id,title,abstract
http://arxiv.org/abs/2508.05636v1,FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space Mixing,"Face recognition technology raises significant privacy concerns, motivating research into methods for protecting facial identities. Existing anonymization techniques often compromise utility by significantly altering facial attributes or lacking cancelability, hindering subsequent analysis or re-identification prevention. To address this, we propose FaceAnonyMixer, a novel framework for generating cancelable anonymized faces through identity-consistent latent space mixing. FaceAnonyMixer leverages a pre-trained StyleGAN2 to disentangle identity and attribute information within the latent space. By mixing identity codes from different individuals while preserving the original attributes of the target face, we generate anonymized faces that maintain realism and utility. Furthermore, the mixing process is designed to be cancelable, allowing for the recovery of the original identity given a secret key. Experiments demonstrate that FaceAnonyMixer effectively obscures facial identities, achieving state-of-the-art face recognition attack success rates while preserving key facial attributes relevant for tasks like emotion recognition and age estimation. This approach offers a practical solution for privacy-preserving face analysis with inherent cancelability."
http://arxiv.org/abs/2508.05635v1,Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation,"Robotic manipulation remains a challenging field, often requiring task-specific training and limiting generalization across diverse environments and objects. Current robotic systems struggle to effectively integrate visual perception, reasoning, and action planning into a cohesive framework, hindering their ability to perform complex manipulation tasks in unstructured environments. We introduce Genie Envisioner, a unified world foundation platform designed to bridge this gap by providing robots with a comprehensive understanding of their surroundings and the ability to predict the consequences of their actions. Genie Envisioner leverages a novel combination of multi-modal large language models (MLLMs) for scene understanding, physics-based simulation for action prediction, and a hierarchical reinforcement learning framework for task planning. This architecture allows robots to ""envision"" the potential outcomes of different manipulation strategies and select the optimal sequence of actions to achieve desired goals. Experimental results demonstrate that Genie Envisioner significantly outperforms existing methods on a range of challenging manipulation tasks, including object rearrangement, tool use, and collaborative assembly, achieving a 30% improvement in task completion rate and a 40% reduction in planning time. This work represents a significant step towards creating more versatile and adaptable robotic manipulation systems capable of operating autonomously in complex real-world scenarios."
http://arxiv.org/abs/2508.05634v1,Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling,"Safe navigation in crowded environments is crucial for autonomous robots, yet current methods often struggle to generalize to unseen scenarios due to the inherent uncertainty in predicting human behavior. This paper addresses the challenge of achieving generalizable safety in crowd navigation by explicitly handling uncertainty in human motion forecasting. We propose a novel framework that integrates conformal prediction with a state-of-the-art social navigation algorithm. Our approach generates prediction sets for future human trajectories with guaranteed coverage probabilities, allowing the robot to reason about the range of possible human actions and plan accordingly. Furthermore, we introduce a novel risk-aware cost function that penalizes robot actions leading to potential collisions based on the predicted uncertainty. Through extensive simulations in diverse and challenging crowd scenarios, we demonstrate that our method significantly improves safety compared to deterministic prediction methods, achieving higher success rates and fewer collisions while maintaining comparable navigation efficiency across various unseen environments. This work provides a principled approach for robust and generalizable safe navigation in dynamic human environments, paving the way for more reliable robot deployments in real-world settings."
http://arxiv.org/abs/2508.05630v1,MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes,"Video object segmentation (VOS) aims to identify and track a target object throughout a video sequence, a crucial task for various applications. Existing VOS datasets often lack the complexity found in real-world scenarios, limiting the generalization ability of trained models. To address this, we introduce MOSEv2, a more challenging video object segmentation dataset designed to push the boundaries of current algorithms. MOSEv2 features a diverse collection of videos with significant object occlusions, complex background clutter, rapid camera motion, and substantial object deformations, all of which are frequently absent in existing datasets. We provide pixel-accurate annotations for multiple objects per video, coupled with detailed attribute annotations describing the specific challenges present. We evaluate several state-of-the-art VOS methods on MOSEv2, demonstrating a significant performance gap compared to their performance on existing datasets, highlighting the increased difficulty and necessitating future research focused on robustness to real-world complexities. MOSEv2 serves as a valuable benchmark for the community, fostering the development of more robust and generalizable VOS algorithms capable of handling the intricacies of real-world video data."
http://arxiv.org/abs/2508.05631v1,GAP: Gaussianize Any Point Clouds with Text Guidance,"Point cloud generation aims to create realistic and diverse 3D shapes, often constrained by the availability of labeled training data. Existing methods struggle with generating high-fidelity point clouds that accurately reflect complex textual descriptions, particularly those requiring fine-grained control over shape attributes. This paper introduces GAP, a novel framework to Gaussianize Any Point Clouds with Text Guidance, enabling controllable and high-quality 3D shape generation. GAP leverages a diffusion-based generative model trained to map noisy point clouds to their corresponding clean shapes conditioned on text embeddings. We introduce a novel Gaussianization process that progressively transforms input point clouds into Gaussian noise, facilitating a more robust and flexible learning process. Furthermore, we incorporate a text encoder to guide the denoising process, allowing for precise control over the generated shape's attributes based on textual descriptions. Experimental results demonstrate that GAP significantly outperforms existing text-to-3D point cloud generation methods in terms of generation quality, shape fidelity, and text alignment, as measured by established metrics. This work offers a promising avenue for generating diverse and controllable 3D content from textual descriptions, with potential applications in various fields, including computer-aided design, virtual reality, and robotics."
http://arxiv.org/abs/2508.05626v1,Physically Controllable Relighting of Photographs,"Relighting photographs is a fundamental problem in computer vision with applications ranging from image editing to augmented reality. Existing methods often struggle to produce physically plausible results, particularly when dealing with complex lighting environments and intricate object geometry, limiting the controllability and realism of the relit images. We address this challenge by introducing a novel framework for physically controllable relighting of photographs that leverages differentiable rendering and a hybrid neural-analytic representation of scene lighting. Our method decomposes the scene into albedo, normals, and a spatially varying environment map, jointly optimized using a differentiable rendering loss and physics-based regularizers. Critically, we introduce a set of physically meaningful light source parameters that allow users to directly manipulate the environment map, enabling intuitive control over the relighting process. Experiments on both synthetic and real-world datasets demonstrate that our approach generates more realistic and controllable relighting results compared to state-of-the-art methods, producing images with improved shading, shadows, and specular highlights under user-specified lighting conditions. This provides a powerful tool for photo editing and scene understanding, allowing for intuitive and physically accurate manipulation of lighting in existing photographs."
http://arxiv.org/abs/2508.05615v1,Test-Time Reinforcement Learning for GUI Grounding via Region Consistency,"GUI grounding, the task of localizing GUI elements given a natural language query, is crucial for enabling effective human-computer interaction. However, existing methods often struggle to generalize to unseen GUIs with novel layouts or element variations due to the limited diversity in training datasets. This paper addresses the challenge of adapting to new GUI environments at test time without requiring any further training data. We propose a novel test-time reinforcement learning (RL) framework that leverages region consistency to guide the grounding process. Specifically, an RL agent learns to iteratively refine its grounding predictions by maximizing the consistency of visual features within the predicted region and minimizing inconsistencies with surrounding regions. This is achieved by rewarding actions that lead to more coherent feature representations within the proposed bounding box, promoting accurate localization. Experiments on diverse GUI datasets demonstrate that our approach significantly improves grounding accuracy compared to existing zero-shot and few-shot adaptation techniques, leading to state-of-the-art performance in unseen GUI environments. This work provides a practical and effective solution for deploying GUI grounding models in real-world applications with varying GUI designs."
http://arxiv.org/abs/2508.05609v1,Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity,"Evaluating the quality of generated 3D assets remains a significant challenge, hindering progress in 3D content creation. Existing 3D generation evaluation metrics often focus on global shape similarity or struggle to capture fine-grained structural inconsistencies, leading to unreliable assessments of generation quality. To address this, we introduce Hi3DEval, a novel hierarchical evaluation framework for 3D generation that assesses validity across multiple levels of abstraction. Hi3DEval leverages a hierarchical decomposition of 3D meshes into semantically meaningful parts, enabling the identification of errors at the component level, such as missing or deformed parts, as well as global shape deviations. Our framework incorporates learned shape priors and geometric consistency checks at each level of the hierarchy to quantify validity, providing a comprehensive and interpretable evaluation score. Experiments on various 3D generation models and datasets demonstrate that Hi3DEval correlates significantly better with human perception of 3D quality compared to existing metrics. Hi3DEval provides a robust and fine-grained evaluation tool that can accelerate the development of high-quality 3D generative models."
http://arxiv.org/abs/2508.05606v1,Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision,"Chain-of-Thought (CoT) prompting has significantly improved the reasoning capabilities of large language models (LLMs) in both language and vision domains. However, current CoT methods typically operate in a unimodal fashion, limiting their ability to leverage synergistic reasoning across text and vision. This paper introduces Uni-CoT, a novel framework towards unified chain-of-thought reasoning that bridges the gap between textual and visual modalities. Uni-CoT employs a shared LLM backbone and modality-specific encoders to process both text and images. It then generates intermediate reasoning steps in a unified representation space, allowing the LLM to reason across modalities and iteratively refine its answer based on information from both. We demonstrate that Uni-CoT achieves state-of-the-art performance on several challenging multimodal reasoning benchmarks, including ScienceQA and Visual Commonsense Reasoning, significantly outperforming unimodal CoT approaches. This work highlights the potential of unified CoT reasoning for building more robust and versatile AI systems capable of tackling complex, real-world problems."
http://arxiv.org/abs/2508.05602v1,LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model,"Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding and generating content based on both visual and textual inputs. However, evaluating the relevancy of an image and text pair remains a challenging task, often requiring subjective human judgment or complex, task-specific models. This paper introduces LLaVA-RE, a novel approach leveraging the LLaVA architecture to directly evaluate the binary relevancy (relevant or irrelevant) of image-text pairs. We fine-tune LLaVA with a binary classification head and train it on a curated dataset of relevant and irrelevant image-text pairs, instructing the model to output a definitive ""relevant"" or ""irrelevant"" assessment. Experimental results on diverse datasets demonstrate LLaVA-RE's ability to achieve high accuracy in determining image-text relevancy, surpassing existing methods in zero-shot and few-shot settings. This streamlined evaluation method offers a significant advancement in automating the assessment of multimodal data alignment, with broad implications for tasks such as image captioning evaluation, cross-modal retrieval, and content moderation."
http://arxiv.org/abs/2508.05599v1,WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction,"Discrete visual tokenization has become a cornerstone for efficient and scalable image and video processing, enabling the application of powerful sequence models like transformers. However, existing methods often struggle to balance compression efficiency with the preservation of fine-grained details, leading to limitations in high-fidelity visual reconstruction. This paper introduces WeTok, a novel discrete tokenization framework designed to significantly improve the accuracy and visual quality of reconstructed images. WeTok leverages a hierarchical vector quantization approach combined with a learned weighting mechanism that dynamically adjusts the contribution of each quantization level based on local image content. Furthermore, we incorporate a perceptual loss during training to better align the token embeddings with human visual perception. Our experiments demonstrate that WeTok achieves state-of-the-art reconstruction performance on several benchmark datasets, outperforming existing methods in terms of PSNR, SSIM, and LPIPS. The enhanced reconstruction fidelity enabled by WeTok unlocks new possibilities for a wide range of downstream vision tasks that rely on accurate and detailed visual representations."
http://arxiv.org/abs/2508.05585v1,DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition,"Open-vocabulary multi-label recognition aims to identify multiple objects within an image using an arbitrary set of textual descriptions, a challenging task requiring the transfer of knowledge from pre-trained vision-language models. A key problem lies in effectively adapting these models to simultaneously handle both visual and semantic nuances across various categories, especially when encountering unseen labels. We introduce DART, a Dual Adaptive Refinement Transfer framework, which addresses this by adaptively refining both visual and semantic representations through a dual-branch approach. DART employs a visual refinement module that learns to focus on relevant image regions guided by label embeddings, and a semantic refinement module that contextualizes label embeddings with visual features, iteratively improving their alignment. Extensive experiments on multiple benchmark datasets demonstrate that DART significantly outperforms existing state-of-the-art methods, achieving substantial gains in both seen and unseen label recognition accuracy. DART offers a robust and generalizable solution for open-vocabulary multi-label recognition, paving the way for more flexible and adaptable vision systems."
http://arxiv.org/abs/2508.05580v1,Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis,"Large Multimodal Models (MLLMs) have shown remarkable capabilities in understanding and interacting with visual and textual data. However, their ability to synthesize coherent and consistent world data, especially under complex, multi-faceted instructions, remains a significant challenge. This paper introduces ""Follow-Your-Instruction"" (FYI), a comprehensive MLLM agent designed to generate realistic and instruction-compliant world data, including scenes, objects, and their relationships. FYI leverages a novel hierarchical decomposition strategy to break down complex instructions into simpler sub-tasks, which are then addressed by specialized modules for scene generation, object placement, and relationship enforcement. Furthermore, we introduce a feedback loop that iteratively refines the generated data based on instruction adherence and realism metrics. Experimental results demonstrate that FYI significantly outperforms existing MLLMs in generating complex scenes with high fidelity to given instructions, achieving a 30% improvement in instruction adherence score and a 20% increase in realism as judged by human evaluators. FYI represents a crucial step towards creating intelligent agents capable of constructing and manipulating virtual worlds, opening new avenues for applications in simulation, training, and content creation."
http://arxiv.org/abs/2508.05568v1,X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment,"Vertical Federated Learning (VFL) enables collaborative model training across multiple parties with non-overlapping feature spaces, protecting data privacy. However, existing VFL methods often suffer from performance degradation due to feature incompleteness and feature distribution divergence across different parties. We address these challenges by proposing X-VFL, a novel VFL framework with Cross Completion and Decision Subspace Alignment. X-VFL employs a cross completion module at each party to impute missing features using information from other parties' models, effectively mitigating the feature incompleteness problem. Furthermore, we introduce a decision subspace alignment strategy to minimize the discrepancy between the decision boundaries learned by each party, thereby addressing feature distribution divergence. Extensive experiments on various benchmark datasets demonstrate that X-VFL consistently outperforms state-of-the-art VFL methods in terms of prediction accuracy and convergence speed. X-VFL offers a robust and privacy-preserving solution for collaborative learning in scenarios with heterogeneous and incomplete data."
http://arxiv.org/abs/2508.05547v1,Adapting Vision-Language Models Without Labels: A Comprehensive Survey,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in various multimodal tasks, but adapting them to new domains or tasks often requires substantial labeled data, which can be expensive and time-consuming to acquire. This survey addresses the problem of adapting VLMs to downstream tasks without relying on explicit human annotations, focusing on the rapidly growing field of label-free VLM adaptation. We comprehensively categorize and analyze existing approaches based on their underlying principles, including techniques that leverage pseudo-labeling, self-training, contrastive learning with automatically generated augmentations, and knowledge distillation from pre-trained models using proxy tasks. Furthermore, we provide a structured comparison of these methods across diverse benchmark datasets and tasks, highlighting their strengths and weaknesses in terms of performance, computational efficiency, and generalization ability. Our analysis reveals that while label-free adaptation achieves promising results, performance gaps remain compared to supervised methods, and robustness across different domains requires further investigation. This survey provides a valuable resource for researchers and practitioners seeking to leverage the power of VLMs in data-scarce scenarios and outlines key directions for future research in label-free multimodal learning."
http://arxiv.org/abs/2508.05531v1,Point cloud segmentation for 3D Clothed Human Layering,"3D clothed human modeling is crucial for various applications, including virtual try-on, animation, and game development. Accurately segmenting a clothed 3D human scan into semantic layers corresponding to clothing items and the underlying body remains a significant challenge due to complex clothing topologies, occlusions, and variations in pose and garment style. This paper introduces a novel point cloud segmentation method for 3D clothed human layering, leveraging a graph neural network (GNN) architecture that incorporates both local geometric features and global contextual information. Specifically, we construct a graph representation of the point cloud and iteratively refine node features using graph convolutions, incorporating learned embeddings from a pre-trained large language model (LLM) to capture garment style and category information. The refined node features are then used to predict per-point semantic labels for clothing layers. Experimental results on a diverse dataset of clothed human scans demonstrate that our approach achieves state-of-the-art segmentation accuracy, outperforming existing methods by a significant margin, particularly in handling complex clothing arrangements. This advancement enables more realistic and controllable 3D clothed human avatars, facilitating further progress in downstream applications."
http://arxiv.org/abs/2508.05529v1,Looking into the Unknown: Exploring Action Discovery for Segmentation of Known and Unknown Actions,"Action segmentation aims to temporally delineate and classify actions within videos, typically relying on pre-defined action vocabularies. However, real-world videos often contain actions not present in the training data, posing a significant challenge to existing segmentation models. This paper addresses the problem of discovering and segmenting both known and unknown actions in video sequences. We propose a novel framework that combines a discriminative action segmentation network trained on known actions with an unsupervised action discovery module. The segmentation network provides frame-level action probabilities, which are then used to identify segments with low confidence for known actions, indicating potential unknown actions. These uncertain segments are clustered based on their feature representations, and a pseudo-labeling strategy is employed to iteratively refine the clusters and update the segmentation network. Experiments on benchmark datasets demonstrate that our approach achieves comparable performance to state-of-the-art methods on known actions while effectively discovering and segmenting unknown actions, leading to improved overall segmentation accuracy. This work provides a crucial step towards more robust and generalizable action segmentation in real-world scenarios."
http://arxiv.org/abs/2508.05527v1,AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety,"Content moderation is crucial for maintaining brand safety on online platforms, traditionally relying on human moderators. However, the scale and complexity of online content necessitate exploring automated solutions, particularly leveraging the capabilities of multimodal Large Language Models (LLMs). This paper addresses the critical need for a comparative evaluation of AI-powered content moderation systems against human moderators in identifying brand-unsafe content across diverse modalities. We propose a novel framework that utilizes state-of-the-art multimodal LLMs, fine-tuned on a curated dataset of text, images, and video content flagged for various brand safety violations. Our evaluation metrics encompass precision, recall, F1-score, and a novel ""contextual understanding"" score to assess the ability to discern nuanced violations. Results demonstrate that while multimodal LLMs achieve comparable precision to human moderators, they often surpass humans in recall, identifying a significantly larger proportion of unsafe content, particularly in complex, context-dependent scenarios. This research highlights the potential of AI-driven content moderation to enhance brand safety by improving efficiency and reducing the risk of overlooking harmful content, ultimately fostering a safer online environment."
http://arxiv.org/abs/2508.05526v1,When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework,"Deepfake technology poses a significant threat due to its ability to generate highly realistic manipulated videos, making it crucial to develop robust detection methods. Current deepfake detection methods often rely on analyzing subtle facial inconsistencies and temporal artifacts, but they can be computationally expensive and vulnerable to adversarial attacks and unseen manipulation techniques. This paper addresses the challenge of building a more unified, lightweight, and generalizable deepfake detection framework. We propose a novel approach that combines facial landmark analysis with Graph Neural Networks (GNNs) to capture both local facial inconsistencies and global facial structure deviations introduced by deepfakes. Our framework first extracts facial landmarks and then constructs a graph representing the relationships between these landmarks. A lightweight GNN is then employed to learn discriminative features from this graph, capturing subtle inconsistencies in facial geometry and dynamics. Experimental results on benchmark datasets demonstrate that our approach achieves comparable or superior performance to state-of-the-art methods with significantly fewer parameters and computational cost. This unified and lightweight GNN-based framework offers a promising direction for developing more efficient and robust deepfake detection systems."
http://arxiv.org/abs/2508.05521v1,Optimal Brain Connection: Towards Efficient Structural Pruning,"Deep neural networks have achieved remarkable success in various computer vision tasks, but their high computational cost and memory footprint hinder deployment on resource-constrained devices. Structural pruning offers a promising solution by removing entire filters or channels, leading to genuine model compression and acceleration. However, identifying the optimal subset of connections to prune remains a challenging combinatorial problem, often relying on heuristics or computationally expensive iterative training. We propose a novel approach, Optimal Brain Connection (OBC), which leverages a differentiable proxy to estimate the importance of each filter based on its contribution to the overall network loss. Specifically, OBC employs a learnable gate for each filter and optimizes the gate values using gradient descent alongside the network weights, encouraging unimportant filters to be automatically shut off. We demonstrate that pruning filters based on OBC significantly outperforms existing state-of-the-art structural pruning methods on benchmark datasets like CIFAR-10 and ImageNet, achieving comparable accuracy with significantly higher compression ratios and reduced inference time. This efficient and effective pruning technique paves the way for deploying high-performance deep learning models on edge devices with limited resources."
http://arxiv.org/abs/2508.05519v1,Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods,"Clinical data cleaning is a crucial yet time-consuming step in healthcare research and practice, often involving manual review and correction of errors and inconsistencies. The increasing volume and complexity of clinical data necessitate more efficient and accurate cleaning methods. This paper addresses the problem of slow and resource-intensive traditional data cleaning processes by exploring the potential of artificial intelligence (AI) to accelerate and improve the process. We propose a novel AI-assisted framework that integrates three distinct AI techniques: (1) a rule-based system for automated error detection, (2) a machine learning model for predicting missing values, and (3) a natural language processing (NLP) module for standardizing free-text clinical notes. We compare the performance of this AI-assisted framework against traditional manual cleaning methods on a real-world dataset of patient records. Our results demonstrate a significant reduction in cleaning time (up to 60%) and a notable improvement in data accuracy (reduction in error rate by 15%) when using the AI-assisted approach. This study highlights the potential of AI to revolutionize clinical data cleaning, leading to faster research cycles, improved data quality, and ultimately, better patient care."
http://arxiv.org/abs/2508.05516v1,FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment,"Image Quality Assessment (IQA) plays a vital role in various vision applications, aiming to predict perceived image quality consistent with human subjective evaluations. However, deep learning-based IQA models are vulnerable to adversarial attacks and small perturbations, raising concerns about their robustness and reliability in real-world scenarios. This paper addresses the challenge of enhancing the robustness of IQA models against such perturbations. We introduce FS-IQA, a novel framework incorporating certified feature smoothing to improve model robustness. FS-IQA leverages randomized smoothing applied directly to feature representations within the IQA network, enabling the derivation of certified robustness guarantees. This allows us to certify a minimum level of quality change for a given perturbation radius around an input image. Extensive experiments on benchmark IQA datasets demonstrate that FS-IQA significantly improves robustness against adversarial attacks and common image corruptions, while maintaining competitive performance on clean images. Our proposed framework provides a practical and theoretically grounded approach to enhance the reliability of IQA models, paving the way for more trustworthy quality assessment in sensitive applications."
http://arxiv.org/abs/2508.05514v1,Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking,"Multi-object tracking (MOT) in crowded scenes remains a challenging task due to frequent occlusions, similar appearances, and complex interactions among pedestrians. Existing pedestrian tracking methods often struggle in crowded environments due to inaccurate pedestrian detection and unreliable data association. To address these limitations, we propose a novel Head Anchor Enhanced Detection and Association (HAA) framework for robust pedestrian tracking. HAA leverages head-specific anchor boxes to improve detection accuracy, particularly in occluded scenarios where the head region is often visible. Furthermore, we introduce a contextual association module that incorporates both appearance and spatial-temporal information, weighted by the detection confidence scores, to enhance tracking robustness and reduce identity switches. Experimental results on the challenging MOT17 and MOT20 datasets demonstrate that HAA significantly outperforms state-of-the-art methods, achieving substantial improvements in both detection accuracy (IDF1 score) and tracking precision (MOTA score), particularly in crowded scenes. The proposed HAA framework offers a practical and effective solution for pedestrian tracking in crowded environments, advancing the state-of-the-art in MOT."
http://arxiv.org/abs/2508.05507v1,Revealing Latent Information: A Physics-inspired Self-supervised Pre-training Framework for Noisy and Sparse Events,"Event cameras offer significant advantages over traditional cameras in high-speed and high dynamic range scenarios, but their data is inherently noisy and sparse, hindering effective learning. This paper addresses the challenge of learning robust representations from noisy and sparse event data without relying on extensive labeled datasets. We introduce a novel physics-inspired self-supervised pre-training framework that leverages principles from event generation to learn latent representations. Our approach models event generation as a stochastic process governed by underlying scene illumination changes, and then uses contrastive learning to encourage representations to be invariant to noise and sparsity while preserving information about the underlying illumination dynamics. We pre-train our model on large-scale synthetic and real-world event datasets and demonstrate significant performance improvements on downstream tasks such as event-based object recognition and motion estimation, surpassing state-of-the-art self-supervised methods by a significant margin, particularly in challenging low-light and high-speed conditions. This work provides a promising direction for unlocking the full potential of event cameras by learning robust and informative representations from their inherent data characteristics."
http://arxiv.org/abs/2508.05506v1,MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips,"Accurate and robust reconstruction of hand-object interactions (HOIs) from monocular videos is crucial for understanding human behavior and enabling realistic virtual interactions. However, the inherent ambiguity in 2D projections, coupled with frequent occlusions and self-similarities, makes 3D HOI reconstruction from short video clips a challenging problem. We introduce MagicHOI, a novel framework that leverages learned 3D priors and temporal consistency to achieve accurate hand and object pose estimation and shape reconstruction from short monocular video sequences. MagicHOI employs a hybrid implicit-explicit representation, utilizing a neural implicit surface for detailed shape representation and explicit 3D pose parameters for efficient optimization. A temporal smoothing loss, guided by learned motion priors, encourages consistent and plausible motion trajectories across frames, further mitigating ambiguities. Experiments on challenging benchmark datasets demonstrate that MagicHOI significantly outperforms state-of-the-art methods, achieving higher accuracy in both pose estimation and shape reconstruction, particularly in scenarios with significant occlusion and complex interactions. This improved reconstruction quality unlocks new possibilities for downstream tasks such as activity recognition and robotic manipulation."
http://arxiv.org/abs/2508.05505v1,Symmetry Understanding of 3D Shapes via Chirality Disentanglement,"Symmetry is a fundamental property of 3D shapes, crucial for tasks ranging from shape analysis to robotic manipulation. However, current symmetry detection methods often struggle with shapes exhibiting chirality, where a shape and its mirror image are not superimposable through rigid transformations. This paper addresses the problem of robustly understanding symmetry in 3D shapes, particularly those with chiral characteristics. We propose a novel approach that explicitly disentangles the intrinsic symmetry properties of a shape from its extrinsic chiral transformation. Our method learns a latent space representation where symmetry parameters and a chirality flag are independently encoded, enabling accurate symmetry detection and reflection completion even in the presence of significant chirality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in symmetry detection accuracy and outperforms existing methods in handling chiral shapes, evidenced by a significant improvement in reflection completion quality. This disentangled representation provides a more comprehensive and robust understanding of 3D shape symmetry, facilitating downstream applications requiring accurate symmetry information."
http://arxiv.org/abs/2508.05504v1,Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection,"Multi-view clustering (MVC) aims to group data points using information from multiple feature sets or ""views,"" often requiring careful parameter tuning. However, existing MVC methods struggle with high-dimensional data and typically involve view weighting parameters that significantly impact performance and necessitate computationally expensive optimization. This paper introduces a novel parameter-free entropy-regularized multi-view clustering framework that incorporates hierarchical feature selection. Our approach adaptively learns view weights by maximizing the entropy of cluster assignments while simultaneously performing hierarchical feature selection within each view based on feature importance scores derived from spectral analysis. This allows the framework to automatically identify and prioritize relevant features within each view without requiring pre-defined parameters. Experiments on several benchmark datasets demonstrate that our method consistently achieves state-of-the-art clustering performance compared to existing MVC algorithms, particularly in high-dimensional settings, while eliminating the need for parameter tuning. The proposed framework offers a robust and efficient solution for multi-view clustering, simplifying the application of MVC to real-world datasets with complex feature structures."
http://arxiv.org/abs/2508.05503v1,AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated Industrial Anomaly Detection,"Automated anomaly detection in industrial environments is crucial for maintaining operational efficiency and safety, but often requires integrating diverse data modalities and expert knowledge. Existing methods struggle to effectively leverage the complementary strengths of multiple anomaly detection agents, especially when incorporating high-level managerial insights to guide the detection process. We introduce AutoIAD, a novel framework for Manager-Driven Multi-Agent Collaboration in Automated Industrial Anomaly Detection. AutoIAD employs a central ""Manager"" module that dynamically allocates tasks and aggregates the outputs of specialized ""Agent"" modules, each trained on a specific data modality or anomaly type. The Manager leverages reinforcement learning, incorporating feedback from human operators (managers) to optimize agent selection and weighting strategies, enabling the system to adapt to evolving industrial conditions and prioritize critical anomalies. Experimental results on a real-world industrial dataset demonstrate that AutoIAD significantly outperforms state-of-the-art anomaly detection methods, achieving a 15% improvement in F1-score and a reduction in false positives, highlighting its potential for practical deployment in complex industrial settings. AutoIAD provides a flexible and robust solution for automated industrial anomaly detection, facilitating effective collaboration between AI agents and human experts."
http://arxiv.org/abs/2508.05502v1,MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs,"Large Multimodal Language Models (MLLMs) have demonstrated remarkable performance in vision-language tasks; however, their reliance on extensive datasets primarily in high-resource languages and cultures results in limited applicability and potential biases when deployed in low-resource settings. This paper addresses the critical gap in culturally-grounded MLLMs for low-resource languages, where both linguistic data and visual representations often lack adequate representation. We introduce MELLA (Multimodal Embedding with Linguistic and Localized Adaptation), a novel framework that bridges linguistic capability and cultural groundedness. MELLA leverages a two-stage training process: first, it employs cross-lingual transfer learning to adapt a pre-trained MLLM to the target low-resource language using a combination of machine translation and back-translation. Second, it incorporates a novel localized visual grounding module, trained on a curated dataset of culturally relevant images and captions, to align visual representations with local contexts. Experiments on a newly created benchmark for a low-resource language demonstrate that MELLA significantly outperforms existing MLLMs, achieving a 25% improvement in cultural understanding and a 18% increase in overall task accuracy. MELLA provides a promising pathway towards building more inclusive and effective MLLMs for diverse linguistic and cultural contexts."
http://arxiv.org/abs/2508.05501v1,SMOL-MapSeg: Show Me One Label,"Semantic segmentation, a fundamental task in computer vision, often requires extensive pixel-level annotations, which are costly and time-consuming to acquire. This paper addresses the challenge of reducing annotation effort by exploring extreme supervision: training semantic segmentation models using only a single labeled instance per class. We introduce SMOL-MapSeg, a novel approach leveraging a combination of self-supervised pretraining, a carefully designed loss function that emphasizes inter-class discrimination and intra-class compactness, and a mapping module that projects features into a shared embedding space. Specifically, we employ a contrastive loss to learn feature representations from unlabeled data, a margin-based cross-entropy loss to encourage distinct class clusters, and a feature mapping network to reduce the impact of domain shift between the single labeled instance and the unlabeled data. Experimental results on the Pascal VOC 2012 dataset demonstrate that SMOL-MapSeg achieves significant performance gains compared to existing few-shot and weakly-supervised segmentation methods under this extreme supervision setting. This work highlights the potential of learning robust semantic segmentation models with minimal annotation effort, paving the way for more scalable and accessible computer vision applications."
http://arxiv.org/abs/2508.05489v1,Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification,"Adversarial purification aims to mitigate the effect of adversarial attacks on deep neural networks, often employing techniques like image compression to remove adversarial perturbations. While compression-based defenses show initial promise, their robustness against adaptive attacks remains a concern. This paper investigates the vulnerabilities of compression-based adversarial purification methods to carefully crafted adversarial attacks designed to bypass their defenses. We propose a novel attack framework, the ""Compression-Aware Adversarial Attack"" (CAAA), that directly optimizes adversarial perturbations in the compressed domain, minimizing the reconstruction error after decompression. This allows the attack to craft perturbations that are less likely to be removed during the compression process. Our experiments demonstrate that CAAA significantly reduces the effectiveness of JPEG and wavelet-based purification methods on CIFAR-10 and ImageNet datasets, achieving success rates close to 100% even at high compression levels. These findings highlight the need for more robust and adaptive defenses against adversarial attacks that consider the intricacies of compression-based purification techniques, furthering the development of reliable and secure deep learning systems."
http://arxiv.org/abs/2508.05465v1,F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery,"Pituitary adenoma resection via endoscopic transsphenoidal surgery is a common neurosurgical procedure, requiring precise anatomical knowledge for safe and effective tumor removal. Accurate segmentation of pituitary anatomy in endoscopic videos remains a challenge due to variable image quality, anatomical variations, and surgical instruments obstructing the view. This paper introduces F2PASeg, a novel feature fusion approach for pituitary anatomy segmentation in endoscopic videos. F2PASeg employs a dual-branch encoder-decoder architecture, where one branch extracts high-resolution spatial features and the other captures rich semantic information using a ResNet backbone. A novel Feature Fusion Module (FFM) adaptively integrates features from both branches at multiple scales, enhancing the representation of fine-grained anatomical boundaries and contextual understanding. Experimental results on a clinically relevant dataset of endoscopic pituitary surgery videos demonstrate that F2PASeg outperforms state-of-the-art segmentation methods, achieving a significant improvement in Dice score and Intersection over Union (IoU) for critical anatomical structures. The proposed method offers a promising solution for real-time surgical guidance and improved patient outcomes in pituitary adenoma resection."
http://arxiv.org/abs/2508.05461v1,How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization,"Unsupervised anomaly detection and localization are critical for identifying deviations from normality without relying on labeled anomalous data. Diffusion models, particularly flow matching, have shown promise in density estimation and generative modeling, but their application to anomaly detection remains relatively unexplored, especially regarding the interpretability of their anomaly scores. This paper addresses the challenge of effectively leveraging flow matching models for both accurate anomaly detection and fine-grained localization in an unsupervised setting. We introduce a novel anomaly scoring framework based on the reverse-time conditional flow of a pre-trained flow matching model. Our approach leverages the learned flow to reconstruct potentially anomalous regions, and then quantifies the anomaly score based on the reconstruction error and the trajectory deviation from the learned normal flow. We demonstrate state-of-the-art performance on several benchmark datasets, including MVTec AD and VisA, surpassing existing unsupervised anomaly detection methods. Importantly, our approach provides interpretable anomaly maps that accurately highlight anomalous regions, demonstrating the potential of flow matching for robust and explainable anomaly detection."
http://arxiv.org/abs/2508.05430v1,Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions,"Vision-Language (VL) encoders learn joint representations of images and text, enabling powerful cross-modal reasoning. Despite their success, understanding which specific visual elements and textual tokens contribute most to similarity scores remains a challenge. This paper addresses the problem of explaining similarity scores in VL encoders by identifying the key interactions between visual regions and textual tokens. We propose a novel approach that leverages the Banzhaf value from cooperative game theory to quantify the contribution of each region-token interaction to the overall similarity score. We further introduce learned weights to modulate these Banzhaf values, allowing the model to prioritize interactions that are most relevant for explaining similarity. Experimental results on benchmark datasets demonstrate that our method generates more faithful and interpretable explanations compared to existing attribution techniques. Our approach provides valuable insights into the inner workings of VL encoders, facilitating model debugging and improving trust in their predictions."
http://arxiv.org/abs/2508.05417v1,Smoothing Slot Attention Iterations and Recurrences,"Slot Attention has emerged as a powerful method for object-centric representation learning, effectively disentangling scenes into distinct entities. However, the iterative refinement process within Slot Attention, particularly when combined with recurrent architectures, can exhibit instability and sensitivity to initialization, hindering convergence and potentially leading to suboptimal representations. We address this problem by introducing a novel smoothing technique applied to both the iterative updates within each Slot Attention block and the recurrent connections between blocks. Specifically, we propose Exponential Moving Average (EMA) smoothing of the slot updates, reducing abrupt changes in slot states and promoting smoother convergence. Furthermore, we apply EMA smoothing to the hidden state passed between recurrent Slot Attention blocks, stabilizing the information flow across time. Empirical evaluations on challenging scene understanding datasets demonstrate that our smoothed Slot Attention significantly improves representation quality, evidenced by enhanced object segmentation and reconstruction performance. These improvements highlight the importance of stabilizing the iterative and recurrent dynamics in Slot Attention for robust object-centric representation learning."
http://arxiv.org/abs/2508.05414v1,Physical Adversarial Camouflage through Gradient Calibration and Regularization,"Adversarial attacks pose a significant threat to the robustness of deep learning models, especially in safety-critical applications like autonomous driving. Physical adversarial camouflage, which involves designing patterns to be printed on real-world objects, presents a particularly challenging attack scenario. This paper addresses the problem of generating robust physical adversarial camouflage that maintains its effectiveness under varying environmental conditions and viewing angles. We introduce a novel gradient calibration and regularization framework that improves the transferability and physical realizability of adversarial camouflage. Our method calibrates gradients using a novel perspective-aware rendering technique, simulating diverse viewpoints during optimization. Furthermore, we introduce a total-variation-based regularization term to promote smoothness and reduce high-frequency noise in the generated camouflage patterns, enhancing their printability and robustness in the physical world. Experimental results demonstrate that our approach significantly outperforms existing state-of-the-art methods in fooling object detectors under diverse physical conditions, achieving a higher attack success rate in real-world experiments. This work contributes to a better understanding of physical adversarial vulnerabilities and provides a practical approach for generating robust and realistic adversarial camouflage."
http://arxiv.org/abs/2508.05409v1,From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization,"Face recognition systems are vulnerable to backdoor attacks, where imperceptible triggers embedded during training can manipulate the system's predictions at inference time. This paper addresses the problem of defending against backdoor attacks in face recognition, specifically focusing on scenarios where the attacker has control over the training data and injects a visually subtle trigger. We propose a novel two-stage framework: (1) Vision-Language Trigger Detection, which leverages CLIP to identify potentially backdoored images by analyzing the semantic similarity between image content and textual descriptions associated with common backdoor triggers; and (2) Noise-Based Neutralization, where detected images are processed with a carefully calibrated noise layer designed to disrupt the trigger's effectiveness without significantly degrading performance on clean data. Empirical evaluations on benchmark datasets demonstrate that our approach effectively mitigates the impact of various backdoor attacks, achieving a significant reduction in attack success rate while maintaining high accuracy on benign samples. The proposed method offers a practical and robust defense mechanism against backdoor attacks in face recognition, enhancing the reliability and security of these systems."
http://arxiv.org/abs/2508.05402v1,DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model,"Autonomous driving systems are increasingly reliant on complex, multi-modal sensor suites to achieve robust perception and control. However, deploying such systems on resource-constrained platforms remains a significant challenge. This paper addresses the problem of efficiently distilling knowledge from a high-capacity, multi-modal autonomous driving system into a smaller, more deployable model without sacrificing performance. We introduce DistillDrive, an end-to-end distillation framework leveraging an isomorphic hetero-source planning model. DistillDrive employs a novel planning-centric distillation strategy that transfers not only behavioral cloning but also the rationale behind the teacher's decisions by aligning the planning trajectories across different sensor modalities. This is achieved through a learned isomorphism that maps heterogeneous sensor inputs (e.g., LiDAR, camera) to a common planning space, enabling efficient knowledge transfer. Experiments on the CARLA simulator demonstrate that DistillDrive significantly outperforms existing distillation methods, achieving a 25% improvement in driving score while using only a fraction of the teacher's parameters. This work offers a promising approach for deploying high-performing autonomous driving systems on computationally limited platforms."
http://arxiv.org/abs/2508.05399v1,UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation,"Text-to-image generation has witnessed remarkable progress with the advent of masked generative transformers, yet challenges remain in precisely aligning generated images with complex textual descriptions, particularly in capturing fine-grained details and compositional relationships. Current approaches often struggle with attention collapse and semantic inconsistencies between text and image representations, leading to suboptimal image quality and fidelity. To address these limitations, we introduce UNCAGE: a novel Contrastive Attention Guidance mechanism for masked generative transformers. UNCAGE leverages contrastive learning to explicitly align text and image attention maps, encouraging the model to attend to relevant image regions based on the textual input. Specifically, we incorporate a contrastive loss that maximizes the similarity between text embeddings and attention-weighted image features, while minimizing the similarity between text embeddings and randomly sampled image features. Experiments on the COCO and Parti datasets demonstrate that UNCAGE significantly improves image quality, text-image alignment, and compositional understanding compared to state-of-the-art methods, as evidenced by improved FID and CLIP scores. This work provides a valuable framework for enhancing the controllability and fidelity of text-to-image generation models."
http://arxiv.org/abs/2508.05391v1,Artificial Intelligence-Based Classification of Spitz Tumors,"Spitz tumors are melanocytic neoplasms that pose a significant diagnostic challenge due to their overlapping histopathological features with melanoma. Accurate differentiation between benign Spitz nevi and atypical Spitz tumors/Spitzoid melanomas is crucial for appropriate patient management, yet inter-observer variability among pathologists remains a persistent issue. This study addresses the problem of improving the accuracy and consistency of Spitz tumor classification using artificial intelligence. We propose a novel deep learning framework incorporating a convolutional neural network (CNN) pre-trained on a large-scale dataset of dermatoscopic images and fine-tuned with histopathological images of Spitz tumors. The framework further integrates an attention mechanism to highlight diagnostically relevant features within the images, enhancing interpretability and performance. Our results demonstrate that the proposed AI-based classification system achieves a significantly higher accuracy (92.5%) and AUC (0.95) compared to traditional diagnostic methods and previously published AI approaches on a held-out test set of challenging Spitz tumor cases. This AI-based tool has the potential to assist pathologists in making more accurate and reliable diagnoses of Spitz tumors, ultimately improving patient outcomes."
http://arxiv.org/abs/2508.05382v1,Deformable Attention Graph Representation Learning for Histopathology Whole Slide Image Analysis,"Whole slide images (WSIs) offer comprehensive views of tissue microstructures, presenting opportunities for automated diagnostic and prognostic analysis. However, the gigapixel size and complex spatial arrangements of cellular components within WSIs pose significant computational and representational challenges. This paper addresses the problem of effectively capturing long-range dependencies and structural variations within WSIs for improved downstream analysis. We propose a novel Deformable Attention Graph Representation Learning (DAGRL) framework. DAGRL first constructs a graph representation of the WSI, where nodes represent tissue patches and edges encode spatial relationships. Then, a deformable attention mechanism dynamically adjusts the receptive fields of nodes, enabling the model to focus on relevant contextual information across varying distances. Finally, graph neural networks are employed to learn node embeddings that capture both local features and global structural context. Experiments on multiple histopathology datasets demonstrate that DAGRL achieves state-of-the-art performance in tasks such as cancer subtype classification and survival prediction. The proposed framework offers a powerful and flexible approach for learning robust and informative representations from complex histopathology images."
http://arxiv.org/abs/2508.05375v1,CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation,"Automated generation of radiology reports from CT scans can significantly improve diagnostic efficiency and reduce radiologist workload. However, existing methods often struggle to accurately capture the complex anatomical relationships and subtle findings necessary for comprehensive report generation. This paper addresses the challenge of generating high-quality CT reports by proposing CT-GRAPH, a novel hierarchical graph attention network that incorporates anatomy-guided information. CT-GRAPH constructs a hierarchical graph representation of the CT volume, where nodes represent anatomical regions and edges encode their spatial relationships at multiple scales. A graph attention mechanism is then employed to selectively aggregate information from relevant anatomical regions, guided by both visual features extracted from the CT scan and anatomical knowledge embedded within the graph structure. Experimental results on a large-scale chest CT dataset demonstrate that CT-GRAPH significantly outperforms existing state-of-the-art methods in terms of both report quality metrics and clinical relevance, as evaluated by experienced radiologists. This anatomy-guided approach enables more accurate and context-aware report generation, paving the way for improved clinical decision support systems."
http://arxiv.org/abs/2508.05369v1,Cross-View Localization via Redundant Sliced Observations and A-Contrario Validation,"Cross-view localization, the task of determining the geographic pose of a query image given a geo-referenced aerial map, is crucial for autonomous navigation and urban scene understanding. This paper addresses the challenge of achieving robust cross-view localization in the presence of significant viewpoint and appearance variations between ground-level images and overhead imagery. We propose a novel approach that leverages redundant sliced observations extracted from the query image and employs an a-contrario validation framework. Specifically, the query image is partitioned into multiple overlapping slices, each independently matched to the aerial map using learned feature embeddings. The resulting candidate locations are then evaluated using an a-contrario model, which statistically assesses the significance of their spatial consistency, effectively filtering out outliers and noisy matches. Experiments on challenging datasets demonstrate that our method significantly outperforms state-of-the-art approaches, achieving higher localization accuracy and robustness, particularly in scenarios with substantial viewpoint differences and occlusions. Our approach provides a principled and effective framework for robust cross-view localization by exploiting redundancy and statistical validation."
http://arxiv.org/abs/2508.05353v1,PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation,"Chest X-ray report generation aims to automatically produce descriptive reports from medical images, assisting radiologists in diagnosis. However, generating comprehensive and accurate reports remains challenging due to the subtle visual cues in X-rays and the complex relationships between image regions and medical concepts. We address this by proposing PriorRG, a novel framework incorporating prior knowledge through contrastive pre-training and a coarse-to-fine decoding strategy. PriorRG leverages a contrastive loss during pre-training to align visual features with corresponding semantic concepts extracted from a large corpus of radiology reports, learning a robust visual representation informed by prior medical knowledge. Furthermore, we introduce a coarse-to-fine decoding module that first generates a skeletal report summarizing key findings and then refines it with detailed descriptions, ensuring both conciseness and completeness. Experimental results on benchmark datasets demonstrate that PriorRG significantly outperforms state-of-the-art methods in terms of both text quality metrics and clinical accuracy as evaluated by radiologists. This highlights the effectiveness of integrating prior knowledge and a structured decoding approach for generating high-quality and clinically relevant chest X-ray reports."
http://arxiv.org/abs/2508.05343v1,3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, but often struggle to efficiently represent scenes with high-frequency details, leading to blurry renderings or requiring computationally expensive architectures. This paper addresses the challenge of efficiently representing and rendering high-frequency details in radiance fields by introducing 3DGabSplat, a novel 3D Gaussian Splatting framework augmented with 3D Gabor functions. Our method represents the scene as a set of 3D Gaussians, each modulated by a 3D Gabor function, enabling localized frequency adaptation and efficient representation of intricate scene details. By optimizing the parameters of both the Gaussians and the Gabor functions, 3DGabSplat learns a frequency-adaptive representation that captures both low-frequency global structure and high-frequency local details. Experimental results demonstrate that 3DGabSplat achieves state-of-the-art rendering quality, particularly in scenes with complex textures and fine geometric details, while maintaining competitive rendering speeds compared to existing NeRF and Gaussian Splatting methods. This work offers a powerful and efficient approach for high-fidelity novel view synthesis by leveraging the frequency-selective properties of Gabor functions within a Gaussian Splatting framework."
http://arxiv.org/abs/2508.05323v1,Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting,"Open-vocabulary object detectors (OVDs) offer the flexibility to detect objects beyond their initial training vocabulary, but adapting them to new datasets or specific object categories remains a challenge. Fine-tuning the entire model can lead to catastrophic forgetting of previously learned knowledge and is computationally expensive. We address the problem of efficiently adapting OVDs to new object categories while mitigating catastrophic forgetting. Our approach leverages Textual Inversion, a technique initially developed for text-to-image synthesis, to learn new object embeddings within the detector's text encoder without modifying the underlying vision model. By optimizing only these new embeddings, we achieve significant adaptation with minimal computational overhead and reduced risk of forgetting previously learned concepts. Experiments on various benchmark datasets demonstrate that our method effectively adapts OVDs to novel categories, achieving comparable or superior performance to full fine-tuning, while substantially reducing training time and mitigating forgetting. This provides a practical and scalable solution for adapting open-vocabulary object detectors to evolving real-world scenarios."
http://arxiv.org/abs/2508.05318v1,mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering,"Visual Question Answering (VQA) demands intricate reasoning that often necessitates external knowledge beyond the image and question. Existing VQA models struggle to effectively integrate and reason with such external knowledge, particularly when represented in complex multimodal knowledge graphs (mKGs). We address this limitation by proposing mKG-RAG, a novel Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation framework for VQA. mKG-RAG first retrieves relevant subgraphs from an mKG using a learned multimodal retriever, conditioned on both the image and question embeddings. Then, a graph-aware reasoning module processes the retrieved subgraph, fusing visual, textual, and structural information to generate a comprehensive context representation. Finally, a decoder leverages this enriched context to produce the answer. Experiments on the OK-VQA and FVQA datasets demonstrate that mKG-RAG significantly outperforms state-of-the-art VQA models, achieving a 5% improvement in overall accuracy on OK-VQA and a 3% improvement on FVQA. These results highlight the effectiveness of mKG-RAG in leveraging multimodal knowledge graphs for enhanced visual reasoning and question answering."
http://arxiv.org/abs/2508.05316v1,"Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning","Semi-supervised continual learning (SSCL) presents a significant challenge in machine learning, requiring models to sequentially learn new tasks from limited labeled data while retaining knowledge of previous tasks and leveraging unlabeled data. Catastrophic forgetting and instability in learning from unlabeled data are exacerbated in this setting, hindering the effective integration of new knowledge. To address these issues, we propose a novel Divide-and-Conquer Semi-supervised Continual Learning (DC-SSCL) framework. DC-SSCL decomposes each task into smaller, more manageable sub-tasks based on a clustering of the unlabeled data, allowing for more focused and stable learning. A knowledge distillation strategy is then employed to consolidate the learned sub-task representations into a unified task representation, mitigating forgetting and promoting positive transfer. Experiments on benchmark SSCL datasets demonstrate that DC-SSCL significantly outperforms existing methods in terms of average accuracy, backward transfer, and stability, particularly in scenarios with limited labeled data. These results highlight the potential of divide-and-conquer strategies to improve the robustness and efficiency of SSCL models, paving the way for more practical applications in dynamic environments."
http://arxiv.org/abs/2508.05307v1,CoCAViT: Compact Vision Transformer with Robust Global Coordination,"Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks, but their computational complexity and large parameter size often hinder deployment in resource-constrained environments. This paper addresses the challenge of designing a compact and efficient ViT architecture without sacrificing performance, particularly in scenarios requiring robust global context understanding. We introduce CoCAViT, a Compact Vision Transformer with robust Global Coordination, which incorporates a novel Global Coordination Attention (GCA) mechanism. GCA aggregates global context information using learnable coordination vectors distributed across the feature map, enabling efficient and effective long-range dependency modeling. Furthermore, we employ a hierarchical architecture with progressive patch merging to reduce spatial redundancy and computational cost. Extensive experiments on ImageNet classification and COCO object detection demonstrate that CoCAViT achieves competitive or superior performance compared to existing compact ViTs, while significantly reducing the number of parameters and FLOPs. Specifically, CoCAViT achieves X% top-1 accuracy on ImageNet with Y% fewer parameters than baseline models. CoCAViT offers a practical and efficient solution for visual understanding tasks, particularly in applications demanding real-time performance and limited computational resources."
http://arxiv.org/abs/2508.05299v1,VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test,"The Drawing Projection Test (DPT) is a valuable tool for assessing mental health conditions like depression, relying on subjective interpretations of drawn figures. Automating this process presents a significant challenge due to the intricate visual and semantic cues embedded within the drawings. This paper introduces VS-LLM, a novel visual-semantic framework leveraging Large Language Models (LLMs) for automated depression assessment from DPT drawings. VS-LLM integrates visual features extracted from the drawings using a pre-trained Convolutional Neural Network (CNN) with semantic information derived from textual prompts describing specific drawing characteristics associated with depression. The LLM then processes this combined visual-semantic input to generate a depression severity score. Experiments on a diverse dataset of DPT drawings demonstrate that VS-LLM achieves competitive performance compared to traditional machine learning approaches and exhibits a strong correlation with clinician-assigned depression scores. This approach offers a more objective and efficient method for DPT analysis, potentially improving the accessibility and scalability of mental health screening."
http://arxiv.org/abs/2508.05271v1,Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection,"Remote sensing change detection plays a crucial role in monitoring environmental changes and urban development. However, detecting subtle changes in complex scenes with varying illumination and atmospheric conditions remains a significant challenge. This paper introduces a novel Wavelet-Guided Dual-Frequency Encoding (WG-DFE) method to enhance change detection accuracy in remote sensing imagery. WG-DFE first decomposes the bi-temporal images using a discrete wavelet transform to extract multi-scale features. A dual-frequency encoding module, guided by the wavelet coefficients, is then applied to capture both high-frequency details representing subtle changes and low-frequency contextual information. Specifically, wavelet coefficients modulate the frequency encoding to adaptively emphasize change-related features at different scales. Experimental results on multiple benchmark datasets demonstrate that WG-DFE achieves superior performance compared to state-of-the-art methods, yielding significant improvements in both overall accuracy and F1-score. The proposed approach offers a robust and effective solution for detecting subtle changes in remote sensing images, contributing to more accurate and reliable environmental monitoring."
http://arxiv.org/abs/2508.05269v1,B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding,"Large Language Models (LLMs) have demonstrated remarkable capabilities in various fields, but their application to 4D LiDAR data, which captures dynamic 3D environments over time, remains largely unexplored. This paper addresses the lack of comprehensive benchmarks for evaluating the spatio-temporal understanding capabilities of LLMs when processing 4D LiDAR data. We introduce B4DL, a novel benchmark comprising a diverse set of tasks including 4D LiDAR captioning, future scene prediction, and event-based question answering. B4DL leverages real-world autonomous driving datasets and is designed to assess LLMs' ability to reason about object movements, predict future states, and answer complex questions based on spatio-temporal relationships within the 4D point cloud sequences. Extensive experiments using state-of-the-art LLMs reveal significant performance gaps, particularly in tasks requiring precise temporal reasoning and long-term dependency modeling. Our benchmark highlights the challenges and opportunities for advancing LLMs in understanding and interacting with dynamic 3D environments, paving the way for more robust and reliable autonomous systems."
http://arxiv.org/abs/2508.05264v1,SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion,"Infrared and visible image fusion aims to integrate complementary information from different modalities to generate a comprehensive representation beneficial for various computer vision tasks. Existing fusion methods often struggle to preserve fine-grained details and avoid artifacts, limiting their applicability in scenarios demanding high fidelity. To address these limitations, we propose SGDFuse, a novel fusion framework leveraging Segment Anything Model (SAM) guided diffusion. Specifically, SAM is employed to extract semantic masks, enabling spatially adaptive fusion within the diffusion process. This allows for targeted injection of source image features into the latent space, guided by semantic understanding. We further introduce a novel detail-aware loss function to enhance the preservation of intricate textures and sharp edges. Experimental results on benchmark datasets demonstrate that SGDFuse significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative assessments, achieving superior detail preservation and artifact reduction. SGDFuse offers a promising approach for generating high-fidelity fused images, enabling more robust and reliable performance in downstream applications."
http://arxiv.org/abs/2508.05262v1,Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging,"Fluorescent cardiac imaging provides valuable insights into cardiac function and disease mechanisms at a cellular level. However, robust tracking of fluorescently labeled structures within these images is challenging due to low signal-to-noise ratios, photobleaching, and complex cardiac motion. This paper addresses the problem of developing a robust and accurate tracking algorithm for fluorescent cardiac imaging sequences, particularly in the presence of these common imaging artifacts. We propose a novel particle filtering framework incorporating a dynamic motion model based on cardiac mechanics and an adaptive observation model that accounts for variations in fluorescence intensity and shape. The motion model is learned from training data and refined online, while the observation model dynamically adjusts its weighting based on local image statistics, mitigating the effects of photobleaching and noise. Experimental results on both synthetic and real fluorescent cardiac imaging data demonstrate that our method outperforms state-of-the-art tracking algorithms, achieving significantly improved accuracy and robustness, especially under challenging imaging conditions. This enhanced tracking capability enables more reliable quantification of cardiac dynamics and improved analysis of cellular processes in cardiac disease."
http://arxiv.org/abs/2508.05254v1,CF3: Compact and Fast 3D Feature Fields,"Representing 3D scenes with continuous feature fields has shown remarkable success in various computer vision tasks, enabling high-quality reconstruction and novel view synthesis. However, the computational and memory demands of existing feature field methods, particularly those relying on large neural networks, limit their applicability in resource-constrained environments and real-time applications. This paper addresses the challenge of creating compact and efficient 3D feature fields without sacrificing reconstruction quality. We introduce CF3, a novel approach that leverages a combination of tensor decomposition and adaptive quantization to compress the feature field representation. Specifically, we decompose the high-dimensional feature grid into a set of low-rank tensors and dynamically quantize the tensor elements based on their contribution to the overall field representation. Experiments on benchmark datasets demonstrate that CF3 achieves comparable or superior reconstruction quality to state-of-the-art methods while significantly reducing memory footprint and inference time. CF3 unlocks the potential for deploying high-fidelity 3D scene representations on edge devices and enabling real-time rendering applications."
http://arxiv.org/abs/2508.05246v1,A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis,"Gender classification from iris images offers a non-invasive biometric modality with potential applications in security, forensics, and personalized healthcare. However, the subtle gender-specific features embedded within iris textures present a challenging classification problem. This paper presents a comprehensive survey and analysis of deep learning-based techniques for gender classification using iris images. We systematically evaluate various convolutional neural network (CNN) architectures, including custom-designed networks and pre-trained models fine-tuned for iris gender recognition. Furthermore, we investigate the impact of different pre-processing steps, such as iris segmentation and normalization, along with data augmentation strategies to enhance model robustness and generalization. Our experimental results, conducted on publicly available iris datasets, demonstrate that fine-tuned CNNs, particularly those leveraging attention mechanisms, achieve state-of-the-art performance, surpassing traditional feature-based methods by a significant margin. Specifically, our best performing model achieves an accuracy of 97.3% on the UBIRIS.v2 dataset. This study provides valuable insights into the efficacy of deep learning for iris-based gender classification and highlights promising directions for future research in this domain."
http://arxiv.org/abs/2508.05244v1,RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding,"Medical image understanding benefits significantly from leveraging both imaging and textual data. However, existing vision-language pre-training methods often struggle to capture fine-grained relationships between image regions and corresponding textual descriptions in the medical domain. To address this, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning pre-trained model for enhanced medical image understanding. RegionMed-CLIP incorporates a novel region proposal network trained specifically on medical images to extract clinically relevant regions of interest. These regions are then encoded and aligned with textual descriptions using a contrastive learning objective, encouraging the model to learn fine-grained associations between image regions and their corresponding textual semantics. Experiments on several medical image classification and report generation tasks demonstrate that RegionMed-CLIP achieves significant performance improvements compared to existing CLIP-based methods and other state-of-the-art pre-trained models. This highlights the importance of region-aware learning for effective multimodal representation learning in the medical domain and its potential to improve downstream clinical applications."
http://arxiv.org/abs/2508.05240v1,Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer,"Multi-modal medical image registration is crucial for various clinical applications, including image-guided surgery and diagnosis. However, the inherent differences in imaging characteristics between modalities like Magnetic Resonance (MR) and Ultrasound (US) pose significant challenges to accurate registration. This paper addresses the problem of robust and accurate MR-US registration by mitigating the impact of disparate imaging styles. We propose a novel coarse-to-fine registration framework that leverages image style transfer to bridge the domain gap between MR and US images. Initially, a CycleGAN-based style transfer network is employed to generate synthetic US images from MR images, enabling a robust coarse registration based on normalized mutual information. Subsequently, a fine registration stage utilizes a deformable registration algorithm guided by gradient correlation, leveraging the style-transferred images to improve feature correspondence. Experimental results on a clinical dataset demonstrate that our method achieves superior registration accuracy compared to state-of-the-art registration techniques, particularly in regions with significant anatomical variations. This improved registration accuracy has the potential to enhance the reliability and precision of image-guided interventions."
http://arxiv.org/abs/2508.05237v1,Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models,"Vision-language models (VLMs) have demonstrated remarkable zero-shot transfer capabilities, but their vulnerability to adversarial attacks remains a significant concern, particularly in safety-critical applications. Achieving robustness in these models often involves complex trade-offs between standard accuracy, adversarial robustness, and computational efficiency. This paper addresses the challenge of navigating this trade-off by presenting a systematic synthesis of defensive strategies for zero-shot adversarial robustness in VLMs. We propose a novel framework that integrates adversarial training with contrastive learning and input pre-processing techniques, allowing for a tunable balance between different performance metrics. Specifically, we explore the synergistic effects of adversarial data augmentation, robust feature alignment, and adversarial purification to mitigate the impact of imperceptible perturbations. Our experiments demonstrate that our integrated approach achieves superior adversarial robustness compared to existing single-defense strategies while maintaining competitive clean accuracy. This research provides valuable insights into the design and implementation of robust VLMs, paving the way for their reliable deployment in real-world scenarios."
http://arxiv.org/abs/2508.05236v1,ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models,"Generating large-scale, diverse, and realistic camera data is crucial for training and evaluating autonomous driving systems, yet real-world data collection is costly and time-consuming. This paper addresses the challenge of generating controllable, arbitrary viewpoint camera data for autonomous driving scenarios, a capability currently lacking in existing generative models. We introduce ArbiViewGen, a novel framework leveraging Stable Diffusion models conditioned on semantic layouts and camera poses to synthesize photorealistic images from arbitrary viewpoints. Specifically, we utilize a custom Stable Diffusion pipeline incorporating a ControlNet conditioned on semantic segmentation maps and a camera pose embedding network that learns a latent space representation of camera transformations. Our experiments demonstrate that ArbiViewGen generates high-quality images with accurate scene geometry and consistent appearance across different viewpoints, achieving state-of-the-art performance in visual fidelity and viewpoint consistency compared to existing methods. ArbiViewGen offers a powerful and efficient tool for generating synthetic data to enhance the development and validation of autonomous driving algorithms."
http://arxiv.org/abs/2508.05227v1,Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2,"Two-phase flows, ubiquitous in industrial processes and natural phenomena, often exhibit complex and irregular interfaces that are challenging to accurately segment. Precise segmentation of these interfaces is crucial for understanding flow dynamics and optimizing system performance, but traditional image processing techniques often struggle with noise, varying illumination, and intricate flow structures. This paper addresses the problem of achieving robust and accurate segmentation of complex and irregular interfaces in real-world two-phase flow imagery. We propose a novel application of the Segment Anything Model (SAM2), leveraging its zero-shot transfer capabilities and prompt engineering to adapt to the specific characteristics of two-phase flow images. We explore various prompting strategies, including bounding boxes and point prompts derived from image statistics, to guide SAM2 in effectively delineating the phase boundaries. Our empirical evaluation on a diverse dataset of real-world two-phase flow images demonstrates that SAM2, with optimized prompting strategies, achieves significantly improved segmentation accuracy compared to traditional edge detection and thresholding methods, exhibiting a Dice score improvement of up to 20%. This work highlights the potential of foundation models like SAM2 for addressing challenging computer vision tasks in scientific domains, paving the way for automated analysis and control of complex two-phase flow systems."
http://arxiv.org/abs/2508.05224v1,Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning,"Federated Learning (FL) enables collaborative model training across distributed clients without sharing sensitive data, but its performance is highly vulnerable to adversarial attacks and client failures. Existing defense strategies often overlook the critical role of the communication topology in fostering resilience, focusing instead on complex aggregation rules or client-side defenses. This paper addresses the problem of designing communication topologies that inherently enhance FL robustness against various threats. We propose a novel topology optimization framework that leverages spectral graph theory to minimize the impact of malicious clients and stragglers. Specifically, we formulate the topology design problem as minimizing the spectral radius of a modified adjacency matrix, promoting faster convergence and isolating adversarial influence. Experimental results on benchmark datasets demonstrate that our optimized topologies significantly improve FL accuracy and convergence speed under both Byzantine attacks and client unavailability, outperforming traditional topologies like star and ring networks. This work highlights the importance of topology design in achieving robust and reliable federated learning systems."
http://arxiv.org/abs/2508.05221v1,ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking,"Long-term vision-language tracking (VLT) demands robust target understanding and reasoning about its evolving state over extended periods. Existing VLT methods often struggle with complex scenarios requiring temporal reasoning and contextual awareness, especially when the target undergoes significant appearance changes, occlusions, or interactions with the environment. To address this limitation, we introduce ReasoningTrack, a novel framework that incorporates chain-of-thought (CoT) reasoning into the VLT pipeline. ReasoningTrack leverages a large language model (LLM) to generate intermediate reasoning steps based on visual and textual cues, effectively decomposing the tracking task into a sequence of manageable sub-problems. These reasoning steps guide the visual tracking module, enabling it to better anticipate target behavior and maintain accurate localization over time. Experiments on challenging long-term VLT datasets, including LaSOT and LLaVA-Bench, demonstrate that ReasoningTrack significantly outperforms state-of-the-art methods, achieving substantial improvements in tracking accuracy and robustness, particularly in scenarios requiring intricate reasoning. This work highlights the potential of integrating LLMs with visual tracking systems to achieve more sophisticated and reliable long-term object tracking."
http://arxiv.org/abs/2508.05213v1,Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation,"Source-free domain adaptation addresses the challenge of adapting models to new target domains without accessing source data, a critical requirement in privacy-sensitive or data-scarce scenarios. This paper tackles the problem of source-free cross-domain few-shot semantic segmentation, where the goal is to segment unseen classes in a target domain using only a few labeled examples, without access to the original source data used for pre-training. We propose a novel Textual and Visual Guided Task Adaptation (TVTA) framework. TVTA leverages the semantic richness of text embeddings, generated from class names, to guide the adaptation process, enabling more robust generalization to novel classes. Specifically, we introduce a text-guided feature alignment module that aligns target domain features with class-specific textual representations and a visual-guided pseudo-labeling strategy that refines segmentation predictions by incorporating visual information from the few-shot support set. Experiments on benchmark datasets demonstrate that TVTA significantly outperforms state-of-the-art source-free domain adaptation methods in few-shot segmentation scenarios, achieving substantial gains in segmentation accuracy for unseen classes. This work offers a practical and effective solution for adapting segmentation models to new domains with limited data and without compromising data privacy."
http://arxiv.org/abs/2508.05211v1,VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization,"Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in understanding and generating content based on both visual and textual inputs. However, the computational cost associated with processing high-resolution images often limits their deployment in resource-constrained environments. This paper addresses the challenge of efficiently processing visual information in LMMs by introducing a token pruning framework that selectively removes less informative image tokens. Our approach, VFlowOpt, leverages visual information flow to guide token pruning, prioritizing the preservation of tokens crucial for maintaining accurate visual representation. We introduce a novel metric to quantify information flow between image tokens and dynamically adjust pruning rates based on this metric. Experiments on various visual question answering and image captioning benchmarks demonstrate that VFlowOpt achieves significant computational savings (up to 40% reduction in FLOPs) with minimal performance degradation compared to existing token pruning methods. This work offers a practical and effective solution for deploying LMMs in real-world applications where computational resources are limited."
http://arxiv.org/abs/2508.05205v1,EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery,"Endoscopic image matching is crucial for various robot-assisted surgery (RAS) applications, including surgical navigation, loop closure detection, and localization. However, current methods often struggle to generalize across diverse endoscopic datasets due to variations in imaging modalities, anatomical structures, and surgical procedures. To address this limitation, we introduce EndoMatcher, a novel endoscopic image matcher pre-trained on a large-scale, multi-domain endoscopic dataset. EndoMatcher leverages a contrastive learning framework with domain-specific data augmentations during pre-training to learn robust and generalizable feature representations. We further employ a lightweight attention mechanism to refine feature matching based on contextual information within endoscopic scenes. Experimental results on several challenging endoscopic datasets demonstrate that EndoMatcher significantly outperforms state-of-the-art methods in image matching accuracy and generalization ability, achieving up to a 20% improvement in recall@1 on unseen datasets. This advancement facilitates more reliable and accurate image matching for improved surgical guidance and automation in RAS."
http://arxiv.org/abs/2508.05202v1,SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images,"Land cover extraction from spectral remote sensing imagery is crucial for environmental monitoring and resource management. However, existing methods often struggle with the inherent complexity and variability of spectral signatures, particularly when distinguishing between spectrally similar land cover types. This paper introduces SPEX, a novel Vision-Language Model (VLM) specifically designed for land cover extraction on spectral remote sensing images. SPEX leverages a pre-trained vision transformer backbone to extract spatial-spectral features from multi-band imagery, which are then aligned with textual descriptions of land cover classes using a contrastive learning objective. A lightweight language model processes these textual descriptions, enabling SPEX to learn a joint embedding space where image patches are closely associated with their corresponding land cover classes. Experimental results on multiple benchmark datasets demonstrate that SPEX achieves state-of-the-art performance, surpassing existing methods by a significant margin, especially in challenging scenarios with high spectral similarity. SPEX offers a robust and generalizable solution for automated land cover extraction, paving the way for more accurate and efficient environmental analysis."
http://arxiv.org/abs/2508.05197v1,QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering,"Knowledge-intensive Visual Question Answering (KI-VQA) demands external knowledge to accurately answer questions about images. Existing methods often struggle to retrieve and integrate relevant knowledge effectively, especially when faced with complex, multi-faceted queries. This paper introduces QA-Dragon, a Query-Aware Dynamic Retrieval-Augmented Generation (RAG) system designed to enhance knowledge integration in KI-VQA. QA-Dragon employs a dynamic retrieval strategy that iteratively refines knowledge retrieval based on question understanding and previous retrieval results. Specifically, it uses a query decomposition module to break down complex questions and then leverages a re-ranking mechanism to select the most pertinent knowledge snippets from a large external knowledge source. Experiments on the challenging OK-VQA dataset demonstrate that QA-Dragon achieves state-of-the-art performance, surpassing existing methods by a significant margin in accuracy and demonstrating improved reasoning capabilities. This work highlights the importance of query-aware dynamic knowledge retrieval for achieving robust and accurate performance in knowledge-intensive visual reasoning tasks."
http://arxiv.org/abs/2508.05187v1,Refining Gaussian Splatting: A Volumetric Densification Approach,"Gaussian Splatting has emerged as a powerful technique for novel view synthesis, offering state-of-the-art rendering quality and speed. However, the initial Gaussian primitives are often sub-optimal, leading to artifacts in regions with complex geometry or insufficient initial density. This paper introduces a novel volumetric densification approach to refine Gaussian Splatting representations. Our method leverages a differentiable volumetric occupancy field, learned concurrently with the Gaussian parameters, to guide the densification and pruning process. Specifically, we sample points within regions of high occupancy and convert them into new Gaussian primitives, while simultaneously pruning redundant or low-occupancy Gaussians. We demonstrate through extensive experiments on benchmark datasets that our approach significantly reduces artifacts and improves rendering quality, particularly in challenging scenarios with thin structures and intricate details. Our method achieves state-of-the-art performance in terms of PSNR, SSIM, and LPIPS, while maintaining competitive rendering speeds, highlighting its potential for real-time and high-fidelity novel view synthesis."
http://arxiv.org/abs/2508.05186v1,Learning to See and Act: Task-Aware View Planning for Robotic Manipulation,"Robotic manipulation often requires integrating visual perception with motor control, a process complicated by occlusions and limited field of view. This work addresses the problem of efficiently acquiring informative viewpoints for robotic manipulation tasks, specifically focusing on how to learn a view planning policy that is aware of the task being performed. We introduce a novel reinforcement learning framework that trains an agent to select optimal viewpoints based on a learned task reward function and a learned state representation encoding both visual information and robot configuration. The agent iteratively observes the scene from different viewpoints, updating its belief state and selecting the next best view until it is confident enough to execute the manipulation. We demonstrate in simulated pick-and-place and part assembly tasks that our task-aware view planning policy significantly outperforms hand-crafted and task-agnostic baselines in terms of task success rate and the number of viewpoints required. Our approach provides a principled and efficient way to integrate perception and action, enabling robots to perform complex manipulation tasks in cluttered environments."
http://arxiv.org/abs/2508.05182v1,SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation,"Domain adaptation (DA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, mitigating the performance degradation caused by domain shift. Existing spectral alignment methods often rely on specific assumptions about the data distribution or require explicit feature correspondence, limiting their applicability in complex scenarios. This paper introduces SPA++, a Generalized Graph Spectral Alignment framework for versatile domain adaptation. SPA++ constructs graph representations of both source and target domains and formulates domain alignment as a graph matching problem solved via spectral decomposition. Crucially, SPA++ incorporates a novel adaptive weighting scheme for graph edges, allowing it to handle varying degrees of domain discrepancy and feature heterogeneity. We further augment the framework with a self-supervision module to enhance target domain representation learning. Extensive experiments on multiple DA benchmarks, including image classification and semantic segmentation tasks, demonstrate that SPA++ consistently outperforms state-of-the-art methods, achieving significant improvements in adaptation accuracy and robustness across diverse domain shifts. The generalized formulation and adaptive capabilities of SPA++ unlock its potential for addressing a wider range of real-world domain adaptation challenges."
http://arxiv.org/abs/2508.05172v1,Multi-tracklet Tracking for Generic Targets with Adaptive Detection Clustering,"Multi-object tracking (MOT) is crucial for numerous vision applications, but its performance heavily relies on the accuracy and consistency of the underlying object detector. This paper addresses the challenge of robust MOT in scenarios with noisy detections, particularly when tracking generic targets without strong appearance models. We propose a novel multi-tracklet tracking framework that incorporates an adaptive detection clustering module. This module dynamically groups detections based on both spatial proximity and appearance similarity, mitigating the impact of false positives and fragmented detections. The resulting clusters are then associated with existing tracklets using a Kalman filter-based motion prediction and a cost function that considers both motion and appearance consistency. Experimental results on standard MOT benchmarks demonstrate that our approach significantly improves tracking accuracy and reduces identity switches compared to existing methods, especially in challenging scenarios with cluttered environments and frequent occlusions. This work provides a more reliable and adaptable tracking solution for a wide range of applications involving generic object tracking."
http://arxiv.org/abs/2508.05168v1,Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations,"Medical image quality assessment (IQA) is crucial for ensuring accurate diagnoses and reliable clinical decision-making. Existing IQA methods predominantly operate on the discrete pixel grid, often overlooking the underlying continuous nature of medical images and limiting their ability to capture subtle artifacts and distortions. This paper addresses the challenge of developing a more nuanced and robust IQA framework by leveraging Implicit Neural Representations (INRs). We propose a novel approach, termed INR-IQA, that represents medical images as continuous functions parameterized by neural networks. This allows for quality assessment by comparing the learned INR of a distorted image against a reference INR, utilizing metrics computed in the continuous domain. Our experiments on various medical imaging modalities, including MRI and CT scans, demonstrate that INR-IQA outperforms state-of-the-art pixel-based IQA methods in accurately predicting perceived image quality and detecting subtle artifacts. This novel framework offers a significant advancement in medical image quality assessment by moving beyond the limitations of discrete pixel representations and enabling a more comprehensive and accurate evaluation of image fidelity."
http://arxiv.org/abs/2508.05167v1,PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems,"Multimodal Large Language Models (MLLMs) are increasingly being integrated into autonomous driving systems to enhance scene understanding and decision-making. However, the robustness of these systems against adversarial attacks, particularly in physically realizable scenarios, remains largely unexplored. This paper addresses the vulnerability of MLLM-based autonomous driving systems to physically realizable adversarial patch attacks. We introduce PhysPatch, a novel method for generating transferable adversarial patches that can be physically printed and deployed to disrupt the perception of MLLMs in real-world driving environments. PhysPatch leverages a differentiable rendering pipeline and incorporates physical constraints such as lighting variations and camera perspectives during the patch optimization process. Extensive experiments in both simulated and real-world settings demonstrate that PhysPatch can effectively mislead MLLMs, causing them to misinterpret traffic signs and make incorrect driving decisions, even when viewed from different angles and under varying lighting conditions. Our findings highlight the critical need for developing robust defense mechanisms against physical adversarial attacks to ensure the safety and reliability of MLLM-driven autonomous vehicles."
http://arxiv.org/abs/2508.05162v1,X-MoGen: Unified Motion Generation across Humans and Animals,"Generating realistic and diverse motion for virtual characters is a fundamental challenge in computer graphics and animation. Existing motion generation methods often focus on specific categories, primarily human motion, and struggle to generalize to the diverse morphologies and movement patterns observed in animals. This paper introduces X-MoGen, a unified motion generation framework capable of producing realistic and varied motions across both human and animal species. X-MoGen leverages a shared latent space learned from a large-scale multi-species motion capture dataset. We propose a novel architecture incorporating species-aware conditional variational autoencoders (CVAEs) and a kinematic adaptation module to translate latent motion representations into realistic joint trajectories for different skeletal structures. Our experiments demonstrate that X-MoGen outperforms state-of-the-art human motion generation methods while also achieving compelling results on a range of animal species, including quadrupeds and bipeds, as evaluated by quantitative metrics and perceptual user studies. The unified framework facilitates the creation of complex interactive simulations and animations featuring diverse characters with realistic and coherent motion styles."
http://arxiv.org/abs/2508.05160v1,Rotation Equivariant Arbitrary-scale Image Super-Resolution,"Image super-resolution (SR) aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, a fundamental task in computer vision. Existing SR methods often struggle with images exhibiting arbitrary rotations and scales, leading to performance degradation and artifacts. To address this limitation, we propose a novel Rotation Equivariant Arbitrary-scale Super-Resolution Network (REASRN) that leverages group convolutional neural networks (G-CNNs) to achieve rotation equivariance and integrates a continuous scale estimation module to handle arbitrary scaling factors. REASRN learns scale-aware features through a meta-learning framework, enabling it to generalize effectively across different scales and orientations. Experimental results on benchmark datasets demonstrate that REASRN significantly outperforms state-of-the-art SR methods, especially when dealing with rotated and scaled LR images, achieving superior PSNR and SSIM scores while maintaining visual fidelity. Our approach offers a robust and generalizable solution for arbitrary-scale SR, which is crucial for real-world applications where image orientations and scales are often unknown."
http://arxiv.org/abs/2508.05138v1,Deep Learning-based Animal Behavior Analysis: Insights from Mouse Chronic Pain Models,"Animal behavior analysis is crucial for understanding neurological disorders and evaluating therapeutic interventions, particularly in preclinical research using animal models. Manual scoring of these behaviors is time-consuming, subjective, and prone to bias, hindering the throughput and reliability of studies. This paper addresses the need for automated and objective assessment of pain-related behaviors in mice using deep learning techniques. We propose a novel pipeline integrating pose estimation with temporal convolutional networks (TCNs) to analyze video recordings of mice in chronic pain models. Specifically, we utilize a pre-trained pose estimation model to extract skeletal keypoints, followed by a TCN to learn temporal dependencies and classify pain-related behaviors such as paw licking, guarding, and flinching. Our results demonstrate that the proposed method achieves high accuracy in identifying and classifying these behaviors, outperforming traditional machine learning approaches based on hand-engineered features. This automated system offers a significant improvement in efficiency and objectivity for preclinical pain research, facilitating the development of more effective pain management strategies."
http://arxiv.org/abs/2508.05137v1,FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images,"Federated learning (FL) enables collaborative training of deep learning models across decentralized datasets without direct data sharing, crucial for sensitive medical imaging data. However, non-independent and identically distributed (non-IID) data distributions across clients, particularly variations in image intensity and contrast due to different acquisition protocols or scanner types, significantly degrade the performance of FL models for organ segmentation. To address this challenge, we propose FedGIN, a novel federated learning framework with Dynamic Global Intensity Non-linear Augmentation. FedGIN dynamically adjusts intensity transformations based on the global intensity distribution learned collaboratively across clients during each communication round. This involves estimating global intensity statistics and generating non-linear intensity mapping functions to augment local client data, thereby mitigating the impact of intensity variations. Experimental results on multi-modal (CT and MRI) abdominal organ segmentation demonstrate that FedGIN consistently outperforms state-of-the-art FL methods, achieving significant improvements in Dice scores, especially for clients with drastically different intensity distributions, while maintaining privacy. FedGIN offers a robust and practical solution for collaborative medical image analysis in real-world federated settings with heterogeneous data."
http://arxiv.org/abs/2508.05123v1,Latent Expression Generation for Referring Image Segmentation and Grounding,"Referring image segmentation (RIS) and referring image grounding (RIG) aim to identify and segment or locate objects in an image based on a natural language expression. Existing methods often struggle with the inherent ambiguity and variability of natural language, particularly in handling complex or nuanced expressions. To address this, we propose a novel framework, Latent Expression Generation (LEG), which learns to generate a diverse set of semantically similar, yet syntactically varied, latent expressions from the original input expression. These latent expressions are generated by a transformer-based model conditioned on both the original expression and the visual context, encouraging the generation of expressions that are visually grounded. The generated latent expressions are then used to augment the original expression during the RIS/RIG process, effectively expanding the search space and improving robustness to linguistic variations. Experiments on multiple benchmark datasets for both RIS and RIG demonstrate that LEG significantly improves performance, achieving state-of-the-art results, particularly on challenging expressions that require a deeper understanding of the visual context. This highlights the importance of modeling expression variability for robust and accurate vision-language understanding."
http://arxiv.org/abs/2508.05114v1,AHDMIL: Asymmetric Hierarchical Distillation Multi-Instance Learning for Fast and Accurate Whole-Slide Image Classification,"Whole-slide images (WSIs) present a significant opportunity for computational pathology, enabling automated disease diagnosis and prognosis. However, their gigapixel resolution poses computational challenges, particularly for training deep learning models. We address the problem of efficiently classifying WSIs using multi-instance learning (MIL) while maintaining high accuracy. We introduce Asymmetric Hierarchical Distillation Multi-Instance Learning (AHDMIL), a novel framework that leverages hierarchical feature extraction and knowledge distillation to accelerate WSI classification. AHDMIL employs a lightweight student network to learn from a pre-trained, computationally expensive teacher network, both operating on image patches extracted at multiple resolutions. Crucially, the distillation process is asymmetric, focusing the student's learning on the most informative regions identified by the teacher, thereby improving its efficiency and accuracy. Experimental results on benchmark datasets demonstrate that AHDMIL achieves comparable or superior classification performance to state-of-the-art MIL methods while significantly reducing computational cost and inference time. This efficient and accurate WSI classification framework has the potential to facilitate broader adoption of deep learning in clinical pathology."
http://arxiv.org/abs/2508.05115v1,RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer,"Creating realistic and expressive talking-head videos from audio has long been a challenging task in computer vision and graphics. Existing audio-driven portrait animation methods often struggle to generate high-fidelity results and maintain temporal coherence, particularly for complex facial expressions and head movements. This paper addresses the problem of generating high-quality, temporally stable, and realistic portrait videos driven solely by audio input in real-time. We introduce RAP, a novel Real-time Audio-driven Portrait animation framework utilizing a Video Diffusion Transformer. RAP leverages a transformer-based architecture to capture long-range temporal dependencies within the generated video sequence and employs a diffusion process to synthesize realistic facial details conditioned on audio features. Furthermore, we introduce a novel training strategy to enhance lip synchronization and expression fidelity. Experimental results demonstrate that RAP significantly outperforms state-of-the-art methods in terms of visual quality, lip synchronization accuracy, and temporal stability, while achieving real-time performance. RAP provides a practical and efficient solution for generating high-quality talking-head videos, enabling various applications in virtual communication, entertainment, and content creation."
http://arxiv.org/abs/2508.05094v1,Sculpting Margin Penalty: Intra-Task Adapter Merging and Classifier Calibration for Few-Shot Class-Incremental Learning,"Few-Shot Class-Incremental Learning (FSCIL) presents a significant challenge in machine learning, requiring models to learn new classes from limited examples while mitigating catastrophic forgetting of previously learned knowledge. A critical bottleneck in FSCIL lies in effectively adapting to new tasks without compromising past performance and ensuring well-calibrated predictions. To address this, we propose a novel approach termed ""Sculpting Margin Penalty"" (SMP) which incorporates intra-task adapter merging and classifier calibration. SMP leverages a margin-sensitive penalty during adapter training to sculpt the feature space, promoting better class separation and reducing overlap. Furthermore, we introduce a temperature scaling-based calibration technique optimized for the merged adapter, ensuring reliable probability estimates for both old and new classes. Experimental results on benchmark datasets demonstrate that SMP achieves state-of-the-art performance in FSCIL, significantly improving both classification accuracy and calibration error compared to existing methods. This work offers a practical and effective solution for building robust and reliable FSCIL systems."
http://arxiv.org/abs/2508.05091v1,PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation,"Generating long human videos with precise pose control remains a significant challenge, often resulting in inconsistencies and artifacts, particularly for unseen subjects. This paper introduces PoseGen, a novel framework for pose-controllable long human video generation leveraging in-context learning with LoRA finetuning. PoseGen employs a two-stage approach: first, a pre-trained text-to-video diffusion model is finetuned with LoRA on a small set of subject-specific pose-image pairs, enabling efficient adaptation to new identities. Second, an in-context module uses these finetuned LoRA weights to guide the video generation process, maintaining pose fidelity and temporal coherence over extended durations. Experiments demonstrate that PoseGen significantly outperforms existing methods in terms of pose accuracy, visual quality, and identity preservation, as evidenced by quantitative metrics and qualitative evaluations. PoseGen offers a practical and efficient solution for generating high-quality, pose-controllable long human videos, opening new avenues for applications in virtual character animation and personalized content creation."
http://arxiv.org/abs/2508.05084v1,AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models,"Pathology foundation models (PFMs) pre-trained on large-scale histopathology images have shown promise for various downstream diagnostic tasks. However, integrating information from multiple PFMs effectively remains a challenge, especially when the strengths of each model vary across different diagnostic scenarios. This paper addresses the problem of adaptively fusing the predictions of multiple PFMs to leverage their complementary strengths during inference. We introduce AdaFusion, a novel prompt-guided inference framework that dynamically weights the outputs of different PFMs based on a learned adaptive fusion network. This network takes as input the original histopathology image, a textual prompt describing the diagnostic task, and the raw predictions from each PFM, learning to optimally combine the predictions for improved accuracy. Extensive experiments on multiple publicly available histopathology datasets demonstrate that AdaFusion consistently outperforms individual PFMs and static fusion strategies, achieving state-of-the-art performance in tasks such as tumor classification and metastasis detection. AdaFusion offers a flexible and effective approach for harnessing the collective power of multiple PFMs, paving the way for more accurate and reliable AI-driven pathology diagnostics."
http://arxiv.org/abs/2508.05069v1,"FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer","Makeup transfer aims to apply the makeup style from a reference image onto a source face, a challenging task requiring precise attribute manipulation while preserving identity. Existing methods often struggle with generating high-fidelity results, maintaining identity consistency, and exhibiting robustness to variations in pose and expression. This paper addresses these limitations by introducing FLUX-Makeup, a novel makeup transfer framework leveraging a diffusion transformer architecture. Our approach employs a facial attribute extraction module to disentangle makeup style from identity and pose information. This information is then encoded into a latent space and used to condition a diffusion transformer, which iteratively refines the source image to incorporate the desired makeup style. The transformer architecture enables robust long-range dependency modeling, facilitating realistic and coherent makeup transfer. Experiments demonstrate that FLUX-Makeup achieves state-of-the-art performance in terms of visual quality, identity preservation, and robustness compared to existing methods, as evidenced by quantitative metrics and qualitative evaluations. This work paves the way for more realistic and controllable virtual makeup applications."
http://arxiv.org/abs/2508.05068v1,Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks,"Image colorization, the task of adding plausible colors to grayscale images, remains a challenging problem in computer vision. This paper addresses the problem of automatically generating realistic colorizations from grayscale images by leveraging the power of Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs). Our proposed method employs a two-stage architecture. First, a CNN-based generator network learns to predict color information from grayscale input. Second, a GAN framework, incorporating a discriminator network, is used to refine the colorized output, encouraging realism and mitigating artifacts often present in CNN-only approaches. The discriminator is trained to distinguish between real color images and colorized images, providing adversarial feedback to the generator. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance, producing visually compelling and perceptually accurate colorizations that significantly outperform existing techniques in terms of both quantitative metrics and qualitative evaluation. This work represents a significant advancement in automatic image colorization, offering a robust and effective approach for enriching grayscale images with vivid and realistic colors."
http://arxiv.org/abs/2508.05065v1,Decoupling Continual Semantic Segmentation,"Continual Semantic Segmentation (CSS) aims to learn new classes sequentially without forgetting previously learned knowledge. However, existing CSS methods often suffer from catastrophic forgetting and struggle to maintain a balance between plasticity and stability, particularly due to the entangled learning of feature extraction and classification. To address this, we propose a novel ""Decoupled Continual Semantic Segmentation"" (DCSS) framework that explicitly separates the learning of shared feature representations from class-specific classification heads. DCSS employs a fixed, pre-trained feature extractor based on a vision transformer, which provides robust and stable feature representations across tasks. A lightweight, dynamically expandable classification head is then learned for each new task, along with knowledge distillation and a regularization term to mitigate forgetting and promote efficient learning of new classes. Experiments on standard CSS benchmarks like Pascal VOC and Cityscapes demonstrate that DCSS significantly outperforms state-of-the-art methods, achieving higher average incremental accuracy and lower forgetting rates. This decoupled approach offers a more effective and efficient strategy for continual learning in semantic segmentation, paving the way for more scalable and adaptable vision systems."
http://arxiv.org/abs/2508.05064v1,A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding,"Language embedding has emerged as a powerful tool for bridging the gap between natural language and visual understanding, particularly in 2D image analysis. However, its application to complex 3D scene understanding remains relatively underexplored despite the potential for significantly enhanced scene interpretation and reasoning. This paper addresses the challenge of effectively utilizing language embeddings to facilitate a more comprehensive understanding of 3D scenes. We propose a novel framework that leverages pre-trained language models to encode scene descriptions and contextual information, subsequently aligning these embeddings with 3D scene representations derived from point clouds and meshes using a contrastive learning objective. This alignment allows us to perform tasks such as text-based 3D object retrieval, scene graph generation from natural language descriptions, and fine-grained scene understanding based on textual queries. Experimental results on benchmark datasets demonstrate that our approach achieves state-of-the-art performance in text-to-3D retrieval accuracy and significantly improves the quality of generated scene graphs compared to existing methods. This work highlights the potential of language embedding as a crucial component for enabling more intuitive and semantically rich interactions with 3D environments."
http://arxiv.org/abs/2508.05060v1,DualMat: PBR Material Estimation via Coherent Dual-Path Diffusion,"Physically-based rendering (PBR) material estimation from a single image is a challenging ill-posed inverse rendering problem. Existing methods often struggle with generating high-fidelity, spatially coherent material maps, particularly for complex real-world materials. To address this, we introduce DualMat, a novel framework leveraging a coherent dual-path diffusion model for PBR material estimation. DualMat consists of two parallel diffusion paths: a global path that captures the overall material properties and appearance, and a local path that focuses on refining fine-grained details and ensuring spatial coherence. These paths are coupled through a cross-attention mechanism, allowing them to collaboratively generate high-quality material maps by leveraging both global context and local texture information. Experiments on synthetic and real-world datasets demonstrate that DualMat significantly outperforms state-of-the-art methods in terms of material estimation accuracy and visual fidelity, producing more realistic and coherent material maps. This improved material estimation has the potential to significantly enhance applications in areas such as virtual reality, augmented reality, and content creation."
http://arxiv.org/abs/2508.05059v1,Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting,"Deep neural networks, while powerful, suffer from catastrophic forgetting, where learning new tasks overwrites knowledge acquired from previous ones. Identifying and predicting which weights are most susceptible to forgetting is crucial for developing effective continual learning strategies. This paper addresses the problem of predicting ""overflowed"" weights  those significantly altered by subsequent learning  by retrodicting forgetting patterns. Our method, Learning from Oblivion (LfO), trains a meta-network to predict weight changes based on the initial weight values and the gradients observed during subsequent task learning. LfO leverages a novel loss function that emphasizes the correlation between predicted weight changes and actual forgetting, enabling the meta-network to learn the characteristics of weights prone to overflow. Experiments on multiple continual learning benchmarks demonstrate that LfO accurately predicts overflowed weights, achieving significantly higher precision and recall compared to baseline weight-importance estimation methods. These predictions can be used to inform weight protection or regularization strategies, leading to improved continual learning performance."
http://arxiv.org/abs/2508.05053v1,Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content. However, their performance on tasks requiring the localization of fine-grained details within images remains largely unexplored. This paper investigates the ability of current MLLMs to accurately locate subtle visual elements, addressing the challenge of ""finding needles in images."" We propose a novel prompting strategy that combines detailed textual descriptions with iterative refinement, guiding the MLLM to focus on specific regions of interest within the image. This involves an initial coarse localization, followed by progressively zoomed-in views and more specific queries. Our experiments, conducted on a newly curated dataset of high-resolution images with challenging, subtle details, reveal that while MLLMs exhibit a limited capacity to initially identify these details, our iterative refinement approach significantly improves localization accuracy, achieving a 25% increase in intersection-over-union (IoU) compared to baseline prompting methods. These findings highlight both the potential and current limitations of MLLMs for fine-grained visual understanding, paving the way for future research in enhancing their perceptual acuity."
http://arxiv.org/abs/2508.05038v1,HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID,"Video-based person re-identification (ReID) is crucial for various applications like surveillance and intelligent transportation, yet remains challenging due to variations in pose, viewpoint, and illumination across video frames. Existing methods often struggle to effectively integrate diverse biometric cues extracted from different body parts and time steps, leading to suboptimal performance. We address this limitation by proposing HAMoBE, a Hierarchical and Adaptive Mixture of Biometric Experts, for robust video-based person ReID. HAMoBE learns a hierarchical representation by first aggregating frame-level features into part-level experts, followed by an adaptive mixture module that dynamically weighs the contribution of each expert based on the input video's characteristics. The adaptive mixture leverages attention mechanisms to focus on the most discriminative body parts and temporal segments, enhancing the robustness to occlusions and viewpoint changes. Extensive experiments on several challenging video ReID datasets, including MARS and DukeMTMC-VideoReID, demonstrate that HAMoBE achieves state-of-the-art performance, outperforming existing methods by a significant margin. This highlights the effectiveness of our hierarchical and adaptive approach in leveraging diverse biometric cues for robust video-based person ReID."
http://arxiv.org/abs/2508.05037v1,A Novel Image Similarity Metric for Scene Composition Structure,"Image similarity metrics are crucial for a variety of computer vision tasks, including image retrieval, scene understanding, and generative modeling. Existing metrics often focus on pixel-level or feature-level comparisons, neglecting the underlying compositional structure of scenes. This paper introduces a novel image similarity metric specifically designed to capture and compare the compositional structure of scenes, addressing the limitation of current methods in discerning subtle differences in object arrangements and relationships. Our approach leverages a graph-based representation of scene composition, where nodes represent objects and edges encode their spatial relationships. The similarity between two images is then computed by measuring the graph edit distance between their respective scene graphs, incorporating learned weights for node and edge transformations based on semantic similarity and spatial context. We demonstrate the effectiveness of our metric on several scene understanding tasks, including scene graph retrieval and compositional image generation evaluation, achieving significantly improved correlation with human perception compared to state-of-the-art image similarity metrics. This novel metric provides a powerful tool for tasks requiring sensitivity to scene composition and arrangement, opening new avenues for research in scene understanding and generation."
http://arxiv.org/abs/2508.05019v1,Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes,"Clinical documentation, specifically Subjective, Objective, Assessment, and Plan (SOAP) notes, is a crucial but time-consuming aspect of dermatological practice. Automating the generation of structured SOAP notes from patient encounters could significantly reduce clinician burden, but requires robust methods for extracting and organizing relevant information. We address the challenge of automatically generating structured SOAP notes from clinical text using only weak supervision in the form of section headings. We introduce Skin-SOAP, a novel weakly supervised framework that leverages a transformer-based sequence-to-sequence model fine-tuned on a large corpus of dermatological text. Skin-SOAP incorporates a multi-task learning objective, simultaneously predicting the SOAP section and generating the corresponding text, guided by attention mechanisms to align input text with the appropriate SOAP category. Experiments on a real-world dermatology dataset demonstrate that Skin-SOAP achieves state-of-the-art performance in generating coherent and clinically relevant SOAP notes, outperforming existing weakly supervised and rule-based methods, as evaluated by both automated metrics and expert clinicians. This framework offers a promising solution for automating clinical documentation, ultimately improving efficiency and allowing clinicians to focus on patient care."
http://arxiv.org/abs/2508.05016v1,AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content,"User-generated content (UGC) is increasingly being enhanced by artificial intelligence (AI) techniques, leading to diverse and often unpredictable quality variations. Existing image quality assessment (IQA) datasets primarily focus on traditional distortions and lack the complexities introduced by AI-driven enhancements. To address this gap, we introduce AU-IQA, a novel benchmark dataset specifically designed for perceptual quality assessment of AI-enhanced UGC. AU-IQA comprises over 20,000 images generated from diverse source content and processed with a wide range of AI models, including those for super-resolution, style transfer, inpainting, and object removal. We collected subjective quality ratings via a large-scale crowdsourcing study, providing reliable Mean Opinion Scores (MOS) for each image. Experimental results demonstrate that existing full-reference and no-reference IQA metrics exhibit limited correlation with human perception on AU-IQA, highlighting the challenge of assessing the quality of AI-enhanced content. This dataset provides a valuable resource for developing and evaluating novel IQA models capable of accurately predicting the perceptual quality of AI-enhanced UGC."
http://arxiv.org/abs/2508.05008v1,Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation,"Medical image segmentation is crucial for accurate diagnosis and treatment planning, but current deep learning models often struggle with generalization across diverse datasets due to spurious correlations. This paper addresses the problem of learning generalizable representations for medical image segmentation by explicitly modeling and mitigating the effects of confounding factors present in multimodal data. We propose a novel Multimodal Causal-Driven Representation Learning (MCDRL) framework. MCDRL leverages causal inference principles to disentangle modality-specific and modality-invariant features, explicitly identifying and removing the influence of confounders such as imaging protocols or patient demographics. We achieve this through a combination of adversarial learning and a novel causal loss function that encourages the model to learn representations invariant to identified confounders while preserving essential anatomical information. Experimental results on three publicly available multimodal medical image segmentation datasets demonstrate that MCDRL consistently outperforms state-of-the-art methods in cross-dataset generalization, achieving significant improvements in segmentation accuracy and robustness. This work provides a principled approach for learning generalizable representations in multimodal medical imaging, paving the way for more reliable and clinically relevant AI-driven diagnostic tools."
http://arxiv.org/abs/2508.05001v1,CRAM: Large-scale Video Continual Learning with Bootstrapped Compression,"Continual learning (CL) in video understanding faces significant challenges due to catastrophic forgetting and the high computational cost of processing large-scale video data. Existing video CL methods often struggle to scale to real-world scenarios with complex and diverse video streams. To address these limitations, we introduce CRAM: a novel large-scale Video Continual Learning framework with Bootstrapped Compression. CRAM leverages a learned compression module, bootstrapped with knowledge distillation from past models, to efficiently reduce the computational burden of processing high-resolution video while retaining crucial information. This compressed representation is then used for continual learning with a dynamically expanding network, mitigating catastrophic forgetting through selective parameter freezing and replay of compressed exemplars. Experiments on large-scale video datasets, including ActivityNet and Kinetics, demonstrate that CRAM significantly outperforms state-of-the-art video CL methods, achieving comparable or superior accuracy with a substantial reduction in computational cost and memory footprint. CRAM offers a practical and scalable solution for deploying continual learning models in dynamic video environments."
http://arxiv.org/abs/2508.04998v1,Attribute Guidance With Inherent Pseudo-label For Occluded Person Re-identification,"Occluded person re-identification (Re-ID) remains a challenging task due to severe feature corruption caused by occlusions and the scarcity of clean data for training. This paper addresses the problem of learning robust and discriminative person representations in the presence of occlusion by leveraging attribute information. We propose an Attribute Guidance with Inherent Pseudo-label (AGIP) framework, which uses attribute prediction as an auxiliary task to guide feature learning and generate inherent pseudo-labels to alleviate the impact of occlusion. Specifically, we first predict attributes from person images and use attribute-aware attention to focus on relevant body regions, enhancing the robustness of feature extraction. Then, we generate pseudo-labels based on the predicted attribute similarity, which are used to refine the feature embedding space, encouraging similar identities with similar attributes to cluster together. Extensive experiments on several challenging occluded Re-ID datasets, including Occluded-DukeMTMC, Occluded-REID, and P-DukeMTMC, demonstrate that our proposed AGIP framework achieves state-of-the-art performance. The proposed method provides a novel and effective way to mitigate the impact of occlusion by exploiting readily available attribute information, advancing the field of occluded person Re-ID."
http://arxiv.org/abs/2508.04988v1,Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks,"The visual cortex exhibits remarkable plasticity, rapidly adapting to contextual regularities to improve perceptual efficiency. However, current deep learning models often struggle to capture this rapid contextual learning observed in biological systems. We address this gap by proposing a novel architecture, the Fast-Weight Deep Autoencoder (FW-DAE), which integrates fast-weight connections within a deep autoencoder framework. Specifically, the FW-DAE learns a compressed representation of visual input through a standard autoencoder, while simultaneously utilizing fast-weight connections to modulate the network's activity based on recent contextual information. These fast-weights, updated via Hebbian-like learning rules on a faster timescale than the autoencoder's primary weights, enable rapid adaptation to changing visual environments. Experiments on benchmark image datasets with manipulated contextual cues demonstrate that FW-DAEs achieve significantly improved performance in detecting and classifying contextually incongruent objects compared to standard autoencoders and other deep learning baselines. This suggests that incorporating fast-weight connections into deep learning architectures offers a promising avenue for modeling rapid contextual learning and enhancing the adaptability of artificial visual systems."
http://arxiv.org/abs/2508.04987v1,Unified modality separation: A vision-language framework for unsupervised domain adaptation,"Unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, addressing the domain shift problem that arises when training data does not reflect real-world deployment scenarios. However, existing UDA methods often struggle with complex domain shifts that manifest differently across modalities, particularly in vision-language tasks. This paper introduces a novel ""Unified Modality Separation"" (UMS) framework for UDA that explicitly addresses modality-specific domain discrepancies. UMS leverages contrastive learning with carefully designed vision and language encoders to disentangle domain-invariant semantic features from domain-specific style features in each modality. Specifically, we train encoders to maximize the agreement between semantic features across domains while minimizing the correlation between style features. We evaluate UMS on several challenging vision-language UDA benchmarks, including visual question answering and image captioning tasks. Our results demonstrate that UMS consistently outperforms state-of-the-art UDA methods, achieving significant improvements in adaptation accuracy and robustness. This work provides a principled approach to handling modality-specific domain shifts, paving the way for more effective and reliable vision-language models in real-world applications."
http://arxiv.org/abs/2508.04984v1,Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion,"Depth completion aims to recover dense depth maps from sparse measurements, a crucial task for various applications like autonomous driving and robotics. Existing depth completion methods often struggle with out-of-distribution (OOD) scenarios where the test data significantly deviates from the training distribution, leading to performance degradation. This paper addresses the challenge of OOD depth completion by leveraging the generalization capabilities of large-scale depth foundation models. We propose a novel depth propagation framework that utilizes a pre-trained depth foundation model to generate initial coarse depth estimates and then refines these estimates by propagating information from the sparse input depth, guided by learned confidence maps. The confidence maps, predicted by a lightweight network, dynamically adjust the influence of the propagated sparse depth based on local data quality and uncertainty. Our experiments on various OOD datasets demonstrate that the proposed method significantly outperforms state-of-the-art depth completion approaches, achieving substantial improvements in accuracy and robustness. This work highlights the potential of depth foundation models for addressing the challenges of generalization in depth completion and opens new avenues for developing more reliable depth perception systems."
http://arxiv.org/abs/2508.04979v1,Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression,"Image compression aims to reduce storage space and transmission bandwidth for images, with deep learning-based methods showing promise. However, achieving high compression ratios while maintaining fidelity often requires complex architectures and iterative refinement, hindering practical applications demanding speed. This paper addresses the challenge of fast and efficient image compression by leveraging a one-step diffusion model steered by a fidelity-rich decoder. Our approach, Fidelity-Steered One-Step Diffusion Compression (FSODC), employs a lightweight diffusion model to directly map the input image to a compressed latent representation. Crucially, we train a powerful decoder, incorporating attention mechanisms and multi-scale feature fusion, to provide rich fidelity cues that guide the diffusion process and ensure accurate reconstruction. Experimental results demonstrate that FSODC achieves competitive or superior rate-distortion performance compared to existing learned compression methods, while offering significantly faster compression and decompression speeds due to its one-step nature. The proposed method offers a practical solution for real-time image compression applications where speed and fidelity are paramount."
http://arxiv.org/abs/2508.04976v1,CSRAP: Enhanced Canvas Attention Scheduling for Real-Time Mission Critical Perception,"Real-time perception is crucial for mission-critical applications like autonomous driving and robotics, demanding high accuracy and low latency. However, resource constraints necessitate efficient allocation of computational effort. This paper addresses the problem of dynamically scheduling attention within convolutional neural networks (CNNs) to maximize performance under tight computational budgets in real-time mission-critical perception tasks. We introduce CSRAP, Canvas Attention Scheduling with Residual-aware Prioritization, which learns to predict the importance of different spatial regions (canvases) of the feature map based on residual information. CSRAP then dynamically allocates computational resources to the most informative canvases, processing them with higher resolution or more complex operations while down-sampling less critical regions. Experiments on benchmark datasets such as Cityscapes and KITTI demonstrate that CSRAP achieves significant improvements in accuracy (up to 5% mIoU) and reduces computational cost (up to 20% FLOPs) compared to state-of-the-art attention mechanisms and dynamic scheduling approaches. CSRAP provides a practical solution for deploying high-performance perception systems on resource-constrained platforms, enabling reliable real-time operation in mission-critical scenarios."
http://arxiv.org/abs/2508.04968v1,UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS,"Reconstructing high-fidelity 3D scenes from sparse views remains a significant challenge in computer vision, particularly for methods relying on differentiable rendering. Neural Radiance Fields (NeRFs) and, more recently, 3D Gaussian Splatting (3DGS), have shown promise, but often struggle with artifacts and incomplete geometry when trained with limited camera viewpoints. To address this, we introduce Uncertainty-Guided Differentiable Opacity and Soft Dropout (UGOD) for enhanced sparse-view 3DGS. UGOD leverages predicted uncertainty to refine the opacity learning process, allowing for more accurate estimation of scene density. Furthermore, we propose a novel soft dropout strategy, guided by uncertainty, that encourages exploration of plausible solutions by selectively regularizing less confident regions of the 3D Gaussian representation. Our experiments demonstrate that UGOD significantly improves the quality and completeness of reconstructed 3D scenes from sparse views, achieving state-of-the-art performance on benchmark datasets. This approach provides a robust and efficient framework for high-fidelity 3D reconstruction from limited viewpoints, advancing the capabilities of differentiable rendering techniques."
http://arxiv.org/abs/2508.04966v1,Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction,"Reconstructing dynamic scenes from multi-view video is a challenging problem in computer vision, requiring accurate representation of both geometry and temporal evolution. Existing methods often struggle with complex deformations and topological changes. This paper addresses the problem of robust and high-fidelity 4D reconstruction of dynamic scenes by integrating Laplacian surface analysis with 3D Gaussian Splatting. Our approach leverages the efficient rendering capabilities of 3D Gaussian Splatting to represent the scene geometry at each time step. Crucially, we introduce a novel Laplacian-based regularization term that enforces temporal coherence by penalizing deviations from smooth surface deformations. This term is incorporated into the splatting optimization process, guiding the Gaussians to move according to a learned dynamics model informed by the Laplacian of the surface. Experiments on challenging benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of reconstruction accuracy, visual fidelity, and temporal smoothness compared to existing dynamic scene reconstruction techniques. Our work provides a powerful framework for capturing and representing complex dynamic scenes, enabling applications in virtual reality, animation, and robotics."
http://arxiv.org/abs/2508.04965v1,Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting,"3D Gaussian Splatting (3D-GS) has emerged as a powerful technique for novel view synthesis, achieving state-of-the-art rendering quality and speed. However, the computational cost associated with splatting hundreds of millions of Gaussians remains a significant bottleneck, hindering its deployment in real-time applications, particularly on resource-constrained devices. To address this, we propose Perceive-Sample-Compress (PSC), a novel framework that dynamically optimizes Gaussian rendering by selectively focusing computation on perceptually relevant regions. PSC first employs a lightweight neural network to predict a perceptual importance map of the rendered image. This map guides a non-uniform sampling strategy that prioritizes Gaussians contributing to high-importance regions, reducing the number of Gaussians processed per frame. Finally, we introduce a differentiable compression module that further reduces memory bandwidth requirements by encoding Gaussian attributes, allowing for efficient decompression during rendering. Experiments demonstrate that PSC achieves comparable rendering quality to the original 3D-GS while achieving a 2-3x speedup and significantly reducing memory bandwidth usage. This enables real-time rendering of complex 3D scenes on consumer-grade hardware, paving the way for wider adoption of 3D-GS in interactive applications."
http://arxiv.org/abs/2508.04962v1,Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework,"Point cloud semantic segmentation is crucial for scene understanding in autonomous driving and robotics, but current methods are limited by their closed-world assumption, struggling with novel, unseen object categories. This paper addresses the challenge of open-world point cloud semantic segmentation, where models must identify known classes while also detecting and adapting to unknown objects in real-time. We propose a novel human-in-the-loop framework that integrates uncertainty-aware deep learning with interactive annotation. Our system first leverages a modified PointNet++ architecture with a Bayesian inference module to identify unknown regions based on high predictive uncertainty. These uncertain regions are then presented to a human annotator for labeling, allowing the model to incrementally learn new classes and refine its understanding of the environment through active learning. We demonstrate that our framework significantly improves segmentation accuracy on both known and novel classes compared to existing open-world segmentation approaches, achieving a 15% increase in overall Intersection-over-Union on a modified SemanticKITTI dataset with simulated unknown objects. This work provides a practical and efficient approach for deploying robust point cloud semantic segmentation in dynamic and unpredictable open-world environments."
http://arxiv.org/abs/2508.04955v1,AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics,"Spatial proteomics, which maps the spatial distribution of proteins within cells and tissues, generates complex, high-dimensional data requiring robust representation learning techniques. However, variations in experimental conditions and imaging platforms introduce significant domain shifts, hindering the generalization of learned representations across datasets. To address this, we propose AdvDINO, a novel domain-adversarial self-supervised learning framework built upon the DINO architecture. AdvDINO integrates a domain discriminator network that encourages the DINO encoder to learn domain-invariant features by minimizing the discriminator's ability to distinguish between different data sources. Simultaneously, the DINO self-supervised learning objective ensures the learned representations capture essential protein spatial relationships. Experiments on multiple spatial proteomics datasets demonstrate that AdvDINO outperforms existing self-supervised methods in downstream tasks such as cell type classification and protein localization prediction, particularly when evaluated on datasets from unseen domains. AdvDINO's ability to learn domain-invariant representations significantly improves the robustness and generalizability of spatial proteomics analysis, facilitating broader application and integration of datasets across diverse experimental settings."
http://arxiv.org/abs/2508.04945v1,Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering,"Visual activity recognition has achieved significant progress, yet its evaluation often relies on simplistic label matching, neglecting the inherent ambiguity of action verbs. This leads to an overestimation of performance, particularly when dealing with subtle variations in activity execution or semantic overlap between actions. We address this problem by introducing a novel evaluation framework that incorporates verb sense disambiguation through clustering. Our approach leverages pre-trained word embeddings and hierarchical clustering to group action verbs into semantically similar clusters, effectively capturing different senses of the same verb. During evaluation, predictions are considered correct if they fall within the same cluster as the ground truth, thereby allowing for more nuanced and robust assessment. Experiments on benchmark datasets demonstrate that our sense-clustering-based evaluation reveals a significant drop in performance compared to traditional exact matching, highlighting the limitations of current models in handling verb ambiguity. This approach provides a more realistic and informative evaluation metric, paving the way for the development of more robust and semantically aware activity recognition systems."
http://arxiv.org/abs/2508.04943v1,TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring,"Dynamic Scene Graph Generation (DynSG) aims to understand evolving relationships between objects in videos, offering rich scene representations for video understanding. However, acquiring dense and accurate annotations for DynSG is expensive, hindering the development of robust models. This paper addresses the challenge of weakly supervised DynSG, where only video-level labels are available. We propose TRKT: a Temporal-enhanced Relation-aware Knowledge Transferring framework. TRKT leverages knowledge from a static scene graph model trained on image data with full annotations and transfers this knowledge to the DynSG task. Specifically, we introduce a temporal-enhanced module to capture long-range dependencies between visual features, and a relation-aware knowledge transferring module to selectively transfer knowledge based on the relevance of static scene graph relations to the dynamic video context. Experiments on the Action Genome dataset demonstrate that TRKT significantly outperforms existing weakly supervised methods, achieving comparable performance to some fully supervised approaches while utilizing only video-level labels. This work provides a promising direction for learning dynamic scene representations with minimal supervision, opening up new possibilities for video understanding tasks."
http://arxiv.org/abs/2508.04942v1,Accelerating Conditional Prompt Learning via Masked Image Modeling for Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in various cross-modal tasks, with conditional prompt learning emerging as an effective paradigm for adapting VLMs to downstream applications. However, training these prompt-based VLMs often requires substantial computational resources and time due to the need to process both image and text modalities during each iteration. This paper addresses the challenge of accelerating conditional prompt learning for VLMs by leveraging Masked Image Modeling (MIM). Our method strategically masks image patches during training, forcing the model to learn more robust and efficient visual representations from the remaining visible regions. This reduces the computational burden associated with processing the entire image while simultaneously encouraging the model to focus on salient features relevant to the prompt. We demonstrate through experiments on multiple vision-language benchmarks that our MIM-based approach significantly accelerates conditional prompt learning, achieving comparable or even superior performance to full image training with a substantial reduction in training time (up to 30%). This work offers a practical and effective strategy for accelerating the adaptation of VLMs to new tasks, making them more accessible for resource-constrained environments."
http://arxiv.org/abs/2508.04941v1,Toward Errorless Training ImageNet-1k,"ImageNet-1k has been instrumental in advancing deep learning, serving as a crucial pre-training dataset for numerous computer vision tasks. However, ImageNet-1k is known to contain label noise, which can negatively impact the performance of models trained on it, particularly at scale. This paper addresses the challenge of mitigating the effects of label noise in ImageNet-1k to enable more robust and accurate model training. We propose a novel framework, Errorless Training with Cross-Attention Filtering (ETCAF), that leverages cross-attention mechanisms within a co-teaching paradigm to dynamically identify and filter out potentially mislabeled images during training. ETCAF utilizes two networks trained on complementary subsets of the data, using cross-attention maps to estimate the reliability of each image's label and selectively update the models with high-confidence examples. Experiments demonstrate that ETCAF significantly improves the classification accuracy of ResNet-50 and other architectures trained on ImageNet-1k, achieving state-of-the-art performance compared to other label-noise robust training methods. This work provides a pathway towards more effective and reliable training on large-scale datasets with inherent label noise, ultimately leading to improved generalization and performance across various downstream tasks."
http://arxiv.org/abs/2508.04937v1,ALScope: A Unified Toolkit for Deep Active Learning,"Active Learning (AL) aims to minimize annotation costs by strategically selecting the most informative samples for labeling. Despite the proliferation of deep learning-based AL methods, a unified and easily extensible toolkit that encompasses diverse algorithms and datasets remains lacking, hindering fair comparisons and widespread adoption. To address this, we introduce ALScope, a comprehensive and modular toolkit designed to streamline the development, evaluation, and deployment of deep active learning strategies. ALScope provides a standardized interface for various AL query strategies, including uncertainty-based, diversity-based, and hybrid approaches, and integrates seamlessly with popular deep learning frameworks like PyTorch and TensorFlow. Furthermore, ALScope supports a wide range of datasets and evaluation metrics, facilitating rigorous benchmarking and analysis. Empirical evaluations across several benchmark datasets demonstrate the effectiveness and versatility of ALScope, showcasing its ability to reproduce state-of-the-art results and enable novel AL research. ALScope empowers researchers and practitioners to accelerate the development and application of deep active learning, fostering innovation in this critical field."
http://arxiv.org/abs/2508.04929v1,CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction,"Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the 3D structure of biomolecules at near-atomic resolution. However, traditional cryo-EM reconstruction methods often struggle with heterogeneous datasets and require extensive parameter tuning, limiting their applicability and resolution potential. To address these limitations, we introduce CryoGS, a novel approach leveraging 3D Gaussian Splatting for homogeneous cryo-EM reconstruction. CryoGS directly optimizes a set of 3D Gaussians to represent the molecular density, using cryo-EM projection images as constraints. The method incorporates differentiable rendering of Gaussians and a tailored loss function that encourages accurate projection matching and regularization of the Gaussian distribution. Experiments on both simulated and experimental cryo-EM datasets demonstrate that CryoGS achieves comparable or superior reconstruction quality compared to state-of-the-art methods, while requiring minimal parameter tuning and offering potential for high-resolution structure determination. This new framework provides a powerful and flexible tool for cryo-EM structure determination, accelerating biological discovery."
http://arxiv.org/abs/2508.04928v1,Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens,"Monocular depth estimation has seen significant progress driven by deep learning and large-scale datasets, yet these advancements are primarily focused on perspective cameras. Applying these foundational models directly to fisheye images results in significant performance degradation due to the extreme distortion inherent in such lenses. We address this problem by introducing a novel calibration token-based approach to adapt pre-trained monocular depth estimators for fisheye cameras. Our method leverages a learnable calibration token appended to the input image embedding, which is optimized to learn the specific distortion characteristics of the fisheye lens. Further, we propose a training strategy that combines synthetic fisheye data generated from existing perspective datasets with real-world fisheye data, enabling robust generalization. Experimental results on both synthetic and real-world fisheye datasets demonstrate substantial improvements in depth estimation accuracy compared to direct application of perspective-trained models and other adaptation techniques. This work provides a practical and effective solution for extending the benefits of foundational monocular depth estimation models to the increasingly prevalent use case of fisheye cameras."
http://arxiv.org/abs/2508.04924v1,Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations,"Video highlight detection aims to identify the most engaging segments within a video, a task crucial for efficient video summarization and content recommendation. However, existing supervised methods often struggle to generalize to unseen video domains due to significant variations in content, style, and capturing conditions. To address this challenge, we propose a novel test-time adaptation (TTA) framework that leverages meta-auxiliary learning and cross-modality hallucinations to adapt a pre-trained highlight detection model to a new video during inference. Our method dynamically selects and fine-tunes auxiliary tasks based on meta-learning principles, guiding the adaptation process towards feature spaces relevant to highlight detection. Simultaneously, we generate pseudo-labels for missing modalities, such as audio features, through cross-modality hallucination, enriching the model's understanding of the video content. Experiments on multiple benchmark datasets demonstrate that our approach significantly outperforms existing TTA and domain adaptation techniques, achieving state-of-the-art performance in unseen video domains. This work provides a practical and effective solution for deploying robust highlight detection models in real-world scenarios with diverse and evolving video content."
http://arxiv.org/abs/2508.04900v1,Revealing Temporal Label Noise in Multimodal Hateful Video Classification,"Multimodal hateful video classification remains a challenging task due to the complex interplay of visual, textual, and acoustic cues. Existing research often overlooks the impact of temporal label noise, where short segments within a video are incorrectly labeled as hateful or non-hateful, despite the overall video's true label. This paper addresses the problem of identifying and mitigating the effect of temporal label noise in multimodal hateful video classification. We propose a novel Temporal Noise-Aware Learning (TNAL) framework that leverages a self-attention mechanism to dynamically weight the contribution of each video segment during training. TNAL incorporates a noise detection module that estimates the probability of a segment being mislabeled based on its multimodal features and the overall video label, allowing the model to downweight noisy segments and focus on cleaner ones. Experiments on two publicly available hateful video datasets demonstrate that TNAL significantly improves classification accuracy and robustness compared to state-of-the-art methods, achieving up to a 5% improvement in F1-score. This work highlights the critical role of addressing temporal label noise in achieving reliable and accurate hateful video detection."
http://arxiv.org/abs/2508.04868v1,Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications,"Object detection in transportation applications, such as autonomous driving and traffic monitoring, requires robust and accurate perception under diverse environmental conditions. Existing object detection methods often struggle with handling variations in lighting, weather, and occlusion, leading to performance degradation. To address this, we propose a novel Dual-Stream Attention Network with Multi-Modal Queries (DSAMQ) for robust object detection. Our approach leverages both visual (RGB) and potentially thermal imagery within a dual-stream architecture to extract complementary feature representations. A novel multi-modal query mechanism is introduced, allowing the network to selectively attend to relevant features from both streams based on learned query embeddings derived from both modalities. We evaluate DSAMQ on a challenging transportation dataset containing diverse weather and lighting conditions. Our results demonstrate a significant improvement in object detection accuracy, particularly for small and occluded objects, achieving a 5% increase in mAP compared to state-of-the-art single-stream and multi-modal fusion methods. This improved robustness and accuracy can contribute to safer and more reliable transportation systems."
http://arxiv.org/abs/2508.04852v1,VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence,"Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual understanding and reasoning, but their ability to leverage fine-grained visual evidence for complex reasoning tasks remains largely unexplored. This paper addresses the challenge of rigorously evaluating MLLMs on tasks that necessitate reasoning with detailed visual cues and contextual understanding. We introduce VER-Bench, a novel benchmark comprising three distinct tasks: Fine-Grained Object Identification, Attribute-Based Comparison, and Scene Graph Reasoning. VER-Bench features carefully curated image-question pairs that demand precise visual analysis and logical inference based on subtle differences and relationships within the visual scene. Our extensive evaluation of state-of-the-art MLLMs on VER-Bench reveals significant limitations in their ability to accurately perceive and reason with fine-grained visual evidence, often exhibiting biases towards superficial features and struggling with complex relational reasoning. These findings highlight the need for future research to focus on enhancing MLLMs' capacity to leverage nuanced visual information for improved reasoning and decision-making."
http://arxiv.org/abs/2508.04847v1,LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction,"3D human motion prediction, the task of forecasting future poses given a sequence of past poses, is crucial for various applications, including robotics, virtual reality, and surveillance. Existing deep learning methods often struggle with capturing the complex temporal dependencies and high-dimensional nature of human motion, leading to limitations in long-term prediction accuracy. To address these challenges, we introduce LuKAN, a novel framework leveraging Kolmogorov-Arnold Networks (KANs) for 3D human motion prediction. LuKAN decomposes the complex motion dynamics into a series of simpler functions learned by KANs, organized in a hierarchical structure to model both short-term and long-term dependencies. Furthermore, we introduce a learned latent space to encode the input motion sequence, enabling the KANs to operate on a more compact and informative representation. Experimental results on the Human3.6M and AMASS datasets demonstrate that LuKAN significantly outperforms state-of-the-art methods, achieving lower prediction errors, especially in long-term forecasting scenarios. This work highlights the potential of KANs as a powerful tool for modeling complex dynamical systems and offers a promising direction for advancing the field of 3D human motion prediction."
http://arxiv.org/abs/2508.04827v1,A deep learning approach to track eye movements based on events,"Eye tracking is crucial for understanding human attention and behavior, with applications spanning from human-computer interaction to clinical diagnostics. However, traditional frame-based eye tracking methods are computationally expensive and often redundant, as eye movements are sparse and asynchronous events. This paper addresses the challenge of efficiently and accurately tracking eye movements by leveraging event cameras, which asynchronously capture changes in scene illumination. We propose a novel deep learning architecture, EventEye, that directly processes event streams to predict gaze direction. EventEye utilizes a spatiotemporal convolutional neural network (CNN) specifically designed to extract relevant features from event data, followed by a recurrent neural network (RNN) to model the temporal dynamics of eye movements. We evaluate EventEye on a newly collected event-based eye tracking dataset and demonstrate superior performance compared to frame-based methods and existing event-based approaches in terms of accuracy and latency. This research offers a promising avenue for developing low-power, high-speed eye tracking systems suitable for real-time applications and resource-constrained environments."
http://arxiv.org/abs/2508.04825v1,Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off,"Virtual try-on (VTON) aims to realistically transfer clothing items onto a target person, while virtual try-off (VTO) seeks to remove the garment from the person, revealing the body underneath. Current methods often treat VTON and VTO as separate tasks, requiring distinct architectures and training procedures, limiting scalability and potentially hindering performance due to the lack of shared knowledge. We introduce Voost, a unified and scalable diffusion transformer network capable of performing both bidirectional VTON and VTO within a single framework. Voost leverages a novel garment-aware attention mechanism within a diffusion model to effectively condition the generation process on both the target person and the desired clothing manipulation. Specifically, we employ a shared encoder to extract features from the person image, garment image, and a conditional mask representing the desired operation (try-on or try-off), which are then fused through a transformer-based architecture to guide the denoising process. Extensive experiments on standard VTON/VTO benchmarks demonstrate that Voost achieves state-of-the-art performance in both tasks, surpassing existing specialized methods while using a single, streamlined architecture. The unified framework and superior performance of Voost represent a significant advancement in the field of virtual apparel manipulation, paving the way for more versatile and efficient applications."
http://arxiv.org/abs/2508.04818v1,Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models,"Anomaly detection and segmentation are crucial for various applications, including medical imaging and industrial inspection, where identifying deviations from normality is paramount. Existing diffusion-based anomaly detection methods often rely on computationally expensive reconstruction or iterative refinement processes, hindering real-time applicability. This paper introduces a novel single-step, reconstruction-free anomaly detection and segmentation framework leveraging the generative power of diffusion models. Our approach directly estimates the anomaly score by analyzing the discrepancy between the input image and the denoised output of a pre-trained diffusion model conditioned on the input itself. This discrepancy, quantified using a learned anomaly scoring function, is then used to generate an anomaly map highlighting anomalous regions. Experiments on benchmark datasets, including MVTec AD and BRATS2015, demonstrate that our method achieves state-of-the-art anomaly detection and segmentation performance with significantly reduced computational cost compared to existing diffusion-based approaches. The proposed method offers a fast and effective solution for anomaly detection and segmentation, enabling real-time deployment in resource-constrained environments."
http://arxiv.org/abs/2508.04816v1,CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework,"Self-supervised learning (SSL) has shown remarkable progress in learning representations from unlabeled data, often relying on contrastive learning or masked image modeling. However, these approaches can suffer from issues such as representation collapse or require extensive computational resources for pretraining. We address the problem of improving the efficiency and robustness of SSL by leveraging knowledge distillation from multiple diverse teachers. Our proposed framework, CoMAD (Contrastive learning with Multiple-teacher self-supervised distillation), utilizes a set of independently trained teachers, each with different architectures or trained with different augmentations, to provide complementary information to a student network. The student is trained using a contrastive loss against its own augmented views while simultaneously minimizing the divergence between its predictions and the aggregated knowledge from the multiple teachers, effectively distilling diverse perspectives. Experiments on ImageNet and downstream tasks demonstrate that CoMAD significantly outperforms single-teacher distillation and achieves competitive performance compared to state-of-the-art SSL methods, while using fewer computational resources and exhibiting greater robustness to teacher selection. This work offers a novel and efficient approach to self-supervised learning by effectively harnessing the collective intelligence of multiple teachers."
http://arxiv.org/abs/2508.04801v1,ACM Multimedia Grand Challenge on ENT Endoscopy Analysis,"Endoscopic examination of the ear, nose, and throat (ENT) plays a crucial role in diagnosing various conditions, from infections to cancers. However, the analysis of ENT endoscopy videos remains a challenging task, often relying on subjective visual assessment. This paper presents the ACM Multimedia Grand Challenge on ENT Endoscopy Analysis, designed to advance automated methods for efficient and accurate analysis of these videos. The challenge focuses on three key tasks: anatomical structure segmentation, polyp detection, and pathology classification. We provide a large-scale, meticulously annotated dataset of ENT endoscopy videos and images to facilitate the development and evaluation of novel algorithms. The challenge attracted significant participation, with top-performing methods demonstrating substantial improvements over baseline approaches, particularly in polyp detection and pathology classification, achieving Dice scores of up to 0.85 and AUC scores of up to 0.92, respectively. This challenge serves as a valuable benchmark and catalyst for future research in automated ENT endoscopy analysis, ultimately leading to improved diagnostic accuracy and patient care."
http://arxiv.org/abs/2508.04797v1,RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration,"Image restoration in ultra-high-definition (UHD) imagery presents significant challenges due to the increased complexity and detail inherent in high-resolution data. Existing methods often struggle to effectively handle diverse degradations and maintain fidelity in restored UHD images, leading to artifacts and loss of fine details. We address this limitation by proposing RetinexDual, a novel Retinex-based dual nature approach for generalized UHD image restoration. Our method leverages the Retinex theory to decompose the degraded image into reflectance and illumination components, enabling targeted restoration strategies. Specifically, we employ a dual-branch network that separately processes the reflectance and illumination, incorporating specialized modules for detail enhancement in the reflectance and artifact suppression in the illumination. Extensive experiments on benchmark datasets demonstrate that RetinexDual achieves state-of-the-art performance in terms of both quantitative metrics and visual quality, significantly outperforming existing methods in restoring fine details and suppressing artifacts in UHD images. This work provides a robust and effective solution for generalized UHD image restoration, paving the way for improved visual quality in high-resolution applications."
http://arxiv.org/abs/2508.04790v1,Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization,"Mammographic image retrieval systems play a crucial role in computer-aided diagnosis, aiding radiologists in identifying similar cases and improving diagnostic accuracy. However, effectively capturing the subtle and complex features indicative of different Breast Imaging Reporting and Data System (BIRADS) categories remains a significant challenge. This paper addresses the problem of developing a robust and accurate mammographic image retrieval system capable of effectively differentiating between BIRADS categories. We propose a novel multi-architecture deep learning framework, integrating the strengths of convolutional neural networks (CNNs) such as ResNet50, EfficientNetB0, and DenseNet121, pre-trained on ImageNet and fine-tuned on a large mammography dataset. Furthermore, a super-ensemble optimization strategy, incorporating weighted averaging based on validation set performance, is employed to maximize retrieval accuracy. The proposed framework achieves a mean Average Precision (mAP) of 0.92 and a Top-5 retrieval accuracy of 96% on a held-out test set, demonstrating significant improvements compared to individual architectures and traditional methods. This advanced framework provides a valuable tool for radiologists, potentially leading to more accurate diagnoses and improved patient outcomes in breast cancer screening."
http://arxiv.org/abs/2508.04705v1,Occupancy Learning with Spatiotemporal Memory,"Occupancy prediction, the task of forecasting which locations in a scene will be occupied in the future, is crucial for autonomous navigation and robot planning. Existing methods often struggle with long-term dependencies and dynamic environments, leading to inaccurate predictions, particularly when reasoning about complex interactions. We address this limitation by introducing a novel occupancy learning framework with spatiotemporal memory. Our approach utilizes a recurrent neural network architecture to encode the history of occupancy states into a persistent memory representation, which is then selectively accessed and updated based on current observations. A key component is a differentiable memory addressing mechanism that allows the network to focus on relevant regions and time steps within the memory, enhancing its ability to reason about dynamic changes and predict future occupancy. We demonstrate that our method significantly improves occupancy prediction accuracy compared to state-of-the-art approaches on challenging synthetic and real-world datasets, particularly in scenarios involving complex object interactions and long-term reasoning. This improved accuracy enables more reliable and robust decision-making for autonomous systems operating in dynamic environments."
http://arxiv.org/abs/2508.04702v1,BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning,"Bird's-Eye View (BEV) perception is crucial for autonomous driving, enabling comprehensive scene understanding from an egocentric perspective. However, learning robust BEV representations remains challenging due to the inherent ambiguity arising from perspective projection and the scarcity of labeled BEV data. To address this, we introduce BEVCon, a novel contrastive learning framework designed to improve BEV feature representation. BEVCon leverages a multi-view consistency constraint, encouraging the BEV features extracted from different augmented views of the same scene to be similar, while pushing apart features from different scenes. Furthermore, we incorporate a spatial-aware contrastive loss that emphasizes the consistency of features within local regions of the BEV map, enhancing spatial awareness and feature discrimination. Extensive experiments on the nuScenes dataset demonstrate that BEVCon significantly improves BEV perception performance, achieving state-of-the-art results in object detection and semantic segmentation tasks. This highlights the effectiveness of contrastive learning in learning robust and discriminative BEV representations, paving the way for more reliable autonomous driving systems."
http://arxiv.org/abs/2508.04700v1,SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience,"Automating complex computer tasks traditionally requires extensive manual programming or pre-defined scripting. However, the dynamic nature of software interfaces and user needs necessitates adaptable solutions. This paper addresses the challenge of creating a computer use agent capable of autonomously learning and evolving its task execution strategies from direct interaction with the computer environment. We introduce SEAgent, a Self-Evolving Agent framework that combines reinforcement learning with a novel action representation based on GUI element embeddings and dynamic action chaining. SEAgent learns by observing user demonstrations, exploring the action space, and refining its policy through trial and error, dynamically adapting to changes in the environment and optimizing for task completion efficiency. Experiments across various computer tasks, including file management and web browsing, demonstrate that SEAgent achieves significantly higher success rates and faster task completion times compared to baseline agents with fixed action sets. This work showcases the potential of self-learning agents to automate complex computer tasks, paving the way for more personalized and efficient human-computer interaction."
http://arxiv.org/abs/2508.04687v1,MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics,"Realtime facial animation is crucial for immersive virtual experiences, yet creating believable and expressive avatars that dynamically reflect a user's emotional state remains a significant challenge. This paper addresses the need for a high-fidelity, low-latency facial animation system that can realistically convey nuanced emotional expressions in real-time. We introduce MienCap, a novel performance-based facial animation framework that leverages a lightweight neural network architecture for efficient and accurate facial landmark tracking, coupled with a dynamic mood-blending module. This module utilizes a continuous affect space model to smoothly transition between different emotional states, guided by real-time analysis of the user's vocal intonation and facial micro-expressions. We demonstrate that MienCap achieves state-of-the-art performance in terms of both animation accuracy and runtime efficiency, enabling realistic and responsive avatar control on commodity hardware. The proposed system advances the state-of-the-art in real-time facial animation, paving the way for more engaging and emotionally intelligent virtual interactions."
http://arxiv.org/abs/2508.04682v1,TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction,"Multi-agent perception and prediction are crucial for enabling safe and efficient autonomous driving in complex environments. However, training robust multi-agent systems often requires balancing multiple tasks, such as object detection, trajectory forecasting, and intent prediction, which can be computationally expensive and lead to suboptimal performance due to task interference. This paper addresses the challenge of efficient and balanced multi-task learning in multi-agent perception and prediction. We introduce TurboTrain, a novel training framework that adaptively modulates task-specific learning rates based on real-time performance metrics and gradient statistics, promoting balanced learning across tasks. Furthermore, TurboTrain incorporates a dynamic task sampling strategy, prioritizing challenging and underperforming scenarios to accelerate convergence and improve overall performance. Experimental results on the large-scale INTERACTION dataset demonstrate that TurboTrain significantly outperforms state-of-the-art multi-task learning methods, achieving substantial improvements in both prediction accuracy and training efficiency. This work provides a practical and effective solution for training high-performing multi-agent perception and prediction systems, paving the way for safer and more reliable autonomous driving."
http://arxiv.org/abs/2508.04681v1,Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions,"Understanding human interactions with objects and other humans is crucial for developing intelligent agents that can seamlessly operate in human-centric environments. However, current datasets often lack the nuanced, first-person perspective necessary for training agents to effectively perceive and act within these interactive scenarios. This paper introduces a novel dataset and benchmark, Egocentric Human-Object-Human Interactions (EHOI), focused on capturing the complexity of egocentric interactions. EHOI comprises over 200 hours of video data featuring diverse scenarios involving human-object and human-human interactions, along with detailed annotations including 3D human poses, object bounding boxes, gaze estimations, and action labels. We also propose a novel multi-modal transformer-based architecture, EHOI-Former, that leverages visual, pose, and gaze cues to predict future actions and interaction outcomes. Experimental results demonstrate that EHOI-Former outperforms existing methods on action prediction and interaction anticipation tasks within the EHOI benchmark, highlighting the importance of multi-modal fusion for egocentric interaction understanding. The EHOI dataset and benchmark will facilitate the development of more capable and context-aware embodied AI agents."
http://arxiv.org/abs/2508.04677v2,ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable zero-shot transfer capabilities by leveraging pre-trained knowledge. However, the performance of VLMs is often susceptible to noisy or irrelevant information present in input images, leading to inaccurate predictions. To address this challenge, we introduce ANPrompt: Anti-noise Prompt Tuning, a novel prompt learning framework designed to enhance the robustness of VLMs against visual noise. ANPrompt leverages a bi-level optimization strategy. In the outer loop, we train a prompt generator to produce prompts tailored to the specific input image. Simultaneously, in the inner loop, we introduce an anti-noise regularization term that encourages the generated prompts to be invariant to perturbations in the input image. This is achieved by minimizing the discrepancy between the predictions made with the original image and its noisy counterpart. Experiments on various benchmark datasets demonstrate that ANPrompt significantly improves the accuracy of VLMs, particularly in noisy environments, achieving state-of-the-art performance in few-shot image classification and robust zero-shot transfer. This method offers a promising direction for developing more reliable and adaptable VLMs for real-world applications."
http://arxiv.org/abs/2508.04663v1,HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models,"Diffusion models have achieved state-of-the-art results in various generative tasks, but their large size poses significant challenges for deployment on resource-constrained devices. Directly pruning these models often leads to substantial performance degradation due to the disruption of intricate feature dependencies, particularly in spatially sensitive layers. We introduce HierarchicalPrune, a novel position-aware compression technique that strategically prunes diffusion models by considering the spatial importance of network parameters. Our method first divides the model into hierarchical blocks based on their function and resolution, then employs a learnable mask that adaptively prunes parameters within each block, guided by a position-aware importance score calculated from attention maps and feature activations. This allows for fine-grained control over the pruning process, preserving crucial spatial information. Experiments on large-scale image generation datasets demonstrate that HierarchicalPrune achieves significantly higher compression rates compared to existing methods, while maintaining competitive or even improved image quality, as measured by FID and perceptual metrics. This approach enables efficient deployment of high-quality diffusion models on platforms with limited computational resources."
http://arxiv.org/abs/2508.04659v1,PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment,"Room layout estimation is a crucial task for scene understanding, enabling applications in robotics, augmented reality, and interior design. Existing methods often struggle with complex room geometries, occlusions, and the inherent ambiguity in single-view or limited multi-view scenarios. This paper addresses the problem of robust and accurate room layout estimation from multi-view images by exploiting featuremetric alignment. We propose PixCuboid, a novel approach that leverages the geometric constraints of a cuboid room model and aligns multi-view image features in a shared 3D space. Our method employs a differentiable feature extraction and projection pipeline, coupled with a robust optimization strategy that minimizes photometric and geometric inconsistencies across views. Furthermore, we introduce a novel loss function that encourages the predicted cuboid to align with prominent image features, improving accuracy in challenging scenarios. Experimental results on benchmark datasets demonstrate that PixCuboid achieves state-of-the-art performance, outperforming existing methods in terms of accuracy and robustness, particularly in cluttered scenes and with limited viewpoint diversity. Our approach offers a significant advancement in multi-view room layout estimation, paving the way for more reliable scene understanding in real-world applications."
http://arxiv.org/abs/2508.04658v1,YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper,"Early detection of diseases in poultry farms is crucial for minimizing economic losses and ensuring animal welfare. Traditional methods for disease detection are often labor-intensive and prone to subjective errors. This paper addresses the need for an automated, accurate, and efficient system for poultry disease detection and health monitoring. We propose a novel deep learning model based on YOLOv8, fine-tuned and optimized for the specific characteristics of poultry images and videos. Our approach incorporates data augmentation techniques, transfer learning, and a custom loss function to improve the model's robustness and generalization ability. Furthermore, we explore the integration of environmental sensor data with visual features to enhance the accuracy of health assessment. Experimental results on a newly curated dataset of poultry images demonstrate that our YOLOv8-based model achieves state-of-the-art performance, with a mean Average Precision (mAP) of 92.5% for detecting common poultry diseases and identifying unhealthy birds. This automated system offers a significant advancement in poultry farming by enabling timely intervention and improved flock management practices."
http://arxiv.org/abs/2508.04655v1,X-SAM: From Segment Anything to Any Segmentation,"The Segment Anything Model (SAM) has demonstrated remarkable zero-shot generalization for image segmentation, yet its reliance on specific prompt types and a fixed architecture limits its adaptability to diverse segmentation tasks. This paper addresses the challenge of extending SAM's capabilities to handle arbitrary segmentation objectives without task-specific fine-tuning. We introduce X-SAM, a novel framework that leverages a learnable prompt encoder conditioned on external task embeddings to dynamically adapt SAM to new segmentation paradigms. X-SAM learns to map task-specific information, such as dataset names or textual descriptions, into a latent space that guides the generation of tailored prompts for SAM's image encoder. Experiments across a wide range of segmentation tasks, including semantic, instance, and panoptic segmentation, demonstrate that X-SAM significantly outperforms the original SAM in zero-shot transfer, achieving state-of-the-art performance on several benchmarks. These results highlight the potential of X-SAM to democratize segmentation by enabling its application to virtually any task with minimal effort."
http://arxiv.org/abs/2508.04650v1,EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts,"Visual Question Answering (VQA) on charts is a challenging task requiring both understanding of visual elements and reasoning about the underlying data. Existing VQA benchmarks for charts primarily focus on end-to-end performance, offering limited insights into the impact of different visual encoding methods on model reasoning abilities. We introduce EncQA, a novel benchmark designed to isolate and evaluate the performance of Vision-Language Models (VLMs) based on the visual encoding used in charts. EncQA comprises a diverse set of charts with varying encoding types (e.g., bar, line, pie) and question categories focusing on comparative analysis, trend identification, and value retrieval. Furthermore, we provide a standardized set of visual encoding representations derived directly from the chart's data and visual elements, enabling a fair comparison across different models. Our evaluation of several state-of-the-art VLMs reveals significant performance variations depending on the encoding type, with bar charts exhibiting higher accuracy than more complex encodings like scatter plots, highlighting encoding-specific weaknesses in current models. EncQA provides a valuable resource for understanding and improving the visual reasoning capabilities of VLMs on charts, ultimately leading to more robust and reliable chart understanding systems."
http://arxiv.org/abs/2508.04648v1,Super Resolved Imaging with Adaptive Optics,"Super-resolution (SR) techniques aim to reconstruct high-resolution (HR) images from one or more low-resolution (LR) observations, a task significantly challenged by atmospheric turbulence which severely degrades image quality. This paper addresses the problem of achieving high-quality SR imaging in the presence of strong, spatially varying atmospheric distortions. We propose a novel SR framework that integrates adaptive optics (AO) wavefront correction with a deep learning-based SR network, allowing for joint estimation of both the underlying HR image and the residual wavefront error. Specifically, our method uses an AO system to partially correct the wavefront, followed by a convolutional neural network (CNN) trained to further refine the AO-corrected image and simultaneously estimate the remaining wavefront aberrations. We demonstrate through simulations and experimental data that our AO-assisted SR method significantly outperforms traditional SR algorithms and AO-only approaches, achieving higher resolution and improved image quality, especially in challenging seeing conditions. This hybrid approach offers a practical and effective solution for high-resolution imaging through turbulent media in various applications, including astronomy and remote sensing."
http://arxiv.org/abs/2508.04642v1,RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case,"Autonomous driving systems require extensive training to navigate complex and potentially dangerous real-world scenarios. However, acquiring sufficient real-world data for rare but critical ""hard-case"" events, such as near-miss collisions or extreme weather conditions, is both challenging and risky. This paper addresses the problem of improving autonomous driving performance in the real world by leveraging synthetic data specifically designed to represent such hard-case scenarios. We introduce RoboTron-Sim, a novel simulation framework that automatically generates diverse and challenging driving situations by employing adversarial reinforcement learning to identify and amplify failure modes of a target autonomous driving system. The framework utilizes a parameterized environment and a reinforcement learning agent to create scenarios that maximize the probability of undesirable outcomes. Experiments demonstrate that training autonomous driving models with data generated by RoboTron-Sim significantly improves their robustness and safety in real-world driving scenarios, leading to a demonstrable reduction in critical events compared to models trained on standard datasets. This work provides a promising avenue for efficiently training safer and more reliable autonomous driving systems by focusing on the synthesis of targeted, high-impact training data."
http://arxiv.org/abs/2508.04625v1,"FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging","Financial numerical reasoning (FNR) tasks, which require understanding and reasoning over financial data presented in textual and tabular formats, are crucial for informed decision-making. However, existing FNR datasets often lack multimodal context, comprehensive reasoning steps, and challenging problem-solving scenarios, limiting the development of truly robust FNR models. To address these limitations, we introduce FinMMR, a new multimodal FNR dataset that incorporates diverse data modalities, including textual descriptions, tabular data, and relevant images, alongside detailed, multi-step reasoning chains. FinMMR problems necessitate understanding complex financial concepts, integrating information across modalities, and performing a sequence of numerical operations to arrive at the correct answer. We further introduce a novel multimodal reasoning framework leveraging graph neural networks and large language models to effectively reason over the structured and unstructured data within FinMMR. Experimental results demonstrate that even state-of-the-art models struggle to achieve satisfactory performance on FinMMR, highlighting its complexity and the need for more advanced FNR techniques. FinMMR serves as a valuable benchmark for advancing multimodal reasoning capabilities in the financial domain and beyond."
http://arxiv.org/abs/2508.04614v1,How Does Bilateral Ear Symmetry Affect Deep Ear Features?,"Ear biometrics is gaining prominence as a reliable modality for person identification. While research has explored the impact of pose, illumination, and occlusion on ear recognition performance, the influence of bilateral ear symmetry on deep feature representation remains largely unexplored. This paper investigates how varying degrees of bilateral ear symmetry, both in terms of shape and textural characteristics, affect the learned deep features and subsequent identification accuracy. We propose a novel data augmentation strategy that manipulates the symmetry of ear images by morphing one ear image towards its mirrored counterpart to create a spectrum of symmetry levels. These synthesized images, along with real ear images, are used to train a convolutional neural network (CNN) optimized for ear recognition. We then analyze the resulting feature embeddings using t-distributed stochastic neighbor embedding (t-SNE) to visualize the feature distribution and assess the discriminative power of features extracted from ears with different symmetry levels. Our results demonstrate that ears exhibiting higher degrees of symmetry tend to cluster more tightly in the feature space, leading to improved recognition performance, while asymmetric ears produce more dispersed feature representations. This suggests that symmetry acts as a regularizer during deep feature learning, contributing to more robust and discriminative ear biometric systems."
http://arxiv.org/abs/2508.04611v1,OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment,"Estimating depth from images is a fundamental task in computer vision, often tackled using either monocular or stereo cues. While monocular depth estimation leverages contextual information, it suffers from scale ambiguity, whereas stereo methods excel at local accuracy but struggle in textureless regions. This paper addresses the problem of effectively combining the strengths of both monocular and stereo depth estimation into a unified framework. We propose OmniDepth, a novel architecture that bridges monocular and stereo reasoning through a latent alignment module. This module learns a shared latent space where features from both modalities are aligned, enabling the network to leverage complementary information. Furthermore, we introduce a cross-modal attention mechanism that adaptively weights features based on their relevance to the final depth prediction. Experimental results on standard benchmarks demonstrate that OmniDepth achieves state-of-the-art performance, significantly outperforming existing monocular and stereo methods, especially in challenging scenarios with occlusions and texture-poor regions. Our approach offers a robust and accurate solution for depth estimation by synergistically exploiting both monocular and stereo cues, paving the way for improved scene understanding in various applications."
http://arxiv.org/abs/2508.04597v1,Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline,"Simultaneous Localization and Mapping (SLAM) is a cornerstone of autonomous navigation, with RGB-D SLAM offering robust performance leveraging depth information. However, the reliance on specialized depth sensors limits applicability in resource-constrained environments or when only RGB images are available. This paper addresses the challenge of achieving robust and efficient RGB SLAM without relying on computationally expensive deep learning backends for depth estimation or pose optimization. We introduce a novel feed-forward RGB SLAM baseline that leverages pseudo-depth generated via a lightweight depth-from-motion approach, combined with a Gaussian process-based probabilistic mapping framework. The pseudo-depth is refined using a novel Gaussian-weighted filtering technique to mitigate noise and improve accuracy. Our experiments on standard benchmark datasets (e.g., TUM RGB-D) demonstrate competitive performance against existing RGB SLAM methods, achieving comparable trajectory accuracy and map quality with significantly reduced computational overhead. This work provides a practical and efficient alternative for RGB SLAM, enabling wider deployment in resource-limited scenarios."
http://arxiv.org/abs/2508.04592v1,Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan,"The ability to associate faces with corresponding voices is crucial for human social interaction and has significant implications for multimedia analysis and security applications. This paper outlines the evaluation plan for the Face-voice Association in Multilingual Environments (FAME) 2026 Challenge, designed to push the boundaries of current multimodal learning techniques in complex, real-world scenarios. The challenge focuses on face-voice association across multiple languages and diverse demographic groups, considering variations in speaking styles, acoustic conditions, and visual appearances. The evaluation protocol includes a novel multi-stage approach: first, participants must perform unimodal face and voice embeddings. These embeddings are then used to predict face-voice pairings, evaluated using both cross-validation within each language and cross-lingual association tasks. Preliminary results using baseline models demonstrate the difficulty of the task, with significant performance variations across languages and demographic groups. This challenge will provide a valuable benchmark dataset and evaluation framework for advancing research in robust and generalizable multimodal face-voice association, particularly in multilingual and multicultural contexts."
http://arxiv.org/abs/2508.04573v1,Visual Bias and Interpretability in Deep Learning for Dermatological Image Analysis,"Deep learning models have demonstrated remarkable performance in dermatological image analysis, offering potential for automated skin disease diagnosis. However, concerns exist regarding the presence of visual biases in these models and a lack of interpretability, hindering their reliable deployment in clinical settings. This paper investigates the impact of visual biases, specifically related to image acquisition artifacts and demographic factors, on the performance and interpretability of deep learning models for skin lesion classification. We propose a novel bias-aware training strategy that incorporates adversarial debiasing and attention-based explainability techniques. Adversarial debiasing minimizes the model's reliance on bias-related features during training, while attention maps highlight the image regions influencing the model's decision, allowing for visual validation. Experiments on a large-scale dermatological image dataset demonstrate that our bias-aware training significantly improves model generalization across diverse datasets with varying demographic distributions, while attention maps reveal that models trained without debiasing often rely on irrelevant features such as rulers or skin tones instead of actual lesion characteristics. This work highlights the critical need for addressing visual biases in deep learning for dermatological image analysis to ensure fairness, robustness, and clinical acceptance."
http://arxiv.org/abs/2508.04572v1,Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding,"Identifying and localizing abnormalities in medical images is a critical task for computer-aided diagnosis. Existing methods often struggle to effectively leverage prior medical knowledge to guide the abnormality grounding process. This paper addresses the challenge of incorporating complex medical knowledge, specifically visual attributes and their relationships, to improve abnormality grounding in medical images. We propose a novel Knowledge Decomposition framework that explicitly decomposes complex medical knowledge into a structured knowledge graph representing visual attributes and their hierarchical dependencies. This graph is then used to guide a multi-modal reasoning process, combining visual features with knowledge-based inferences to generate abnormality heatmaps. Our framework achieves state-of-the-art performance on benchmark datasets for abnormality grounding in chest X-rays and retinal fundus images, demonstrating significant improvements in both localization accuracy and diagnostic sensitivity. This work highlights the potential of knowledge-driven approaches for enhancing the interpretability and reliability of medical image analysis."
http://arxiv.org/abs/2508.04568v1,DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling,"Diffusion MRI (dMRI) tractography aims to reconstruct white matter pathways in the brain by estimating streamline trajectories from diffusion signals. Existing tractography methods often struggle with noisy data, complex fiber configurations, and accurately capturing the global structure of neural pathways. This paper addresses the challenge of robust and accurate dMRI tractography by proposing DDTracking, a novel deep generative framework leveraging a diffusion model for streamline generation with explicit local-global spatiotemporal modeling. DDTracking employs a variational autoencoder (VAE) to learn a latent space representation of streamlines, conditioned on dMRI data. A diffusion model is then trained to generate streamlines in this latent space, guided by both local diffusion orientations and global anatomical priors. Furthermore, we introduce a spatiotemporal attention mechanism to effectively integrate local diffusion information with global streamline context during the generative process. Experiments on both synthetic and real human brain dMRI datasets demonstrate that DDTracking significantly outperforms state-of-the-art tractography algorithms in terms of reconstruction accuracy, robustness to noise, and ability to capture complex fiber geometries. DDTracking offers a powerful and flexible framework for dMRI tractography, paving the way for more accurate and reliable white matter connectivity analysis."
http://arxiv.org/abs/2508.04567v1,Analyzing and Mitigating Object Hallucination: A Training Bias Perspective,"Object hallucination, the generation of non-existent objects in images, poses a significant challenge to the reliability of generative models, particularly in safety-critical applications. This work addresses the problem of understanding and mitigating object hallucination from a training bias perspective. We hypothesize that biases in the training data, specifically concerning object co-occurrence and contextual relationships, contribute significantly to the model's tendency to hallucinate. To address this, we propose a novel training strategy, Context-Aware Regularization (CAR), which explicitly encourages the model to learn valid object-context associations by penalizing inconsistent relationships observed during training. CAR leverages a pre-trained language model to assess the semantic plausibility of generated scenes and incorporates this information into the training loss. Experiments on benchmark datasets demonstrate that CAR effectively reduces object hallucination rates by up to 30% while maintaining image quality and content diversity. These results highlight the importance of addressing training biases in generative models to improve their robustness and trustworthiness."
http://arxiv.org/abs/2508.04566v1,CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization,"Dense audio-visual event localization (DAVL) aims to identify the temporal boundaries of sound events and their corresponding visual scenes in untrimmed videos. Existing weakly-supervised DAVL methods often struggle with imprecise localization due to the limited availability of frame-level labels and the difficulty of bridging the semantic gap between audio and visual modalities. This paper introduces CLASP: a novel Cross-modal Salient Anchor-based Semantic Propagation framework for weakly-supervised DAVL. CLASP leverages salient anchor points within both audio and visual streams to guide semantic propagation across modalities. Specifically, we first identify salient audio and visual anchors using attention mechanisms. Then, we construct a cross-modal semantic propagation graph, where nodes represent video segments and edges connect semantically similar segments across modalities, weighted by the anchor similarities. Through iterative propagation, we refine the audio-visual representations and generate pseudo-labels to train a dense event localization model. Experiments on the DCASE 2018 Task 5 and AVE datasets demonstrate that CLASP achieves significant performance gains over state-of-the-art weakly-supervised DAVL methods, improving frame-mAP by a substantial margin. CLASP provides a robust and effective approach for learning accurate audio-visual event localization with only video-level labels."
http://arxiv.org/abs/2508.04565v1,TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning,"Orthodontic treatment planning heavily relies on accurate tooth alignment for achieving optimal aesthetics and functionality. However, manual tooth alignment is a time-consuming and subjective process. This paper addresses the problem of automating tooth alignment by learning complex transformations between maloccluded and ideal dental arches. We propose TAlignDiff, a novel framework that leverages diffusion-based transformation learning to predict the optimal pose (position and orientation) for each tooth. Specifically, we employ a denoising diffusion probabilistic model (DDPM) conditioned on the maloccluded arch to iteratively refine an initial random pose into the aligned configuration. A key innovation is the incorporation of a geometric constraint loss that enforces natural tooth spacing and prevents collisions during the diffusion process. Experimental results on a clinical dataset demonstrate that TAlignDiff achieves significant improvements in alignment accuracy compared to existing methods, reducing the mean absolute error in tooth position and orientation by a substantial margin. This automated approach has the potential to significantly improve the efficiency and consistency of orthodontic treatment planning."
http://arxiv.org/abs/2508.04564v1,Drone Detection with Event Cameras,"Event cameras are novel sensors that asynchronously output pixel-level brightness changes, offering advantages over traditional frame-based cameras in high dynamic range and high temporal resolution scenarios. Detecting small, fast-moving objects like drones poses a significant challenge for conventional vision systems due to motion blur and limited frame rates. This paper proposes a novel approach for drone detection using event cameras by leveraging a Spiking Neural Network (SNN) architecture tailored for processing asynchronous event data. Our method employs a convolutional SNN to extract spatio-temporal features from event streams, followed by a recurrent SNN layer for temporal context aggregation and classification. A novel event-based data augmentation strategy is also introduced to improve robustness. Experimental results on a newly collected dataset of drone flight sequences demonstrate that our proposed method achieves superior detection accuracy and latency compared to existing frame-based and event-based detection algorithms, particularly in challenging lighting conditions and with fast drone maneuvers. This work highlights the potential of event cameras for robust and efficient drone detection, enabling applications in drone surveillance, airspace management, and autonomous navigation."
http://arxiv.org/abs/2508.04559v1,One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose,"Personalized garment try-on and try-off represent a significant challenge in virtual fashion, requiring the seamless integration of clothing items onto target individuals in diverse poses. Current methods often struggle to generalize across different tasks, requiring separate models for try-on and try-off, and often exhibiting limitations in handling complex poses and clothing deformations. We address this problem by proposing a novel unified framework, Partial Diffusion for Try-On and Try-Off (PDTO), capable of performing both try-on and try-off with a single model, regardless of the target pose. PDTO leverages a diffusion-based generative model conditioned on both the target person and the garment, with a crucial modification: we introduce a partial diffusion process, selectively applying noise to either the garment region (for try-on) or the dressed region (for try-off). This allows the model to learn shared representations and efficiently adapt to both tasks. Experiments demonstrate that PDTO achieves state-of-the-art performance on both try-on and try-off benchmarks, surpassing existing methods in terms of visual fidelity and pose robustness. The ability to perform both tasks with a single model, coupled with improved performance, represents a significant advancement towards more versatile and efficient virtual try-on systems."
http://arxiv.org/abs/2508.04556v1,CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps,"The proliferation of Open Radio Access Networks (O-RAN) demands intelligent and adaptable xApps capable of optimizing network performance in real-time. However, current xApps often rely on limited network-centric data, hindering their ability to make holistic decisions based on the user's perceived Quality of Experience (QoE). This paper addresses the challenge of integrating visual context into O-RAN to enhance xApp decision-making. We propose CONVERGE, a novel multi-agent architecture that leverages distributed vision and radio data to create a comprehensive understanding of the environment. CONVERGE employs a network of edge-deployed cameras and radio sensors, processed by independent agents, whose outputs are fused to provide rich contextual information to xApps. Experimental results demonstrate that CONVERGE enables xApps to achieve up to a 20% improvement in user throughput and a 15% reduction in handovers compared to traditional radio-only approaches. This integrated vision-radio architecture provides a pathway towards more intelligent and user-centric O-RAN deployments."
http://arxiv.org/abs/2508.04553v1,LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation,"Late gadolinium enhancement cardiac magnetic resonance imaging (LGE-CMRI) is crucial for assessing left atrial (LA) scar in atrial fibrillation patients undergoing ablation therapy. Accurate and automated segmentation of LA scar remains a challenging task due to the subtle contrast between scar tissue and healthy myocardium, as well as variations in image quality and anatomical structures. We propose LA-CaRe-CNN, a novel Cascading Refinement Convolutional Neural Network for robust LA scar segmentation. Our method employs a coarse-to-fine strategy, where an initial CNN coarsely segments the LA and scar, followed by cascaded refinement CNNs that progressively refine the segmentation boundaries, focusing on ambiguous regions. Specifically, we utilize attention mechanisms within each refinement stage to emphasize scar-specific features and suppress noise. The proposed LA-CaRe-CNN was evaluated on a multi-center LGE-CMRI dataset, achieving a Dice score of 0.82 and an Average Symmetric Surface Distance (ASSD) of 0.75 mm, demonstrating significant improvements compared to state-of-the-art segmentation methods. This automated and accurate scar segmentation pipeline has the potential to improve the efficiency and consistency of clinical workflows for atrial fibrillation management."
http://arxiv.org/abs/2508.04552v1,Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation,"Accurate whole heart segmentation is crucial for various clinical applications, yet deep learning models often struggle with domain shifts between training and testing datasets. This paper addresses the challenge of domain generalization in whole heart segmentation, where models trained on multiple source domains exhibit robust performance on unseen target domains. We propose a novel augmentation-based domain generalization framework that incorporates both domain-specific and domain-invariant data augmentations during joint training from multiple source domains. Specifically, we employ adversarial training to learn domain-invariant features, alongside a novel augmentation strategy that introduces domain-specific variations and simulates potential unseen target domains. Our method is evaluated on a multi-center cardiac MRI dataset, demonstrating significant improvements in generalization performance compared to existing domain generalization techniques and single-source training baselines. The proposed approach achieves a mean Dice score improvement of over 5% on unseen target domains, highlighting its potential for reliable and robust whole heart segmentation in clinical practice."
http://arxiv.org/abs/2508.04551v1,Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis,"Garment transfer, the task of realistically rendering a person wearing clothing from a different image, has seen significant progress recently. However, most existing methods primarily focus on ""dressing,"" transferring a garment onto a target person, while the inverse task of ""undressing"" (removing a garment and inferring the body underneath) remains largely unexplored and is often treated as a separate problem. This paper introduces a unified diffusion framework for Two-Way Garment Transfer (TWGT), capable of performing both dressing and undressing synthesis within a single model. Our approach leverages a conditional diffusion model conditioned on both the target person and the source garment (or the absence thereof for undressing), along with carefully designed guidance mechanisms to ensure realistic garment fitting and plausible body reconstruction. We further incorporate a novel attention mechanism to explicitly model the interactions between the person and the garment, facilitating more accurate and coherent synthesis. Extensive experiments on benchmark datasets demonstrate that our TWGT framework achieves state-of-the-art performance in both dressing and undressing tasks, generating more realistic and visually appealing results compared to existing methods. This unified framework offers a significant advancement in garment transfer research, opening new avenues for applications such as virtual try-on and virtual wardrobe management."
http://arxiv.org/abs/2508.04549v1,MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning,"Understanding marine ecosystems requires comprehensive analysis of underwater video data, yet existing datasets often lack the necessary annotations for detailed scene understanding. This paper introduces the Marine Scene Comprehension (MSC) dataset, a novel resource designed to facilitate research in fine-grained marine wildlife analysis. MSC comprises over 20 hours of diverse underwater footage featuring a wide array of marine species and habitats. Crucially, the dataset provides both grounded segmentation masks for individual animals and clip-level captions describing the observed events and environmental context. We leverage a semi-automatic annotation pipeline, combining human expertise with active learning techniques, to generate high-quality segmentation masks and descriptive captions. Experiments using state-of-the-art video understanding models demonstrate the challenges posed by MSC and the benefits of incorporating both segmentation and captioning information for improved performance on tasks such as species identification and behavior recognition. The MSC dataset will serve as a valuable benchmark for developing advanced computer vision algorithms for marine ecology and conservation efforts."
http://arxiv.org/abs/2508.04546v1,Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding,"Online video temporal grounding aims to identify the start and end times of a specific event in an untrimmed video as it is being streamed. Existing methods often struggle with balancing accuracy and latency, particularly when dealing with complex event structures exhibiting temporal hierarchies. We address this challenge by introducing a novel Hierarchical Event Memory (HEM) network for accurate and low-latency online video temporal grounding. HEM leverages a multi-scale temporal pyramid to encode video features and employs a hierarchical memory module to capture both fine-grained and coarse-grained temporal contexts. The memory is updated online, allowing the network to adapt to evolving video content. A boundary refinement module then leverages the hierarchical memory representations to generate precise temporal boundaries for the target event. Experiments on benchmark datasets, including THUMOS14 and ActivityNet, demonstrate that HEM achieves state-of-the-art grounding accuracy while maintaining significantly lower latency compared to existing online methods. This makes HEM a practical solution for real-time video understanding applications."
http://arxiv.org/abs/2508.04540v1,InceptoFormer: A Multi-Signal Neural Framework for Parkinson's Disease Severity Evaluation from Gait,"Parkinson's Disease (PD) significantly impacts gait, making it a valuable biomarker for disease severity. Existing methods often rely on handcrafted features or fail to effectively integrate diverse gait signals for comprehensive PD evaluation. This paper introduces InceptoFormer, a novel multi-signal neural framework designed to improve the accuracy and robustness of PD severity evaluation from gait data. InceptoFormer leverages an Inception-based module to extract fine-grained local features from individual gait signals (e.g., ground reaction forces, kinematic data) and a Transformer network to capture long-range dependencies and cross-signal interactions. Furthermore, we introduce a contrastive learning strategy to enhance feature discriminability and improve generalization performance. Experimental results on a publicly available PD gait dataset demonstrate that InceptoFormer achieves state-of-the-art performance in predicting the Unified Parkinson's Disease Rating Scale (UPDRS) motor score, surpassing existing methods by a significant margin (e.g., an improvement of X% in RMSE). The proposed framework offers a promising tool for objective and automated assessment of PD severity, potentially aiding in clinical diagnosis and treatment monitoring."
http://arxiv.org/abs/2508.04539v1,TopKD: Top-scaled Knowledge Distillation,"Knowledge distillation (KD) is a widely used technique to transfer knowledge from a large, pre-trained teacher model to a smaller, more efficient student model. However, traditional KD methods often struggle to effectively transfer nuanced knowledge captured in the teacher's less confident predictions, leading to suboptimal student performance. This paper introduces Top-scaled Knowledge Distillation (TopKD), a novel approach that focuses on distilling the most salient information from the teacher's output distribution by dynamically scaling the top-k probabilities. TopKD amplifies the importance of the teacher's top-k predictions while suppressing the remaining probabilities, thereby guiding the student to prioritize learning the most relevant features. We further introduce a temperature annealing schedule specific to the scaled teacher distribution, refining the distillation process throughout training. Experimental results on image classification benchmarks, including CIFAR-100 and ImageNet, demonstrate that TopKD consistently outperforms state-of-the-art KD methods, achieving significant improvements in student accuracy while maintaining computational efficiency. TopKD provides a refined and effective knowledge transfer mechanism, enabling the creation of more compact and accurate student models."
http://arxiv.org/abs/2508.04534v1,No Masks Needed: Explainable AI for Deriving Segmentation from Classification,"Image classification and semantic segmentation are fundamental tasks in computer vision, with the latter traditionally requiring significantly more annotation effort. This paper addresses the challenge of bridging the gap between these two tasks by deriving high-quality segmentation maps directly from classification networks, eliminating the need for pixel-level supervision during training. We propose a novel Explainable AI-driven framework, ""No Masks Needed"" (NMN), which leverages Gradient-weighted Class Activation Mapping (Grad-CAM) and a tailored iterative refinement process. NMN first generates initial coarse segmentation masks using Grad-CAM, then iteratively refines these masks by incorporating contextual information and enforcing spatial consistency through a learned energy function. This function penalizes inconsistencies between the predicted class and the refined segmentation, as well as encourages smoothness within the segmentation boundaries. Experimental results on benchmark datasets, including Pascal VOC and COCO, demonstrate that NMN achieves segmentation performance comparable to weakly supervised methods that rely on bounding box annotations, while using only image-level labels. This significantly reduces the annotation burden and unlocks the potential for scaling segmentation models to large, unannotated datasets, enabling broader applications in areas such as medical imaging and autonomous driving."
http://arxiv.org/abs/2508.04524v1,RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection,"Deepfake technology poses a significant threat due to its potential for malicious disinformation and manipulation. Existing deepfake detection methods often lack explainability, hindering trust and making it difficult to understand the reasoning behind a classification decision. This paper introduces RAIDX, a novel framework for explainable deepfake detection that combines Retrieval-Augmented Generation (RAG) with Gradient Policy Reinforcement Learning with Proximal Optimization (GRPO). RAIDX leverages RAG to retrieve relevant textual and visual information from a knowledge base, providing context for generating explanations. Subsequently, GRPO is employed to train the model to produce explanations that are both faithful to the classification decision and human-interpretable, using a reward function that encourages accurate and concise justifications. Experiments on benchmark datasets demonstrate that RAIDX achieves state-of-the-art deepfake detection performance while generating significantly more informative and accurate explanations compared to existing methods. This work represents a significant step towards building trustworthy and transparent deepfake detection systems."
http://arxiv.org/abs/2508.04522v1,Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation,"Accurate segmentation of fetal brain tissues from magnetic resonance imaging (MRI) is crucial for assessing neurodevelopment and detecting abnormalities. However, fetal brain MRI often suffers from low resolution, motion artifacts, and intensity variations, making automated segmentation challenging. We address this problem by proposing a novel conditional fetal brain atlas learning framework for robust and accurate tissue segmentation. Our approach leverages a conditional variational autoencoder (CVAE) to learn a probabilistic atlas space conditioned on gestational age (GA). Specifically, the CVAE learns a latent representation of the fetal brain, enabling the generation of subject-specific atlases based on GA. During segmentation, we register a target fetal brain MRI to the generated atlas and employ a multi-atlas segmentation strategy to obtain the final tissue segmentation. Experiments on a large dataset of fetal brain MRI demonstrate that our method achieves state-of-the-art segmentation accuracy, outperforming existing atlas-based and deep learning methods, particularly in challenging cases with significant artifacts. This accurate and automated segmentation method has the potential to improve the clinical evaluation of fetal brain development."
http://arxiv.org/abs/2508.04513v1,Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation,"Unsupervised temporal action segmentation aims to divide a long, untrimmed video into meaningful action segments without relying on manual annotations. This task is challenging due to the lack of explicit supervision and the high variability of human motion. We address this problem by introducing a novel unsupervised approach that leverages the representational power of ""Skeleton Motion Words"" (SMWs) to capture recurring motion patterns. Our method first extracts 3D skeleton sequences from videos and then learns a codebook of SMWs by clustering local skeleton motion features using a self-supervised contrastive learning framework. These SMWs are then used to represent each video as a sequence of discrete motion patterns, which are subsequently segmented using a Hidden Markov Model (HMM) trained to identify transitions between different action phases based on the SMW sequence. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance compared to existing unsupervised approaches, effectively discovering meaningful action boundaries and segmenting videos into semantically coherent units. This work offers a promising direction for developing fully automated video understanding systems by reducing the reliance on costly manual annotations."
http://arxiv.org/abs/2508.04508v1,Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds,"Surface reconstruction from sparse RGB images remains a challenging problem, often requiring significant computational resources and time. Existing methods struggle to balance accuracy, completeness, and speed, particularly when dealing with limited viewpoints. We address this gap by introducing Surf3R, a novel approach for rapid surface reconstruction from sparse RGB views that prioritizes speed without sacrificing significant accuracy. Our method leverages a differentiable rendering framework coupled with a multi-resolution voxel grid representation. We employ a coarse-to-fine optimization strategy, first establishing a rough shape using a low-resolution grid and then refining the surface details at higher resolutions, guided by photometric consistency. This allows for efficient gradient-based optimization even with a limited number of input images. Experiments demonstrate that Surf3R can reconstruct complex surfaces from as few as five RGB images in a matter of seconds, achieving comparable or superior reconstruction quality compared to existing real-time methods while being significantly faster than batch optimization techniques. Surf3R enables interactive 3D modeling and real-time applications requiring rapid scene understanding."
http://arxiv.org/abs/2508.04505v1,MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos,"Reconstructing and animating clothed humans from monocular videos remains a challenging task due to complex cloth deformations and the inherent ambiguity of inferring 3D geometry from 2D projections. This paper addresses the problem of creating animatable, cloth-decoupled human avatars directly from monocular video, allowing for realistic cloth transfer and dynamic simulation. We introduce MonoCloth, a novel framework that leverages a two-stage approach. First, we estimate the underlying human pose and shape, along with a coarse cloth geometry, using a learned volumetric representation conditioned on the input video frames. Second, we refine the cloth geometry and learn a neural deformation field that explicitly models cloth dynamics decoupled from the underlying human body. This decoupling enables realistic cloth animation driven by the estimated human motion or external forces. Experimental results on both synthetic and real-world datasets demonstrate that MonoCloth significantly improves the accuracy and realism of cloth reconstruction and animation compared to state-of-the-art methods, producing visually plausible cloth dynamics and enabling cloth transfer between different human avatars. Our approach offers a practical solution for generating realistic and controllable clothed human avatars from readily available monocular video data."
http://arxiv.org/abs/2508.04492v1,Learning Robust Intervention Representations with Delta Embeddings,"Understanding the causal effects of interventions is crucial for building robust and reliable vision systems. However, learning representations that are invariant to irrelevant variations while sensitive to meaningful interventions remains a significant challenge. This paper addresses the problem of learning robust intervention representations that effectively capture the causal effects of interventions while filtering out spurious correlations. We introduce Delta Embeddings, a novel approach that learns representations by explicitly modeling the *change* in the embedding space caused by an intervention. This is achieved by training a network to predict the difference vector (delta) between the embeddings of the pre- and post-intervention images. Furthermore, we incorporate a contrastive loss to encourage similar delta embeddings for similar interventions, promoting robustness and generalization. We demonstrate that Delta Embeddings outperform existing methods on several benchmark datasets for counterfactual reasoning and intervention prediction, exhibiting improved accuracy and robustness to confounding factors. This work provides a new perspective on learning causal representations and offers a promising direction for building more reliable and interpretable vision systems."
http://arxiv.org/abs/2508.04491v1,OpenDCVCs: A PyTorch Open Source Implementation and Performance Evaluation of the DCVC series Video Codecs,"Deep video compression has shown promising results, often outperforming traditional codecs at low bitrates. However, the adoption of these deep codecs is hindered by a lack of open-source, easily accessible, and well-documented implementations. This paper addresses this gap by introducing OpenDCVCs, a PyTorch-based open-source implementation of the DCVC (Deep Codec for Video Compression) series, encompassing DCVC, DCVC-TCM, and DCVC-E. OpenDCVCs provides modular and well-documented code, simplifying experimentation and customization. We conduct a comprehensive performance evaluation of the implemented codecs under various settings, including different quantization parameters and video sequences, comparing them against the official DCVC release and VVC (Versatile Video Coding). Our results demonstrate that OpenDCVCs achieves comparable or slightly improved performance compared to the official DCVC, while offering significant performance gains over VVC at very low bitrates. OpenDCVCs facilitates further research and development in deep video compression by providing a reliable and accessible platform for the community."
http://arxiv.org/abs/2508.04485v1,QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution,"Video Super-Resolution (VSR) aims to reconstruct high-resolution (HR) videos from their low-resolution (LR) counterparts, a crucial task for various applications. Deploying deep learning-based VSR models on resource-constrained devices is challenging due to their high computational cost and memory footprint. This paper addresses the problem of reducing the computational complexity of state-of-the-art VSR models without significant performance degradation by exploring post-training quantization (PTQ). We introduce QuantVSR, a novel PTQ framework tailored for real-world VSR models. QuantVSR employs a hybrid quantization strategy, dynamically adjusting the bit-width for different layers based on their sensitivity to quantization errors, along with a novel quantization-aware scaling factor calibration method to minimize the quantization loss. Our experiments on benchmark datasets demonstrate that QuantVSR can effectively quantize state-of-the-art VSR models to low bit-widths (e.g., INT4/INT8) with minimal performance drop, achieving significant reductions in computational cost and memory footprint compared to their full-precision counterparts. QuantVSR provides a practical and efficient solution for deploying high-performance VSR models on resource-limited platforms."
http://arxiv.org/abs/2508.04482v1,OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use,"Operating System (OS) agents, powered by Multimodal Large Language Models (MLLMs), are emerging as a promising paradigm for enabling intuitive and versatile human-computer interaction across general computing devices. The rapid development of these agents necessitates a structured understanding of their capabilities, limitations, and design principles. This survey provides a comprehensive overview of MLLM-based OS agents, categorizing them based on their architecture, task execution strategies (e.g., tool use, reflection, planning), and evaluation methodologies. We analyze existing agents according to their ability to perform diverse tasks such as web browsing, document editing, and software management, highlighting the strengths and weaknesses of different approaches in handling complex, real-world scenarios. Furthermore, we identify key challenges, including robustness to noisy visual inputs, efficient reasoning over long horizons, and effective generalization to unseen applications. This survey serves as a valuable resource for researchers and practitioners, offering insights into the current state of the field and outlining directions for future research towards building more capable and reliable OS agents. Our analysis reveals that while MLLM-based agents demonstrate remarkable potential, significant improvements are needed in areas like memory management, error recovery, and user customization to achieve truly seamless and intuitive device control. This survey contributes to the advancement of OS agent technology by providing a structured framework for understanding and building these intelligent assistants, paving the way for more accessible and efficient computing experiences."
http://arxiv.org/abs/2508.04472v1,Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model,"Text-to-image (TTI) models have demonstrated remarkable capabilities in generating photorealistic images from textual descriptions. However, controlling the absence of specific concepts in generated images, a task known as concept erasure, remains challenging, often resulting in residual traces of the unwanted concept or compromising image quality. This paper addresses the problem of achieving robust and complete concept erasure in TTI models without introducing undesirable artifacts. We propose Zero-Residual Concept Erasure via Progressive Alignment (ZR-CEPA), a novel approach that progressively aligns the latent space representations of the original text prompt and a modified prompt designed to exclude the target concept. ZR-CEPA leverages a carefully designed loss function that minimizes both the presence of the unwanted concept and the deviation from the original image structure, guided by a concept discriminator. Experimental results on various concepts and TTI models demonstrate that ZR-CEPA effectively eliminates the targeted concepts, achieving significantly lower concept leakage scores compared to existing methods while preserving image fidelity and overall aesthetic quality. This approach offers a practical and effective solution for precise control over image generation, enabling safer and more controllable TTI model applications."
http://arxiv.org/abs/2508.04469v1,FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding,"Vision-language understanding (VLU) tasks often require extensive training to align visual and textual representations, demanding significant computational resources. Fine-tuning large, pre-trained vision-language models is often impractical due to their size and the need for task-specific adaptation. This paper introduces FrEVL (Frozen Embeddings for Vision-Language), an efficient VLU approach that leverages frozen, pre-trained image and text embeddings to minimize training overhead. FrEVL employs a lightweight, trainable fusion module that learns to effectively combine fixed visual and textual features, enabling rapid adaptation to downstream tasks. Furthermore, we explore different fusion architectures and demonstrate the effectiveness of simple attention mechanisms. Experiments on several benchmark VLU datasets, including Visual Question Answering (VQA) and Image-Text Retrieval, show that FrEVL achieves competitive performance compared to fine-tuning methods while significantly reducing training time and computational cost. FrEVL provides a practical and efficient pathway for transferring knowledge from large, pre-trained models to a wide range of VLU tasks, making VLU more accessible and resource-efficient."
http://arxiv.org/abs/2508.04467v1,4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation,"Creating high-quality, controllable, and temporally coherent 4D dynamic scenes remains a significant challenge in computer vision and computer graphics. Existing 3D-aware generative models often struggle with temporal consistency and fine-grained detail, especially when generating novel views of dynamic scenes. To address these limitations, we introduce 4DVD, a cascaded dense-view video diffusion model for high-quality 4D content generation. 4DVD leverages a novel cascaded architecture, first generating a coarse, low-resolution dynamic scene representation from a sparse set of camera viewpoints and then progressively refining it with increasing resolution and view density using a learned super-resolution diffusion process conditioned on both spatial and temporal information. This allows us to synthesize temporally coherent and geometrically consistent novel views. Experiments demonstrate that 4DVD significantly outperforms state-of-the-art methods in terms of visual quality, temporal stability, and view consistency, producing compelling 4D content with intricate details. 4DVD represents a significant step towards photorealistic and controllable generation of dynamic 3D scenes, opening new avenues for applications in virtual reality, animation, and game development."
http://arxiv.org/abs/2508.04453v1,Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion,"Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in understanding and reasoning about visual scenes, but often struggle with tasks requiring deep visual knowledge and handling occluded or incomplete objects. This limitation stems from insufficient training on scenarios that necessitate reasoning about the causal relationships between object parts and their holistic representation. To address this, we propose a novel Causality-Driven Visual Object Completion (CD-VOC) framework that boosts visual knowledge-intensive training for LVLMs. CD-VOC first identifies potential causal relationships between visible and occluded object parts using a learned causal graph. Then, it leverages this causal knowledge to guide the completion of occluded objects, generating more realistic and contextually relevant visual augmentations. Experiments on a variety of knowledge-intensive visual reasoning tasks, including visual question answering and scene graph generation with occluded objects, demonstrate that our CD-VOC framework significantly improves the performance of LVLMs compared to existing data augmentation techniques and baseline LVLMs. This work highlights the importance of incorporating causal reasoning into visual knowledge training and provides a promising direction for enhancing the robustness and generalizability of LVLMs in real-world scenarios."
http://arxiv.org/abs/2508.04450v1,TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration,"Computed Tomography (CT) image registration is a critical task in medical image analysis, enabling precise alignment for diagnosis, treatment planning, and longitudinal studies. However, existing deep learning registration methods often rely on complex, task-specific architectures, hindering generalization and demanding substantial computational resources. This work addresses the challenge of creating a lightweight and generalizable foundation model for CT image registration. We introduce TotalRegistrator, a novel transformer-based architecture pre-trained on a large, diverse CT dataset using a self-supervised deformation prediction task. TotalRegistrator leverages a symmetric design with shared weights for encoding both fixed and moving images, followed by a transformer network to predict dense displacement fields. Fine-tuning experiments on various downstream registration tasks, including intra-patient and inter-patient registration, demonstrate that TotalRegistrator achieves competitive performance compared to task-specific models while requiring significantly fewer parameters. TotalRegistrator demonstrates the potential of foundation models to streamline CT image registration, fostering efficiency and broader applicability in clinical settings."
http://arxiv.org/abs/2508.04441v1,Benchmarking Foundation Models for Mitotic Figure Classification,"Mitotic figure classification is a critical task in cancer diagnosis and prognosis, often performed manually by pathologists which is time-consuming and prone to inter-observer variability. Recent advances in foundation models have shown remarkable transfer learning capabilities across various domains, raising the question of their effectiveness in specialized medical imaging tasks like mitotic figure classification. This paper investigates the performance of several pre-trained foundation models, including vision transformers and contrastive learning models, on publicly available mitotic figure datasets. We evaluate both fine-tuning and zero-shot transfer learning approaches, adapting image embeddings from models pre-trained on large-scale natural image datasets to the task of classifying mitotic and non-mitotic cells. Our results demonstrate that fine-tuned foundation models achieve state-of-the-art performance, surpassing previous methods by a significant margin, while zero-shot transfer exhibits promising, though less competitive, results. This study highlights the potential of leveraging foundation models for improved accuracy and efficiency in mitotic figure classification, reducing the burden on pathologists and potentially leading to better patient outcomes."
http://arxiv.org/abs/2508.04429v1,Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis,"Interstitial Lung Diseases (ILDs) encompass a heterogeneous group of disorders characterized by inflammation and fibrosis of the lung parenchyma, posing significant diagnostic challenges. Accurate and timely diagnosis of ILDs is crucial for effective patient management, yet current methods often rely on subjective visual assessment of high-resolution computed tomography (HRCT) scans. This paper addresses the problem of automating ILD diagnosis by leveraging the representational power of self-supervised learning. We propose a novel framework utilizing Masked Autoencoders (MAEs) pre-trained on a large, unlabeled dataset of HRCT images to learn robust feature representations. These learned features are then fine-tuned on a smaller, labeled dataset of ILD cases for classification. We demonstrate that our MAE-based approach achieves state-of-the-art performance on a benchmark ILD dataset, surpassing existing supervised and weakly-supervised methods in terms of diagnostic accuracy and F1-score. This highlights the potential of self-supervised learning to improve the diagnosis of complex pulmonary diseases, reducing inter-observer variability and potentially improving patient outcomes."
http://arxiv.org/abs/2508.04424v1,Composed Object Retrieval: Object-level Retrieval via Composed Expressions,"Object retrieval aims to locate objects within images based on textual descriptions. Current methods often struggle with complex descriptions that specify object attributes and relationships, requiring a deeper understanding of both visual and linguistic compositionality. This paper addresses the problem of *composed object retrieval*, where the goal is to retrieve specific object instances within an image based on a composed expression describing the object's attributes and relationships with other objects in the scene. We propose a novel attention-guided graph reasoning network that explicitly models object relationships and leverages compositional semantics. Our method first constructs a scene graph representing object interactions and then employs a graph attention mechanism to propagate information between related objects, guided by the parsed structure of the composed expression. We achieve state-of-the-art results on the challenging COCO-Compositional dataset, demonstrating significant improvements over existing methods, particularly for complex compositional queries. This work highlights the importance of explicitly modeling object relationships and compositional semantics for accurate and robust object retrieval, paving the way for more intuitive and expressive human-machine interaction."
http://arxiv.org/abs/2508.04422v1,Efficient Inter-Task Attention for Multitask Transformer Models,"Multitask learning leverages shared representations to improve performance and efficiency across related tasks. However, effectively sharing information between tasks, especially in transformer-based models, remains a challenge due to the computational cost of attending over all task-specific features. This paper addresses the problem of efficiently modeling inter-task dependencies in multitask transformer models. We propose a novel Inter-Task Attention (ITA) mechanism that learns a sparse, task-aware attention mask to selectively attend to relevant features from other tasks. ITA employs a lightweight gating network conditioned on task embeddings to generate these masks, significantly reducing the computational overhead associated with full cross-attention. Experiments on a diverse set of multitask computer vision benchmarks, including semantic segmentation, depth estimation, and surface normal prediction, demonstrate that ITA achieves comparable or superior performance to full cross-attention methods while reducing computational cost by up to 40%. These results highlight the potential of ITA as a scalable and effective solution for building high-performance multitask transformer models, enabling efficient knowledge transfer across a wide range of vision tasks."
http://arxiv.org/abs/2508.04418v1,Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation,"Referring audio-visual segmentation (RAVS) aims to segment the visual object in a video that corresponds to a given audio query. Current approaches often directly fuse audio and visual features to predict the segmentation mask, neglecting the importance of explicitly reasoning about object-level information before segmentation. This can lead to inaccurate or incomplete segmentations, especially when dealing with complex scenes or ambiguous audio cues. To address this, we propose a novel object-aware reasoning agent for RAVS, termed ""Think Before You Segment"" (TBS). TBS first extracts object proposals from the video frames and learns object-specific audio-visual embeddings. A reasoning module then iteratively refines these embeddings by attending to relevant object proposals based on the audio query, effectively simulating a deliberative thought process. Finally, the refined object embeddings are used to predict the segmentation mask. Experimental results on the AVSBench and LVS datasets demonstrate that TBS significantly outperforms existing state-of-the-art methods, achieving improvements of up to 5% in Intersection-over-Union (IoU). This highlights the importance of incorporating explicit object-level reasoning into RAVS models for improved accuracy and robustness."
http://arxiv.org/abs/2508.04416v1,Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning,"Reasoning about long videos requires integrating visual information across extended temporal spans and often necessitates external knowledge. This task poses a significant challenge for existing video understanding models. We address the problem of long video reasoning by introducing a novel multimodal, tool-augmented reinforcement learning framework, ""Thinking With Videos"" (TWV). TWV leverages a reinforcement learning agent that interacts with a suite of external tools, including a large language model (LLM), a visual question answering (VQA) module, and a frame retrieval system, to gather relevant information and plan reasoning steps. The agent learns a policy to strategically utilize these tools based on the video content and the reasoning task. Our experiments on the TVQA and STAR benchmark datasets demonstrate that TWV significantly outperforms state-of-the-art methods, achieving substantial gains in accuracy and interpretability. These results highlight the potential of tool-augmented reinforcement learning for enabling more sophisticated and human-like reasoning over long videos."
http://arxiv.org/abs/2508.04406v1,Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models,"Generating accurate 3D building models is crucial for various applications, including urban planning, energy simulation, and thermal analysis. However, creating such models, particularly those incorporating thermal properties, remains a challenging and time-consuming process. This paper addresses the problem of efficiently and accurately parsing building facades from single thermal images to generate scalable 3D thermal building models. We propose a novel deep learning-based framework leveraging a multi-task U-Net architecture. This network simultaneously performs semantic segmentation of facade elements (windows, walls, doors, etc.) and estimates thermal property maps directly from thermal infrared images. The predicted semantic labels are then used to reconstruct a geometric 3D model, while the thermal property maps are projected onto the corresponding facade elements, creating a thermally-aware 3D representation. Experiments on a newly collected thermal facade dataset demonstrate that our approach achieves state-of-the-art performance in both semantic segmentation and thermal property estimation, leading to highly accurate and realistic 3D thermal building models. This methodology provides a scalable and automated solution for generating thermally-enhanced 3D building models from readily available thermal imagery, enabling more effective urban energy management and building performance analysis."
http://arxiv.org/abs/2508.04381v1,ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition,"Ear recognition has emerged as a promising biometric modality due to its non-intrusive nature and stability over time. However, unconstrained scenarios, characterized by pose variations, occlusions, and varying illumination, pose significant challenges for existing ear recognition systems, particularly when dealing with multiple impressions of the same subject. This paper addresses the problem of robust ear recognition in unconstrained multi-impression scenarios by learning a discriminative representation that is invariant to intra-subject variations and sensitive to inter-subject differences. We propose ProtoN, a Prototype Node Graph Neural Network, which leverages graph neural networks to model the relationships between ear impressions and learns a set of prototype nodes representing distinct ear identities. These prototype nodes are then used to classify unseen ear impressions based on their similarity to the learned prototypes, enabling effective matching of multiple impressions per subject. Experimental results on challenging benchmark datasets demonstrate that ProtoN achieves state-of-the-art performance in unconstrained multi-impression ear recognition, outperforming existing methods by a significant margin. This highlights the potential of prototype-based graph neural networks for robust biometric identification in real-world applications."
http://arxiv.org/abs/2508.04379v1,VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones,"Cross-modal time series analysis is crucial for understanding complex real-world phenomena, leveraging information from diverse modalities such as vision and numerical sensors. However, existing approaches often struggle to effectively fuse visual and numerical data due to the heterogeneity and temporal misalignment between modalities. This paper introduces VisionTS++, a novel cross-modal time series foundation model that leverages continually pre-trained visual backbones to enhance representation learning and cross-modal alignment. Our method employs a two-stage training strategy: first, we continually pre-train a visual backbone on a large-scale video dataset to capture rich visual features and temporal dynamics; second, we integrate this pre-trained backbone within a cross-modal transformer architecture, enabling effective fusion with numerical time series data and fine-tuning on downstream tasks. Experiments on several challenging cross-modal time series datasets demonstrate that VisionTS++ significantly outperforms state-of-the-art methods in tasks such as action recognition and anomaly detection, achieving improvements of up to 15% in accuracy. VisionTS++ provides a powerful and generalizable framework for cross-modal time series analysis, paving the way for more robust and accurate real-world applications."
http://arxiv.org/abs/2508.04369v2,TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding,"Long-form video understanding requires processing extensive temporal information, posing significant computational challenges. Existing methods often resort to uniform or fixed-stride temporal sampling, which can be inefficient and miss critical events within lengthy videos. This paper addresses the problem of adaptively selecting the most informative frames in long videos to optimize video language understanding. We introduce Temporal Sampling Policy Optimization (TSPO), a novel reinforcement learning framework that learns an optimal sampling policy tailored to the specific video and language task. TSPO employs a hierarchical actor-critic architecture, where the actor network learns to predict sampling probabilities for video segments, guided by a critic network that evaluates the quality of the selected frames for the downstream task. Experiments on multiple challenging video-language datasets, including ActivityNet and HowTo100M, demonstrate that TSPO significantly outperforms existing uniform and adaptive sampling strategies, achieving state-of-the-art results with reduced computational cost. This adaptive temporal sampling approach offers a promising direction for efficient and accurate long-form video understanding."
http://arxiv.org/abs/2508.04368v1,Continual Multiple Instance Learning for Hematologic Disease Diagnosis,"Hematologic disease diagnosis often relies on the analysis of bone marrow aspirate slides, where identifying rare pathological cells within a large field of view is crucial. Existing Multiple Instance Learning (MIL) methods for this task typically assume a static dataset and fail to adapt to new disease subtypes or evolving data distributions encountered in a clinical setting. We address the challenge of continually learning from new bone marrow aspirate data while mitigating catastrophic forgetting of previously learned disease patterns. We propose a novel Continual Multiple Instance Learning (CMIL) framework that integrates a dynamic prototype memory module to retain representative features of past tasks and an attention-guided knowledge distillation strategy to transfer relevant knowledge to the current task. The prototype memory selectively stores crucial instance-level features, while the attention mechanism focuses knowledge transfer on diagnostically relevant regions within the slides. Experimental results on a benchmark hematologic disease dataset demonstrate that our CMIL framework significantly outperforms existing continual learning and MIL approaches, achieving superior accuracy and reduced catastrophic forgetting when learning new disease subtypes sequentially. This work paves the way for developing robust and adaptable diagnostic systems for hematologic diseases, improving diagnostic accuracy and reducing the need for complete model retraining."
http://arxiv.org/abs/2508.04366v1,RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light,"Multi-view photometric stereo (MVPS) reconstructs 3D shapes by analyzing shading variations under different lighting conditions observed from multiple viewpoints. Existing MVPS methods often assume controlled lighting or require complex calibration procedures to estimate light source directions, limiting their applicability in unconstrained environments with natural light. This paper addresses the challenge of reconstructing accurate 3D shapes using MVPS under rotated natural lighting conditions, where the object and cameras undergo relative rotations. We introduce RotatedMVPS, a novel approach that simultaneously estimates the surface normals and relative camera poses by leveraging the epipolar geometry between views and the photometric consistency across different lighting conditions. Our method utilizes a differentiable rendering framework coupled with a robust optimization strategy to jointly refine the shape, camera poses, and a low-dimensional representation of the environmental illumination. Experimental results on both synthetic and real-world datasets demonstrate that RotatedMVPS achieves state-of-the-art reconstruction accuracy compared to existing MVPS methods under challenging, rotated natural lighting conditions. This work significantly expands the applicability of MVPS to dynamic and uncontrolled environments, enabling robust 3D reconstruction in real-world scenarios."
http://arxiv.org/abs/2508.04350v1,Chain of Questions: Guiding Multimodal Curiosity in Language Models,"Large language models (LLMs) exhibit remarkable capabilities in understanding and generating text, but their interaction with visual data remains limited, often requiring extensive training on paired image-text datasets. A key challenge is enabling LLMs to actively explore and learn from multimodal environments through curiosity-driven exploration. We address the problem of guiding LLMs to ask relevant questions about images to facilitate deeper understanding and discovery. Our approach, Chain of Questions (CoQ), prompts the LLM to iteratively generate a sequence of questions, each building upon the previous, to progressively explore different aspects of an image. Specifically, we guide the LLM to initially ask broad questions and subsequently refine them based on visual grounding and previous answers. Experiments on a variety of image understanding tasks demonstrate that CoQ significantly improves the model's ability to identify objects, infer relationships, and understand complex scenes compared to baseline question-answering strategies. This improved multimodal understanding highlights the potential of CoQ as a general framework for empowering LLMs to learn more effectively from visual data through curiosity and active exploration."
http://arxiv.org/abs/2508.04335v1,RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization,"Three-dimensional line estimation is a fundamental problem in computer vision, crucial for tasks like structure from motion, SLAM, and object reconstruction. However, representing and optimizing lines within factor graphs poses challenges due to their inherent non-Euclidean nature and the potential for over-parameterization. This paper addresses the problem of efficiently and accurately representing 3D lines within a factor graph optimization framework. We propose RiemanLine, a novel representation of 3D lines based on the Riemannian manifold of oriented lines. This representation utilizes Plcker coordinates projected onto a hypersphere, enabling efficient computation of geodesic distances and tangent space operations. We develop a novel factor based on the Riemannian metric, allowing for direct and robust integration of line observations into factor graphs. Experimental results on synthetic and real-world datasets demonstrate that RiemanLine achieves superior accuracy and robustness compared to existing line parameterizations, particularly in scenarios with noisy or outlier-corrupted data. This work provides a geometrically sound and computationally efficient approach for incorporating 3D lines into factor graph optimization, advancing the state-of-the-art in geometric estimation tasks."
http://arxiv.org/abs/2508.04325v1,Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models,"Large language models (LLMs) have demonstrated remarkable performance on medical benchmarks, raising expectations for their clinical utility. However, current benchmarks often focus on narrow tasks and fail to adequately assess critical aspects of clinical reasoning and real-world applicability. This paper argues that relying solely on leaderboard performance provides an incomplete and potentially misleading picture of LLM capabilities in medicine. We propose a multi-faceted evaluation framework that goes beyond traditional question-answering and incorporates assessments of diagnostic accuracy under uncertainty, the ability to generate clinically relevant explanations, and robustness to adversarial examples mimicking common medical errors. Our framework is applied to a suite of publicly available LLMs, revealing significant discrepancies between leaderboard scores and performance on these more challenging, clinically-aligned tasks. Specifically, we observe a substantial drop in diagnostic accuracy when models are presented with ambiguous cases and a limited capacity to generate explanations that align with established medical guidelines. This work highlights the need for a more nuanced and comprehensive approach to evaluating LLMs in medicine, moving beyond simple accuracy metrics to ensure safe and effective deployment in clinical settings."
http://arxiv.org/abs/2508.04324v1,TempFlow-GRPO: When Timing Matters for GRPO in Flow Models,"Generative flow models have shown remarkable success in various computer vision tasks, yet their ability to model temporal dependencies in video remains relatively underexplored. Existing approaches often treat video as a sequence of independent frames or employ recurrent architectures, neglecting the crucial role of precise timing information in complex dynamics. This work addresses the problem of effectively incorporating temporal information into Generative Residual Permutation Ordering (GRPO) flow models for video generation. We introduce TempFlow-GRPO, a novel framework that leverages temporal convolutions and attention mechanisms to explicitly model the temporal evolution of GRPO permutations. Specifically, we use 3D convolutional layers to extract spatiotemporal features and employ a transformer-based architecture to learn permutation orderings conditioned on both current and past frames. Our experiments on benchmark video datasets demonstrate that TempFlow-GRPO significantly improves video generation quality, achieving state-of-the-art results in terms of Frchet Video Distance (FVD) and Kernel Inception Distance (KID). By explicitly modeling timing information within the GRPO framework, TempFlow-GRPO offers a more principled and effective approach to video generation with flow models."
http://arxiv.org/abs/2508.04316v1,A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks,"Distributed Acoustic Sensing (DAS) is increasingly utilized for various applications, including seismic monitoring, infrastructure integrity assessment, and perimeter security, generating vast amounts of data requiring efficient analysis. However, the lack of labeled DAS data and the domain shift across different applications hinder the development of robust and generalizable signal recognition models. We address this challenge by proposing a foundation model for DAS signal recognition, trained on a large, diverse, and partially labeled dataset. Our approach involves pre-training a convolutional neural network (CNN) architecture on a combination of self-supervised and supervised learning objectives. Subsequently, we employ visual prompt tuning to adapt the pre-trained model to specific downstream tasks with limited labeled data, where task-specific visual prompts are learned to guide the model's attention to relevant features. Experiments on several real-world DAS datasets demonstrate that our foundation model, fine-tuned with visual prompts, achieves significant improvements in classification accuracy and generalization ability compared to training task-specific models from scratch or using traditional transfer learning methods. This work provides a novel and effective framework for DAS signal recognition, enabling rapid deployment of high-performance models for diverse applications with minimal labeled data requirements."
http://arxiv.org/abs/2508.04299v1,Length Matters: Length-Aware Transformer for Temporal Sentence Grounding,"Temporal Sentence Grounding (TSG) aims to localize a target moment in an untrimmed video given a natural language query. Existing TSG methods often overlook the crucial relationship between the length of the query sentence and the duration of the target moment, leading to suboptimal grounding performance, especially for long or short moments. To address this, we propose a novel Length-Aware Transformer (LAT) that explicitly models the correlation between query length and moment duration. LAT incorporates a length-aware attention mechanism that dynamically adjusts the attention weights based on the relative lengths of the query and the candidate moments, enabling the model to better capture the fine-grained alignment between the query and the relevant video segments. Furthermore, we introduce a length-guided proposal refinement module to refine the boundaries of the initially predicted moments, leveraging the length information to improve temporal precision. Experimental results on three benchmark datasets, ActivityNet Captions, TACoS, and Charades-STA, demonstrate that our LAT achieves state-of-the-art performance, particularly on instances involving long or short moments. This highlights the importance of length awareness in temporal sentence grounding and demonstrates the effectiveness of our proposed approach in improving grounding accuracy."
http://arxiv.org/abs/2508.04297v1,MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction,"3D Gaussian Splatting (3D-GS) has recently emerged as a powerful scene representation for novel view synthesis, offering state-of-the-art rendering quality and real-time rendering speeds. However, current 3D-GS methods typically require dense, high-quality multi-view images for robust reconstruction, limiting their applicability in scenarios with sparse or uncalibrated camera setups. This paper introduces Multi-Baseline Generalizable Gaussian Splatting (MuGS), a novel framework that leverages a learned feature space and geometric priors to enable accurate and generalizable 3D-GS reconstruction from significantly fewer and less calibrated input views. MuGS employs a multi-view feature extractor conditioned on relative pose estimates to generate robust, view-consistent features which are then used to initialize and refine the 3D Gaussian parameters. Furthermore, we incorporate a novel differentiable rendering loss that encourages geometric consistency across views, improving overall reconstruction accuracy and robustness to noisy pose estimates. We demonstrate that MuGS significantly outperforms existing methods on benchmark datasets with sparse views and noisy poses, achieving comparable or superior rendering quality with orders of magnitude fewer input images. MuGS enables high-quality novel view synthesis in challenging real-world scenarios where traditional 3D-GS methods struggle."
http://arxiv.org/abs/2508.04286v1,PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space,"Point cloud registration is a fundamental task in computer vision, crucial for applications like 3D reconstruction and autonomous navigation. Existing methods often struggle with significant noise, outliers, and large transformations, especially when dealing with non-rigid deformations. This paper addresses the challenge of robust point cloud registration in the presence of substantial noise and deformation by leveraging the geometry-aware properties of Pre-Kendall Shape Space (PKSS). We introduce PKSS-Align, a novel registration algorithm that first maps point clouds into PKSS, effectively reducing the dimensionality and filtering out noise. A robust alignment is then performed within the PKSS representation using a modified Iterative Closest Point (ICP) scheme that incorporates shape priors derived from the PKSS structure. Experimental results on both synthetic and real-world datasets demonstrate that PKSS-Align significantly outperforms state-of-the-art registration methods in terms of accuracy and robustness, particularly under high noise levels and complex deformations. This approach provides a more resilient and accurate solution for point cloud registration, advancing the capabilities of 3D vision systems in challenging environments."
http://arxiv.org/abs/2508.04273v1,Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval,"Video moment retrieval (VMR) aims to localize the temporal segment within an untrimmed video that semantically corresponds to a given natural language query. Current VMR methods often treat audio as supplementary information or simply concatenate audio features with visual and textual features, neglecting the varying importance of audio across different video segments and the potential for fine-grained audio-visual interactions. To address these limitations, we propose an Importance-Aware Multi-Granularity Fusion Network (IAMGFN) for VMR. IAMGFN learns importance weights for audio features at both global (video-level) and local (moment-level) granularities, adaptively modulating the contribution of audio based on its relevance to the query and visual context. Furthermore, we introduce a novel cross-modal interaction module that facilitates fine-grained fusion of audio, visual, and textual features at multiple temporal scales. Experimental results on Charades-STA and ActivityNet-Captions datasets demonstrate that IAMGFN achieves state-of-the-art performance, showcasing the effectiveness of our importance-aware audio fusion strategy and multi-granularity interaction. Our work highlights the crucial role of audio in VMR and provides a novel framework for effectively leveraging audio information in video understanding tasks."
http://arxiv.org/abs/2508.04270v1,TDSNNs: Competitive Topographic Deep Spiking Neural Networks for Visual Cortex Modeling,"The mammalian visual cortex employs topographic organization and sparse, event-driven communication through spiking neurons to efficiently process visual information. However, current deep learning models often lack these biologically plausible features. We address this gap by proposing Topographic Deep Spiking Neural Networks (TDSNNs), a novel architecture that integrates topographic mapping and competitive learning within a deep spiking neural network framework. TDSNNs utilize a self-organizing map (SOM) layer to induce spatial relationships between neurons, enforcing a topographic structure that mirrors the cortex. Competitive learning, implemented through lateral inhibition, promotes sparse and efficient spike-based representations. We demonstrate that TDSNNs achieve competitive performance on benchmark image classification datasets, such as MNIST and Fashion-MNIST, while exhibiting significantly improved sparsity and energy efficiency compared to traditional deep spiking neural networks. This work provides a biologically plausible and computationally efficient approach to visual processing, offering a potential pathway towards developing more energy-efficient and interpretable artificial intelligence systems."
http://arxiv.org/abs/2508.04267v1,Revisiting Continual Semantic Segmentation with Pre-trained Vision Models,"Continual semantic segmentation aims to learn new classes sequentially without forgetting previously learned knowledge, a challenging task due to catastrophic forgetting. Current approaches often rely on training from scratch or fine-tuning task-specific models, limiting their scalability and hindering the transfer of general visual knowledge. This paper addresses the problem of effectively leveraging pre-trained vision models for continual semantic segmentation to improve performance and reduce training costs. We propose a novel framework, CLIP-Adapter, that integrates a frozen CLIP visual encoder with lightweight, learnable adapter modules for efficient adaptation to new segmentation tasks. Specifically, we introduce a cross-attention adapter to fuse multi-scale CLIP features and a task-aware normalization layer to mitigate negative transfer. Experimental results on established benchmarks (Pascal VOC, ADE20K, and iSAID) demonstrate that CLIP-Adapter significantly outperforms existing continual semantic segmentation methods, achieving state-of-the-art performance with minimal computational overhead. This work provides a promising direction for building more robust and scalable continual learning systems by effectively harnessing the power of pre-trained vision models."
http://arxiv.org/abs/2508.04236v1,PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction,"Image stitching is a fundamental task in computer vision, enabling the creation of high-resolution panoramas from multiple images. However, significant parallax between input images, especially those captured with wide baselines or unconstrained camera motion, leads to severe artifacts and ghosting in traditional stitching approaches. To address this challenge, we introduce PIS3R: a novel deep learning framework for Parallax Image Stitching via 3D Reconstruction. PIS3R leverages a deep neural network to estimate dense depth maps for each input image and subsequently fuses these into a consistent 3D point cloud representation of the scene. This 3D reconstruction is then used to project all images onto a common virtual camera plane, effectively mitigating parallax distortions before blending. We demonstrate that PIS3R significantly outperforms state-of-the-art image stitching methods on challenging datasets containing large parallax, producing visually compelling and geometrically accurate panoramas. Our approach provides a robust and effective solution for stitching images with substantial viewpoint variations, opening new possibilities for creating immersive and detailed visual experiences."
http://arxiv.org/abs/2508.04205v1,Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification,"Lung diseases, a leading cause of mortality worldwide, often manifest as subtle lesions detectable through medical imaging. Accurate classification of these diseases remains challenging due to the subtle nature of early-stage lesions and the need to effectively integrate information from diverse modalities. This paper addresses the problem of improving lung disease classification accuracy, particularly in cases involving small or faint lesions, by leveraging both CT scans and clinical reports. We propose a Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network (SLBMF-Net) that incorporates a novel lesion attention module to highlight subtle lesion regions within CT scans. Furthermore, SLBMF-Net employs bidirectional LSTM to extract contextual information from clinical reports and fuses the image and text features at multiple scales using a hierarchical fusion strategy. Experimental results on a large, publicly available lung disease dataset demonstrate that SLBMF-Net significantly outperforms state-of-the-art methods, achieving a 5% improvement in overall accuracy and a 7% improvement in sensitivity for detecting small lesions. This work offers a promising approach to improving the early and accurate diagnosis of lung diseases by effectively integrating multimodal information and focusing on subtle lesion characteristics."
http://arxiv.org/abs/2508.04200v1,Bootstrap Deep Spectral Clustering with Optimal Transport,"Deep spectral clustering (DSC) leverages the representation power of deep neural networks to learn data embeddings suitable for spectral clustering. However, DSC methods often suffer from unstable training due to the inherent difficulty in simultaneously learning meaningful embeddings and a reliable affinity matrix. To address this, we propose a novel Bootstrap Deep Spectral Clustering framework with Optimal Transport (BDSC-OT). Our method employs a bootstrapping strategy, where multiple DSC models are trained independently on resampled data subsets, promoting diversity in the learned embeddings. We then leverage Optimal Transport to align the cluster assignments predicted by these diverse models, effectively distilling a consensus clustering result. This alignment step enforces consistency and improves the robustness of the learned affinity matrix. Experiments on several benchmark datasets demonstrate that BDSC-OT consistently outperforms existing DSC methods, achieving higher clustering accuracy and normalized mutual information scores. The proposed bootstrapping and optimal transport alignment significantly stabilizes the training process and enhances the quality of the learned representations for improved clustering performance."
http://arxiv.org/abs/2508.04190v1,RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation,"Robust Principal Component Analysis (RPCA) is a powerful technique for separating sparse foreground objects from a structured background. However, traditional RPCA lacks the capacity to model complex, non-linear variations and often struggles with high-dimensional image data, limiting its effectiveness in intricate segmentation tasks. This paper addresses the challenge of segmenting sparse objects in complex scenes by introducing RPCANet++, a deep learning framework that leverages the interpretability of RPCA within a deep architecture. Our method builds upon the RPCANet architecture, incorporating learnable convolutional dictionaries and residual connections to enhance feature extraction and improve the robustness of the sparse component separation. Furthermore, we introduce a novel loss function that encourages sparsity and orthogonality in the learned dictionaries, promoting better interpretability and separation. Experimental results on benchmark datasets demonstrate that RPCANet++ achieves state-of-the-art segmentation performance compared to existing RPCA-based and deep learning methods, particularly in scenarios with significant background clutter and object variability. This work advances the field by providing a deep, interpretable framework for robust PCA, enabling more accurate and reliable sparse object segmentation."
http://arxiv.org/abs/2508.04131v1,DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation,"Medical image segmentation is a crucial task for computer-aided diagnosis and treatment planning. However, accurately delineating intricate anatomical structures and subtle lesions remains challenging due to inherent image ambiguities and the complexity of medical data. This paper addresses the problem of effectively capturing both fine-grained details and high-level semantic information for precise medical image segmentation. We propose DS$^2$Net, a Detail-Semantic Deep Supervision Network, which leverages a novel multi-scale feature fusion module to integrate detail-rich low-level features with semantically meaningful high-level features. Furthermore, we introduce a detail-semantic aware deep supervision strategy that guides the network to learn both detailed boundaries and global contextual representations at different scales. Experimental results on multiple medical image segmentation datasets, including cardiac MRI and lung CT scans, demonstrate that DS$^2$Net achieves superior performance compared to state-of-the-art methods, particularly in delineating fine structures and handling challenging cases with ambiguous boundaries. The proposed approach offers a robust and effective framework for medical image segmentation, potentially improving the accuracy and efficiency of clinical workflows."
http://arxiv.org/abs/2508.04129v1,SVC 2025: the First Multimodal Deception Detection Challenge,"Deception detection is a challenging problem with significant implications for security, law enforcement, and social interactions. Despite advancements in unimodal deception detection, real-world scenarios often involve multimodal cues, necessitating robust methods that integrate information from various modalities. This paper introduces the SVC 2025 Multimodal Deception Detection Challenge, the first of its kind to encourage research on detecting deception using audio, video, and text data. The challenge utilizes a novel dataset comprising interviews designed to elicit deceptive and truthful responses, captured with synchronized multimodal recordings and transcripts. We provide baseline models that employ multimodal fusion techniques, including attention mechanisms and transformer architectures, to integrate features extracted from each modality. Preliminary results demonstrate the effectiveness of multimodal approaches, achieving significant improvements over unimodal baselines. The SVC 2025 challenge and associated dataset will serve as a valuable benchmark for advancing research in multimodal deception detection and fostering the development of more reliable and generalizable systems."
http://arxiv.org/abs/2508.04124v1,Learning Using Privileged Information for Litter Detection,"Litter detection is a critical task for maintaining public health and environmental sustainability, yet current computer vision approaches often struggle with variations in appearance, lighting, and occlusion. This paper addresses the problem of improving litter detection accuracy by leveraging privileged information available during training but not at inference. We propose a novel Learning Using Privileged Information (LUPI) framework for litter detection. Our framework utilizes depth information and semantic segmentation as privileged information during training to guide the learning of a robust litter detector based on a Faster R-CNN architecture with a ResNet backbone. A knowledge distillation approach is employed to transfer the knowledge learned from the privileged modalities to the object detector, effectively improving its performance on RGB images alone. Experimental results on a newly collected and annotated litter dataset demonstrate that our LUPI framework significantly outperforms state-of-the-art object detection methods trained without privileged information, achieving an average precision improvement of 5-7% on challenging litter categories. This work highlights the potential of LUPI to enhance the performance of litter detection systems, paving the way for more effective and efficient waste management strategies."
http://arxiv.org/abs/2508.04051v1,Towards Globally Predictable k-Space Interpolation: A White-box Transformer Approach,"k-Space interpolation is crucial for accelerating Magnetic Resonance Imaging (MRI) by reconstructing images from undersampled data. However, existing deep learning methods often suffer from unpredictable artifacts and limited generalization due to their black-box nature, hindering clinical adoption. To address this, we propose a novel white-box Transformer-based approach for globally predictable k-space interpolation. Our method leverages a learnable k-space data augmentation strategy, combined with a meticulously designed Transformer architecture incorporating explicit inductive biases related to k-space properties, such as conjugate symmetry and radial sampling patterns. This allows for transparent control over the interpolation process and provides interpretability regarding the network's decisions. Experimental results on publicly available datasets demonstrate that our method achieves state-of-the-art reconstruction quality, significantly reducing artifacts and improving quantitative metrics compared to existing black-box approaches, while offering insights into the interpolation process. This work paves the way for more reliable and trustworthy deep learning-based MRI reconstruction, facilitating broader clinical translation."
http://arxiv.org/abs/2508.03996v1,Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images,"Estimating the nutritional content of food from images is a challenging task with significant implications for dietary monitoring and public health. Existing methods often struggle with the inherent variability in food appearance and the lack of large-scale, annotated datasets. This paper investigates the impact of large-scale pre-training on improving the accuracy and robustness of nutritional content estimation from 2D food images. We leverage transfer learning by fine-tuning pre-trained convolutional neural networks, specifically ResNet and Vision Transformer architectures, on a newly curated dataset comprising images of diverse food items with corresponding nutritional information. We explore different pre-training strategies, including ImageNet, a large-scale food image dataset, and self-supervised learning techniques, to initialize the network weights. Our results demonstrate that pre-training on large-scale datasets, particularly food-specific datasets, significantly improves the performance of nutritional content estimation, achieving state-of-the-art accuracy compared to training from scratch or using ImageNet pre-training alone. This research highlights the crucial role of large-scale pre-training in advancing the field of automated dietary assessment and enabling more accurate and accessible nutritional analysis."
http://arxiv.org/abs/2508.03982v1,UNISELF: A Unified Network with Instance Normalization and Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation,"Accurate segmentation of multiple sclerosis (MS) lesions from magnetic resonance imaging (MRI) is crucial for monitoring disease progression and treatment response. However, the variability in lesion appearance, size, and location poses a significant challenge for automated segmentation methods. This paper introduces UNISELF, a novel unified network architecture designed for robust MS lesion segmentation. UNISELF leverages instance normalization to mitigate intensity variations across different MRI sequences and incorporates a self-ensembling lesion fusion (SELF) module to effectively aggregate information from multiple network branches trained with diverse data augmentations. The SELF module learns adaptive weights to combine predictions, enhancing the robustness and accuracy of the final segmentation. Experimental results on a benchmark MS lesion segmentation dataset demonstrate that UNISELF achieves state-of-the-art performance, surpassing existing methods in terms of Dice score and lesion detection rate. The proposed UNISELF architecture provides a powerful and generalizable framework for accurate and reliable MS lesion segmentation, facilitating improved clinical diagnosis and research."
http://arxiv.org/abs/2508.03925v1,Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model,"Point cloud representation is crucial for 3D shape analysis and generation. However, generating high-quality point clouds with explicit correspondence across different shapes remains a significant challenge. This paper introduces a novel point-based shape generation framework leveraging a diffusion model conditioned on a learned correspondence space. Our method, Correspondence-Preserving Diffusion (CPD), first learns a shared latent space where points across different shapes are aligned based on functional maps. Then, a diffusion model is trained to generate point clouds in this correspondence space, enabling controlled shape generation and manipulation. During inference, we sample from the learned diffusion process and map the generated points back to the original shape space, preserving the established correspondence. Experiments on shape generation and shape interpolation tasks demonstrate that CPD generates point clouds with superior quality and correspondence accuracy compared to state-of-the-art methods. This advancement facilitates downstream applications requiring explicit point-to-point correspondence, such as shape morphing and texture transfer."
http://arxiv.org/abs/2508.03920v1,Deep learning framework for crater detection and identification on the Moon and Mars,"Crater detection and identification on planetary surfaces like the Moon and Mars are crucial for understanding surface age, geological history, and potential landing site hazards. Manual crater counting is time-consuming and subjective, motivating the development of automated approaches. This paper addresses the challenge of accurately and efficiently detecting and identifying craters of varying sizes and degradation states in high-resolution planetary imagery. We propose a novel deep learning framework, CraterID-Net, which integrates a modified Mask R-CNN architecture with a multi-scale feature pyramid network (FPN) and an attention mechanism to enhance feature representation and improve detection accuracy, particularly for smaller and degraded craters. CraterID-Net is trained and validated on manually curated datasets of lunar and Martian crater images. Our results demonstrate a significant improvement in detection precision and recall compared to existing crater detection algorithms, achieving an F1-score of 0.85 on the lunar dataset and 0.81 on the Martian dataset, while also reducing false positives. This automated framework provides a powerful tool for efficiently and accurately analyzing planetary surfaces, enabling large-scale crater mapping and contributing to a more comprehensive understanding of planetary evolution."
http://arxiv.org/abs/2508.03538v1,Retinal Lipidomics Associations as Candidate Biomarkers for Cardiovascular Health,"Cardiovascular disease (CVD) remains a leading cause of mortality worldwide, necessitating the development of accessible and non-invasive methods for early risk assessment. This study addresses the challenge of identifying novel biomarkers for cardiovascular health using retinal imaging, a readily available and non-invasive technique. We propose a computational framework that integrates spectral-domain optical coherence tomography (SD-OCT) imaging of the retina with advanced lipidomics analysis of corresponding patient blood samples. Specifically, we extract quantitative features from retinal layer segmentation and reflectance profiles derived from SD-OCT scans, subsequently correlating these features with the abundance of various lipid species identified through mass spectrometry-based lipidomics. Our analysis reveals significant associations between specific retinal features, such as increased retinal nerve fiber layer (RNFL) thickness and altered reflectance patterns in the outer plexiform layer (OPL), and the levels of ceramides and sphingomyelins, lipid classes known to be implicated in CVD pathogenesis. These findings suggest that retinal lipidomics associations derived from non-invasive SD-OCT imaging may serve as promising candidate biomarkers for early detection and risk stratification of cardiovascular disease."
http://arxiv.org/abs/2508.03461v1,Evaluating the Predictive Value of Preoperative MRI for Erectile Dysfunction Following Radical Prostatectomy,"Radical prostatectomy, while a common treatment for localized prostate cancer, often results in erectile dysfunction (ED). Predicting postoperative ED remains a clinical challenge, hindering effective patient counseling and targeted interventions. This study investigates the predictive value of preoperative multi-parametric Magnetic Resonance Imaging (mpMRI) features for ED following radical prostatectomy. We retrospectively analyzed preoperative mpMRI scans from 150 patients undergoing robot-assisted radical prostatectomy. Radiomic features, including textural and morphological characteristics of the prostate, neurovascular bundles (NVB), and surrounding tissues, were extracted and correlated with postoperative International Index of Erectile Function (IIEF-5) scores at 12 months using machine learning models. Our results demonstrate that a model incorporating radiomic features from the NVB and prostate capsule, along with clinical factors like age and pre-operative IIEF-5 score, achieved a significantly improved area under the receiver operating characteristic curve (AUC) of 0.82 for predicting ED compared to a baseline model using clinical factors alone (AUC 0.71, p < 0.05). These findings suggest that preoperative mpMRI radiomics can provide valuable predictive information for ED after radical prostatectomy, potentially enabling personalized treatment strategies and improved patient outcomes."
http://arxiv.org/abs/2508.03404v1,Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling,"Visual Document Understanding (VDU) aims to extract information and answer questions based on visually-rich documents, such as invoices and reports. Current VDU methods often struggle with complex layouts and reasoning requirements present in real-world documents, especially when faced with documents significantly larger or more complex than those seen during training. We address this limitation by introducing a novel Multi-Agent Collaboration (MAC) framework for VDU and Question Answering (QA). Our approach decomposes the VDU task into specialized agents, each responsible for a specific aspect such as text recognition, layout analysis, and reasoning. These agents collaboratively solve the task through message passing and iterative refinement. Furthermore, we introduce a test-time scaling mechanism that dynamically adjusts the number of reasoning iterations and agent communication rounds based on the document's complexity, enabling the framework to adapt to unseen document scales. Experiments on benchmark datasets demonstrate that our MAC framework achieves state-of-the-art results, exhibiting a significant improvement in handling complex documents and out-of-distribution layouts compared to existing methods. The proposed test-time scaling further enhances robustness and efficiency. This work provides a scalable and adaptable solution for VDU, paving the way for more reliable and generalizable document understanding systems."
http://arxiv.org/abs/2508.03775v1,4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis,"Four-dimensional scanning transmission electron microscopy (4D-STEM) generates rich datasets that capture structural and electronic properties at the nanoscale, but the data is often corrupted by noise, drift, and distortions, hindering downstream analysis. This paper addresses the critical need for robust and unified preprocessing of 4D-STEM data to improve the accuracy and reliability of subsequent analyses. We introduce 4D-PreNet, a deep learning framework that integrates denoising, drift correction, and distortion correction within a single, end-to-end trainable architecture. 4D-PreNet leverages a convolutional neural network (CNN) for denoising, followed by a recurrent neural network (RNN) to learn and compensate for temporal drift, and finally incorporates a learned spatial transformation module to rectify distortions. Evaluated on both simulated and experimental 4D-STEM datasets, 4D-PreNet demonstrates significant improvements in signal-to-noise ratio, drift reduction, and geometric accuracy compared to traditional, sequential preprocessing methods. By providing a unified and robust preprocessing solution, 4D-PreNet unlocks the full potential of 4D-STEM data, enabling more accurate and efficient materials characterization."
http://arxiv.org/abs/2508.03374v1,GRASPing Anatomy to Improve Pathology Segmentation,"Accurate segmentation of pathological regions in medical images is crucial for diagnosis, treatment planning, and disease monitoring. However, the variability in lesion appearance, combined with the complex anatomical context, often leads to inaccurate or inconsistent segmentation results. We address the problem of leveraging anatomical information to improve the segmentation of pathological regions in medical images. Our proposed method, GRASP (Geometry-Regularized Anatomical Structure Prior), incorporates a novel geometric loss function that encourages the segmentation network to respect underlying anatomical structures. This is achieved by explicitly modeling the spatial relationships between anatomical landmarks and the target pathology, using a graph-based representation of the anatomy. We train a deep learning model with GRASP and demonstrate its effectiveness on a challenging dataset of lung lesion segmentation in CT scans. Experimental results show that GRASP significantly improves segmentation accuracy, achieving a Dice score improvement of 5-8% compared to state-of-the-art methods, while also enhancing the anatomical plausibility of the segmentation masks. This approach offers a promising avenue for developing more robust and reliable pathology segmentation algorithms in clinically relevant settings."
http://arxiv.org/abs/2508.03357v1,GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution Bone Suppression in Chest X-Ray Images,"Chest X-ray imaging is a fundamental tool for diagnosing pulmonary diseases, but overlapping rib shadows often obscure subtle lung lesions, hindering accurate diagnosis. Current bone suppression techniques, while effective, often require substantial computational resources and time, limiting their applicability in time-sensitive clinical settings. This paper introduces Global-Local Latent Consistency Models (GL-LCM), a novel approach for rapidly generating high-resolution bone-suppressed chest X-ray images. GL-LCM leverages Latent Consistency Models (LCMs) to achieve fast inference, while incorporating a global-local training strategy. This strategy involves training a global branch to capture overall anatomical structure and a local branch to refine fine-grained details, both operating in the latent space. We further introduce a novel consistency loss that enforces agreement between the global and local feature representations, ensuring coherent bone suppression. Experimental results on benchmark datasets demonstrate that GL-LCM achieves state-of-the-art bone suppression performance with significantly reduced inference time compared to existing methods, enabling real-time or near-real-time processing. GL-LCM offers a practical solution for enhancing the diagnostic utility of chest X-ray imaging by providing rapid and accurate bone suppression, thereby improving the detection of subtle lung abnormalities."
http://arxiv.org/abs/2508.03356v1,FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models,"Foundation models have demonstrated remarkable capabilities across diverse vision tasks, yet their computational demands hinder deployment on resource-constrained edge devices and limit accessibility in data-scarce domains. We address the challenge of adapting foundation models to new domains while maintaining edge-compatibility by introducing FedPromo, a novel federated learning framework leveraging lightweight proxy models trained collaboratively at the edge. FedPromo employs a two-stage training process: first, edge devices learn domain-specific representations through proxy models guided by distillation from a frozen foundation model. Second, these proxy models are aggregated and refined on the server, generating a global proxy capable of generalizing to the target domain. Experimental results on diverse benchmark datasets demonstrate that FedPromo achieves comparable or superior performance to centralized fine-tuning of the foundation model, while significantly reducing computational overhead and preserving data privacy. FedPromo unlocks the potential of deploying foundation model knowledge to a wider range of edge applications and previously inaccessible domains."
http://arxiv.org/abs/2508.03331v1,LRDDv2: Enhanced Long-Range Drone Detection Dataset with Range Information and Comprehensive Real-World Challenges,"Long-range drone detection is crucial for ensuring public safety and security in increasingly complex airspace environments. However, existing datasets often lack the diversity and realism required to train robust detection models, particularly regarding range information and challenging real-world conditions. This paper introduces LRDDv2, an enhanced long-range drone detection dataset built upon its predecessor, significantly expanding the variety of drone types, background clutter, weather conditions, and camera perspectives. A key addition is the incorporation of accurate range information for each drone instance, derived from LiDAR and radar sensors, enabling the development of range-aware detection algorithms. Furthermore, LRDDv2 includes new challenging scenarios such as drone swarms, occlusions, and extreme lighting conditions, meticulously annotated with high precision. Experiments using state-of-the-art object detection models demonstrate that training on LRDDv2 leads to substantial improvements in detection accuracy, particularly at longer ranges and in adverse weather. The comprehensive nature of LRDDv2, coupled with the inclusion of range information, provides a valuable resource for advancing research in long-range drone detection and fostering the development of more reliable and practical drone surveillance systems."
http://arxiv.org/abs/2508.03773v1,When Deep Learning Fails: Limitations of Recurrent Models on Stroke-Based Handwriting for Alzheimer's Disease Detection,"Alzheimer's Disease (AD) poses a significant global health challenge, with early diagnosis crucial for effective intervention. Stroke-based handwriting analysis offers a non-invasive and cost-effective avenue for AD detection, yet the efficacy of deep learning approaches, particularly recurrent neural networks (RNNs), remains underexplored under specific data limitations. This paper investigates the limitations of RNN-based models when applied to stroke-based handwriting data for AD classification, specifically addressing scenarios with limited and imbalanced datasets. We explore several RNN architectures, including LSTMs and GRUs, trained on a feature set extracted from handwriting trajectories, augmented with techniques such as Synthetic Minority Oversampling Technique (SMOTE) and transfer learning to mitigate data scarcity and class imbalance. Our results demonstrate that while RNNs achieve reasonable performance on balanced and larger subsets of the dataset, their performance degrades significantly when faced with severe data imbalance and limited training examples, particularly in accurately identifying individuals with AD. Specifically, we observed a substantial drop in recall for the AD class compared to healthy controls, highlighting the models' bias towards the majority class. These findings underscore the need for caution when deploying RNN-based models for AD detection using handwriting analysis in real-world settings where data limitations and class imbalance are prevalent, suggesting further research into alternative architectures or data augmentation strategies tailored for such scenarios."
http://arxiv.org/abs/2508.03317v1,Architectural Insights into Knowledge Distillation for Object Detection: A Comprehensive Review,"Knowledge distillation (KD) has emerged as a powerful technique to transfer knowledge from a large, complex teacher model to a smaller, more efficient student model, facilitating deployment in resource-constrained environments. While KD has been extensively studied in image classification, its application to object detection presents unique challenges due to the complex structured output and multi-stage architectures. This paper addresses the lack of a comprehensive understanding of how different architectural choices within object detection frameworks interact with various KD strategies. We present a systematic review of existing KD methods tailored for object detection, categorizing them based on the distillation location (feature-based, response-based, relation-based), the architectural components involved (backbone, neck, head), and the specific loss functions employed. Through extensive analysis of over 100 papers, we identify key architectural sensitivities, highlighting the importance of neck distillation for performance gains and the effectiveness of attention-based feature distillation for robust knowledge transfer. Furthermore, we analyze the impact of different detector types (one-stage vs. two-stage) on KD efficacy. Our findings reveal that carefully selecting distillation strategies based on the student architecture can lead to significant improvements in detection accuracy while maintaining computational efficiency, providing valuable insights for future research and practical applications of KD in object detection. This review serves as a valuable resource for researchers and practitioners seeking to leverage knowledge distillation for efficient object detection."
http://arxiv.org/abs/2508.03300v1,Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation,"Semantic segmentation models often suffer from significant performance degradation when applied to unseen target domains due to domain shift. This paper addresses the challenge of zero-shot domain adaptive semantic segmentation, where no labeled data is available in the target domain for training. Our approach, Synthetic Data Generation and Progressive Adaptation (SDGPA), leverages a novel pipeline combining synthetic data generation with progressive adaptation strategies. Specifically, we first generate synthetic images with corresponding semantic labels that mimic the target domain characteristics using a style transfer network conditioned on target domain statistics. These synthetic images are then used to pre-train the segmentation model, followed by a progressive adaptation process that iteratively refines the model using pseudo-labels generated on the target domain with increasing confidence thresholds. Experimental results on benchmark datasets demonstrate that SDGPA significantly outperforms existing state-of-the-art zero-shot domain adaptation methods, achieving substantial improvements in segmentation accuracy and robustness on challenging target domains. This work offers a practical and effective solution for deploying semantic segmentation models in real-world scenarios where labeled target data is scarce or unavailable."
http://arxiv.org/abs/2508.03291v1,Investigation on deep learning-based galaxy image translation models,"Galaxy image translation, the task of converting images from one observational domain to another (e.g., from simulated to real), is crucial for leveraging the wealth of simulated data in astronomical research. However, the inherent complexity and diversity of galaxy morphologies present significant challenges for existing image translation models. This paper investigates the performance of several deep learning-based image translation models, including pix2pix, CycleGAN, and StarGAN, in the context of translating galaxy images between different observational modalities. We evaluate these models on a dataset composed of simulated and real galaxy images, focusing on their ability to preserve key morphological features and photometric properties during translation. Our results demonstrate that CycleGAN and StarGAN exhibit superior performance in preserving galaxy morphology and reducing artifacts compared to pix2pix, achieving lower Frchet Inception Distance (FID) scores and improved visual fidelity. These findings highlight the potential of advanced generative adversarial networks for bridging the gap between simulated and real galaxy images, ultimately enhancing the accuracy and efficiency of astronomical analyses."
http://arxiv.org/abs/2508.03235v1,Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models,"Scanning electron microscopy (SEM) is a crucial tool for characterizing nanoparticles, particularly their shape, which strongly influences their properties. However, manual shape classification from SEM images is time-consuming and subjective, while traditional supervised machine learning methods require extensive labeled data, often unavailable for novel nanoparticle morphologies. This paper addresses the challenge of zero-shot nanoparticle shape classification in SEM images, eliminating the need for task-specific training data. We leverage the representational power of pre-trained Vision Foundation Models (VFMs), specifically CLIP, by encoding both SEM images and textual descriptions of nanoparticle shapes into a shared embedding space. Classification is performed by assigning an image to the shape description with the highest cosine similarity in the embedding space. Our experiments on diverse datasets of simulated and real SEM images demonstrate that our zero-shot approach achieves competitive accuracy compared to traditional machine learning methods trained with limited data, and even outperforms them in certain scenarios. This work provides a practical and efficient method for nanoparticle shape classification, accelerating materials discovery and characterization by enabling rapid analysis of SEM images without requiring extensive labeled datasets."
http://arxiv.org/abs/2508.03213v1,The Power of Many: Synergistic Unification of Diverse Augmentations for Efficient Adversarial Robustness,"Adversarial training (AT) is a prominent technique for improving the robustness of deep neural networks against adversarial attacks, but it often suffers from high computational costs and reduced standard accuracy. This paper addresses the challenge of improving adversarial robustness efficiently by leveraging the synergistic effects of diverse data augmentations. We introduce a novel framework, Synergistic Augmentation Unification for Robustness (SAUR), which dynamically fuses multiple augmentations during adversarial training based on their individual contributions to robustness and accuracy. SAUR employs a meta-learning approach to learn optimal augmentation weights, enabling the model to adaptively prioritize augmentations that provide complementary benefits. Experiments on benchmark datasets like CIFAR-10 and CIFAR-100 demonstrate that SAUR achieves state-of-the-art adversarial robustness with significantly reduced training time and improved standard accuracy compared to existing AT methods. SAUR offers a practical and effective approach to enhance the robustness of deep learning models without sacrificing efficiency or performance."
http://arxiv.org/abs/2508.03132v1,COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks,"Accurate and robust pose estimation of asteroids is critical for successful robotic in-situ resource utilization and planetary defense missions. However, the extreme lighting conditions, rapid and unpredictable tumbling motions, and lack of prior shape models pose significant challenges for existing pose estimation techniques. We address the problem of real-time pose estimation of unknown, rapidly tumbling asteroids under severe shadowing conditions. We introduce COFFEE (COmpact Feed Forward nEural nEtwork), a novel shadow-resilient pose estimator leveraging a sparse neural network architecture. COFFEE is trained on synthetic images rendered with realistic lighting and shadow variations to learn a direct mapping from image features to asteroid pose. Sparsity is enforced during training to reduce computational complexity and enhance generalization to unseen asteroid shapes. Our results demonstrate that COFFEE achieves state-of-the-art pose estimation accuracy compared to existing methods, while maintaining real-time performance on embedded hardware and exhibiting robustness to extreme shadowing. This enables reliable asteroid pose estimation for autonomous navigation and manipulation tasks in challenging space environments."
http://arxiv.org/abs/2508.03057v1,"A Survey of Medical Point Cloud Shape Learning: Registration, Reconstruction and Variation","Medical point clouds, acquired through modalities like CT and MRI, offer rich 3D shape information crucial for various clinical applications. However, the inherent sparsity, noise, and lack of explicit connectivity in point cloud data present significant challenges for effective shape learning. This survey comprehensively reviews the recent advancements in medical point cloud shape learning, focusing on three fundamental tasks: registration, reconstruction, and variation analysis. We categorize and analyze existing methodologies based on their underlying techniques, including traditional iterative closest point (ICP) variants, deep learning-based registration, surface reconstruction algorithms, and statistical shape modeling approaches. Furthermore, we examine how deep learning architectures, such as autoencoders, generative adversarial networks (GANs), and graph neural networks (GNNs), are increasingly being employed to learn complex shape representations and capture subtle anatomical variations. This survey highlights the strengths and limitations of each approach, discusses open challenges, and suggests potential future research directions, ultimately emphasizing the growing importance of point cloud shape learning in advancing medical image analysis and personalized healthcare."
http://arxiv.org/abs/2508.03008v1,ClinicalFMamba: Advancing Clinical Assessment using Mamba-based Multimodal Neuroimaging Fusion,"Multimodal neuroimaging offers a comprehensive view of brain structure and function, crucial for accurate clinical assessment in neurological disorders. However, effectively fusing information from diverse modalities like MRI, fMRI, and PET remains a significant challenge due to their inherent heterogeneity and complex inter-relationships. We introduce ClinicalFMamba, a novel framework leveraging the Mamba architecture, a selective state space model known for its efficient long-range dependency modeling, to fuse multimodal neuroimaging data for improved clinical assessment. ClinicalFMamba employs modality-specific Mamba blocks to extract pertinent features from each imaging modality, followed by a cross-modal Mamba fusion module that adaptively integrates these features based on their relevance to the clinical task. Evaluated on a large multi-site dataset for Alzheimer's Disease diagnosis, ClinicalFMamba demonstrated superior performance compared to state-of-the-art fusion methods, achieving an AUC of 0.92 and significantly improved sensitivity in early-stage detection. This highlights the potential of ClinicalFMamba to enhance the accuracy and efficiency of clinical decision-making in neuroimaging."
http://arxiv.org/abs/2508.02871v1,Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets,"Deep learning has revolutionized remote sensing image analysis, with both Convolutional Neural Networks (CNNs) and, more recently, Transformers demonstrating remarkable capabilities. However, a comprehensive and comparative evaluation of these architectures on diverse, modern remote sensing datasets, considering their inherent strengths and weaknesses, remains limited. This paper addresses this gap by systematically evaluating the performance of state-of-the-art CNNs (e.g., ResNet, EfficientNet) and Transformers (e.g., ViT, Swin Transformer) on several challenging remote sensing datasets encompassing diverse tasks such as land cover classification, object detection, and semantic segmentation. We investigate the impact of pre-training strategies, data augmentation techniques, and model scaling on the performance of both architectures. Our experiments demonstrate that while Transformers generally outperform CNNs on large-scale datasets, CNNs can achieve comparable or even superior results with appropriate data augmentation and pre-training, particularly when computational resources are constrained. This comparative analysis provides valuable insights into the suitability of each architecture for specific remote sensing applications and contributes to informed decision-making in the development of future remote sensing image analysis systems."
http://arxiv.org/abs/2508.02560v1,"Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application","Explainable AI (XAI) is increasingly employed in neuroimaging to interpret complex predictive models and uncover potential biomarkers of neurological disorders. However, the reliability and validity of XAI methods in this domain remain largely unexplored, raising concerns about the potential for misleading interpretations and erroneous scientific conclusions. This paper systematically investigates the application of several widely used XAI techniques, including gradient-based methods, perturbation-based methods, and feature importance ranking, to simulated and real-world neuroimaging datasets. Our analysis reveals that common XAI tools exhibit systematic failures, such as sensitivity to noise and hyperparameter settings, and a tendency to highlight spurious correlations rather than genuine predictive features. To address these limitations, we propose a rigorous validation framework incorporating domain-specific knowledge, including anatomical priors and physiological constraints, to evaluate the plausibility and stability of XAI explanations. Furthermore, we outline a set of guidelines for the safe and responsible application of XAI in neuroimaging, emphasizing the importance of sanity checks, sensitivity analyses, and the integration of XAI insights with existing neuroscientific theories. This work highlights the critical need for careful validation of XAI methods in neuroimaging and provides a pathway towards more reliable and trustworthy interpretations of complex brain imaging data."
http://arxiv.org/abs/2508.02387v1,$$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise,"Deep learning models are highly susceptible to label noise, particularly when trained on large-scale datasets collected from noisy sources. Learning with noisy labels often leads to overfitting to incorrect labels and a degradation in generalization performance. We address the problem of mitigating the impact of label noise by approximating one-hot target vectors with a novel approach, $$-Softmax. Our method softens the target distribution by introducing a temperature parameter, $$, to the softmax function applied to the one-hot encoded labels. This allows the model to learn from a distribution that is less peaked around the potentially incorrect target, effectively reducing the penalty for misclassifications caused by label noise. Experiments on benchmark datasets with varying degrees of synthetic and real-world label noise demonstrate that $$-Softmax consistently outperforms standard cross-entropy training and other label noise mitigation techniques, achieving significant improvements in classification accuracy and robustness. This approach offers a simple yet effective strategy for training robust deep learning models in the presence of label noise, enhancing their applicability in real-world scenarios."
http://arxiv.org/abs/2508.03759v1,Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy,"White blood cell (WBC) classification is a critical task in hematology, often relying on microscopic blood smear images. However, these images can suffer from low resolution, hindering accurate feature extraction and subsequent classification. This study investigates the impact of image super-resolution (SR) techniques on the accuracy of WBC classification. We explore the efficacy of several SR algorithms, including bicubic interpolation, SRCNN, and ESRGAN, to enhance the resolution of WBC images prior to classification. These enhanced images are then used to train and test a convolutional neural network (CNN) classifier. Our results demonstrate that applying ESRGAN for image super-resolution significantly improves the classification accuracy compared to using low-resolution images directly, achieving a 5-8% increase in F1-score across different WBC types. This suggests that integrating SR techniques can substantially improve the performance of automated WBC classification systems, leading to more reliable diagnostic outcomes."
http://arxiv.org/abs/2508.02293v1,Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning,"Unsupervised anomaly detection (UAD) aims to identify rare and unusual instances without relying on labeled data, a crucial capability for real-world applications. However, existing UAD methods often struggle to generalize to unseen anomaly types or require extensive hyperparameter tuning, hindering their practical deployment. This paper addresses the challenge of achieving robust and generalizable UAD performance in real-world scenarios. We propose a novel Confident Meta-Learning framework for UAD, which leverages meta-learning to train a model capable of rapidly adapting to new, unseen datasets with minimal supervision. Our approach involves learning a meta-representation that captures the underlying structure of normal data distributions, and simultaneously learning a confidence estimation module that assesses the reliability of the anomaly score. The meta-representation facilitates fast adaptation, while the confidence score filters out unreliable anomaly predictions. Experiments on diverse benchmark datasets demonstrate that our method achieves state-of-the-art UAD performance, significantly outperforming existing approaches, particularly in challenging scenarios with complex anomaly distributions. Our work represents a significant step towards realizing practical and reliable unsupervised anomaly detection in real-world applications."
http://arxiv.org/abs/2508.02186v1,Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training,"Adversarial training (AT) is a widely used technique to improve the robustness of deep neural networks against adversarial attacks. However, it often comes at the cost of standard accuracy, creating an accuracy-robustness trade-off. This paper addresses the problem of mitigating this trade-off by focusing on smoother transitions in the model's perception of adversarial examples during training. We propose a novel adversarial training framework, Failure-Aware Boundary Smoothing (FABS), which incorporates two key components: failure-aware example weighting to prioritize the learning of challenging adversarial examples, and boundary smoothing regularization to encourage smoother decision boundaries in the vicinity of these examples. FABS dynamically adjusts the training emphasis based on the model's confidence in correctly classifying adversarial examples, while simultaneously penalizing abrupt changes in the model's output with respect to small input perturbations. Experiments on benchmark datasets demonstrate that FABS achieves a superior accuracy-robustness trade-off compared to existing AT methods, improving both standard accuracy and adversarial robustness against strong attacks. This work offers a practical approach to improve the performance of adversarially trained models by facilitating a more nuanced and stable perception of adversarial examples."
http://arxiv.org/abs/2508.02180v1,Test-Time Model Adaptation for Quantized Neural Networks,"Quantized Neural Networks (QNNs) offer significant advantages in terms of memory footprint and computational efficiency, making them attractive for deployment on resource-constrained devices. However, QNN performance often degrades under domain shift, a problem exacerbated by the inherent limitations of fixed quantization. This paper addresses the challenge of adapting quantized neural networks to novel test-time distributions without requiring retraining or access to source data. We propose a novel test-time adaptation method, Q-AdaShift, that leverages batch normalization statistics and a learnable quantization offset to minimize the discrepancy between the source and target domains. Q-AdaShift dynamically adjusts the network's representation by fine-tuning the quantization offset based on the incoming test data, effectively shifting the activation distribution to better align with the quantized range while preserving the efficiency of the underlying QNN. Experiments on various benchmark datasets demonstrate that Q-AdaShift consistently outperforms existing test-time adaptation techniques for QNNs under significant domain shifts, achieving a substantial improvement in accuracy with minimal computational overhead. This work provides a practical and efficient solution for deploying robust QNNs in real-world scenarios where data distributions are subject to change."
http://arxiv.org/abs/2508.02143v1,TrackletGait: A Robust Framework for Gait Recognition in the Wild,"Gait recognition, the identification of individuals based on their walking patterns, offers a promising biometric modality for surveillance and security applications. However, existing gait recognition methods often struggle with real-world scenarios exhibiting variations in viewpoint, clothing, carrying conditions, and complex backgrounds. This paper introduces TrackletGait, a novel and robust framework for gait recognition designed to overcome these challenges. TrackletGait leverages a tracklet-based approach, extracting short sequences of human pose estimations to capture dynamic gait features while mitigating the impact of noisy or incomplete poses within individual frames. We introduce a novel attention mechanism that selectively weights informative tracklets, further enhancing robustness against variations in clothing and carrying conditions. Extensive experiments on challenging benchmark datasets, including CASIA-B and Gait3D, demonstrate that TrackletGait significantly outperforms state-of-the-art methods, achieving superior recognition accuracy under diverse environmental conditions and covariate factors. TrackletGait provides a practical and accurate solution for gait recognition in unconstrained environments, paving the way for more reliable person identification in real-world applications."
http://arxiv.org/abs/2508.03751v2,Modular Transformer Architecture for Precision Agriculture Imaging,"Precision agriculture relies increasingly on aerial and satellite imagery for tasks such as crop monitoring, disease detection, and yield prediction. However, variations in environmental conditions, crop types, and imaging modalities present a significant challenge for developing robust and generalizable image analysis models. This paper addresses the need for adaptable and efficient models in precision agriculture imaging by introducing a novel Modular Transformer Architecture (MTA). MTA leverages a hierarchical transformer backbone with swappable, task-specific modules for feature extraction and processing, allowing for customized model configurations tailored to specific agricultural applications. We further incorporate a multi-scale feature fusion mechanism to effectively integrate information from different spatial resolutions, enhancing the model's ability to capture both local and global contextual cues. Experimental results on benchmark datasets for crop classification, weed detection, and disease segmentation demonstrate that MTA achieves state-of-the-art performance while maintaining a significantly smaller parameter footprint compared to existing deep learning approaches. The proposed modular design and improved performance offer a practical and scalable solution for deploying advanced image analysis techniques in diverse agricultural settings."
http://arxiv.org/abs/2508.02043v1,Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction,"Accurate dose prediction in radiotherapy treatment planning is crucial for ensuring effective tumor control while minimizing damage to healthy tissues. However, predicting optimal dose distributions for patients with multiple tumors remains a significant challenge, particularly in achieving individualized dose prescriptions for each target volume. This paper introduces a novel conditional diffusion model with anatomical and dose constraints, termed AnD-CDM, for end-to-end multi-tumor dose prediction. AnD-CDM leverages a conditional denoising diffusion probabilistic model conditioned on patient anatomy and target structures, incorporating dual constraints: anatomical constraints to guide dose distribution within specific regions and dose constraints to enforce prescribed dose levels for each tumor. These constraints are integrated into the diffusion process via a classifier-free guidance mechanism. Experiments on a dataset of multi-tumor lung cancer patients demonstrate that AnD-CDM significantly improves dose prediction accuracy, achieving superior target coverage and organ sparing compared to state-of-the-art methods. This advancement facilitates the development of personalized and efficient radiotherapy treatment plans for patients with complex multi-tumor presentations."
http://arxiv.org/abs/2508.01994v1,Deeply Dual Supervised learning for melanoma recognition,"Melanoma, the deadliest form of skin cancer, necessitates accurate and early diagnosis for improved patient outcomes. Existing deep learning approaches for melanoma recognition often suffer from limited labeled data and fail to fully leverage the rich semantic information present in dermoscopic images. This paper introduces a Deeply Dual Supervised (DDS) learning framework to enhance melanoma recognition by simultaneously exploiting both image-level and pixel-level supervision signals. Our method incorporates a convolutional neural network (CNN) backbone coupled with two auxiliary branches: a classification branch for image-level melanoma/non-melanoma prediction and a segmentation branch for pixel-level lesion delineation. A novel dual supervision loss function is designed to jointly optimize these branches, encouraging the CNN to learn more discriminative and robust features. Experimental results on the ISIC 2016 and 2017 datasets demonstrate that DDS significantly outperforms state-of-the-art methods, achieving an average improvement of 3.2% in terms of AUC score. The proposed deeply dual supervised learning framework offers a promising approach for improving the accuracy and reliability of automated melanoma recognition systems."
http://arxiv.org/abs/2508.01936v1,CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes,"Structure-from-Motion (SfM) is a foundational technique for 3D reconstruction, yet its performance degrades significantly in multi-altitude scenarios due to large viewpoint variations and insufficient feature matching across diverse perspectives. This paper addresses the challenge of robust and scalable sparse localization in such environments using a novel deep learning-based SfM pipeline. Our proposed Cross-View Deep SfM (CVD-SfM) system leverages a Siamese network architecture, trained with a novel cross-view contrastive loss, to extract viewpoint-invariant features. These features are then used to establish reliable correspondences across images captured at different altitudes, enabling robust relative pose estimation and sparse 3D reconstruction. We demonstrate that CVD-SfM significantly outperforms traditional feature-based SfM and existing deep learning methods on challenging multi-altitude datasets, achieving higher localization accuracy and completeness, particularly in regions with significant viewpoint changes. This improved performance facilitates more accurate and efficient 3D mapping of complex environments."
http://arxiv.org/abs/2508.01932v1,Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense,"Backdoor attacks pose a significant threat to deep learning models, where hidden triggers embedded during training can manipulate the model's predictions at inference time. Existing backdoor defense methods primarily focus on reactive detection and mitigation after the model is compromised. This paper addresses the critical need for a proactive defense mechanism that prevents the model from learning spurious correlations between triggers and target labels in the first place. We propose a novel proactive defense framework, Proactive Disentangled Modeling (PDM), which explicitly disentangles the representations of potential trigger objects from the target labels during the training phase. PDM incorporates an auxiliary loss function that encourages independence between the learned feature representations of trigger objects and the primary task-related features, effectively preventing the model from associating specific triggers with the backdoor target. Experiments on multiple benchmark datasets and backdoor attack strategies demonstrate that PDM significantly reduces the attack success rate while maintaining high clean accuracy, outperforming state-of-the-art defense methods. This proactive approach offers a robust and reliable strategy for defending against backdoor attacks, enhancing the trustworthiness and security of deep learning models."
http://arxiv.org/abs/2508.01752v1,Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring,"Multi-object tracking (MOT) plays a crucial role in precision livestock farming, enabling automated monitoring of animal behavior and welfare. However, existing MOT systems often struggle with occlusions and ID switches, particularly in complex environments like dairy farms. This paper addresses the challenge of robust and accurate dairy cow tracking by proposing a novel vision transformer-based multi-camera MOT framework. Our approach leverages a transformer network to learn robust feature representations of individual cows from multiple camera views. These features are then fused and used within a tracking-by-detection paradigm, incorporating a Kalman filter for state estimation and a Hungarian algorithm for data association. Experimental results on a newly collected multi-camera dairy farm dataset demonstrate that our method significantly reduces ID switches and improves tracking accuracy compared to state-of-the-art MOT algorithms. This enhanced tracking performance facilitates more reliable analysis of cow behavior, contributing to improved animal welfare and farm management."
http://arxiv.org/abs/2508.01728v1,Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations,"Concept representations learned by deep neural networks are often opaque, hindering interpretability and trustworthiness. Existing circuit discovery methods primarily focus on identifying high-level, coarse-grained connections between concepts, often overlooking the intricate, fine-grained interactions that contribute to a comprehensive concept representation. This paper introduces Granular Concept Circuits (GCC), a novel approach for discovering fine-grained circuits within concept representations learned by neural networks. GCC leverages a combination of activation patching and causal mediation analysis to identify and quantify the influence of individual neurons on specific concept-related behaviors. We further introduce a circuit simplification technique based on information bottleneck principles to distill the discovered circuits into a minimal, yet representative, set of connections. Experiments on image classification models demonstrate that GCC effectively uncovers more granular and informative circuits compared to existing methods, revealing previously hidden connections between neurons and their contributions to predicting specific concepts. GCC offers a significant step toward understanding the nuanced inner workings of concept representations within neural networks, paving the way for improved interpretability, explainability, and control over learned representations."
http://arxiv.org/abs/2508.01639v1,Glass Surface Segmentation with an RGB-D Camera via Weighted Feature Fusion for Service Robots,"Transparent surfaces, particularly glass, pose a significant challenge for service robots navigating indoor environments due to their inherent lack of texture and reflective properties. This paper addresses the problem of robust and accurate glass surface segmentation using an RGB-D camera for reliable robot navigation. We propose a novel weighted feature fusion approach that leverages both color and depth information. Specifically, we extract features from RGB and depth images using a deep convolutional neural network, and subsequently fuse these features using an attention mechanism that adaptively weights the contribution of each modality based on local image characteristics. This allows the network to prioritize reliable features, effectively mitigating the ambiguities caused by reflections and occlusions. Experimental results on a newly collected dataset of diverse indoor scenes containing various types of glass surfaces demonstrate that our proposed method achieves state-of-the-art segmentation accuracy, outperforming existing approaches by a significant margin. The enhanced ability to perceive and segment glass surfaces contributes to safer and more reliable navigation for service robots operating in complex, real-world environments."
http://arxiv.org/abs/2508.01633v1,Rate-distortion Optimized Point Cloud Preprocessing for Geometry-based Point Cloud Compression,"Geometry-based point cloud compression (G-PCC) offers high compression efficiency but is sensitive to noise and outliers in the input point cloud. Directly compressing raw point clouds often leads to suboptimal rate-distortion performance due to the allocation of bits to encode irrelevant geometric details. This paper addresses the problem of optimizing point cloud preprocessing specifically for G-PCC, aiming to improve the rate-distortion trade-off by intelligently simplifying the geometry before compression. We propose a novel rate-distortion optimized point cloud simplification framework that integrates mesh decimation, outlier removal, and smoothing operations. Our framework formulates the preprocessing as a constrained optimization problem, balancing the distortion introduced by simplification with the rate reduction achieved during subsequent compression. The optimization leverages a differentiable approximation of the G-PCC rate to guide the simplification process. Experimental results on a diverse set of point clouds demonstrate that our method achieves significant bitrate savings compared to traditional simplification techniques and direct compression, while maintaining comparable or even improved visual quality. This optimized preprocessing pipeline provides a practical and effective approach for enhancing the performance of geometry-based point cloud compression."
http://arxiv.org/abs/2508.01620v1,IMU: Influence-guided Machine Unlearning,"Machine unlearning, the ability to selectively remove the influence of specific data points from a trained model, is becoming increasingly important due to growing privacy concerns and regulatory demands. Existing unlearning methods often struggle with computational efficiency, accuracy, or applicability to complex deep learning models. This paper introduces Influence-guided Machine Unlearning (IMU), a novel approach that leverages the concept of influence functions to efficiently approximate the effect of removing specific data points. IMU first identifies the most influential parameters for the target data to be unlearned, then performs a targeted retraining of only these parameters using a modified learning objective designed to counteract the influence of the removed data. We demonstrate through extensive experiments on image classification and object detection tasks that IMU achieves comparable unlearning performance to retraining from scratch, while significantly reducing computational cost. IMU offers a practical and efficient solution for machine unlearning, enabling the responsible and compliant deployment of deep learning models in sensitive applications."
http://arxiv.org/abs/2508.01594v1,CLIMD: A Curriculum Learning Framework for Imbalanced Multimodal Diagnosis,"Multimodal medical diagnosis leverages diverse data sources like imaging and clinical reports to improve diagnostic accuracy. However, real-world datasets often suffer from significant class imbalance and varying difficulty levels across different modalities, hindering the performance of multimodal diagnostic models. This paper introduces CLIMD, a novel Curriculum Learning framework for Imbalanced Multimodal Diagnosis. CLIMD employs a difficulty-aware curriculum strategy that gradually exposes the model to increasingly challenging samples within each modality, mitigating the impact of class imbalance. Furthermore, it incorporates a modality-aware weighting scheme that dynamically adjusts the contribution of each modality based on its perceived difficulty and relevance during training. Extensive experiments on a chest X-ray diagnosis dataset demonstrate that CLIMD significantly outperforms state-of-the-art methods, achieving a 5-8% improvement in F1-score for minority classes and improved overall diagnostic accuracy. CLIMD provides a robust and effective solution for training multimodal diagnostic models in the presence of class imbalance and varying modality difficulty."
http://arxiv.org/abs/2508.01592v1,DMTrack: Spatio-Temporal Multimodal Tracking via Dual-Adapter,"Multimodal tracking leverages complementary information from different sensor modalities to achieve robust and accurate object tracking, especially in challenging scenarios. However, effectively fusing spatio-temporal information from multiple modalities with varying characteristics remains a key challenge. We introduce DMTrack, a novel spatio-temporal multimodal tracking framework utilizing a Dual-Adapter module for enhanced feature fusion. DMTrack employs separate adapters to process spatial and temporal information independently for each modality before adaptively fusing them. This decoupling allows the model to learn modality-specific representations at different scales and handle the heterogeneity between modalities more effectively. Furthermore, we introduce a cross-modal attention mechanism within each adapter to refine feature representations based on inter-modal correlations. Extensive experiments on multiple benchmark datasets demonstrate that DMTrack significantly outperforms state-of-the-art multimodal tracking algorithms, achieving substantial gains in both accuracy and robustness. This highlights the effectiveness of our Dual-Adapter design in capturing and integrating complex spatio-temporal relationships across diverse modalities, leading to improved tracking performance."
http://arxiv.org/abs/2508.01574v1,TopoImages: Incorporating Local Topology Encoding into Deep Learning Models for Medical Image Classification,"Medical image classification is crucial for accurate diagnosis and treatment planning. However, current deep learning models often overlook the importance of local topological features that can be indicative of subtle disease-related changes. This paper addresses the challenge of effectively incorporating local topological information into deep learning models for improved medical image classification. We introduce TopoImages, a novel approach that extracts persistent homology features from local image patches and encodes them as topological descriptors. These descriptors are then integrated into a convolutional neural network (CNN) architecture as additional input channels, providing the model with explicit topological information alongside the raw image data. We evaluate TopoImages on two distinct medical image classification tasks: differentiating between benign and malignant lung nodules in CT scans and classifying Alzheimer's disease stages from MRI scans. Experimental results demonstrate that TopoImages consistently improves classification accuracy compared to baseline CNN models and other feature extraction techniques, highlighting the value of integrating local topology into deep learning for medical image analysis. This approach provides a promising direction for developing more robust and interpretable medical image classification systems."
http://arxiv.org/abs/2508.01565v1,Deeply Supervised Multi-Task Autoencoder for Biological Brain Age estimation using three dimensional T$_1$-weighted magnetic resonance imaging,"Accurate estimation of biological brain age from structural MRI provides a valuable biomarker for neurodegenerative diseases and cognitive decline. However, capturing subtle age-related changes in complex brain structures from 3D MRI remains a challenge. We address this challenge by proposing a deeply supervised multi-task autoencoder (DS-MTAE) that simultaneously learns a compact latent representation of 3D T$_1$-weighted MRI scans and predicts brain age. The DS-MTAE incorporates a 3D convolutional autoencoder architecture with multiple auxiliary tasks, including regional volume regression and tissue segmentation, to enhance feature learning. Deep supervision is applied at multiple levels of the encoder to guide the learning process and ensure robust feature extraction. Experiments on a large dataset of healthy adults demonstrate that our DS-MTAE achieves state-of-the-art brain age prediction accuracy, significantly outperforming existing methods. This suggests that the learned latent space effectively captures age-related brain changes, providing a powerful tool for early detection and monitoring of neurodegenerative processes."
http://arxiv.org/abs/2508.01555v1,MGCR-Net:Multimodal Graph-Conditioned Vision-Language Reconstruction Network for Remote Sensing Change Detection,"Remote sensing change detection (RSCD) aims to identify land surface changes by analyzing multi-temporal images, a crucial task for environmental monitoring and urban planning. However, existing RSCD methods often struggle with complex and subtle changes due to limited feature representation capabilities and ineffective fusion of multimodal data. To address these limitations, we propose MGCR-Net, a Multimodal Graph-Conditioned Vision-Language Reconstruction Network for RSCD. MGCR-Net leverages graph convolutional networks to model contextual relationships within and between bitemporal image features and incorporates a novel vision-language reconstruction module. This module utilizes textual descriptions of the scene to guide the reconstruction of image features, thereby enhancing the discriminative power of the learned representations and facilitating effective multimodal fusion. Experimental results on multiple benchmark datasets demonstrate that MGCR-Net achieves significant improvements in change detection accuracy compared to state-of-the-art methods. This highlights the potential of our approach for robust and accurate change detection in complex remote sensing scenarios."
http://arxiv.org/abs/2508.01396v2,Spatial-Frequency Aware for Object Detection in RAW Image,"Object detection in RAW images offers significant advantages over processing sRGB images, including increased dynamic range and finer control over image processing pipelines. However, existing object detectors are primarily designed for sRGB images and often struggle with the unique characteristics of RAW data, particularly the spatial-frequency distribution resulting from Bayer pattern sampling and demosaicing artifacts. This paper addresses the challenge of effectively detecting objects directly in RAW images by explicitly considering the spatial-frequency domain characteristics. We propose a novel Spatial-Frequency Aware Object Detection (SFAOD) framework. SFAOD incorporates a learnable spatial-frequency decomposition module to extract frequency-specific features from RAW images. These features are then adaptively fused with spatial features through an attention mechanism, allowing the detector to focus on relevant frequency components for accurate object localization and classification. Experimental results on a newly created RAW object detection dataset demonstrate that SFAOD significantly outperforms state-of-the-art sRGB-based detectors adapted for RAW images, achieving a 5.2% improvement in average precision (AP). This work highlights the importance of spatial-frequency awareness for robust object detection in RAW images, paving the way for improved performance in low-light and high-dynamic-range scenarios."
http://arxiv.org/abs/2508.01380v1,Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using Vision-Language Models,"Generating realistic and diverse damage data is crucial for training robust computer vision models for infrastructure inspection and disaster response. However, acquiring sufficient real-world damaged imagery is often expensive, time-consuming, and potentially dangerous. This paper addresses the challenge of generating high-quality synthetic damage data by fusing visual information with human knowledge using vision-language models (VLMs). Our method leverages a pre-trained VLM to semantically guide image generation based on textual descriptions of damage types, severity levels, and affected structural components. We introduce a novel pipeline that combines text-to-image generation with image in-painting and blending techniques, allowing for precise control over the location and appearance of synthetic damage within existing imagery. Experiments demonstrate that our approach generates more realistic and diverse damage datasets compared to purely generative methods, leading to significant improvements in the performance of downstream damage detection models, particularly for rare and complex damage scenarios. This research offers a cost-effective and scalable solution for creating comprehensive damage datasets, ultimately contributing to more reliable and efficient infrastructure monitoring systems."
http://arxiv.org/abs/2508.01352v2,Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study,"Lung adenocarcinoma (LUAD) is a leading cause of cancer-related deaths, with epidermal growth factor receptor (EGFR) mutations being a crucial biomarker for targeted therapy. Accurately predicting EGFR mutation status from histopathological whole-slide images (WSIs) offers a non-invasive and cost-effective alternative to genetic sequencing, particularly in resource-constrained settings. This study addresses the challenge of predicting EGFR mutation status in LUAD patients from WSIs using a deep learning approach applied to a unique Indian cohort. We leverage a pretrained vision foundation model, specifically DINOv2, and employ a transfer learning strategy to adapt the model to our dataset. The WSIs were processed into tiles, and DINOv2 features were extracted and fed into a downstream classifier. We further investigated the impact of different tile aggregation strategies and classifier architectures. Our method achieved an AUC of 0.82 and an accuracy of 78% in predicting EGFR mutation status on an independent validation set, demonstrating robust performance despite the limited size and specific characteristics of the Indian cohort. This study highlights the potential of pretrained foundation models and transfer learning for predicting clinically relevant genomic alterations from histopathology, paving the way for personalized treatment strategies in LUAD patients, especially within the Indian population."
http://arxiv.org/abs/2508.01322v1,SWAN: Synergistic Wavelet-Attention Network for Infrared Small Target Detection,"Infrared small target detection (IRSTD) is a crucial task in various applications, including military surveillance and search and rescue, yet it remains challenging due to the low signal-to-noise ratio and lack of distinct features. Existing methods often struggle to effectively suppress complex backgrounds while preserving the weak target signal, leading to high false alarm rates. We propose a Synergistic Wavelet-Attention Network (SWAN) for robust IRSTD. SWAN leverages the multi-resolution analysis capabilities of wavelet transform to decompose the infrared image into different frequency subbands, enabling targeted feature extraction. Subsequently, we introduce a novel synergistic attention mechanism that integrates channel and spatial attention modules to adaptively enhance target-related features and suppress background clutter across all subbands. Experimental results on benchmark datasets demonstrate that SWAN achieves state-of-the-art performance, surpassing existing methods in terms of detection accuracy and robustness, particularly in challenging scenarios with complex backgrounds. This improved performance highlights the potential of SWAN for real-world IRSTD applications."
http://arxiv.org/abs/2508.01316v1,Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust,"Distal myopathies, a heterogeneous group of inherited muscle disorders, present diagnostic challenges due to overlapping clinical and pathological features. This paper addresses the critical need for improved diagnostic accuracy in distal myopathy by leveraging multimodal data fusion. We propose a novel attention-aware fusion network that integrates clinical data (patient history, physical examination findings) with muscle biopsy images (histopathology) to predict specific distal myopathy subtypes. Our model employs separate attention mechanisms for each modality to identify salient features, followed by a fusion module that adaptively weights contributions based on learned attention scores. Experiments on a dataset of clinically and genetically confirmed distal myopathy cases demonstrate that our approach significantly outperforms unimodal baselines and existing fusion strategies, achieving state-of-the-art diagnostic accuracy. Furthermore, visualization of attention maps provides interpretable insights into the model's decision-making process, enhancing clinician trust and facilitating integration into clinical workflows for improved diagnostic outcomes."
http://arxiv.org/abs/2508.01254v1,Self-Enhanced Image Clustering with Cross-Modal Semantic Consistency,"Image clustering aims to group unlabeled images into distinct clusters based on visual similarity. However, the inherent ambiguity in visual features and the lack of explicit supervision often lead to suboptimal clustering performance. This paper addresses the challenge of improving image clustering accuracy by leveraging self-enhancement and cross-modal semantic consistency. We propose a novel Self-Enhanced Image Clustering framework with Cross-Modal Semantic Consistency (SEIC-CMSC). Our approach initially constructs a pseudo-label set through self-supervised learning and refines it iteratively by exploiting the agreement between visual features and text descriptions generated via an image captioning model. Specifically, we enforce consistency between the predicted visual cluster assignments and the semantic information derived from the generated captions, thereby enhancing the robustness and accuracy of the clustering process. Extensive experiments on several benchmark datasets demonstrate that our SEIC-CMSC significantly outperforms state-of-the-art clustering methods. The proposed framework offers a promising direction for unsupervised image understanding by effectively integrating visual and semantic information for improved clustering performance."
http://arxiv.org/abs/2508.01219v1,Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis,"Deep neural networks have achieved remarkable success in various computer vision tasks, but their generalization ability remains a challenge, often requiring extensive data augmentation or domain adaptation techniques. This paper addresses the problem of enhancing the generalization capability of vision models by explicitly incorporating an eigenbasis representation within the network architecture. We introduce the Eigen Neural Network (ENN), which learns a data-driven eigenbasis from the input feature space and projects subsequent feature maps onto this learned basis. This projection enforces a structured representation, capturing the principal components of the data distribution and filtering out irrelevant noise. Experiments on image classification (CIFAR-10, CIFAR-100, ImageNet) and object detection (COCO) demonstrate that ENNs consistently outperform standard convolutional neural networks and other regularization techniques, especially in out-of-distribution generalization scenarios. The proposed ENN offers a novel and effective approach to improve the robustness and generalization of vision models by leveraging the power of eigenbasis representation."
http://arxiv.org/abs/2508.01210v1,RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification,"Road surface classification is crucial for autonomous driving, impacting safety and performance by enabling adaptive vehicle control. Existing methods often struggle with long-range dependencies and computational efficiency when processing high-resolution road images. To address these limitations, we introduce RoadMamba, a novel dual-branch visual state space model specifically designed for road surface classification. RoadMamba leverages a Mamba-based branch to capture global contextual information and long-range dependencies in the road scene, complemented by a convolutional branch for extracting detailed local features. These features are then fused to provide a comprehensive representation of the road surface. Experimental results on publicly available road datasets demonstrate that RoadMamba achieves state-of-the-art classification accuracy while significantly reducing computational cost compared to transformer-based methods. This improved efficiency and accuracy make RoadMamba a promising solution for real-time road surface classification in autonomous driving systems."
http://arxiv.org/abs/2508.01206v1,Deep Learning for Pavement Condition Evaluation Using Satellite Imagery,"Pavement condition evaluation is crucial for maintaining safe and efficient transportation infrastructure, yet traditional methods are often costly, time-consuming, and subjective. This paper addresses the challenge of efficiently and accurately assessing pavement distresses over large areas by leveraging the accessibility and scalability of satellite imagery. We propose a novel deep learning framework that combines a convolutional neural network (CNN) for feature extraction with a transformer-based architecture for contextual understanding of pavement distress patterns from high-resolution satellite imagery. Specifically, we employ a pre-trained ResNet50 model as the CNN backbone to extract relevant visual features, followed by a transformer encoder to capture long-range dependencies and spatial relationships between different distress types. Our experiments, conducted on a newly curated dataset of satellite imagery and corresponding pavement condition labels, demonstrate that our proposed method achieves state-of-the-art performance in pavement distress classification, surpassing existing CNN-based approaches by a significant margin. This work provides a cost-effective and scalable solution for large-scale pavement condition monitoring, enabling proactive maintenance strategies and improved infrastructure management."
http://arxiv.org/abs/2508.01137v1,Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach,"Anomaly detection in brain MRI is crucial for early diagnosis and treatment of neurological disorders. However, the scarcity of labeled anomaly data poses a significant challenge for supervised learning approaches. We address this problem by proposing a novel semi-supervised anomaly detection framework leveraging deep reinforcement learning (DRL) in a domain-agnostic manner. Our method trains a DRL agent to navigate the latent space of a pre-trained variational autoencoder (VAE), learning to distinguish between normal and anomalous brain MRI slices based on reconstruction error and feature distribution discrepancies. The agent is rewarded for identifying anomalous regions and penalized for misclassifications, guided by a small set of labeled anomalous examples while leveraging a large pool of unlabeled data. Experiments on multiple publicly available brain MRI datasets demonstrate that our approach achieves state-of-the-art performance in anomaly detection, significantly outperforming existing semi-supervised and unsupervised methods, particularly when the labeled anomaly set is limited. This work presents a robust and generalizable solution for anomaly detection in medical imaging, paving the way for more effective clinical decision support systems."
http://arxiv.org/abs/2508.03745v1,Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision,"Terrain feature detection from remote sensing data is crucial for numerous applications, including environmental monitoring and autonomous navigation. However, obtaining dense and accurate labels for training deep learning models remains a significant bottleneck. This paper addresses the challenge of detecting terrain features under weak supervision by leveraging Tobler's First Law of Geography (TFL), which states that ""everything is related to everything else, but near things are more related than distant things."" We introduce a spatially explicit deep learning model that incorporates a TFL-inspired regularization term during training. This term encourages spatial consistency in the predicted terrain feature labels, effectively propagating weak labels to unlabeled neighboring pixels based on spatial proximity and feature similarity. We evaluate our method on a diverse dataset of aerial imagery, demonstrating significant improvements in terrain feature detection accuracy compared to baseline models trained with the same weak supervision. The proposed approach provides a novel and effective framework for leveraging readily available, albeit noisy, spatial information to enhance the performance of deep learning models in GeoAI applications."
http://arxiv.org/abs/2508.01087v1,COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition,"Open set recognition (OSR) aims to correctly classify known classes while rejecting unknown samples. Existing OSR methods often struggle with high-dimensional feature spaces and noisy data, leading to degraded performance and misclassification of novel instances as known classes. We address this challenge by introducing COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition. COSTARR employs a novel attenuation mechanism within a deep neural network to dynamically suppress irrelevant or noisy features, enhancing the discrimination between known and unknown samples. Furthermore, we consolidate the feature space by promoting intra-class compactness and inter-class separation through a tailored loss function that incorporates both classification accuracy and open space risk. Experiments on benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that COSTARR significantly outperforms state-of-the-art OSR methods, achieving higher accuracy on known classes and improved rejection rates of unknown samples, particularly under challenging noise conditions. COSTARR provides a robust and effective solution for open set recognition, improving the reliability of real-world computer vision applications."
http://arxiv.org/abs/2508.01074v1,Evading Data Provenance in Deep Neural Networks,"Deep neural networks (DNNs) are increasingly deployed in sensitive applications, raising concerns about data provenance and model security. Existing watermarking and fingerprinting techniques aim to trace model ownership and training data, but these methods often neglect the potential for adversarial attacks specifically designed to remove or evade such provenance mechanisms. This paper addresses the critical problem of how an adversary can effectively evade data provenance techniques embedded within DNNs, thereby obscuring the origin and ownership of the model. We propose a novel adversarial training framework, termed Provenance Evasion via Adversarial Fine-tuning (PEAF), which utilizes a bi-level optimization strategy to fine-tune a pre-trained, watermarked model. PEAF simultaneously minimizes the primary task loss while maximizing the error in watermark detection, effectively weakening or removing the embedded provenance information. Our experiments on image classification tasks demonstrate that PEAF significantly reduces the accuracy of existing watermarking techniques by up to 75% while maintaining acceptable performance on the original task. This work highlights a significant vulnerability in current data provenance techniques and underscores the need for more robust and resilient methods for securing DNNs against adversarial provenance manipulation."
http://arxiv.org/abs/2508.00974v1,ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling,"Thermography provides valuable insights into the thermal behavior of objects, yet its application in dynamic environments like cycling is hampered by the lack of labeled thermal datasets. Obtaining accurate, pixel-wise thermogram labels is challenging due to the inherent noise and low resolution of thermal cameras, as well as the difficulty of manual annotation. We introduce ThermoCycleNet, a novel stereo-based framework for automatically generating labeled thermograms for cycling scenarios. Our approach leverages calibrated thermal and RGB stereo cameras to project high-resolution RGB semantic segmentations onto the corresponding thermal images. To address discrepancies arising from parallax and sensor differences, we employ a learned disparity refinement network trained on synthetic thermal data and fine-tuned with a self-supervised thermal consistency loss. Experiments conducted on a newly collected real-world cycling dataset demonstrate that ThermoCycleNet significantly improves the accuracy of thermogram labeling compared to direct projection methods, achieving a mean Intersection over Union (mIoU) of 68.3% for cyclist segmentation. This enables the training of robust thermal perception models for cycling, facilitating advancements in rider safety and performance analysis."
http://arxiv.org/abs/2508.00755v1,AI-Driven Collaborative Satellite Object Detection for Space Sustainability,"The increasing number of artificial objects in Earth's orbit poses a significant threat to space sustainability, necessitating accurate and timely satellite object detection. Current satellite object detection methods often suffer from limitations in resolution, varying illumination conditions, and the scarcity of labeled data, hindering comprehensive space situational awareness. This paper introduces an AI-driven collaborative framework for satellite object detection, leveraging federated learning and domain adaptation to address these challenges. Our approach facilitates knowledge sharing between multiple satellite observation platforms without compromising data privacy. Specifically, we employ a novel adversarial domain adaptation module within a federated learning scheme to mitigate domain discrepancies arising from diverse sensor characteristics and orbital geometries across participating satellites. Experimental results on a newly curated satellite object detection dataset demonstrate a significant improvement in detection accuracy (15% increase in mAP) and robustness compared to traditional centralized and non-collaborative methods, particularly in low-illumination and low-resolution scenarios. This collaborative AI framework provides a pathway towards a more sustainable and safer space environment by enabling more accurate and comprehensive tracking of space objects."
http://arxiv.org/abs/2508.00698v1,Can Large Pretrained Depth Estimation Models Help With Image Dehazing?,"Image dehazing aims to restore clear visibility in images degraded by atmospheric haze, a common challenge for outdoor computer vision applications. While deep learning has significantly advanced dehazing, performance often relies on large-scale, task-specific training datasets. This paper investigates whether large pretrained depth estimation models, trained on diverse and extensive datasets, can be leveraged to improve image dehazing performance without requiring extensive dehazing-specific fine-tuning. We propose a novel dehazing framework that incorporates depth information extracted from a large pretrained depth estimation model as a prior. Specifically, we utilize the estimated depth map to guide the estimation of atmospheric light and transmission map, key parameters in the atmospheric scattering model. Our experiments on synthetic and real-world datasets demonstrate that leveraging pretrained depth information significantly improves dehazing performance compared to existing methods, particularly in challenging scenarios with complex scene geometry and varying haze density. This work highlights the potential of transfer learning from related tasks, specifically depth estimation, to enhance the robustness and generalizability of image dehazing algorithms."
http://arxiv.org/abs/2508.00553v2,HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models,"Vision-language pre-training has significantly advanced various downstream vision tasks by leveraging large-scale datasets and cross-modal learning. However, the computational cost associated with processing high-resolution images and lengthy text prompts remains a significant bottleneck, particularly when deploying these models on resource-constrained devices. This paper addresses the challenge of efficient inference in vision-language models by introducing HiPrune, a novel training-free token pruning method that leverages hierarchical attention information. HiPrune constructs a hierarchical representation of attention scores, calculated across different layers and modalities, to identify and prune less informative visual tokens dynamically. Specifically, we utilize the attention entropy within each hierarchical level to guide the pruning process, ensuring the preservation of crucial visual features while reducing computational overhead. Experiments on image classification and visual question answering tasks demonstrate that HiPrune achieves significant speedups (up to 2x) with minimal performance degradation compared to the unpruned baseline and existing training-based pruning methods. The proposed method offers a practical and efficient solution for deploying large vision-language models in real-world applications without requiring retraining."
http://arxiv.org/abs/2508.00552v1,DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification,"Adversarial training (AT) has emerged as a dominant paradigm for defending neural networks against adversarial attacks, but its high computational cost remains a significant bottleneck. Knowledge distillation offers a promising avenue for efficient adversarial purification; however, existing methods often struggle to maintain robustness and accuracy, particularly when dealing with strong adversarial perturbations. This paper addresses the challenge of distilling robust knowledge from computationally expensive AT models into lightweight, efficient student models without sacrificing performance. We introduce Noise Bridge Consistency Distillation (NBCD), a novel approach that leverages a carefully designed noise bridge to connect the feature spaces of the teacher and student models during training. NBCD encourages the student to maintain consistent predictions under varying levels of noise applied to the adversarial examples, effectively transferring the teacher's robustness to the student. Experiments on CIFAR-10 and CIFAR-100 demonstrate that NBCD achieves comparable or superior adversarial robustness to state-of-the-art distillation methods, while significantly reducing computational cost and maintaining high clean accuracy. The proposed distillation strategy offers a practical and effective solution for deploying robust deep learning models in resource-constrained environments."
http://arxiv.org/abs/2508.03744v1,Do We Need Pre-Processing for Deep Learning Based Ultrasound Shear Wave Elastography?,"Shear wave elastography (SWE) is a valuable ultrasound technique for non-invasive tissue characterization, where tissue stiffness is estimated from shear wave speed. Deep learning has shown promise in automating and potentially improving SWE analysis; however, the necessity of traditional pre-processing steps, such as noise filtering and shear wave arrival time estimation, remains unclear when employing these data-driven methods. This paper investigates whether pre-processing pipelines commonly used in conventional SWE algorithms are essential for deep learning models trained to estimate tissue stiffness from raw ultrasound SWE data. We trained convolutional neural networks (CNNs) on both pre-processed and raw SWE data, comparing their performance in stiffness estimation accuracy and robustness to noise. Our results indicate that CNNs trained directly on raw SWE data can achieve comparable, and in some cases superior, performance to those trained on pre-processed data, particularly when dealing with noisy acquisitions, while also reducing computational complexity. This suggests that deep learning models can inherently learn relevant features from raw SWE data, potentially obviating the need for complex and potentially biased pre-processing steps, paving the way for simpler and more robust SWE analysis pipelines."
http://arxiv.org/abs/2508.00531v1,The Repeated-Stimulus Confound in Electroencephalography,"Electroencephalography (EEG) offers a non-invasive method for studying neural activity associated with visual perception and cognition. However, a common paradigm involves repeated presentations of the same stimuli to elicit measurable event-related potentials (ERPs), potentially introducing systematic biases. This paper addresses the confound that repeated stimulus presentations can induce in EEG studies, specifically the interaction between stimulus repetition, adaptation, and neural responses. We propose a novel analytical framework that leverages representational similarity analysis (RSA) to disentangle stimulus-specific neural representations from repetition-related effects. This framework models the evolution of EEG patterns over repeated stimulus presentations, allowing us to quantify the degree to which neural responses are driven by the visual features of the stimuli versus adaptation-driven changes. Our results demonstrate that repetition significantly alters the representational geometry of EEG signals, particularly in later processing stages, and that the proposed framework effectively mitigates this confound, revealing underlying stimulus-specific representations obscured by adaptation. This work provides a crucial tool for improving the interpretability of EEG data and offers insights into the neural mechanisms underlying visual processing and adaptation."
http://arxiv.org/abs/2508.00496v2,LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI,"Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a crucial tool for breast cancer diagnosis and treatment monitoring, with accurate lesion segmentation being paramount. However, segmenting small breast lesions in DCE-MRI is challenging due to their subtle appearance, low contrast, and the presence of noise and artifacts. This paper addresses the problem of improving the segmentation accuracy of small breast lesions in longitudinal DCE-MRI by jointly modeling temporal contrast enhancement patterns and clinical information. We propose LesiOnTime, a novel deep learning framework that leverages a 3D convolutional neural network backbone augmented with a temporal attention mechanism to capture dynamic contrast uptake patterns across multiple time points. Crucially, LesiOnTime incorporates clinical metadata, such as patient age and hormone receptor status, through a feature fusion module, enabling the model to learn lesion characteristics conditioned on patient-specific factors. Experiments on a large longitudinal DCE-MRI dataset demonstrate that LesiOnTime significantly outperforms state-of-the-art segmentation methods, achieving a Dice score improvement of over 8% for small lesions. This improved segmentation accuracy, facilitated by temporal dynamics and clinical context, has the potential to enhance diagnostic precision and personalized treatment planning for breast cancer patients."
http://arxiv.org/abs/2508.00438v1,Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection,"Coronary artery disease (CAD) is a leading cause of death globally, and accurate detection of coronary stenosis is crucial for timely intervention. Deep learning models show promise in this domain, but their performance is often limited by the availability of annotated medical images, particularly for specific stenosis severities. We address this challenge by proposing a novel diffusion-based data augmentation framework guided by user-defined stenosis characteristics for coronary angiography images. Our method leverages a conditional diffusion model to generate synthetic images with specified stenosis severity, location, and morphology, controlled through a user-defined mask and condition embeddings. A key innovation is the integration of spatial control within the diffusion process, enabling precise manipulation of the generated stenotic regions. Experiments on a benchmark coronary angiography dataset demonstrate that training a stenosis detection model with augmented data generated by our approach significantly improves performance, achieving a 5% increase in F1-score for severe stenosis detection compared to models trained with traditional augmentation techniques. This user-guided data augmentation strategy offers a powerful tool for enhancing the performance and robustness of deep learning models in medical image analysis, particularly when dealing with limited and imbalanced datasets."
http://arxiv.org/abs/2508.00413v1,DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space,"Diffusion models have achieved state-of-the-art results in generative tasks, but their iterative denoising process is computationally expensive, leading to slow convergence. This paper addresses the problem of accelerating diffusion model training and inference by learning a more structured and informative latent space. We introduce DC-AE 1.5, an enhanced Diffusion-based Conditional Autoencoder that incorporates a novel hierarchical vector quantization (VQ) module within the autoencoder's latent space. This structured latent space is then leveraged by a diffusion model conditioned on the discrete latent codes. Our experiments on benchmark image datasets demonstrate that DC-AE 1.5 significantly reduces the number of diffusion steps required for high-quality sample generation, achieving comparable or superior FID scores with a 2x-4x reduction in sampling time compared to baseline diffusion models. The proposed approach offers a practical solution to improve the efficiency of diffusion models without compromising generation quality, making them more accessible for resource-constrained applications."
http://arxiv.org/abs/2508.00387v2,STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers,"Spiking Neural Networks (SNNs) offer potential advantages in energy efficiency and latency compared to traditional Artificial Neural Networks, particularly when deployed on neuromorphic hardware. However, training deep and complex SNNs remains a challenge, and existing Spiking Transformer architectures often struggle to effectively capture temporal dependencies in input spike trains. This paper introduces STF, a novel approach that incorporates Shallow-Level Temporal Feedback to enhance the performance of Spiking Transformers. STF augments the initial layers of the Spiking Transformer with recurrent connections that provide feedback from preceding time steps, allowing the network to better retain short-term temporal information before processing by the subsequent Transformer layers. Our experiments on neuromorphic datasets, including Spiking Heidelberg Digits (SHD) and Spiking Speech Commands (SSC), demonstrate that STF significantly improves classification accuracy compared to state-of-the-art Spiking Transformer architectures, achieving a relative improvement of up to 5% while maintaining competitive latency. These results highlight the importance of capturing shallow-level temporal dynamics in SNNs and pave the way for more efficient and accurate processing of temporal information with Spiking Transformers."
http://arxiv.org/abs/2508.03740v1,VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission,"Digital semantic communication aims to transmit the meaning of data efficiently and reliably over bandwidth-constrained channels. However, current approaches often struggle to adapt effectively to varying channel conditions, leading to suboptimal performance in real-world image transmission scenarios. This paper addresses the challenge of robust and efficient semantic image transmission by introducing VQ-DeepISC, a novel Vector Quantized-enabled Deep Image Semantic Communication system. VQ-DeepISC leverages a deep learning architecture incorporating vector quantization to extract and compress semantic image features into discrete codebook indices. These indices are then mapped to channel symbols using a learned modulation scheme optimized for the current channel state information, enabling adaptive and robust transmission. Experimental results demonstrate that VQ-DeepISC achieves significant improvements in perceptual quality, measured by LPIPS and PSNR, compared to existing deep semantic communication methods and traditional JPEG compression, particularly under noisy channel conditions. This work provides a promising direction for developing practical and robust semantic communication systems for image transmission in dynamic environments."
http://arxiv.org/abs/2508.00235v1,Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior,"Intracranial aneurysms (IA) are pathological dilations of cerebral arteries and pose a significant risk of rupture, leading to severe neurological consequences. Accurate detection and segmentation of IAs in Magnetic Resonance Angiography (MRA) images are crucial for timely diagnosis and treatment planning. However, obtaining precise pixel-level annotations for training deep learning models is laborious and requires specialized expertise. This paper introduces a weakly supervised framework for IA detection and segmentation in MRA images using a multi-task UNet architecture incorporating a vesselness prior. Our approach leverages readily available bounding box annotations for IAs and combines them with automatically generated vesselness maps as a spatial prior to guide the segmentation process. The multi-task UNet simultaneously predicts IA presence within the bounding box and refines the segmentation mask using the vesselness information. Experimental results on a clinical dataset demonstrate that our weakly supervised method achieves comparable detection and segmentation performance to fully supervised methods, while significantly reducing annotation effort. This approach offers a practical and efficient solution for automated IA analysis in MRA imaging, facilitating improved clinical workflows and patient outcomes."
http://arxiv.org/abs/2508.00197v1,Graph Lineages and Skeletal Graph Products,"Graph representations are prevalent in computer vision, enabling structured analysis of complex scenes and objects. However, reasoning about the evolutionary relationships between graphs, particularly in dynamic settings or hierarchical decompositions, remains a challenge. This paper addresses the problem of defining and analyzing lineages of graphs and their products, focusing on skeletal graph representations commonly used for shape analysis and pose estimation. We introduce the concept of a ""graph lineage,"" representing a sequence of graphs related by a set of defined transformations, and develop a novel ""skeletal graph product"" operation that combines information from multiple graphs within a lineage while preserving their underlying skeletal structure. We demonstrate the efficacy of our approach in two applications: tracking articulated objects and generating hierarchical shape decompositions, achieving improved robustness and accuracy compared to existing methods. The proposed framework provides a powerful tool for reasoning about the evolution and composition of graph structures, opening new avenues for research in dynamic scene understanding and shape analysis."
http://arxiv.org/abs/2508.00155v1,GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation,"Accurate 3D tooth segmentation is crucial for various dental applications, including orthodontic treatment planning, surgical simulation, and computer-aided design/manufacturing of dental prostheses. However, the complex morphology of teeth, variations in shape and size, and the presence of dental artifacts pose significant challenges for automated segmentation. This paper introduces GEPAR3D, a novel geometry prior-assisted learning framework for robust and accurate 3D tooth segmentation. GEPAR3D leverages a multi-stage approach, first employing a coarse-to-fine segmentation network to generate initial predictions. Subsequently, a geometry prior module, incorporating shape priors learned from a statistical shape model, refines the segmentation by enforcing geometric consistency and smoothness. Finally, a refinement network further enhances the segmentation details and corrects potential errors based on the initial prediction and shape prior. Experimental results on a large-scale dental CBCT dataset demonstrate that GEPAR3D outperforms state-of-the-art methods, achieving significant improvements in Dice score and Hausdorff distance. GEPAR3D offers a promising solution for accurate and reliable 3D tooth segmentation, paving the way for improved clinical workflows and enhanced patient care in dentistry."
http://arxiv.org/abs/2508.03739v1,A Modified VGG19-Based Framework for Accurate and Interpretable Real-Time Bone Fracture Detection,"Bone fracture detection from X-ray images is a critical task in medical diagnosis, often requiring expert radiologist interpretation. However, the increasing demand for timely diagnosis necessitates automated and efficient solutions. This paper addresses the challenge of developing an accurate and interpretable real-time bone fracture detection system applicable in resource-constrained environments. We propose a modified VGG19-based framework incorporating attention mechanisms and knowledge distillation. Specifically, we introduce a novel attention module integrated within the VGG19 architecture to highlight fracture-relevant regions, enhancing feature extraction. Furthermore, we employ knowledge distillation, transferring knowledge from a larger, pre-trained VGG19 model to a smaller, streamlined version, enabling real-time performance. Experimental results on a large-scale bone fracture X-ray dataset demonstrate that our approach achieves a detection accuracy of 94.7% and a processing speed of 28 frames per second on a standard GPU, surpassing state-of-the-art methods while maintaining model interpretability through attention map visualization. This work offers a practical and reliable solution for assisting clinicians in rapid and accurate bone fracture diagnosis."
http://arxiv.org/abs/2508.00098v1,Stress-Aware Resilient Neural Training,"Neural network training is susceptible to various stressors, including hardware faults, data corruption, and adversarial attacks, leading to degraded performance and unreliable models. This paper addresses the critical problem of maintaining training resilience in the face of such stressors. We introduce Stress-Aware Resilient Neural Training (SARNT), a novel framework that integrates stress detection and mitigation strategies directly into the training loop. SARNT employs a lightweight stress detection module to identify anomalies during training and dynamically adjusts the learning rate and applies gradient clipping based on the severity and type of stress detected. Our experiments demonstrate that SARNT significantly improves the resilience of trained models against a range of stressors, including bit flips, label noise, and adversarial perturbations, achieving up to a 20% improvement in accuracy compared to standard training methods under stressful conditions. SARNT offers a practical and effective approach to building more robust and reliable neural networks for real-world applications."
http://arxiv.org/abs/2507.23763v2,Topology Optimization in Medical Image Segmentation with Fast Euler Characteristic,"Medical image segmentation is crucial for diagnosis and treatment planning, yet traditional methods often struggle with topological inaccuracies, leading to disconnected or overly connected regions. This paper addresses the challenge of incorporating topological constraints into medical image segmentation while maintaining computational efficiency. We propose a novel topology optimization framework that integrates a fast Euler characteristic calculation directly into the segmentation loss function. Our method leverages a computationally efficient algorithm for computing the Euler characteristic based on marching cubes, enabling its use within deep learning segmentation pipelines. We further introduce a differentiable approximation of the Euler characteristic to facilitate gradient-based optimization. Experiments on diverse medical imaging datasets, including brain MRI and lung CT scans, demonstrate that our approach significantly improves the topological accuracy of segmentation results compared to existing methods, achieving comparable segmentation performance with significantly reduced topological errors. This approach offers a practical and efficient solution for topology-aware medical image segmentation, enhancing the clinical utility of automated analysis pipelines."
http://arxiv.org/abs/2507.23715v1,DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching,"Non-rigid shape matching aims to find correspondences between deformable objects, a fundamental task in computer vision and graphics. Existing methods often struggle with significant deformations, topological noise, and category variations due to limitations in capturing intrinsic shape properties. This paper introduces DiffuMatch, a novel category-agnostic shape matching approach leveraging spectral diffusion geometry and diffusion probabilistic models. DiffuMatch learns a prior distribution over point-wise descriptors, representing the likelihood of a given descriptor to be a valid embedding of the underlying shape's spectral diffusion geometry. Specifically, we train a diffusion model on a diverse set of shapes to generate high-quality descriptors, which are then used to guide the matching process. Experiments on challenging benchmark datasets demonstrate that DiffuMatch achieves state-of-the-art performance in both within-category and cross-category shape matching scenarios, exhibiting robustness to large deformations and topological noise. Our approach significantly improves the accuracy and robustness of non-rigid shape matching by incorporating learned spectral diffusion priors, paving the way for more reliable shape analysis and understanding."
http://arxiv.org/abs/2507.23709v1,Explainable Image Classification with Reduced Overconfidence for Tissue Characterisation,"Deep learning models have achieved remarkable success in image classification tasks, including tissue characterisation from medical images. However, their inherent black-box nature and tendency towards overconfident predictions limit their clinical applicability. This paper addresses the critical need for explainable and reliable image classification models in tissue characterisation by reducing overconfidence and providing insights into model decision-making. We propose a novel framework that integrates a modified cross-entropy loss function incorporating a temperature scaling mechanism to mitigate overconfidence, coupled with a Grad-CAM based explainability module. This framework generates visual explanations highlighting salient image regions contributing to the classification decision. Our experiments on a histopathology dataset demonstrate a significant reduction in model overconfidence, as measured by Expected Calibration Error, while maintaining competitive classification accuracy. Furthermore, the generated Grad-CAM heatmaps provide clinically relevant insights, aligning with expert annotations and improving trust in the model's predictions. This explainable and well-calibrated approach enhances the potential of deep learning for reliable and transparent tissue characterisation in diagnostic pathology."
http://arxiv.org/abs/2507.23704v1,Enhanced Velocity Field Modeling for Gaussian Video Reconstruction,"Gaussian Splatting has emerged as a powerful technique for novel view synthesis, offering real-time rendering and high-quality results. However, accurately modeling motion, especially in dynamic scenes with complex deformations, remains a significant challenge for Gaussian video reconstruction. This paper addresses the limitations of existing methods in capturing intricate velocity fields, which often lead to blurry or distorted reconstructions in regions with rapid or non-rigid motion. We introduce an enhanced velocity field modeling approach that incorporates a learnable deformation field regularized by a smoothness prior and coupled with a novel adaptive sampling strategy for velocity estimation. Our method leverages optical flow information to initialize and guide the learning process, enabling more robust and accurate estimation of Gaussian velocities. Experiments on a diverse set of challenging dynamic scenes demonstrate that our approach significantly improves the quality of video reconstruction, achieving state-of-the-art results in terms of PSNR, SSIM, and LPIPS. The improved velocity field modeling enables faithful representation of complex motions, leading to more realistic and visually appealing dynamic scene reconstructions."
http://arxiv.org/abs/2507.23648v1,Towards Field-Ready AI-based Malaria Diagnosis: A Continual Learning Approach,"Microscopic examination of Giemsa-stained blood smears remains the gold standard for malaria diagnosis, but requires trained personnel and is prone to subjectivity, especially in resource-limited settings. Current deep learning models for automated malaria detection often suffer from performance degradation when deployed in new geographical locations or when faced with variations in staining protocols, due to the domain shift problem. We address this challenge by proposing a continual learning framework that enables a malaria diagnosis model to adapt to new, unseen data distributions without forgetting previously learned knowledge. Our approach leverages a combination of knowledge distillation and dynamic expansion of network capacity to incrementally learn from new datasets while preserving performance on the original training data. Experiments on a diverse collection of malaria slide images from different sources demonstrate that our continual learning approach significantly outperforms fine-tuning and other baseline methods in adapting to new domains, achieving an average F1-score improvement of 15% on the new datasets while maintaining comparable performance on the original data. This work demonstrates the feasibility of deploying robust and adaptable AI-based malaria diagnostic tools in real-world settings where data distributions are constantly evolving."
http://arxiv.org/abs/2507.23609v1,Consistent Point Matching,"Point matching is a fundamental problem in computer vision, enabling tasks such as image registration, 3D reconstruction, and object recognition. However, establishing accurate correspondences between point sets is challenging, especially in the presence of noise, outliers, and geometric distortions. This paper addresses the problem of achieving consistent point matching across multiple views or instances, ensuring that the correspondences respect underlying geometric constraints. We propose a novel graph-based approach that leverages cycle consistency to enforce globally optimal matches. Specifically, we construct a graph where nodes represent points and edges represent potential correspondences weighted by similarity scores. We then formulate the matching problem as a quadratic assignment problem, incorporating cycle consistency constraints to penalize inconsistent loops in the graph. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, showing significant improvements in matching accuracy and robustness compared to existing state-of-the-art techniques. Our approach provides a robust and reliable solution for consistent point matching, facilitating downstream applications that rely on accurate correspondences."
http://arxiv.org/abs/2508.03736v1,Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities,"Smart cities generate vast quantities of data from diverse sources, including radio frequency (RF) signals and visual imagery, offering opportunities for enhanced environmental understanding. However, effectively fusing these heterogeneous data modalities to improve mapping tasks remains a significant challenge. This paper addresses the problem of integrating pervasive RF data, such as Wi-Fi signal strength, with spatial images to create more robust and informative maps in urban environments. We propose a novel fusion architecture based on Vision Transformers (ViTs) to learn joint representations from the two modalities. Specifically, we encode RF data as a pseudo-image and leverage cross-attention mechanisms within the ViT to enable information exchange between RF and visual feature embeddings. The fused representation is then used for tasks such as semantic segmentation and localization. Experimental results on a newly collected dataset demonstrate that our approach significantly outperforms state-of-the-art methods that rely on either single modality or naive fusion techniques, achieving an average improvement of 8% in Intersection over Union (IoU) for semantic segmentation. This work demonstrates the potential of ViT-based multimodal fusion for creating more accurate and comprehensive maps, benefiting a wide range of smart city applications."
http://arxiv.org/abs/2507.23487v1,Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions,"Precise estimation of fruit yield is crucial for optimizing resource allocation and improving harvesting strategies in agriculture. However, accurately assessing the mass of individual fruits like strawberries in real-time field conditions presents significant challenges due to variable lighting, complex backgrounds, and frequent occlusions. This paper addresses the problem of online strawberry mass estimation in table-top grown systems under realistic field conditions where occlusions are prevalent. We propose a novel approach that combines a Mask R-CNN for strawberry detection and segmentation with a multi-view geometry-based 3D reconstruction module. The 3D reconstruction leverages stereo vision to generate a point cloud of each detected strawberry, and a deep learning model trained on synthetic data predicts the mass from the reconstructed point cloud, effectively mitigating the impact of occlusions. Experimental results on a newly collected dataset of table-top grown strawberries demonstrate a significant improvement in mass estimation accuracy compared to methods relying solely on 2D image features, achieving a mean absolute percentage error (MAPE) of 8.2% even with occlusion rates exceeding 30%. This robust and efficient approach offers a valuable tool for automated yield monitoring and precision agriculture applications."
http://arxiv.org/abs/2507.23480v1,FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction,"3D point cloud data offers rich geometric information crucial for various applications, yet its unstructured nature poses significant computational challenges for deep learning inference. Existing point cloud networks often require extensive processing of numerous points, leading to high latency, particularly on resource-constrained devices. This paper addresses the problem of accelerating point cloud model inference by strategically reducing the number of points processed without sacrificing accuracy. We introduce FastPoint, a novel framework that learns to predict the distances between sample points and the underlying object surface. This distance prediction enables informed point sampling, prioritizing points closer to the surface and discarding redundant or noisy points. Furthermore, we employ a lightweight module to refine the selected points, enhancing feature representation and mitigating information loss from the reduced point set. Experimental results on ModelNet40 and ScanObjectNN datasets demonstrate that FastPoint achieves comparable or even superior classification accuracy compared to state-of-the-art methods, while significantly reducing the number of points processed and accelerating inference speed by up to 40%. FastPoint offers a practical and efficient approach for deploying point cloud models in real-time applications and resource-limited environments."
http://arxiv.org/abs/2508.03734v1,A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models,"Ophthalmic diagnostics increasingly rely on multimodal imaging, integrating data from modalities like fundus photography, optical coherence tomography (OCT), and fluorescein angiography to provide a comprehensive view of ocular health. However, the field currently grapples with a fragmented landscape of task-specific models, each tailored for a narrow range of diagnostic applications and struggling to generalize across different datasets and modalities. This survey addresses this problem by providing a structured overview of multimodal ophthalmic diagnostic techniques, ranging from traditional task-specific approaches to emerging foundational models. We categorize existing methods based on their architecture, training strategy, and application, highlighting the strengths and limitations of each. Furthermore, we analyze the recent trend towards self-supervised learning and large language models in ophthalmology, exploring their potential for creating versatile diagnostic tools. Our analysis reveals a significant shift from handcrafted feature engineering and modality-specific architectures towards end-to-end trainable models leveraging large-scale datasets, demonstrating improved performance in tasks like disease detection, segmentation, and progression prediction. This survey offers a comprehensive resource for researchers and clinicians, charting the evolution of multimodal ophthalmic diagnostics and paving the way for the development of more robust and generalizable AI-powered solutions in ophthalmology."
http://arxiv.org/abs/2507.23398v1,Smart Video Capsule Endoscopy: Raw Image-Based Localization for Enhanced GI Tract Investigation,"Video Capsule Endoscopy (VCE) is a minimally invasive procedure for examining the gastrointestinal (GI) tract, but lacks real-time localization capabilities, hindering targeted diagnosis and intervention. This paper addresses the challenge of accurately localizing the VCE capsule within the GI tract using only raw image data, without relying on external sensors or anatomical landmarks. We propose a novel deep learning framework that combines a contrastive learning-based feature extractor with a transformer-based sequence model to learn spatio-temporal representations directly from VCE video frames. The contrastive learning component learns discriminative features robust to illumination variations and motion blur, while the transformer captures long-range dependencies within the video sequence to infer capsule position. Our method achieves state-of-the-art localization accuracy on a large-scale VCE dataset, demonstrating a significant improvement over existing image-based localization techniques. This advancement enables the development of smart VCE systems capable of providing real-time feedback and targeted interventions, ultimately improving diagnostic yield and patient outcomes."
http://arxiv.org/abs/2507.23359v1,Pixel Embedding Method for Tubular Neurite Segmentation,"Precise segmentation of neurites, especially tubular-shaped structures, is crucial for quantitative analysis in neuroscience, enabling insights into neuronal connectivity and function. However, the complex morphology of neurites, combined with image noise and varying intensity, poses significant challenges for existing segmentation techniques. This paper introduces a novel pixel embedding method for robust and accurate tubular neurite segmentation. Our approach learns a low-dimensional embedding space where pixels belonging to the same neurite are clustered together, while pixels from different neurites are separated. We achieve this by training a convolutional neural network to predict pixel-wise embeddings, incorporating a contrastive loss function that encourages intra-neurite compactness and inter-neurite separation. Experiments on both synthetic and real microscopy datasets demonstrate that our method outperforms state-of-the-art segmentation algorithms, achieving significantly higher accuracy and robustness, especially in densely packed and noisy environments. The proposed pixel embedding method offers a powerful tool for automated neurite tracing and analysis, facilitating large-scale studies of neuronal circuits."
http://arxiv.org/abs/2507.23341v1,"The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models","Face detection is a fundamental task in computer vision, serving as a crucial initial step for various applications like facial recognition and video surveillance. The performance of face detection models can be significantly affected by the quality of input images, particularly their resolution. This paper investigates the impact of image resolution on the performance of three state-of-the-art face detection models: MTCNN, YOLOv XI, and YOLOv XII. We conduct a comparative analysis by evaluating the models on a benchmark dataset downsampled to various resolutions, ranging from high-resolution (1024x1024) to low-resolution (64x64). Performance is measured using metrics such as precision, recall, F1-score, and average precision (AP). Our experiments reveal that while all models experience a performance degradation with decreasing resolution, YOLOv XII demonstrates a more robust performance at lower resolutions compared to MTCNN and YOLOv XI, particularly in terms of recall. These findings highlight the importance of considering image resolution when selecting a face detection model for specific applications and provide valuable insights for future research on developing resolution-invariant face detection algorithms."
http://arxiv.org/abs/2507.23315v1,Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification,"Lightweight deep learning models are crucial for real-time image classification on resource-constrained devices. However, achieving optimal accuracy with these models often requires careful tuning of hyperparameters, a computationally expensive and time-consuming process. This paper investigates the impact of hyperparameter optimization (HPO) on the accuracy of lightweight convolutional neural networks (CNNs) designed for real-time image classification. We employ a Bayesian optimization strategy, specifically Tree-structured Parzen Estimator (TPE), to efficiently search the hyperparameter space of MobileNetV3 and ShuffleNetV2, focusing on learning rate, optimizer parameters, and regularization strengths. Furthermore, we incorporate a custom early stopping mechanism tailored to prevent overfitting during HPO. Our experiments on the CIFAR-10 and ImageNet datasets demonstrate that optimized hyperparameters significantly improve the classification accuracy of both MobileNetV3 and ShuffleNetV2, achieving relative gains of up to 5% compared to default settings while maintaining real-time performance. These results highlight the critical role of HPO in maximizing the performance of lightweight models, enabling their wider adoption in real-world, resource-limited applications."
http://arxiv.org/abs/2507.23256v1,EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan Africa using MedNeXt V2 with Deep Supervision,"Brain tumor segmentation is crucial for accurate diagnosis and treatment planning, yet its application in resource-constrained settings like Sub-Saharan Africa (SSA) faces unique challenges due to limited data and computational resources. Existing deep learning models often struggle to generalize effectively to the diverse and underrepresented patient populations in SSA. To address this, we propose EMedNeXt, an enhanced brain tumor segmentation framework specifically designed for the SSA context. EMedNeXt leverages the MedNeXt V2 architecture, known for its efficiency and performance, and integrates deep supervision to improve feature learning and segmentation accuracy with limited data. We further augment the training data with region-specific augmentations to improve the model's robustness to variations in image acquisition and patient demographics prevalent in SSA. Evaluated on a newly curated dataset of brain tumor MRI scans from multiple hospitals in SSA, EMedNeXt achieved a significantly improved Dice score of 0.82 and Hausdorff distance of 12.5 compared to state-of-the-art methods. This work demonstrates the potential of tailored deep learning approaches to improve brain tumor diagnosis and treatment in resource-limited environments, contributing to improved healthcare outcomes in Sub-Saharan Africa."
http://arxiv.org/abs/2507.23225v1,YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection,"Road damage detection is crucial for ensuring road safety and efficient infrastructure maintenance. Existing deep learning-based methods often suffer from high computational costs and large model sizes, hindering their deployment on resource-constrained devices for real-time applications. This paper introduces YOLO-ROC, an ultra-lightweight and high-precision object detection model specifically designed for real-time road damage detection. YOLO-ROC leverages a novel receptive-field-optimized backbone network, incorporating depthwise separable convolutions and efficient channel attention mechanisms to significantly reduce the model's parameter count and computational complexity. Furthermore, we introduce a refined loss function that emphasizes the detection of small and occluded road damage instances, improving overall detection accuracy. Experimental results on a publicly available road damage dataset demonstrate that YOLO-ROC achieves state-of-the-art performance, exhibiting a 5% improvement in mean Average Precision (mAP) compared to existing lightweight models while maintaining a frame rate exceeding 60 FPS on embedded hardware. YOLO-ROC offers a practical and efficient solution for real-time road damage detection, enabling proactive infrastructure management and enhanced road safety."
http://arxiv.org/abs/2507.23129v1,MRpro - open PyTorch-based MR reconstruction and processing package,"Magnetic Resonance Imaging (MRI) is a crucial medical imaging modality, but its data acquisition and reconstruction processes are complex, often relying on proprietary software. This dependency hinders reproducibility and innovation in MRI research. To address this, we introduce MRpro, an open-source PyTorch-based package designed for MRI reconstruction and processing. MRpro provides a modular and extensible framework for implementing and evaluating various reconstruction algorithms, including parallel imaging, compressed sensing, and deep learning-based methods. It offers tools for data simulation, pre-processing (e.g., coil combination, gridding), and post-processing (e.g., image registration, segmentation). We demonstrate MRpro's capabilities through benchmark reconstructions on publicly available datasets, showcasing its performance and flexibility in implementing both conventional and state-of-the-art reconstruction pipelines. MRpro empowers researchers and clinicians with a transparent and customizable platform for advancing MRI technology and accelerating translation to clinical practice."
http://arxiv.org/abs/2507.23110v1,Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation,"Domain generalization (DG) in medical image segmentation aims to develop models robust to distribution shifts across unseen clinical sites or scanner configurations, a crucial requirement for real-world deployment. However, existing DG methods often overlook the unique challenges presented by heterogeneous sequence Magnetic Resonance Imaging (MRI), where variations in pulse sequences (e.g., T1, T2, FLAIR) significantly impact image appearance and feature distributions. This paper addresses the problem of generalizing segmentation models across unseen MRI sequence combinations. We propose a novel Sequence-invariant Feature Disentanglement and Adaptation (SFDA) framework. SFDA first disentangles sequence-specific and sequence-invariant features using a novel adversarial learning strategy coupled with a sequence-aware attention mechanism. It then adapts the sequence-invariant features to the target domain through a meta-learning based domain alignment technique. Experiments on a multi-sequence brain MRI dataset demonstrate that SFDA significantly outperforms existing DG methods, achieving an average Dice score improvement of over 5% on unseen sequence combinations. These results highlight the importance of explicitly addressing sequence heterogeneity for robust domain generalization in MRI segmentation and demonstrate the effectiveness of the proposed SFDA framework."
http://arxiv.org/abs/2507.23027v1,Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging,"Echocardiography is a crucial non-invasive imaging modality for diagnosing cardiovascular diseases, but its diagnostic accuracy is often limited by image quality, particularly in resource-constrained settings where lower resolution transducers are common. This work addresses the challenge of improving diagnostic performance from low-resolution echocardiograms by leveraging super-resolution techniques. We propose a novel framework integrating a deep learning-based super-resolution module with a downstream classification network. Specifically, we employ a residual-in-residual dense block (RRDB) network, pre-trained on a large external dataset of natural images, to enhance the resolution of input echocardiograms before feeding them into a convolutional neural network classifier trained to identify various cardiac conditions. Experimental results on a clinically relevant dataset demonstrate that our super-resolution-aided classification pipeline significantly improves diagnostic accuracy compared to classification directly from low-resolution images, achieving a 7% increase in overall F1-score. This approach has the potential to enhance the clinical utility of echocardiography in resource-limited environments, allowing for more accurate diagnoses with existing imaging infrastructure."
http://arxiv.org/abs/2507.23021v1,Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction,"Human gaze behavior is a complex and stochastic process, crucial for understanding visual attention and intention. Existing scanpath prediction models often struggle to capture the inherent uncertainty and multimodality of gaze patterns, leading to limited performance in diverse visual environments. This paper addresses the challenge of accurately modeling and predicting human gaze scanpaths by proposing a novel diffusion-based framework. Our approach leverages a conditional diffusion model to learn the distribution of gaze trajectories conditioned on visual stimuli and prior gaze locations. Specifically, we formulate scanpath prediction as a denoising process, iteratively refining a noisy trajectory towards a realistic gaze sequence guided by a learned score function. Extensive experiments on several benchmark datasets demonstrate that our model achieves state-of-the-art performance in scanpath prediction, surpassing existing methods in terms of accuracy and diversity of generated gaze patterns. The proposed diffusion-based approach offers a powerful and flexible framework for modeling complex human gaze behavior, paving the way for improved applications in areas such as human-computer interaction, visual analytics, and cognitive science."
http://arxiv.org/abs/2507.23001v1,LesionGen: A Concept-Guided Diffusion Model for Dermatology Image Synthesis,"Dermatology image synthesis is crucial for training robust diagnostic algorithms, especially for rare or under-represented skin conditions. However, existing generative models often lack fine-grained control over lesion characteristics, limiting their utility in creating diverse and clinically relevant datasets. This paper introduces LesionGen, a novel concept-guided diffusion model for synthesizing high-quality dermatology images with precise control over lesion attributes. LesionGen leverages a concept bank of textual embeddings representing different lesion characteristics (e.g., color, texture, shape) and incorporates them into the diffusion process via cross-attention mechanisms. This allows us to directly manipulate the generated lesion's appearance by modulating the influence of specific concepts during image generation. Experiments demonstrate that LesionGen generates realistic dermatology images with superior fidelity and diversity compared to existing GAN-based and diffusion-based methods, as evaluated by Frchet Inception Distance (FID) and perceptual metrics. Furthermore, we show that models trained on LesionGen-synthesized data exhibit improved performance on downstream classification tasks, particularly for challenging skin conditions. LesionGen offers a powerful tool for generating customized and clinically relevant dermatology datasets, accelerating the development and deployment of AI-powered diagnostic solutions."
http://arxiv.org/abs/2507.23000v1,Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation,"Urban heat islands exacerbate the impacts of climate change, posing significant health risks to city dwellers. Accurately predicting and mitigating urban heat stress requires a holistic understanding of complex interactions between urban morphology, climate, and human activity, which current methods often fail to capture. This paper introduces a multimodal AI framework for predicting and mitigating urban heat stress by simulating the impact of urban landscape transformations. Our framework integrates satellite imagery, meteorological data, building footprints, and traffic patterns into a deep learning model that predicts surface temperature with high spatial resolution. Furthermore, we incorporate a reinforcement learning agent that optimizes urban greening strategies, such as tree placement and vegetation cover, based on predicted temperature reductions and various urban planning constraints. Experimental results on a case study in [City Name] demonstrate that our framework can accurately predict surface temperature (RMSE = X.X C) and identify optimal greening strategies that significantly reduce urban heat stress, decreasing average surface temperatures by Y.Y C in targeted areas while adhering to pre-defined budgetary and spatial limitations. This framework provides a powerful tool for urban planners to design cooler, more resilient, and sustainable cities."
http://arxiv.org/abs/2507.22859v1,Mesh based segmentation for automated margin line generation on incisors receiving crown treatment,"Accurate margin line generation is crucial for the design and fabrication of dental crowns, impacting the restoration's fit, aesthetics, and longevity. Manual margin line delineation on 3D scans of prepared teeth is time-consuming and subject to inter-operator variability. This paper addresses the challenge of automating margin line generation on incisors receiving crown treatment by developing a novel mesh-based segmentation approach. Our method leverages a deformable mesh model initialized based on anatomical landmarks and refined through a combination of geometric and curvature-based energy terms. Specifically, the mesh evolves to conform to the prepared tooth surface, while a segmentation algorithm identifies the optimal margin line by analyzing local surface curvature changes and minimizing the distance to user-defined anchor points. Experiments on a dataset of 50 incisor scans demonstrate that our method achieves a mean margin line accuracy of 0.25 mm, significantly reducing the need for manual adjustments and improving the efficiency of the crown design workflow. This automated approach offers a reliable and efficient solution for margin line generation, ultimately contributing to improved dental restoration outcomes."
http://arxiv.org/abs/2507.22832v1,Tapping into the Black Box: Uncovering Aligned Representations in Pretrained Neural Networks,"Pretrained neural networks have demonstrated remarkable success across various computer vision tasks, yet their internal representations remain largely opaque. This work addresses the critical problem of understanding how semantic concepts are encoded and aligned within the layers of these ""black box"" models. We introduce a novel methodology, Aligned Representation Probing (ARP), which combines representational similarity analysis with targeted intervention techniques to identify and manipulate concept-specific subspaces within a network's activation space. ARP involves first identifying layers where representations are most similar to known concept embeddings, then applying targeted adversarial perturbations to isolate and enhance the aligned subspace. Our experiments on several benchmark datasets and network architectures reveal that surprisingly compact, semantically meaningful subspaces exist and can be effectively manipulated to influence downstream task performance, demonstrating a degree of modularity previously unappreciated. These findings offer valuable insights into the internal workings of pretrained neural networks and pave the way for more interpretable and controllable AI systems."
http://arxiv.org/abs/2507.22817v1,Wall Shear Stress Estimation in Abdominal Aortic Aneurysms: Towards Generalisable Neural Surrogate Models,"Wall Shear Stress (WSS) is a critical biomechanical indicator in the progression of Abdominal Aortic Aneurysms (AAAs), offering insights into regions susceptible to rupture. However, obtaining accurate WSS estimations typically requires computationally expensive Computational Fluid Dynamics (CFD) simulations for each patient-specific AAA geometry. This work addresses the challenge of developing a generalisable and computationally efficient method for WSS estimation in AAAs by introducing a neural surrogate model trained on a diverse dataset of AAA geometries and corresponding CFD-derived WSS fields. Our approach leverages a graph neural network (GNN) architecture to directly predict WSS from geometric representations of the AAA, incorporating both local shape features and global contextual information. We demonstrate that our GNN-based surrogate model achieves high accuracy in predicting WSS distributions on unseen AAA geometries, exhibiting a significant speedup compared to traditional CFD simulations while maintaining strong correlation with CFD results (R > 0.85). This enables rapid and reliable WSS estimation, facilitating improved risk assessment and personalized treatment planning for AAA patients."
http://arxiv.org/abs/2507.22813v1,DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion,"Neural network trojans pose a significant threat to the reliability of AI systems, as subtle input perturbations can trigger malicious behavior. Identifying and understanding these trigger inputs is crucial for developing robust defenses. However, existing trojan detection methods often require access to training data or a clean validation set, which may not be available in real-world scenarios. This paper addresses the challenge of data-free trojan trigger inversion, aiming to reconstruct suspicious input patterns that activate the embedded backdoor without relying on any training data. We introduce DISTIL, a novel approach that leverages the generative power of pre-trained latent diffusion models to explore the input space and identify potential triggers. DISTIL utilizes a carefully crafted objective function that combines trojan activation likelihood with the diffusion model's prior, guiding the generation process towards realistic and effective trigger inputs. Experiments on benchmark datasets demonstrate that DISTIL can successfully invert trojan triggers with high attack success rates, even in the absence of training data, outperforming existing data-free methods by a significant margin. Our findings highlight the vulnerability of trojaned models and provide a valuable tool for security auditing and robust AI development."
http://arxiv.org/abs/2507.22791v1,Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques,"Feature matching is a fundamental task in computer vision, enabling applications such as image registration, object recognition, and 3D reconstruction. However, the performance of traditional feature matching algorithms degrades significantly when applied to images acquired from different modalities due to variations in appearance, noise characteristics, and information content. This paper presents a comprehensive review of both single-modality and cross-modality feature matching techniques, focusing on recent advances in addressing the challenges posed by modality differences. We categorize existing methods based on their core strategies, including hand-crafted feature descriptors, learned feature embeddings, generative adversarial networks for modality synthesis, and domain adaptation techniques. We then provide a detailed analysis of their strengths and weaknesses, highlighting the key factors influencing their performance across various modality pairs. Furthermore, we critically evaluate the performance of representative methods on publicly available datasets, providing a quantitative comparison of their accuracy and robustness. This review identifies promising research directions and serves as a valuable resource for researchers seeking to develop more effective and robust feature matching algorithms for multi-modal computer vision applications."
http://arxiv.org/abs/2507.22699v1,Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints,"Shape-from-template methods offer a powerful approach to 3D reconstruction by deforming a known template mesh to align with observed data. However, accurate and robust deformation, especially under significant viewpoint changes and partial occlusions, remains a challenge. This paper addresses the problem of reconstructing accurate 3D shapes from single-view images by leveraging a template mesh and enforcing inextensibility constraints during deformation. Our method combines image-based photometric error with a novel inextensibility energy term that penalizes deviations from the template mesh's original edge lengths. This energy is incorporated into an optimization framework that iteratively refines the mesh vertices to minimize both the photometric error and the inextensibility violation. We demonstrate that our approach improves reconstruction accuracy and robustness compared to existing methods, particularly in scenarios with significant perspective distortion and texture-less regions. The results show a significant reduction in reconstruction error on benchmark datasets and real-world images, suggesting the effectiveness of inextensibility constraints in guiding shape-from-template algorithms. This work provides a valuable contribution to the field by enabling more reliable and accurate 3D reconstruction from 2D images."
http://arxiv.org/abs/2507.22675v1,MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model,"Unsupervised change detection in remote sensing imagery is crucial for monitoring environmental changes and urban development. However, existing methods often struggle with complex scenes and require extensive parameter tuning. This paper addresses the challenge of unsupervised change detection by leveraging the Segment Anything Model (SAM), a powerful zero-shot image segmentation model, in a novel framework called MergeSAM. Our approach first extracts multi-scale features from bi-temporal remote sensing images. Then, SAM is applied to each image to generate semantic segmentation masks. These masks are then merged based on spectral similarity and spatial proximity to create a unified representation of each image. A change map is generated by comparing these merged segmentations using a modified fuzzy entropy difference measure. Experimental results on several real-world remote sensing datasets demonstrate that MergeSAM achieves competitive or superior performance compared to state-of-the-art unsupervised change detection methods, particularly in complex urban environments. The proposed framework offers a robust and adaptable solution for unsupervised change detection, reducing the need for extensive training data and manual parameter adjustments."
http://arxiv.org/abs/2507.22635v1,trAIce3D: A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images,"Semantic segmentation of microglial cells in large-scale 3D microscopy images is crucial for understanding neuroinflammation and various neurological disorders. However, the inherent complexity of microglial morphology and the sheer size of 3D datasets pose significant challenges for accurate and efficient segmentation. We introduce trAIce3D, a novel prompt-driven transformer-based U-Net architecture for semantic segmentation of microglial cells in large-scale 3D microscopy images. trAIce3D leverages a 3D U-Net backbone enhanced with transformer modules to capture long-range dependencies and global context within the volumetric data. Furthermore, we incorporate a prompt-driven mechanism, allowing users to guide the segmentation process by providing sparse annotations or cues, thereby improving accuracy and reducing the need for extensive manual labeling. Experiments on a large-scale dataset of mouse brain tissue demonstrate that trAIce3D achieves state-of-the-art segmentation performance, surpassing existing methods in terms of Dice score and Hausdorff distance, while also exhibiting improved efficiency in handling large 3D volumes. This work provides a powerful and user-friendly tool for automated and accurate analysis of microglial morphology in 3D microscopy, facilitating deeper insights into neuroinflammatory processes."
http://arxiv.org/abs/2507.22527v1,FGFP: A Fractional Gaussian Filter and Pruning for Deep Neural Networks Compression,"Deep neural networks (DNNs) have achieved remarkable success in various computer vision tasks, but their high computational cost and large memory footprint hinder their deployment on resource-constrained devices. To address this challenge, we introduce a novel compression technique, FGFP, which combines a Fractional Gaussian Filter (FGF) with a structured pruning strategy. Specifically, FGF leverages fractional calculus to construct filters that capture finer details and smoother variations in feature maps, leading to improved representational power with fewer parameters. We then employ a structured pruning method that selectively removes entire filters based on their contribution to the network's overall performance, guided by the FGF's learned importance scores. Experiments on benchmark datasets, including CIFAR-10 and ImageNet, demonstrate that FGFP achieves significant compression ratios (up to 10x) with minimal accuracy degradation compared to state-of-the-art pruning techniques. This allows for efficient deployment of high-performing DNNs on edge devices, facilitating real-time applications with limited resources."
http://arxiv.org/abs/2507.22446v1,RCR-AF: Enhancing Model Generalization via Rademacher Complexity Reduction Activation Function,"Deep neural networks often struggle to generalize well to unseen data, particularly when training and testing distributions diverge. This limitation stems from the model's capacity to memorize training data, leading to overfitting and poor performance on new examples. To address this issue, we introduce RCR-AF, a novel Rademacher Complexity Reduction Activation Function designed to improve model generalization by explicitly minimizing the empirical Rademacher complexity during training. RCR-AF dynamically adjusts its activation based on the local data distribution, promoting smoother decision boundaries and reducing the effective model complexity. This is achieved through a learnable parameter that modulates the non-linearity, guided by a complexity regularization term added to the standard loss function. Experiments on several benchmark datasets, including CIFAR-10, CIFAR-100, and a domain adaptation task using MNIST-M, demonstrate that RCR-AF consistently improves generalization performance compared to standard activation functions like ReLU and ELU, particularly in scenarios with noisy labels or domain shift. These results highlight the effectiveness of directly minimizing Rademacher complexity as a strategy to enhance the robustness and generalization capabilities of deep neural networks."
http://arxiv.org/abs/2507.22407v1,Moir Zero: An Efficient and High-Performance Neural Architecture for Moir Removal,"Moir patterns, caused by interference between a camera's sensor and repeating patterns in the captured scene, significantly degrade image quality and hinder downstream computer vision tasks. Removing these artifacts effectively remains a challenging problem, particularly when computational resources are limited. This paper introduces Moir Zero, a novel and efficient neural architecture designed for high-performance moir removal. Our approach leverages a lightweight U-Net backbone incorporating frequency separation and attention mechanisms to effectively disentangle moir patterns from underlying image content. Specifically, we decompose the input image into high-frequency and low-frequency components, process them separately with tailored residual blocks, and employ channel and spatial attention to refine the moir suppression. Extensive experiments on benchmark datasets demonstrate that Moir Zero achieves state-of-the-art moir removal performance with significantly fewer parameters and computational cost compared to existing methods. The proposed architecture offers a practical solution for real-world applications demanding efficient and effective moir artifact reduction."
http://arxiv.org/abs/2507.22361v1,Object Recognition Datasets and Challenges: A Review,"Object recognition is a cornerstone of computer vision, enabling machines to perceive and interact with the visual world. The progress in this field is intrinsically linked to the availability of comprehensive and challenging datasets. This paper addresses the critical need for a structured overview of prominent object recognition datasets and the inherent challenges they present to current state-of-the-art algorithms. We conduct a systematic review of existing datasets, categorizing them based on their characteristics, annotation methodologies (e.g., bounding boxes, segmentation masks, 3D models), and the specific recognition tasks they support (e.g., classification, detection, instance segmentation). Furthermore, we analyze the challenges associated with each dataset, including issues like intra-class variation, occlusion, viewpoint changes, and domain shift. Our analysis reveals that while significant strides have been made, challenges remain in handling fine-grained recognition, few-shot learning, and adapting to real-world scenarios with complex and noisy data. This review provides a valuable resource for researchers and practitioners, guiding dataset selection and highlighting critical areas for future research to advance robust and generalizable object recognition systems."
http://arxiv.org/abs/2507.22336v1,A Segmentation Framework for Accurate Diagnosis of Amyloid Positivity without Structural Images,"Amyloid deposition in the brain is a hallmark of Alzheimer's disease, typically assessed using Positron Emission Tomography (PET) imaging. However, accurate diagnosis of amyloid positivity often relies on anatomical co-registration with structural Magnetic Resonance Imaging (MRI) to correct for partial volume effects and anatomical variability, limiting its accessibility in resource-constrained settings or retrospective studies lacking MRI. This paper addresses the challenge of accurate amyloid positivity classification directly from PET images, without requiring structural MRI. We propose a novel segmentation framework leveraging a deep learning architecture incorporating a 3D U-Net with attention mechanisms trained on a large, multi-center dataset of amyloid PET scans and corresponding amyloid status. The network directly segments the PET image into regions indicative of amyloid accumulation, guided by attention layers focusing on relevant features for accurate diagnosis. Our results demonstrate that the proposed framework achieves comparable or superior performance to methods requiring MRI co-registration, reaching an AUC of 0.92 in classifying amyloid positivity on a held-out test set. This MRI-free segmentation approach offers a more accessible and cost-effective method for amyloid positivity assessment, broadening the applicability of amyloid imaging in clinical research and practice."
http://arxiv.org/abs/2507.22274v1,HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification,"Retinal image analysis is crucial for early diagnosis and management of various ocular diseases. While Convolutional Neural Networks (CNNs) have shown promising results in retinal image classification, they often require large datasets and can be sensitive to variations in image quality and subtle pathological features. This paper addresses the challenge of improving the performance and robustness of CNNs in retinal image classification, particularly when dealing with limited data and nuanced retinal pathologies. We propose HOG-CNN, a novel hybrid approach that integrates Histogram of Oriented Gradients (HOG) features with CNNs. Specifically, HOG features are extracted from pre-processed retinal images and used to create feature maps, which are then concatenated with the initial layers of a pre-trained CNN. This allows the network to leverage both the learned representations from the CNN and the explicit, interpretable gradient information captured by HOG. Experimental results on publicly available datasets for diabetic retinopathy and glaucoma classification demonstrate that HOG-CNN achieves significantly improved accuracy and F1-score compared to baseline CNN models and other feature-based methods, especially in scenarios with limited training data. The proposed method offers a more effective and data-efficient approach to retinal image classification, potentially leading to more reliable and accessible diagnostic tools."
http://arxiv.org/abs/2507.22194v1,Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception,"Unsupervised semantic segmentation is crucial for enabling robots to autonomously understand their environment without relying on labeled data. However, existing unsupervised methods often struggle with temporal inconsistency, leading to flickering segmentations that hinder downstream tasks like navigation and object interaction. This paper addresses the challenge of achieving temporally consistent unsupervised semantic segmentation for mobile robot perception. We introduce a novel framework that combines a contrastive learning-based segmentation network with a temporal regularization module. Our approach leverages optical flow to propagate segmentation masks across frames, enforcing consistency by penalizing changes in segment assignments for corresponding pixels. Additionally, we incorporate a self-supervision loss based on motion coherence to improve segmentation accuracy and robustness. Experiments on the TartanAir and Cityscapes datasets demonstrate that our method significantly reduces temporal instability compared to state-of-the-art unsupervised segmentation techniques, while maintaining competitive segmentation performance. This leads to more stable and reliable environment representations, paving the way for improved robot autonomy in dynamic environments."
http://arxiv.org/abs/2507.22152v1,Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset,"Accurate and efficient segmentation of paediatric brain tumours is crucial for diagnosis, treatment planning, and monitoring disease progression. However, the diverse range of tumour types and locations, coupled with limited labelled data, poses a significant challenge for developing robust automated segmentation methods. This work addresses the need for efficient and accurate segmentation models that can generalize across the pathological heterogeneity observed in paediatric brain tumours. We propose a novel deep learning framework incorporating a modified U-Net architecture with attention mechanisms and a multi-scale feature fusion strategy. Furthermore, we leverage a pathologically diverse single-center clinical dataset to train and evaluate our model, incorporating data augmentation techniques to enhance robustness and generalization. Our method achieves state-of-the-art performance on our dataset, demonstrating significant improvements in Dice score, Hausdorff distance, and inference time compared to existing methods. This improved efficiency and accuracy can potentially facilitate clinical workflows and improve patient outcomes by enabling faster and more reliable tumour volume estimation."
http://arxiv.org/abs/2507.22101v1,"AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock","Artificial intelligence (AI) is rapidly transforming various sectors, and agriculture is no exception, offering solutions to enhance efficiency, productivity, and sustainability. However, the breadth of AI applications within agriculture, encompassing crops, fisheries, and livestock, remains fragmented across numerous studies, hindering a holistic understanding of the field's potential. This survey comprehensively reviews deep learning techniques applied to these three key agricultural domains, providing a structured overview of current research and identifying promising avenues for future exploration. We systematically analyze literature focusing on computer vision-based deep learning models for tasks such as crop disease detection, yield prediction, fish species identification, livestock health monitoring, and automated feeding systems. Our analysis encompasses model architectures, datasets utilized, performance metrics achieved, and limitations encountered. This survey reveals a strong trend towards convolutional neural networks (CNNs) for image-based tasks, while recurrent neural networks (RNNs) and transformers are gaining traction for time-series data analysis in livestock monitoring. Furthermore, we observe a growing interest in transfer learning and domain adaptation techniques to address data scarcity and variability in agricultural environments. This review serves as a valuable resource for researchers and practitioners seeking to leverage deep learning for advancing sustainable and efficient agricultural practices."
http://arxiv.org/abs/2507.22099v1,Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?,"Physics engine based software systems (PES), ubiquitous in robotics, gaming, and simulation, often suffer from runtime failures, leading to unpredictable and potentially catastrophic outcomes. Identifying the root causes of these failures is challenging due to the complex interactions between the physics engine, the environment, and the control algorithms. This paper investigates the limits of automated failure hunting in PES, exploring the effectiveness of different search strategies in uncovering failure-inducing parameter configurations. We propose a novel hybrid approach that combines gradient-based optimization with evolutionary algorithms to efficiently navigate the high-dimensional parameter space of PES. Our method leverages differentiable physics engines to estimate gradients and guide the search towards failure regions, while the evolutionary component ensures exploration of diverse configurations, mitigating the risk of getting trapped in local minima. Experimental results on several benchmark PES scenarios demonstrate that our hybrid approach significantly outperforms state-of-the-art failure hunting techniques, achieving a higher failure detection rate with fewer simulation runs. This work provides valuable insights into the capabilities and limitations of automated failure hunting in PES, paving the way for more robust and reliable physics-based systems."
http://arxiv.org/abs/2507.22041v1,Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning,"Fine-grained few-shot learning (FGFSL) presents a significant challenge due to the compounded difficulties of subtle inter-class variations and limited data availability. Existing approaches often rely on complex deep architectures and meta-learning strategies to address this problem, leading to increased computational costs and potential overfitting. This paper challenges the prevailing trend by demonstrating that carefully designed shallow convolutional neural networks can achieve competitive, and in some cases superior, performance in FGFSL tasks. We propose a novel shallow network architecture coupled with a tailored training strategy that emphasizes feature disentanglement and robust metric learning. Specifically, our method utilizes a shallow, wide convolutional network with a bottleneck attention module to extract discriminative features, followed by a cosine similarity-based classifier optimized with a margin-ranking loss. Extensive experiments on benchmark FGFSL datasets, including CUB-200-2011 and Stanford Dogs, demonstrate that our shallow model achieves comparable or better accuracy than many deeper and more complex meta-learning approaches, while significantly reducing computational demands. These findings highlight the potential of shallow deep learning for resource-constrained applications and suggest that architectural simplicity, when combined with appropriate training methodologies, can be a powerful tool for tackling challenging computer vision problems."
http://arxiv.org/abs/2507.21985v1,ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models,"Deep neural networks are vulnerable to adversarial attacks, even in scenarios where the model has not been explicitly trained on the target data. This paper addresses the challenge of crafting zero-shot adversarial attacks that are both effective and aligned with a specific, attacker-defined intent, targeting models that have not seen the attacked instance during training (unlearned models). We introduce ZIUM, Zero-shot Intent-aware adversarial attack on Unlearned Models, a novel approach that leverages a combination of pre-trained vision-language models and gradient-free optimization techniques to generate perturbations. ZIUM first extracts a semantic representation of the desired attack intent. It then employs a particle swarm optimization (PSO) strategy, guided by the intent representation, to iteratively refine adversarial perturbations. Our experiments on ImageNet and other benchmark datasets demonstrate that ZIUM can successfully generate adversarial examples that reliably induce targeted misclassifications in unlearned models, achieving significantly higher attack success rates compared to baseline zero-shot methods while maintaining strong visual fidelity. This work highlights a critical vulnerability in modern deep learning systems and underscores the need for robust defense mechanisms against intent-driven, zero-shot adversarial threats."
http://arxiv.org/abs/2507.21968v1,A Deep Learning Pipeline Using Synthetic Data to Improve Interpretation of Paper ECG Images,"Electrocardiograms (ECGs) recorded on paper remain a significant part of clinical practice, particularly in resource-limited settings. However, interpreting digitized paper ECGs is challenging due to variations in paper quality, scanning artifacts, and inherent degradation over time. This paper addresses the problem of accurately interpreting low-quality paper ECG images by developing a robust deep learning pipeline trained primarily on synthetic data. Our method leverages a generative model to create a large, diverse dataset of realistic ECG waveforms with simulated paper degradation, noise, and scanning distortions. A convolutional neural network is then trained on this synthetic dataset for ECG classification and feature extraction, followed by fine-tuning on a limited set of real, annotated paper ECGs. Experimental results demonstrate that our approach significantly improves the accuracy of ECG interpretation on real paper ECG images, achieving a 15% improvement in F1-score compared to models trained solely on real data or augmented real data. This synthetic data-driven approach offers a practical and scalable solution for enhancing the usability of archived and newly acquired paper ECG records in various clinical applications."
http://arxiv.org/abs/2507.21945v1,Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment,"Long-term Action Quality Assessment (AQA) is crucial for evaluating complex activities like gymnastics or diving, where performance unfolds over extended durations. Existing methods often struggle to effectively fuse information from different modalities and capture long-range temporal dependencies, hindering accurate quality prediction. This paper addresses the challenge of aligning multimodal features across long temporal horizons for improved AQA. We propose an Attention-Driven Multimodal Alignment (ADMA) framework, which leverages self-attention mechanisms to dynamically weight and align visual and kinematic features at each time step. ADMA further incorporates a hierarchical temporal modeling module to capture dependencies across varying temporal scales, enabling a comprehensive understanding of the action's progression. Experimental results on the large-scale AQA-7 and AQA-8 datasets demonstrate that ADMA significantly outperforms state-of-the-art methods, achieving improvements of up to 5% in performance metrics. This highlights the effectiveness of our attention-driven alignment strategy in capturing crucial inter-modal relationships and temporal dynamics for robust long-term AQA."
http://arxiv.org/abs/2507.21917v1,ArtSeek: Deep artwork understanding via multimodal in-context reasoning and late interaction retrieval,"Understanding artwork remains a challenging task due to its inherent complexity, encompassing visual style, artistic movements, and contextual information. Existing methods often struggle with integrating diverse data modalities and reasoning about intricate relationships within artwork. This paper introduces ArtSeek, a novel framework for deep artwork understanding that leverages multimodal in-context reasoning and late interaction retrieval. ArtSeek employs a transformer-based architecture to process visual features extracted from artwork images alongside textual information such as artist biographies and artwork descriptions. Crucially, it utilizes in-context learning to dynamically adapt to different artwork understanding tasks by conditioning on a set of exemplar artworks. Furthermore, ArtSeek incorporates a late interaction retrieval mechanism that allows for fine-grained comparison between the query artwork and retrieved contextual information, enhancing the overall understanding process. Experimental results on a diverse dataset of artwork demonstrate that ArtSeek significantly outperforms state-of-the-art methods in tasks such as artwork classification, style recognition, and artist attribution, achieving a substantial improvement in accuracy and F1-score. ArtSeek provides a powerful and versatile approach to deep artwork understanding, paving the way for more sophisticated art analysis and appreciation tools."
http://arxiv.org/abs/2507.21912v2,Predict Patient Self-reported Race from Skin Histological Images,"Skin cancer disproportionately affects individuals of different racial and ethnic groups, highlighting the importance of equitable dermatological care. However, self-reported race, a crucial factor in tailoring treatment and understanding disease prevalence, is often unavailable in retrospective histological datasets. This paper investigates the feasibility of predicting patient self-reported race directly from skin histological images using deep learning. We propose a multi-stage training approach leveraging a convolutional neural network (CNN) architecture. First, the CNN is pre-trained on a large-scale, publicly available image dataset for feature extraction. Subsequently, the pre-trained model is fine-tuned on a dataset of skin histological images annotated with self-reported race, employing data augmentation techniques to mitigate potential biases and improve generalization. Our experimental results demonstrate that the proposed method achieves a promising accuracy in predicting self-reported race from skin histological images, exceeding baseline methods by a significant margin. This capability could facilitate retrospective studies on health disparities and improve the understanding of racial variations in skin disease pathology, ultimately contributing to more equitable healthcare outcomes."
http://arxiv.org/abs/2507.21863v1,VidFuncta: Towards Generalizable Neural Representations for Ultrasound Videos,"Ultrasound (US) imaging is a widely used medical diagnostic tool, but its interpretation relies heavily on specialized training. Developing automated systems to analyze US videos requires robust feature representations that can generalize across diverse anatomical regions and scanning protocols. This paper addresses the challenge of learning generalizable neural representations from US videos, a problem exacerbated by the inherent speckle noise, limited labeled data, and high inter-patient variability in US imaging. We introduce VidFuncta, a novel approach that combines contrastive learning with functional data analysis (FDA) to extract spatiotemporal features robust to noise and variations. VidFuncta first encodes individual frames using a convolutional neural network, then aggregates temporal information via FDA, representing video segments as smooth functional curves. Contrastive learning is then applied to these functional representations to learn embeddings that group semantically similar videos while separating dissimilar ones. Experiments on a diverse dataset of US videos demonstrate that VidFuncta outperforms state-of-the-art video representation learning methods in downstream tasks such as anatomical structure classification and pathology detection, achieving significant improvements in accuracy and robustness. This work paves the way for more reliable and generalizable AI-powered diagnostic tools for ultrasound imaging."
http://arxiv.org/abs/2507.21761v1,MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions,"Vision Transformers (ViTs) have achieved remarkable performance in various computer vision tasks but often suffer from high computational costs due to the quadratic complexity of self-attention. This computational burden hinders their deployment in resource-constrained environments. To address this, we propose MOR-ViT, an efficient ViT architecture leveraging a novel Mixture-of-Recursions (MoR) block. The MoR block replaces the standard self-attention mechanism with a mixture of multiple lightweight recursive convolutional modules operating at different depths. This allows the model to capture both short-range and long-range dependencies with significantly reduced computational complexity. Each recursive module learns to propagate information across different spatial locations through a shared convolutional kernel, enabling efficient feature aggregation. Experiments on ImageNet classification demonstrate that MOR-ViT achieves comparable or superior accuracy to state-of-the-art ViT models while significantly reducing computational costs, achieving up to 2x faster inference speed and a substantial reduction in parameter count. Our results highlight the potential of recursive convolutions as an efficient alternative to self-attention in ViTs, paving the way for deploying high-performance vision models on edge devices."
http://arxiv.org/abs/2507.21756v1,LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection,"Driver fatigue is a significant contributor to traffic accidents, necessitating robust and real-time detection systems. Existing methods often rely on computationally expensive deep learning models, hindering their deployment in resource-constrained environments. This paper addresses the challenge of achieving accurate and efficient driver fatigue detection suitable for real-time applications. We propose LiteFat, a novel lightweight spatio-temporal graph learning framework. LiteFat constructs a dynamic facial landmark graph to capture subtle facial expression changes indicative of fatigue. A pruned graph convolutional network (GCN) then extracts spatial features, followed by a lightweight temporal module that leverages efficient recurrent units to model temporal dependencies. Experiments on benchmark datasets demonstrate that LiteFat achieves state-of-the-art performance with significantly reduced computational complexity, exhibiting a 5x speedup compared to existing GCN-based methods while maintaining comparable accuracy. This makes LiteFat a practical and effective solution for real-time driver fatigue detection systems."
http://arxiv.org/abs/2507.22092v1,Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss,"Foundation models in digital pathology hold immense promise for democratizing diagnostic capabilities; however, their generalization across different scanners remains a significant hurdle. This paper identifies and quantifies the scanner-specific bias inherent in pathology foundation models, leading to degraded performance on unseen scanners. To address this, we introduce a novel Contrastive ScanGen Loss during pre-training, which encourages the model to learn scanner-invariant features by contrasting representations from different scanners while simultaneously generating realistic slide representations. Our comprehensive benchmark, utilizing a diverse set of whole slide images from multiple scanners and diagnostic tasks, demonstrates that models pre-trained with our Contrastive ScanGen Loss exhibit significantly improved generalization performance on unseen scanners compared to standard pre-training, achieving up to a 15% improvement in diagnostic accuracy. This work highlights the critical need for scanner-aware training strategies in pathology foundation models and provides an effective solution for improving their real-world applicability."
http://arxiv.org/abs/2507.21649v1,The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM,"Video anomaly detection (VAD) is a critical task in computer vision, enabling automated surveillance and proactive security measures. Traditional deep neural network (DNN)-based methods often struggle with generalizing to unseen anomalies and require extensive training data. This paper addresses the limitations of existing VAD techniques by proposing a unified framework that leverages the power of Multi-Modal Large Language Models (MLLMs) to enhance anomaly detection capabilities. Our framework integrates visual features extracted from pre-trained DNNs with textual descriptions of normal events, creating a rich contextual representation that MLLMs can effectively reason about. We introduce a novel prompt engineering strategy that guides the MLLM to identify deviations from expected behavior based on both visual and textual cues, enabling zero-shot or few-shot anomaly detection. Experimental results on benchmark datasets demonstrate that our MLLM-based framework significantly outperforms state-of-the-art DNN methods, particularly in detecting novel and complex anomalies. This work paves the way for more robust and adaptable video anomaly detection systems by harnessing the reasoning capabilities of MLLMs, reducing reliance on extensive labeled data, and improving generalization to unseen scenarios."
http://arxiv.org/abs/2507.21611v1,Wind Turbine Feature Detection Using Deep Learning and Synthetic Data,"Wind turbines play a crucial role in renewable energy generation, and automated visual inspection is essential for their efficient maintenance. However, the limited availability of labeled real-world data for training robust feature detection models presents a significant challenge. This paper addresses the problem of accurate and reliable detection of critical features on wind turbines, such as bolts, leading edges, and nacelle components, using deep learning. We propose a novel approach that leverages synthetically generated data to train a convolutional neural network (CNN)-based object detection model. Our pipeline incorporates photorealistic rendering of wind turbine models under varying environmental conditions and camera perspectives, combined with domain randomization techniques to improve generalization to real-world images. The resulting model is then fine-tuned on a small set of real-world labeled images using transfer learning. Experimental results demonstrate that our approach achieves state-of-the-art performance in detecting key features on wind turbines, significantly outperforming models trained solely on real-world data. This research provides a cost-effective and scalable solution for automated wind turbine inspection, facilitating predictive maintenance and reducing operational costs."
http://arxiv.org/abs/2507.21588v1,Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning,"Multi-modal incremental learning aims to continuously adapt models to new tasks while mitigating catastrophic forgetting of previously learned knowledge. However, directly applying existing incremental learning strategies to audio-visual data presents challenges due to the complex cross-modal relationships and the need to maintain balance between stability and plasticity. This paper addresses the problem of catastrophic forgetting in audio-visual multi-task incremental learning by proposing a novel Progressive Homeostatic and Plastic Prompt Tuning (PHPPT) framework. PHPPT leverages prompt tuning to adapt pre-trained audio and visual encoders to new tasks while preserving prior knowledge. It introduces a homeostatic regulation mechanism that dynamically adjusts the learning rate of prompts based on task similarity and forgetting levels, promoting stability. Furthermore, a progressive prompt expansion strategy gradually increases the number of trainable parameters as new tasks are encountered, enhancing plasticity without compromising efficiency. Experiments on challenging audio-visual datasets demonstrate that PHPPT significantly outperforms state-of-the-art incremental learning methods in terms of both average accuracy and forgetting metrics. The proposed method provides a robust and efficient solution for continual learning in dynamic audio-visual environments, paving the way for more adaptable and long-lived AI systems."
http://arxiv.org/abs/2507.21455v2,"Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation","Dataset distillation aims to synthesize a small, informative dataset from a large, original dataset, enabling efficient training of machine learning models. However, distilling effective datasets for self-supervised learning (SSL) remains challenging due to the complexity of capturing the underlying data distribution without labels. This paper addresses the problem of improving the efficacy of distilled datasets for SSL by introducing a novel distillation framework that leverages parameterization, predefined augmentation policies, and gradient approximation techniques. Our method parameterizes the distilled dataset images and optimizes them using a bi-level optimization process, where the inner level trains a student model on the distilled set and the outer level updates the distilled images to minimize the student model's loss on the original dataset when trained with predefined, fixed augmentation policies. Furthermore, we employ gradient approximation techniques to reduce the computational burden of the bi-level optimization. Empirical results on benchmark datasets demonstrate that our approach significantly outperforms existing dataset distillation methods for SSL, achieving superior downstream task performance with distilled datasets of comparable size. This work provides a practical and effective strategy for creating compact, high-quality distilled datasets for self-supervised representation learning, facilitating efficient model training and deployment in resource-constrained environments."
http://arxiv.org/abs/2507.21364v1,Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers,"Accurate and automated identification of African wildlife species is crucial for biodiversity monitoring, conservation efforts, and combating poaching. However, the task remains challenging due to variations in pose, lighting conditions, background clutter, and the sheer diversity of species. This paper addresses the problem of evaluating the performance of various deep learning architectures for classifying images of African wildlife. We systematically investigate the effectiveness of Convolutional Neural Networks (CNNs), specifically DenseNet121, ResNet50, and EfficientNetB0, and compare their performance against Vision Transformers (ViT) and Swin Transformers, pre-trained on ImageNet and fine-tuned on a curated dataset of African wildlife images encompassing diverse species and environmental conditions. Our experiments demonstrate that while CNNs achieve respectable accuracy, the Swin Transformer outperforms all other models, achieving a top-1 accuracy of 87.5% on our test set, demonstrating its superior ability to capture long-range dependencies and fine-grained features essential for accurate species identification. These findings highlight the potential of transformer-based architectures for advancing automated wildlife monitoring and conservation strategies."
http://arxiv.org/abs/2507.21349v1,Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging,"Brain Magnetic Resonance Imaging (MRI) plays a crucial role in diagnosing and monitoring neurological disorders, but long scan times can lead to patient discomfort and increased costs. This paper addresses the problem of accelerating brain MRI acquisition while maintaining high image quality, particularly in longitudinal studies where prior subject-specific scans are available. We propose a novel deep learning reconstruction framework, Prior-Guided Deep Reconstruction Network (PG-DRN), which leverages previously acquired high-quality MRI data of the same subject to enhance the reconstruction of accelerated acquisitions. PG-DRN incorporates a registration module to align prior images with the current under-sampled k-space data, followed by a deep convolutional neural network that fuses the prior information with the under-sampled data to generate a high-quality reconstruction. Experimental results on publicly available datasets demonstrate that PG-DRN significantly outperforms state-of-the-art reconstruction methods, achieving higher Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) while effectively reducing artifacts. This approach offers a promising avenue for reducing MRI scan times and improving image quality, particularly beneficial in longitudinal studies and clinical settings where prior imaging data is readily available."
http://arxiv.org/abs/2507.21247v1,Dual Guidance Semi-Supervised Action Detection,"Action detection, aiming to identify and localize human actions within videos, remains a challenging task, especially when labeled data is scarce. Semi-supervised learning offers a promising avenue to leverage abundant unlabeled videos, yet effectively exploiting the information in unlabeled data for action detection is still an open problem. This paper introduces a novel Dual Guidance Semi-Supervised Action Detection (DGSAD) framework. Our method leverages two complementary guidance strategies: a pseudo-label guidance that refines noisy pseudo-labels generated from a teacher model through a consistency learning approach, and a feature-level guidance that encourages the student model to align its feature representations with those of the teacher model in a self-distillation manner. Specifically, we employ a temporal-aware attention mechanism within the feature-level guidance to emphasize relevant action-related features. Experimental results on the widely used AVA dataset demonstrate that DGSAD significantly outperforms existing semi-supervised action detection methods, achieving state-of-the-art performance with a considerable margin under various labeled data ratios. The proposed dual guidance mechanism effectively exploits unlabeled data, paving the way for more robust and efficient action detection systems in real-world scenarios."
http://arxiv.org/abs/2507.21246v1,On Explaining Visual Captioning with Hybrid Markov Logic Networks,"Visual captioning models have achieved impressive performance in generating descriptive sentences for images, yet understanding the reasoning behind their predictions remains a significant challenge. This paper addresses the problem of explaining the decision-making process of visual captioning models, specifically focusing on identifying the visual and semantic cues that contribute to the generated captions. We propose a novel approach leveraging Hybrid Markov Logic Networks (HMLNs) to represent the complex relationships between image regions, objects, attributes, and words in the generated caption. Our HMLN integrates visual features extracted from a pre-trained object detector with semantic knowledge derived from a language model and external knowledge bases, allowing us to infer the most probable explanations for the captioning model's output by identifying the weighted logical rules that are activated. Experiments on benchmark datasets demonstrate that our approach can generate more coherent and informative explanations compared to existing attention-based methods, revealing the underlying reasoning process of the captioning model and highlighting the importance of specific visual elements and their semantic relationships. This work provides a valuable tool for improving the transparency and trustworthiness of visual captioning systems."
http://arxiv.org/abs/2507.21045v2,Reconstructing 4D Spatial Intelligence: A Survey,"Understanding and reconstructing the dynamic 3D world, often referred to as 4D spatial intelligence, is a fundamental challenge in computer vision with applications ranging from autonomous navigation to augmented reality. This survey addresses the problem of systematically categorizing and analyzing the diverse approaches employed to reconstruct and interpret dynamic 3D scenes from various sensor modalities. We provide a comprehensive review of existing techniques, organizing them based on their underlying principles, including multi-view geometry, deep learning-based methods, and hybrid approaches that combine both. Further, we analyze these methods based on their ability to handle challenges such as occlusions, complex object motion, and varying lighting conditions. Our analysis reveals a trend towards end-to-end deep learning methods for robust and efficient 4D reconstruction, while highlighting the continued importance of geometric priors for improved accuracy and generalization. This survey serves as a valuable resource for researchers seeking to navigate the rapidly evolving field of 4D spatial intelligence and identify promising avenues for future research."
http://arxiv.org/abs/2507.21205v1,Learning from Limited and Imperfect Data,"Deep learning models have achieved remarkable success in various computer vision tasks, but their performance heavily relies on large, perfectly labeled datasets, which are often unavailable in real-world scenarios. This paper addresses the challenge of training robust and accurate computer vision models when faced with both limited training data and imperfect labels. We introduce a novel meta-learning framework, ""Meta-LID,"" that learns to adaptively weight training examples based on their estimated quality and representativeness. Meta-LID employs a bi-level optimization strategy: an inner loop optimizes the main model using weighted examples, while an outer loop learns the optimal weighting scheme by minimizing a meta-loss computed on a small, held-out validation set. Experiments on image classification and object detection tasks with synthetic and real-world noisy datasets demonstrate that Meta-LID consistently outperforms state-of-the-art methods for learning with limited and imperfect data, achieving significant improvements in accuracy and robustness. This work provides a promising approach for deploying deep learning models in data-scarce and noisy environments, broadening their applicability to a wider range of real-world problems."
http://arxiv.org/abs/2507.21018v1,Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark,"Human motion rehabilitation is crucial for restoring motor function after injury or surgery, often relying on expert assessment of movement quality. Automating this assessment process using computer vision, particularly skeleton-based analysis, offers the potential for objective, accessible, and scalable solutions. However, the lack of standardized datasets and evaluation protocols hinders the development and comparison of deep learning models for this task. This paper addresses this gap by introducing a comprehensive benchmark for deep learning-based skeleton-based human motion rehabilitation assessment. We curate and annotate a novel dataset comprising diverse rehabilitation exercises performed by individuals with varying levels of impairment. Furthermore, we systematically evaluate a range of state-of-the-art deep learning architectures, including recurrent neural networks, transformers, and graph convolutional networks, adapted for motion assessment. Our experiments demonstrate the effectiveness of temporal modeling approaches, achieving promising results in classifying movement quality and identifying specific errors. This benchmark provides a valuable resource for researchers and practitioners, facilitating the development of robust and reliable automated tools for human motion rehabilitation."
http://arxiv.org/abs/2507.20980v2,LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering,"Multi-view clustering (MvC) aims to group data points leveraging complementary information from multiple feature sets. However, existing deep MvC methods often struggle with the computational demands and memory limitations imposed by large-scale datasets. This paper addresses the challenge of efficiently clustering large-scale, multi-view data by introducing LargeMvC-Net, an anchor-based deep unfolding network. LargeMvC-Net iteratively refines cluster assignments by explicitly constructing and optimizing view-specific anchor graphs within a deep learning framework. The network unfolds the steps of a traditional anchor-based spectral clustering algorithm into learnable layers, enabling end-to-end optimization and significantly reducing computational complexity compared to traditional methods that require eigendecomposition on the entire affinity matrix. Experiments on several large-scale multi-view datasets demonstrate that LargeMvC-Net achieves competitive or superior clustering performance while drastically reducing computational time and memory footprint, making it a practical solution for large-scale MvC problems. This work provides a scalable and efficient deep learning framework for effectively leveraging multi-view information in real-world applications."
http://arxiv.org/abs/2507.20976v1,Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision,"Vehicle detection in aerial imagery is crucial for various applications, including urban planning and traffic monitoring. However, vehicle detectors trained on one aerial dataset often exhibit poor generalization performance when applied to images from different geographical locations or captured under varying conditions, a phenomenon known as domain shift. This paper addresses the challenge of adapting vehicle detectors trained on a source aerial domain to unseen target domains with minimal supervision. We propose a novel weakly supervised domain adaptation framework that leverages readily available unlabeled target data and weak, image-level supervision in the form of vehicle density estimates. Our method employs an adversarial learning strategy to align feature distributions between the source and target domains, coupled with a consistency regularization term that encourages the detector's predictions to agree with the provided density maps on the target domain. Experiments on multiple aerial vehicle detection datasets demonstrate that our approach significantly improves the detector's performance on unseen target domains compared to existing unsupervised and semi-supervised adaptation techniques. These results highlight the effectiveness of our weakly supervised domain adaptation framework in bridging the domain gap and enabling robust vehicle detection in diverse aerial environments."
http://arxiv.org/abs/2508.00900v1,Sparse 3D Perception for Rose Harvesting Robots: A Two-Stage Approach Bridging Simulation and Real-World Applications,"Automated harvesting of delicate crops like roses presents a significant challenge due to the need for precise 3D perception in unstructured environments. Accurately perceiving rose stems and blossoms with sufficient precision for robotic manipulation is difficult, especially given the computational constraints of mobile robots and the variability of real-world agricultural settings. This paper introduces a novel two-stage approach to sparse 3D perception for rose harvesting robots, designed to bridge the gap between simulated training environments and real-world deployment. Our method leverages a simulation-based training pipeline using synthetic rose gardens to learn robust feature extraction and initial pose estimation with a lightweight neural network. This is followed by a real-time refinement stage employing an efficient Iterative Closest Point (ICP) variant, seeded with the network's output, to achieve high-precision 3D localization of target roses using sparse point cloud data acquired from a low-cost depth sensor. Experimental results demonstrate that our approach achieves a 92% success rate in identifying and localizing harvestable roses in real-world test environments, exhibiting a significant improvement in accuracy and robustness compared to traditional ICP-based methods and direct deployment of networks trained solely on real data. This work contributes a practical and efficient solution for 3D perception in agricultural robotics, facilitating the development of automated harvesting systems for delicate crops."
http://arxiv.org/abs/2507.20884v2,"The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?","Vision-based Sign Language Recognition (SLR) offers a promising solution for bridging communication gaps between deaf and hearing communities. However, the relative contribution of different facial regions to accurate sign recognition remains underexplored. This paper investigates the importance of specific facial features  eyes, mouth, and the full face  in vision-based SLR. We propose a multi-stream convolutional neural network (CNN) architecture that independently processes video sequences of each facial region. The feature representations learned from each stream are then fused using a late fusion strategy, allowing for a comparative analysis of their individual and combined impact on recognition performance. Experiments conducted on a publicly available sign language dataset demonstrate that while the mouth region provides critical information, combining it with the eye region or utilizing the full face consistently yields superior results, achieving a significant improvement in recognition accuracy compared to relying on any single region alone. These findings highlight the complementary nature of facial features in SLR and emphasize the importance of holistic facial analysis for robust and accurate sign language interpretation."
http://arxiv.org/abs/2507.20881v1,Endoscopic Depth Estimation Based on Deep Learning: A Survey,"Endoscopic procedures rely heavily on visual information for diagnosis and surgical navigation. Accurate depth estimation in endoscopic images is crucial for various applications, including 3D reconstruction, surgical navigation, and computer-aided diagnosis; however, the unique characteristics of endoscopic images, such as low texture, specular reflections, and limited field of view, pose significant challenges for traditional depth estimation techniques. This survey comprehensively reviews recent advancements in deep learning-based methods for endoscopic depth estimation. We categorize these methods based on their architectural designs, training strategies, and supervision signals, including monocular depth estimation, stereo matching, and depth from structured light. Furthermore, we analyze the strengths and weaknesses of each category, focusing on their performance on publicly available endoscopic datasets and their robustness to common endoscopic artifacts. Our analysis reveals that while deep learning has significantly improved depth estimation accuracy in endoscopy, challenges remain in generalizing to unseen environments and handling complex scenarios such as bleeding and occlusions. This survey highlights promising research directions for future development, ultimately aiming to improve the accuracy and reliability of depth perception in endoscopic procedures."
http://arxiv.org/abs/2507.20809v1,SCANet: Split Coordinate Attention Network for Building Footprint Extraction,"Building footprint extraction from high-resolution remote sensing imagery is crucial for urban planning, disaster management, and population estimation. However, accurately delineating building boundaries remains challenging due to complex backgrounds, varying building appearances, and occlusions. To address these challenges, we propose SCANet, a Split Coordinate Attention Network for precise building footprint extraction. SCANet leverages a novel split coordinate attention (SCA) mechanism that adaptively captures long-range dependencies along both horizontal and vertical directions of feature maps, while simultaneously reducing computational cost. Specifically, SCA splits input features into parallel branches, applies coordinate attention to each branch, and then fuses the attention-weighted features to enhance the representation of building boundaries. We integrate SCA modules into a U-Net architecture and evaluate SCANet on benchmark datasets, demonstrating significant improvements over existing state-of-the-art methods in terms of intersection-over-union (IoU) and F1-score. The superior performance of SCANet highlights the effectiveness of the proposed split coordinate attention mechanism for enhancing feature representation and achieving accurate building footprint extraction."
http://arxiv.org/abs/2507.20798v1,An Efficient Machine Learning Framework for Forest Height Estimation from Multi-Polarimetric Multi-Baseline SAR data,"Forest height is a crucial parameter for biomass estimation, carbon accounting, and biodiversity monitoring. Synthetic Aperture Radar (SAR) interferometry, particularly utilizing multi-polarimetric and multi-baseline (PolInSAR) data, offers a powerful remote sensing technique for forest height retrieval. However, traditional PolInSAR inversion methods often involve complex mathematical models and computationally intensive optimization procedures, hindering their applicability for large-scale forest monitoring. This paper introduces an efficient machine learning framework for forest height estimation directly from multi-polarimetric multi-baseline SAR data. Our approach leverages a Random Forest regression model trained on a rich feature set extracted from PolInSAR coherency matrices, including polarimetric diversity and baseline decorrelation information. We demonstrate the framework's performance on L-band SAR data acquired over a boreal forest test site, achieving a root mean squared error (RMSE) of 3.2 meters and a correlation coefficient of 0.85 with independent field measurements. This machine learning-based approach provides a significantly faster and more accurate alternative to conventional PolInSAR inversion techniques, enabling efficient and reliable forest height mapping over extensive areas."
http://arxiv.org/abs/2507.20766v4,"Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback","Visual Reinforcement Learning (VRL) enables agents to learn complex tasks directly from raw pixel inputs, offering the potential for robotic skill acquisition without manual feature engineering. However, current VRL methods often struggle with tasks requiring reasoning about object properties and their spatial relationships, especially when rewards are sparse. This paper introduces a novel framework, Learning with Images (LWI), that leverages a differentiable renderer and a reasoning module within a reinforcement learning loop to address this limitation. LWI learns a latent representation of the environment, predicts future states through a differentiable renderer conditioned on the agent's actions, and uses a reasoning module to infer object properties and relationships from the rendered images. The agent is trained solely on images, receiving visual feedback in the form of rendered images and learned rewards based on the reasoning module's output. Experiments on challenging simulated manipulation tasks demonstrate that LWI significantly outperforms state-of-the-art VRL algorithms, achieving higher success rates and faster learning speeds. This work demonstrates the potential of combining differentiable rendering and reasoning within a VRL framework to enable agents to learn complex visual tasks efficiently and effectively, paving the way for more robust and generalizable robotic learning systems."
http://arxiv.org/abs/2507.20765v1,Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural Network,"Hyperspectral imagery (HSI) provides rich spectral information beneficial for numerous remote sensing applications, but its spatial resolution is often limited due to sensor constraints. This limitation hinders the effective analysis and exploitation of HSI data, especially in onboard processing scenarios with stringent computational resources. We propose a novel Deep Pushbroom Neural Network (DPNN) specifically designed for onboard hyperspectral super-resolution (SR). DPNN leverages the pushbroom scanning mechanism inherent in many HSI sensors, employing recurrent connections along the spectral dimension to capture spectral dependencies and spatial smoothness effectively. Furthermore, we introduce a lightweight network architecture optimized for embedded deployment, incorporating techniques such as depthwise separable convolutions and quantization-aware training. Experimental results on benchmark hyperspectral datasets demonstrate that DPNN achieves competitive SR performance compared to state-of-the-art methods while significantly reducing computational complexity and memory footprint. This makes DPNN a promising solution for real-time, onboard HSI super-resolution, enabling enhanced downstream analysis and decision-making directly on resource-constrained platforms."
http://arxiv.org/abs/2507.21200v1,PanoGAN A Deep Generative Model for Panoramic Dental Radiographs,"Panoramic dental radiographs are essential diagnostic tools in dentistry, providing a wide view of the oral structures. However, acquiring high-quality panoramic radiographs can be challenging due to patient movement, anatomical variations, and equipment limitations. This paper addresses the problem of generating realistic and diverse panoramic dental radiographs using a deep learning approach, aiming to augment existing datasets and potentially assist in automated diagnosis. We introduce PanoGAN, a novel generative adversarial network (GAN) specifically designed for panoramic dental radiograph synthesis. PanoGAN incorporates a spatially-aware generator architecture that leverages both global contextual information and local details to produce high-resolution, anatomically plausible radiographs. Furthermore, we employ a multi-scale discriminator to enforce realism at different levels of granularity. Experimental results demonstrate that PanoGAN generates radiographs that are perceptually similar to real radiographs, achieving a Frchet Inception Distance (FID) score significantly lower than baseline GAN models. Our work demonstrates the potential of GANs for generating realistic medical images, opening avenues for data augmentation, anomaly detection, and educational applications in dentistry."
http://arxiv.org/abs/2508.00898v1,Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models,"Video frame prediction, the task of generating future frames based on past observations, is crucial for various applications, including video compression, action anticipation, and autonomous driving. However, accurately predicting future frames remains challenging due to the inherent complexity and temporal dependencies within video sequences. This paper investigates the benefits of incorporating explicit feature extraction and temporal sequence analysis within deep learning models for video frame prediction. We propose a hybrid architecture that integrates convolutional autoencoders for extracting robust feature representations from individual frames with recurrent neural networks, specifically LSTMs, for modeling the temporal evolution of these features. Furthermore, we explore the effectiveness of attention mechanisms within the LSTM to selectively focus on relevant features across time. Experimental results on benchmark datasets, including Moving MNIST and the KTH action dataset, demonstrate that our hybrid model achieves significantly improved prediction accuracy and perceptual quality compared to state-of-the-art methods, especially in scenarios with complex motion and occlusions. These findings highlight the importance of combining spatial feature extraction with temporal modeling for effective video frame prediction, paving the way for more robust and reliable video understanding systems."
http://arxiv.org/abs/2507.20680v1,Lightweight Transformer-Driven Segmentation of Hotspots and Snail Trails in Solar PV Thermal Imagery,"Defect detection in solar photovoltaic (PV) modules is crucial for maintaining energy production efficiency and ensuring system longevity. Identifying hotspots and snail trails in thermal imagery is a common practice, but traditional methods often struggle with computational efficiency and accuracy, especially when deployed at scale. This paper addresses the need for a lightweight and accurate segmentation model capable of identifying these defects in PV thermal images. We propose a novel transformer-driven segmentation network, leveraging a MobileViT backbone for efficient feature extraction and a streamlined transformer decoder for precise pixel-level classification. The decoder incorporates a spatial attention mechanism to focus on relevant regions, further enhancing segmentation performance. Experimental results on a publicly available dataset demonstrate that our model achieves comparable or superior segmentation accuracy to state-of-the-art methods, while significantly reducing computational cost and model size. This lightweight transformer-based approach offers a practical solution for real-time defect detection in solar PV systems, enabling timely maintenance and improved energy yield."
http://arxiv.org/abs/2507.20650v1,Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution,"The proliferation of large-scale machine learning models raises concerns about unauthorized distribution and usage, necessitating effective watermarking techniques. Existing black-box watermarking methods often suffer from high computational overhead or require extensive data for watermark embedding and verification, hindering their applicability to large models. We introduce Hot-Swap MarkBoard, a novel and efficient black-box watermarking approach for large-scale model distribution. Our method leverages a strategically designed ""MarkBoard,"" a small set of input samples that are perturbed and swapped during watermark embedding. These perturbations are carefully crafted to maximize the watermark signal while minimizing performance degradation. Watermark verification is achieved by querying the model with the MarkBoard and analyzing the output distribution to detect the presence of the watermark. Experiments on various large-scale image classification models demonstrate that Hot-Swap MarkBoard achieves high watermark detection accuracy with minimal impact on model performance and significantly reduced computational cost compared to existing methods. This efficient and effective watermarking scheme provides a practical solution for protecting the intellectual property of large-scale machine learning models in real-world deployment scenarios."
http://arxiv.org/abs/2507.20623v1,Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit,"Remote sensing scene classification is crucial for various applications, including urban planning and environmental monitoring. However, deploying deep learning models for this task on resource-constrained edge devices remains challenging due to their high computational cost and memory requirements. This paper addresses the problem of enabling lightweight and efficient remote sensing scene classification on edge devices without significantly sacrificing accuracy. We propose a novel framework that combines knowledge distillation and early-exit techniques. First, we distill knowledge from a large, pre-trained teacher model into a smaller student model using a multi-stage distillation loss. Second, we augment the student model with multiple early-exit branches, allowing the model to make predictions at intermediate layers and reduce computational overhead for simpler input scenes. Experimental results on benchmark remote sensing datasets demonstrate that our approach achieves a significant reduction in computational cost (up to 40% reduction in FLOPs) and latency (up to 30% speedup) compared to state-of-the-art lightweight models, while maintaining comparable or even improved classification accuracy. This work provides a practical solution for deploying remote sensing scene classification models on edge devices, facilitating real-time and on-site analysis."
http://arxiv.org/abs/2507.20608v1,Enhanced Deep Learning DeepFake Detection Integrating Handcrafted Features,"DeepFakes, AI-synthesized videos convincingly manipulating facial features, pose a significant threat to information integrity. Detecting these forgeries remains challenging, particularly when facing novel manipulation techniques and low-quality video. This paper addresses the limitations of purely data-driven DeepFake detection methods by integrating handcrafted features with deep learning models. Our approach combines a convolutional neural network (CNN) to extract high-level semantic features with handcrafted features capturing low-level image artifacts, such as frequency domain inconsistencies and blending boundary irregularities, which are known to be prevalent in DeepFakes. We then fuse these feature sets using a late fusion strategy, allowing the model to leverage both global contextual information and local artifact signatures. Experimental results on benchmark datasets, including FaceForensics++ and DeepFakeDetection, demonstrate that our integrated approach achieves state-of-the-art performance, significantly improving detection accuracy and robustness compared to methods relying solely on deep learning. The proposed method offers a more reliable and generalizable DeepFake detection solution, crucial for mitigating the spread of misinformation."
http://arxiv.org/abs/2507.20590v2,Harnessing Diffusion-Yielded Score Priors for Image Restoration,"Image restoration aims to recover high-quality images from degraded observations, a fundamental task in computer vision. While deep learning-based methods have shown promise, they often struggle with ill-posed problems and can produce unsatisfactory results with limited data or severe degradations. This paper addresses the challenge of robust image restoration by leveraging the powerful generative capabilities of diffusion models. We propose a novel framework, Diffusion-Yielded Score Prior Restoration (DySPR), which incorporates score-based priors derived from pre-trained diffusion models into the restoration process. Specifically, DySPR iteratively refines the restored image by minimizing a combination of data fidelity and a regularization term based on the score function learned by the diffusion model. This score prior guides the restoration towards realistic and plausible solutions consistent with the data distribution learned by the diffusion model. Experimental results demonstrate that DySPR significantly outperforms state-of-the-art image restoration methods across various degradation types and severity levels, achieving superior PSNR and SSIM scores while generating visually more appealing results. This work highlights the potential of diffusion models as powerful priors for tackling challenging inverse problems in image processing."
http://arxiv.org/abs/2507.20589v1,Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation,"Reticular structures, characterized by their complex, interconnected networks, are prevalent in various domains, including vegetation mapping, infrastructure inspection, and geological surveys. Accurate segmentation of these structures from 3D LiDAR data is crucial for downstream tasks like structural analysis and automated modeling. This paper addresses the challenge of effectively segmenting reticular structures from noisy and incomplete LiDAR point clouds. We comparatively evaluate three distinct segmentation methods: a region-growing approach incorporating geometric and topological constraints, a graph-based method leveraging minimum spanning trees for network extraction, and a deep learning approach utilizing a 3D convolutional neural network trained on synthetic and real-world LiDAR data. Each method is adapted and optimized for the specific characteristics of reticular structures. Experimental results on diverse datasets demonstrate that the deep learning approach achieves the highest overall segmentation accuracy, while the graph-based method excels in preserving topological connectivity. This comparative analysis provides valuable insights into the strengths and limitations of different approaches for reticular structure segmentation, facilitating informed method selection for specific applications and datasets."
http://arxiv.org/abs/2507.20507v1,Investigating the Effect of Spatial Context on Multi-Task Sea Ice Segmentation,"Sea ice segmentation is crucial for climate monitoring and safe navigation in polar regions. Current deep learning approaches often utilize multi-task learning to improve segmentation performance by simultaneously predicting related variables such as ice thickness or age. However, the influence of explicitly incorporating spatial context within these multi-task frameworks remains largely unexplored. This paper investigates the effect of spatial context on multi-task sea ice segmentation performance. We propose a novel multi-task learning architecture that integrates a contextual attention module, leveraging both global and local spatial relationships, into a shared encoder network followed by task-specific decoders for segmentation and ice property regression. Our experiments on satellite imagery demonstrate that incorporating spatial context significantly improves segmentation accuracy, particularly in regions with complex ice formations, achieving a 5% increase in Intersection over Union (IoU) compared to a baseline multi-task model without contextual attention. This highlights the importance of spatial reasoning in multi-task sea ice analysis and provides a pathway towards more robust and accurate sea ice monitoring systems."
http://arxiv.org/abs/2507.20418v1,Can Foundation Models Predict Fitness for Duty?,"Foundation models have demonstrated remarkable capabilities across diverse domains, leveraging pre-training on massive datasets to achieve strong performance with minimal task-specific fine-tuning. However, their potential in high-stakes domains like predicting fitness for duty (FFD), where subtle behavioral cues and contextual understanding are critical, remains largely unexplored. This paper addresses the challenging problem of automatically assessing an individual's fitness for duty based on multimodal data, specifically focusing on visual cues extracted from video recordings of simulated work scenarios. We propose a novel approach that combines a pre-trained vision-language transformer (CLIP) with a temporal attention mechanism to capture both spatial and temporal dependencies in the video data. The CLIP model extracts semantically rich visual features, which are then aggregated over time using the attention mechanism to generate a comprehensive representation of the individual's behavior. Our experiments on a newly collected FFD dataset demonstrate promising results, achieving a significant improvement over baseline methods in predicting FFD status. This work highlights the potential of foundation models to contribute to automated and objective assessments in safety-critical occupations."
http://arxiv.org/abs/2507.20389v1,Solving Scene Understanding for Autonomous Navigation in Unstructured Environments,"Autonomous navigation in unstructured environments, such as off-road terrains and disaster zones, presents significant challenges due to the lack of predefined roadmaps and the presence of complex, dynamic obstacles. Robust scene understanding is paramount for safe and efficient navigation in these scenarios, yet current methods often struggle with the variability and ambiguity inherent in such environments. This paper addresses the problem of achieving reliable scene understanding for autonomous navigation in unstructured environments by proposing a novel multi-modal fusion framework that integrates visual and LiDAR data within a deep learning architecture. Our approach leverages a modified PointNet++ architecture for LiDAR point cloud segmentation and a ResNet-based network for image semantic segmentation, subsequently fusing these representations through a cross-attention mechanism to enhance contextual awareness and improve segmentation accuracy. Experimental results on a challenging off-road dataset demonstrate that our method significantly outperforms state-of-the-art approaches in scene understanding tasks, achieving a 15% improvement in mean Intersection-over-Union (mIoU) for traversable terrain segmentation. The improved scene understanding capabilities directly translate to more robust and reliable autonomous navigation in unstructured environments, paving the way for safer and more efficient robotic deployments in these critical settings."
http://arxiv.org/abs/2507.20388v1,ModalFormer: Multimodal Transformer for Low-Light Image Enhancement,"Low-light image enhancement (LLIE) is a crucial task in computer vision, aiming to improve the visibility and perceptual quality of images captured in poorly illuminated environments. Existing deep learning methods often struggle with effectively handling complex noise and color distortion inherent in low-light images, particularly due to their limited ability to model long-range dependencies and multimodal information. To address these limitations, we propose ModalFormer, a novel multimodal transformer network for LLIE. ModalFormer leverages a transformer-based architecture to capture global context and integrates multiple modalities, including RGB, frequency, and gradient information, through dedicated modality-specific encoders and a cross-modal fusion module. This fusion module utilizes a learnable attention mechanism to dynamically weigh and integrate the features from different modalities, enabling robust and adaptive enhancement. Experiments on benchmark datasets demonstrate that ModalFormer significantly outperforms state-of-the-art LLIE methods in terms of both quantitative metrics and visual quality, effectively suppressing noise, correcting color distortions, and preserving image details. This work highlights the potential of multimodal transformers for addressing challenging low-light image enhancement problems."
http://arxiv.org/abs/2507.20363v1,Generative Pre-training for Subjective Tasks: A Diffusion Transformer-Based Framework for Facial Beauty Prediction,"Facial beauty prediction, a subjective and nuanced task, remains challenging due to the lack of large-scale labeled datasets and the inherent ambiguity in aesthetic perception. This paper addresses the problem of effectively leveraging unlabeled facial data to improve the performance of facial beauty prediction models, particularly in low-data regimes. We propose a novel Generative Pre-training strategy employing a Diffusion Transformer-based framework. Specifically, we pre-train a Transformer backbone using a diffusion model conditioned on facial attributes extracted from unlabeled data, enabling the model to learn a rich representation of facial features and their correlations. This pre-trained model is then fine-tuned on a smaller, labeled dataset for the beauty prediction task. Experimental results demonstrate that our proposed method significantly outperforms state-of-the-art approaches, especially when training data is limited, achieving improvements of up to 15% in correlation metrics. This highlights the effectiveness of generative pre-training for subjective tasks and offers a promising direction for improving performance in similar domains with scarce labeled data."
http://arxiv.org/abs/2507.20311v1,SWIFT: A General Sensitive Weight Identification Framework for Fast Sensor-Transfer Pansharpening,"Pansharpening, the fusion of high-spatial resolution panchromatic (PAN) and low-spatial resolution multispectral (MS) images, is crucial for various remote sensing applications. However, traditional pansharpening methods often struggle with performance degradation when applied to data from different sensors due to variations in spectral and spatial characteristics. This paper addresses the challenge of achieving robust and efficient sensor-transfer pansharpening. We propose SWIFT, a Sensitive Weight Identification Framework for Fast Sensor-Transfer Pansharpening. SWIFT leverages a pre-trained deep learning model on a source sensor and introduces a lightweight weight identification module to adapt the model to a target sensor. This module identifies and adjusts only the most sensitive weights within the pre-trained model, enabling rapid adaptation with minimal computational overhead. Experiments on multiple benchmark datasets demonstrate that SWIFT achieves state-of-the-art performance in sensor-transfer pansharpening, exhibiting superior fusion quality and significantly reduced training time compared to existing methods. SWIFT offers a practical and effective solution for deploying pansharpening models across diverse remote sensing platforms."
http://arxiv.org/abs/2507.20284v1,Controllable Feature Whitening for Hyperparameter-Free Bias Mitigation,"Bias amplification in deep neural networks is a critical concern, potentially leading to unfair or discriminatory outcomes. Existing bias mitigation techniques often require extensive hyperparameter tuning, limiting their practical applicability and hindering reproducibility. We address this challenge by introducing Controllable Feature Whitening (CFW), a novel and hyperparameter-free approach for mitigating bias during feature extraction. CFW leverages the inherent properties of feature whitening to decorrelate and normalize feature representations, effectively reducing the influence of biased attributes. Critically, we introduce a controllable weighting mechanism that allows for targeted adjustment of the whitening strength across different feature dimensions, enabling precise bias mitigation without requiring sensitive hyperparameter optimization. Experiments on benchmark datasets demonstrate that CFW significantly reduces bias across various fairness metrics, achieving comparable or superior performance to state-of-the-art methods while eliminating the need for hyperparameter tuning. This work offers a readily deployable and robust solution for mitigating bias in deep learning models, facilitating fairer and more equitable AI systems."
http://arxiv.org/abs/2507.20239v1,Decomposing Densification in Gaussian Splatting for Faster 3D Scene Reconstruction,"3D Gaussian Splatting (3D-GS) has emerged as a powerful technique for novel view synthesis, offering real-time rendering and competitive image quality. However, the densification process, crucial for refining the 3D scene representation, remains a significant computational bottleneck, especially for large-scale scenes. This paper addresses the problem of inefficient and often redundant densification in 3D-GS by proposing a novel decomposition strategy. Our method decouples the densification process into two distinct stages: a global scene-aware densification focusing on regions with high reconstruction error, and a local, geometry-adaptive refinement focusing on areas with geometric inconsistencies. This decomposition leverages both image-space rendering errors and 3D geometric properties, such as point cloud curvature, to guide the densification process. We demonstrate through extensive experiments on benchmark datasets that our decomposed densification strategy significantly reduces the computational cost associated with scene reconstruction, achieving up to a 2x speedup in training time while maintaining comparable or improved rendering quality and geometric accuracy. This enables faster and more efficient 3D scene reconstruction using Gaussian Splatting, paving the way for real-time applications on resource-constrained platforms."
http://arxiv.org/abs/2507.20221v1,Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans,"Lung cancer remains a leading cause of cancer-related deaths worldwide, with early detection significantly improving survival rates. This paper addresses the challenge of improving the accuracy and robustness of lung cancer detection in computed tomography (CT) scans, particularly in distinguishing between malignant and benign nodules. We propose a novel Multi-Attention Stacked Ensemble (MASE) framework. MASE leverages the strengths of multiple pre-trained convolutional neural networks (CNNs) by incorporating a multi-attention mechanism within each base model to emphasize salient nodule features and suppress irrelevant background noise. These attention-enhanced features are then fed into a meta-learner, trained to optimally combine the predictions of the base models, creating a powerful ensemble. Experimental results on the LIDC-IDRI dataset demonstrate that MASE achieves state-of-the-art performance, surpassing existing methods with an improved F1-score of 0.89 and a reduced false positive rate. The proposed MASE framework offers a significant advancement in computer-aided diagnosis for lung cancer, potentially leading to earlier and more accurate detection in clinical settings."
http://arxiv.org/abs/2508.04234v1,A machine learning approach for image classification in synthetic aperture RADAR,"Synthetic Aperture Radar (SAR) provides valuable Earth observation data, especially in areas with persistent cloud cover or limited daylight. However, the inherent speckle noise and complex scattering mechanisms in SAR imagery pose significant challenges for accurate and automated land cover classification. This paper addresses the problem of robust and accurate image classification in SAR data by leveraging the power of machine learning. We propose a novel approach that combines a convolutional neural network (CNN) architecture optimized for speckle noise reduction with a self-attention mechanism to capture long-range dependencies and contextual information within SAR images. The CNN learns robust feature representations from raw SAR data, while the self-attention module refines these features by weighting their importance based on global context. Experimental results on benchmark SAR datasets demonstrate that our proposed method achieves state-of-the-art classification accuracy, outperforming traditional machine learning algorithms and existing deep learning approaches. This research provides a significant advancement in SAR image analysis, enabling more reliable and efficient land cover mapping and monitoring applications."
http://arxiv.org/abs/2508.04049v1,Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation,"Sign language generation (SLG) aims to automatically translate spoken language into sign language, primarily represented by 3D human pose sequences. A significant challenge in SLG is generating realistic and temporally coherent pose sequences that accurately reflect the nuanced dynamics of sign language, often resulting in jerky or unnatural movements. We address this by proposing a novel framework, ""Motion is the Choreographer,"" that learns latent pose dynamics conditioned on gloss embeddings to achieve seamless SLG. Our method employs a hierarchical variational autoencoder (VAE) architecture. First, a pose VAE learns a compact latent representation of human pose. Second, a dynamics VAE models the temporal evolution of these latent poses, conditioned on gloss embeddings representing the semantic content. This allows us to generate smooth, contextually relevant pose sequences by sampling from the learned latent dynamics. Experiments on benchmark datasets demonstrate that our approach significantly improves the fluency and naturalness of generated sign language, achieving state-of-the-art results in terms of Frchet Inception Distance (FID) and sign language specific metrics. This work provides a new direction for SLG by explicitly modeling pose dynamics, paving the way for more accessible and realistic sign language communication systems."
http://arxiv.org/abs/2508.03669v1,OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World,"Estimating the 3D shape and pose of objects from single images is a fundamental problem in computer vision, crucial for scene understanding and robotic manipulation. However, existing methods often rely on extensive training data with 3D annotations or struggle with ambiguous viewpoints and occlusions, leading to inaccurate or incomplete predictions. We address the challenge of zero-shot multi-hypothesis shape and pose estimation in real-world scenarios by introducing OmniShape, a novel framework that leverages pre-trained vision-language models and differentiable rendering. Our approach generates a diverse set of shape and pose hypotheses by sampling from a learned latent space conditioned on both the input image and a textual description of the object category. These hypotheses are then refined through a differentiable rendering pipeline, guided by image-based losses and a novel shape prior derived from large-scale text-to-3D diffusion models. Experiments on challenging real-world datasets demonstrate that OmniShape significantly outperforms state-of-the-art zero-shot methods, generating multiple plausible shape and pose estimates with improved accuracy and robustness to occlusions. This enables more reliable downstream applications in robotics and augmented reality by providing a richer understanding of object geometry and spatial relationships."
http://arxiv.org/abs/2508.03625v1,AttZoom: Attention Zoom for Better Visual Features,"Attention mechanisms have become integral to modern computer vision, enabling models to focus on salient image regions. However, standard attention often treats all attended regions equally, neglecting the varying importance of fine-grained details within these regions. This paper addresses the limitation of uniform processing within attended regions by introducing AttZoom, an attention zoom mechanism that dynamically adjusts the level of detail extracted from different parts of an image. AttZoom leverages a learnable scaling factor derived from attention weights to zoom into more informative regions, effectively creating multi-resolution features. Specifically, we use attention weights to predict a zoom factor, which is then applied to the input feature map before further processing, allowing the network to focus on finer details in highly attended areas. Experimental results on ImageNet classification and COCO object detection demonstrate that AttZoom consistently improves the performance of various baseline models, achieving significant gains in accuracy and mAP with minimal additional computational cost. AttZoom offers a simple yet effective approach to refine attention mechanisms, leading to enhanced visual feature extraction and improved performance across diverse computer vision tasks."
http://arxiv.org/abs/2508.03618v1,FPG-NAS: FLOPs-Aware Gated Differentiable Neural Architecture Search for Efficient 6DoF Pose Estimation,"Accurate and efficient 6DoF pose estimation is crucial for numerous applications, including robotics, augmented reality, and autonomous driving. However, deploying deep learning-based pose estimation models on resource-constrained platforms remains challenging due to their high computational cost. This paper addresses the problem of automatically designing efficient neural architectures for 6DoF pose estimation under strict computational constraints. We introduce FPG-NAS, a FLOPs-Aware Gated Differentiable Neural Architecture Search framework. FPG-NAS incorporates a novel FLOPs predictor within the search space to guide the architecture search towards Pareto-optimal solutions balancing accuracy and computational cost. A gated mechanism further refines the search by adaptively selecting the most promising architectural components. Experiments on standard benchmarks demonstrate that FPG-NAS achieves state-of-the-art accuracy with significantly reduced FLOPs compared to existing hand-crafted and NAS-based pose estimation models. The proposed approach enables the deployment of accurate and efficient 6DoF pose estimation on resource-limited devices."
http://arxiv.org/abs/2508.03596v1,MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy,"Metalenses offer a promising avenue for miniaturizing endoscopic systems, enabling minimally invasive diagnostics and interventions. However, the severe optical aberrations inherent in ultra-small metalenses, coupled with limited space for complex optical correction, pose a significant challenge for generating high-quality images. This paper addresses the problem of reconstructing clear images from highly distorted raw data acquired through an ultra-micro metalens endoscope. We introduce MetaScope, an optics-driven neural network architecture that leverages the physics of light propagation and metalens design parameters to guide image restoration. Specifically, MetaScope incorporates a learnable point spread function (PSF) estimation module informed by the metalens design and a subsequent deconvolution network to mitigate aberrations. We demonstrate that MetaScope significantly outperforms conventional image processing techniques and existing deep learning methods, achieving a substantial improvement in image quality and resolution on both simulated and experimentally captured metalens endoscopic images. This work paves the way for the development of high-performance, ultra-miniature endoscopic devices with enhanced diagnostic capabilities."
http://arxiv.org/abs/2508.03564v1,A Scalable Machine Learning Pipeline for Building Footprint Detection in Historical Maps,"Historical maps represent a rich, largely untapped source of geospatial information crucial for understanding urban development and environmental change. However, extracting structured data, such as building footprints, from these maps is a challenging task due to their inherent variability, distortions, and degradation over time. This paper addresses the problem of automatically and efficiently detecting building footprints in large collections of digitized historical maps. We propose a scalable machine learning pipeline that combines image preprocessing steps tailored for historical map characteristics with a deep learning-based object detection model, specifically a Mask R-CNN architecture fine-tuned for building footprint segmentation. Furthermore, we introduce a novel data augmentation strategy that leverages synthetic map generation to enhance the model's robustness to variations in map style and quality. Experiments on a diverse dataset of historical maps demonstrate that our pipeline achieves state-of-the-art performance in building footprint detection, significantly outperforming existing methods in terms of accuracy and efficiency. This automated approach enables the extraction of valuable geospatial data from historical maps at scale, facilitating large-scale studies of urban evolution and providing critical insights for historical research."
http://arxiv.org/abs/2508.03411v1,SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation,"Unsupervised video object segmentation aims to discover and segment objects in a video without manual annotations, a crucial capability for autonomous scene understanding. A key challenge lies in learning temporally consistent object representations that can handle complex object motions and occlusions. We introduce SlotMatch, a novel framework that distills temporally consistent object-centric representations by leveraging the inherent structure of slot-based attention mechanisms. Our approach employs a slot attention module to decompose each video frame into object-centric slots and then enforces temporal consistency through a novel matching loss that encourages slots representing the same object to maintain similar features across frames. Furthermore, we introduce a slot-dropout regularization technique to promote robustness to occlusions and improve object discovery. Experiments on standard video segmentation benchmarks, including MOVi and YouTube-VIS, demonstrate that SlotMatch significantly outperforms existing unsupervised methods, achieving state-of-the-art performance in terms of both segmentation accuracy and temporal stability. This work advances the field of unsupervised video understanding by providing a robust and effective method for learning temporally coherent object-centric representations."
http://arxiv.org/abs/2508.03375v1,GaitAdapt: Continual Learning for Evolving Gait Recognition,"Gait recognition, the identification of individuals based on their walking patterns, has emerged as a promising biometric modality. However, real-world gait recognition systems face the challenge of evolving gait patterns due to factors like aging, injury, or changes in footwear, leading to performance degradation over time. This paper addresses the problem of catastrophic forgetting in gait recognition models when adapting to new gait patterns encountered sequentially. We introduce GaitAdapt, a novel continual learning framework specifically designed for evolving gait recognition. GaitAdapt leverages a combination of knowledge distillation, prototype-based regularization, and adaptive learning rate scaling to effectively incorporate new gait information while preserving previously learned representations. Experiments on challenging gait datasets demonstrate that GaitAdapt significantly outperforms existing continual learning methods and naive fine-tuning, achieving substantial improvements in accuracy and forgetting mitigation. GaitAdapt offers a practical and effective solution for building robust and adaptive gait recognition systems capable of handling evolving gait patterns in real-world scenarios."
http://arxiv.org/abs/2508.03324v2,Live Demonstration: Neuromorphic Radar for Gesture Recognition,"Gesture recognition is a crucial component for intuitive human-computer interaction, often relying on vision-based systems. However, these systems can struggle in low-light conditions and compromise privacy. This demonstration addresses the challenge of robust and privacy-preserving gesture recognition by leveraging the unique capabilities of neuromorphic radar. We present a system that combines a novel Frequency Modulated Continuous Wave (FMCW) radar with spiking neural network (SNN) processing. The radar captures micro-Doppler signatures of hand gestures, which are then directly encoded into spike trains using an event-based encoding scheme. These spike trains are fed into a multi-layer SNN trained to classify a set of predefined gestures. Our live demonstration showcases real-time gesture recognition with high accuracy and low latency, even in challenging lighting conditions. This work highlights the potential of neuromorphic radar as a viable and privacy-respecting alternative to traditional vision-based gesture recognition systems, paving the way for more robust and energy-efficient interactive technologies."
http://arxiv.org/abs/2508.03244v1,Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution,"Event cameras offer significant advantages over traditional cameras in high-speed and high dynamic range scenarios, but their output often suffers from low spatial resolution. This limitation hinders their application in tasks requiring fine-grained detail. This paper addresses the challenge of super-resolving event streams using a novel and efficient Spiking Neural Network (SNN) architecture. We propose an ultralight polarity-split neuromorphic SNN, where positive and negative events are processed by separate, highly compact SNN branches. This polarity splitting allows for specialized processing of each event type, while the neuromorphic structure enables efficient temporal processing of the asynchronous event stream. We further introduce a novel training strategy that leverages both supervised learning on high-resolution images and unsupervised learning on the event stream itself. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art super-resolution performance with significantly fewer parameters and computational resources compared to existing deep learning-based approaches. This work paves the way for deploying high-performance event-based vision systems on resource-constrained platforms."
http://arxiv.org/abs/2508.03221v1,BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models,"Text-to-image diffusion models have achieved remarkable success in generating high-fidelity and diverse images from textual descriptions, making them increasingly prevalent in various applications. However, the susceptibility of these models to backdoor attacks raises significant security concerns, particularly in scenarios where models are deployed in untrusted environments or rely on pre-trained weights. This paper introduces ""BadBlocks,"" a novel and low-cost backdoor attack specifically designed to compromise text-to-image diffusion models while maintaining stealth. BadBlocks strategically injects imperceptible ""bad blocks"" into the latent space during training, associating specific trigger phrases with targeted visual outputs. Crucially, these bad blocks are optimized to minimize their impact on clean image generation and require minimal parameter modification, significantly reducing the computational overhead compared to existing backdoor methods. Experimental results demonstrate that BadBlocks achieves high attack success rates with minimal degradation in image quality and maintains strong stealthiness, effectively evading detection by existing backdoor defense mechanisms. Our findings highlight the vulnerability of diffusion models to subtle latent space manipulations and underscore the need for robust defense strategies against such targeted attacks."
http://arxiv.org/abs/2508.02995v1,VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision,"Inspired by the hierarchical organization and recurrent processing of the primate visual cortex, we explore the potential of incorporating these principles into artificial vision systems to enhance robustness. Current deep learning models, while achieving impressive performance on standard benchmarks, often exhibit fragility in the face of adversarial attacks or distributional shifts. This paper introduces VCNet, a novel architecture designed to mimic key computational mechanisms of the visual cortex, including hierarchical feature extraction, lateral connections for contextual integration, and feedback pathways for iterative refinement. VCNet utilizes a multi-stage architecture with convolutional layers organized into cortical-inspired areas, interconnected by both feedforward and feedback connections. Crucially, lateral connections within each area facilitate contextual reasoning. Experimental results demonstrate that VCNet exhibits significantly improved robustness against adversarial perturbations and common corruptions compared to standard convolutional neural networks, while maintaining competitive accuracy on clean images. This work highlights the promise of biologically-inspired architectures for developing more robust and reliable artificial vision systems."
http://arxiv.org/abs/2508.02987v1,Adversarial Attention Perturbations for Large Object Detection Transformers,"Large object detection transformers have demonstrated impressive performance; however, their robustness to adversarial attacks remains a significant concern. We address the vulnerability of these models by introducing a novel adversarial attack strategy, Adversarial Attention Perturbations (AAP), which directly manipulates the attention maps within the transformer architecture. AAP crafts subtle, yet effective, perturbations to the attention weights, specifically targeting regions crucial for accurate object localization and classification. This is achieved through a gradient-based optimization process that maximizes the classification loss for the true object while minimizing the confidence score for its correct location. We demonstrate that AAP significantly degrades the performance of state-of-the-art object detection transformers on benchmark datasets like COCO, achieving substantial drops in Average Precision (AP) compared to baseline attacks. The proposed AAP offers a powerful tool for evaluating and improving the robustness of attention-based object detectors against adversarial threats, paving the way for more secure and reliable vision systems."
http://arxiv.org/abs/2508.02831v1,GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, but their interactive editing remains a significant challenge due to the implicit scene representation and entanglement of appearance and geometry. Directly manipulating NeRFs often results in unintended artifacts and requires retraining, hindering intuitive and efficient editing workflows. We introduce GENIE, Gaussian Encoding for Neural Radiance Fields Interactive Editing, a novel approach that leverages explicit 3D Gaussian representations as an intermediate layer within a NeRF architecture to facilitate localized and controllable scene modifications. GENIE encodes the scene with a set of 3D Gaussians, whose parameters are then decoded into density and color used by a neural volume rendering function. This explicit representation enables direct manipulation of Gaussian properties, such as position, scale, and color, corresponding to intuitive scene edits. Our experiments demonstrate that GENIE allows for interactive object translation, scaling, color adjustment, and removal with minimal artifacts and without requiring retraining. GENIE offers a significant step towards democratizing NeRF editing, making it accessible to users without specialized expertise."
http://arxiv.org/abs/2508.02806v1,PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation,"3D human pose estimation from monocular images is a challenging task with applications in various fields, including virtual reality, robotics, and healthcare. Existing methods often struggle with accurately capturing long-range dependencies and fine-grained details in complex poses. This paper introduces PyCAT4, a novel hierarchical vision transformer-based framework for 3D human pose estimation. PyCAT4 leverages a pyramidal architecture to extract multi-scale features, combined with cross-attention transformers (CATs) at each scale to effectively model global context and relationships between body joints. Furthermore, we introduce a novel pose-aware attention mechanism within the CATs to emphasize relevant features based on prior pose estimations, refining the representation and improving accuracy. Experimental results on benchmark datasets, such as Human3.6M and MPI-INF-3DHP, demonstrate that PyCAT4 achieves state-of-the-art performance, significantly outperforming existing transformer-based and CNN-based methods, particularly in challenging scenarios with occlusions and self-similar poses. The proposed framework provides a robust and accurate solution for 3D human pose estimation, paving the way for more realistic and reliable human-computer interaction."
http://arxiv.org/abs/2508.02439v2,Glioblastoma Overall Survival Prediction With Vision Transformers,"Glioblastoma (GBM) is the most aggressive type of brain cancer, exhibiting significant heterogeneity in imaging features that influence patient survival. Accurate prediction of overall survival (OS) in GBM patients remains a crucial challenge for personalized treatment planning. This paper addresses the problem of predicting OS in GBM patients using pre-operative multi-parametric Magnetic Resonance Imaging (mpMRI) scans. We propose a novel approach leveraging Vision Transformers (ViTs) to learn discriminative features directly from the imaging data. Specifically, we adapt a 3D ViT architecture, pre-trained on a large-scale natural image dataset, and fine-tune it on the mpMRI scans (T1, T1Gd, T2, FLAIR) of GBM patients. We further investigate the impact of different patch sizes and pre-training strategies on model performance. Our results on the benchmark BraTS 2021 dataset demonstrate that the ViT-based model achieves state-of-the-art performance in OS prediction, surpassing existing Convolutional Neural Network (CNN)-based methods, with a C-index of 0.76. These findings highlight the potential of ViTs for extracting relevant prognostic information from medical images, paving the way for improved clinical decision-making and patient outcomes in GBM."
http://arxiv.org/abs/2508.02409v1,Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion,"Leaf wetness duration (LWD) is a critical microclimatic variable influencing plant disease development. Traditional LWD sensors often suffer from limitations in spatial resolution and accuracy, hindering precise disease management. This paper addresses the challenge of accurate and high-resolution LWD sensing by introducing Hydra, a novel multi-modal system that fuses millimeter-wave (mm-Wave) radar and camera data. Hydra leverages mm-Wave radar to detect water presence on leaf surfaces and employs a deep learning-based computer vision model to segment individual leaves and extract relevant features. A fusion network then integrates the mm-Wave and visual features to predict the LWD for each leaf. Experimental results demonstrate that Hydra achieves significantly improved LWD estimation accuracy compared to single-modal approaches and traditional resistive grid sensors, exhibiting a mean absolute error reduction of over 40%. This technology enables spatially-resolved and accurate LWD monitoring, facilitating precision agriculture and optimized disease control strategies."
http://arxiv.org/abs/2508.02320v1,Zero-shot Compositional Action Recognition with Neural Logic Constraints,"Compositional action recognition, identifying activities formed by the structured combination of simpler actions, is crucial for understanding complex human behavior. However, existing approaches often struggle to generalize to unseen compositions due to the exponential growth in data requirements as the number of possible action combinations increases. This paper addresses the challenge of zero-shot compositional action recognition, where the goal is to recognize novel action compositions without requiring any training examples for those specific combinations. We propose a novel framework that leverages neural networks to learn representations of individual actions and their relationships, coupled with a neural-symbolic approach to enforce logical constraints derived from action grammar. Specifically, we use a graph neural network to model action dependencies and integrate a differentiable relaxation of logical rules to guide the learning process, ensuring that predicted compositions adhere to predefined semantic structures. Experiments on benchmark datasets demonstrate that our method significantly outperforms existing zero-shot compositional action recognition techniques, achieving state-of-the-art results and exhibiting strong generalization capabilities to unseen action compositions. Our approach offers a promising direction for building more robust and adaptable action recognition systems that can reason about complex human activities in open-world scenarios."
http://arxiv.org/abs/2508.03758v1,FUTransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation,"Foot ulcers are a major complication of diabetes, often leading to amputation if not detected and treated early. Accurate segmentation of foot ulcers from clinical images is crucial for quantitative assessment and effective treatment planning, but is challenging due to variations in ulcer appearance, lighting conditions, and image quality. This paper addresses the problem of automated and explainable foot ulcer segmentation by proposing FUTransUNet-GradCAM, a novel hybrid architecture that combines the strengths of Transformers and U-Nets. Our model leverages a Transformer encoder with self-attention mechanisms to capture global contextual information, coupled with a U-Net decoder for precise pixel-level segmentation. Furthermore, we integrate Grad-CAM to generate visual explanations highlighting the regions of the image that the model uses to make its segmentation decisions. Experimental results on a publicly available foot ulcer dataset demonstrate that FUTransUNet-GradCAM achieves state-of-the-art segmentation performance, outperforming existing U-Net and Transformer-based approaches in terms of Dice score and Intersection over Union (IoU). The Grad-CAM visualizations provide valuable insights into the model's decision-making process, enhancing trust and facilitating clinical adoption of the proposed method."
http://arxiv.org/abs/2508.02157v1,Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes,"Category-level object detection and pose estimation are fundamental tasks for robotic manipulation and scene understanding, requiring generalization across object instances within a category. Existing methods often treat these tasks separately or struggle with accurate pose estimation, especially for objects with limited texture or symmetry. This paper proposes a unified framework for category-level object detection and 6D pose estimation from RGB images using 3D prototypes. Our approach learns a set of category-specific 3D prototypes that represent canonical object shapes and appearance variations. Given an input image, we predict dense pixel-wise correspondences to these prototypes and simultaneously estimate object bounding boxes and 6D poses by aligning the prototypes to the observed image features. Experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both object detection and pose estimation, particularly for challenging object categories. The unified framework and the use of 3D prototypes offer a more robust and accurate solution for category-level perception, leading to improved performance in downstream robotic tasks."
http://arxiv.org/abs/2508.02131v1,A Neural Quality Metric for BRDF Models,"Bidirectional Reflectance Distribution Functions (BRDFs) are crucial for realistic rendering, but evaluating their quality remains challenging, often relying on subjective human perception or computationally expensive ground-truth comparisons. This paper addresses the problem of automating BRDF quality assessment by learning a neural quality metric that correlates with human perception. We propose a novel deep learning architecture, the BRDF Quality Network (BQN), which directly predicts a perceptual quality score from BRDF parameters, without requiring ground truth BRDFs or rendered images. BQN leverages a Siamese network structure to compare BRDF patches and learns to identify subtle differences that impact visual quality, trained on a dataset of synthetically generated BRDFs labeled with human-annotated quality scores. Experimental results demonstrate that BQN achieves state-of-the-art performance in predicting BRDF quality, outperforming existing metrics and exhibiting strong correlation with human perception across a diverse range of materials. This automated and perceptually aligned metric enables efficient BRDF evaluation, optimization, and selection for realistic rendering applications."
http://arxiv.org/abs/2508.02129v1,VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling,"Modeling dynamic urban scenes with high fidelity and temporal consistency remains a significant challenge in computer vision. Existing 4D Gaussian Splatting methods often struggle with complex motions and disocclusions, leading to artifacts and reduced rendering quality. To address these limitations, we introduce VDEGaussian, a novel Video Diffusion Enhanced 4D Gaussian Splatting framework. Our approach leverages a video diffusion prior to guide the optimization of 4D Gaussians, enabling more robust handling of challenging dynamic scenarios. Specifically, we incorporate a temporal consistency loss derived from the diffusion model's latent space, encouraging neighboring Gaussians to evolve smoothly over time. Furthermore, we introduce a diffusion-based inpainting technique to fill disoccluded regions, improving the completeness of the scene representation. Experiments on a diverse set of dynamic urban scenes demonstrate that VDEGaussian significantly outperforms state-of-the-art methods in terms of both visual quality and quantitative metrics, achieving improvements in PSNR, SSIM, and LPIPS. This work presents a promising direction for high-fidelity dynamic scene modeling by effectively integrating the power of diffusion models with 4D Gaussian Splatting."
http://arxiv.org/abs/2508.02111v1,Tackling Ill-posedness of Reversible Image Conversion with Well-posed Invertible Network,"Reversible image conversion, encompassing tasks like image enhancement and restoration, aims to transform images while preserving information for perfect reconstruction. However, the inherent ill-posedness of these tasks often leads to amplified noise and artifacts in the converted images, particularly when using deep invertible networks. To address this challenge, we propose a novel well-posed invertible network architecture specifically designed to mitigate the ill-posedness issue. Our approach incorporates a spectral-normalized flow module that constrains the Lipschitz constant of the network, promoting stability and preventing excessive amplification of high-frequency noise. Furthermore, we introduce a learnable regularization term within the invertible mapping to encourage smoothness and plausibility in the converted image domain. Experimental results on benchmark datasets for image denoising and super-resolution demonstrate that our proposed method achieves state-of-the-art performance, exhibiting superior perceptual quality and reduced artifact generation compared to existing invertible and non-invertible approaches. This work offers a significant step towards robust and reliable reversible image conversion by explicitly addressing the inherent ill-posedness of the problem."
http://arxiv.org/abs/2508.02056v1,StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion,"3D human pose estimation from monocular videos is a challenging task due to depth ambiguity and occlusions. Existing methods often struggle to produce temporally consistent and accurate pose sequences, especially when dealing with complex motions. To address this, we introduce StarPose, a novel spatial-temporal autoregressive diffusion framework for 3D human pose estimation. Our method leverages a diffusion model conditioned on past and future pose estimates to iteratively refine the current pose, effectively capturing long-range temporal dependencies. We further incorporate a spatial attention mechanism to focus on relevant body joints during the diffusion process, enhancing the accuracy of individual pose predictions. Experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that StarPose achieves state-of-the-art performance in terms of both accuracy and temporal smoothness, surpassing existing autoregressive and diffusion-based methods. This work highlights the potential of diffusion models for robust and temporally coherent 3D human pose estimation from video."
http://arxiv.org/abs/2508.01941v1,Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation,"Accurate and efficient 3D medical image segmentation is crucial for various clinical applications, yet deep learning models often require significant computational resources, hindering their deployment in resource-constrained environments. This paper addresses the challenge of developing lightweight and accurate 3D medical image segmentation models suitable for deployment in scenarios with limited computational power. We introduce AMBER-AFNO, a novel benchmark dataset tailored for evaluating the performance of lightweight models, consisting of diverse abdominal organs with annotations derived from the AFNO challenge. Furthermore, we propose a streamlined segmentation framework employing Axial Fusion Network with Optimized Bottleneck Residual blocks (AFNO) integrated with knowledge distillation techniques to enhance the performance of a compact student model. Experimental results on the AMBER-AFNO benchmark demonstrate that our proposed framework achieves a competitive Dice score of 82.5% with a significantly reduced parameter count and computational cost compared to state-of-the-art, heavier models. This work facilitates the development and deployment of efficient 3D medical image segmentation solutions, broadening accessibility and enabling real-time clinical decision support."
http://arxiv.org/abs/2508.01845v1,Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems,"Adversarial attacks, crafted perturbations imperceptible to humans, pose a significant threat to the reliability and security of computer vision systems. While initially conceived as vulnerabilities, adversarial examples have paradoxically emerged as a valuable tool for improving model robustness. This survey provides a comprehensive overview of adversarial attacks in computer vision, moving beyond their conventional portrayal as solely threats to explore their dual role as defenses. We categorize attacks based on their threat model (e.g., white-box, black-box), perturbation type (e.g., L_p norm bounded), and target (e.g., targeted, untargeted). Subsequently, we analyze how adversarial attacks are leveraged in various defense mechanisms, including adversarial training, certified defenses, and input transformation techniques. We critically examine the strengths and weaknesses of each defense strategy, highlighting their effectiveness against different attack types and computational costs. Furthermore, we discuss the transferability of attacks and defenses across different architectures and datasets, revealing crucial insights into the generalizability of robustness. Our analysis demonstrates that adversarial attacks, when strategically employed, can significantly enhance the resilience and trustworthiness of computer vision systems, paving the way for more robust and secure real-world applications."
http://arxiv.org/abs/2508.01772v1,LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation,"Subarachnoid hemorrhage (SAH) is a life-threatening condition requiring prompt diagnosis and treatment, where accurate segmentation of hematoma is crucial for clinical decision-making. However, training deep learning models for SAH segmentation demands large, labeled datasets, which are often scarce, limiting the applicability of complex models. This paper investigates the use of Low-Rank Adaptation (LoRA) for efficient transfer learning on a U-Net architecture to address the data scarcity challenge in SAH segmentation. We fine-tune a pre-trained U-Net, initialized with weights from a large publicly available medical imaging dataset, by incorporating LoRA modules into the convolutional layers. These modules allow for adaptation to the SAH segmentation task using only a small fraction of trainable parameters, significantly reducing computational cost and mitigating overfitting. Experimental results on a clinically acquired SAH dataset demonstrate that LoRA-tuned U-Net achieves comparable segmentation performance to a fully fine-tuned U-Net, while requiring substantially fewer trainable parameters (up to 90% reduction). This highlights the potential of LoRA-based transfer learning as a practical and effective approach for SAH segmentation, particularly in data-constrained scenarios."
http://arxiv.org/abs/2508.01386v1,Construction of Digital Terrain Maps from Multi-view Satellite Imagery using Neural Volume Rendering,"Digital Terrain Maps (DTMs) are crucial for various applications, including urban planning, environmental monitoring, and autonomous navigation. While traditional methods for DTM generation from multi-view satellite imagery rely on stereo matching and photogrammetry, they often struggle in areas with low texture, repetitive patterns, or occlusions. This paper addresses the challenge of generating high-resolution DTMs from multi-view satellite imagery by leveraging neural volume rendering. We propose a novel framework that learns a continuous volumetric representation of the terrain from a set of satellite images with known camera poses. Our method employs a multi-layer perceptron (MLP) to encode spatial locations into density and feature vectors, which are then ray-marched and aggregated to render synthetic images that are compared to the input satellite images. The MLP is trained end-to-end using a photometric loss, enabling the recovery of a detailed and accurate DTM. Experiments on diverse satellite datasets demonstrate that our approach outperforms state-of-the-art stereo matching techniques, particularly in challenging areas with complex topography, achieving significantly lower elevation errors and improved visual quality. This work presents a robust and accurate method for DTM generation, paving the way for enhanced geospatial analysis and modeling."
http://arxiv.org/abs/2508.01385v1,Lightweight Backbone Networks Only Require Adaptive Lightweight Self-Attention Mechanisms,"Deep learning models have achieved remarkable success in computer vision tasks, but their computational demands often hinder deployment on resource-constrained devices. This paper addresses the challenge of designing efficient and effective lightweight backbone networks for image recognition without sacrificing accuracy. We propose that lightweight backbones benefit most from self-attention mechanisms that are themselves lightweight and adapt to the specific features extracted at each layer. Our Adaptive Lightweight Self-Attention (ALSA) dynamically adjusts the attention span and complexity based on the feature map's statistics, minimizing computational overhead while maximizing representational power. ALSA integrates seamlessly into existing lightweight architectures, requiring minimal modifications. Experiments on ImageNet demonstrate that lightweight backbones equipped with ALSA achieve comparable or even superior performance to heavier self-attention-equipped networks, while significantly reducing computational cost and parameter count. These results highlight the importance of adaptive and lightweight self-attention mechanisms for optimizing the performance of resource-efficient computer vision models."
http://arxiv.org/abs/2508.01381v1,ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers,"Reconstructing 3D clothed humans from single images remains a challenging task due to complex clothing deformations and occlusions. Existing methods often struggle to accurately model layered clothing and disentangle the underlying body shape from garment details. We address this problem by proposing ReMu, a novel framework for Reconstructing Multi-layer 3D clothed humans from Image Layers. ReMu leverages a layered representation, explicitly modeling the human body, inner clothing, and outer clothing as separate deformable meshes. These layers are reconstructed sequentially, starting with the body shape estimated from an image, followed by inner and outer garments predicted using a learned deformation field conditioned on the body and previous layers. We introduce a novel layer-aware loss function that encourages accurate layer separation and realistic clothing interactions. Experiments on synthetic and real-world datasets demonstrate that ReMu significantly improves the accuracy and realism of 3D clothed human reconstruction, especially in challenging scenarios with complex clothing. This layered reconstruction approach offers a more detailed and controllable representation for downstream applications such as virtual try-on and animation."
http://arxiv.org/abs/2508.01350v1,Classification of Brain Tumors using Hybrid Deep Learning Models,"Brain tumor classification from magnetic resonance imaging (MRI) is crucial for accurate diagnosis and treatment planning. Manual analysis of MRI scans is time-consuming and prone to inter-observer variability, highlighting the need for automated and reliable classification methods. This paper addresses the problem of accurately classifying brain tumors into different types using deep learning techniques. We propose a novel hybrid deep learning model that combines the strengths of Convolutional Neural Networks (CNNs) for feature extraction and Recurrent Neural Networks (RNNs), specifically Long Short-Term Memory (LSTM), for contextual information processing. The CNN component, pre-trained on a large image dataset, extracts relevant features from MRI slices, which are then fed into the LSTM network to capture spatial dependencies between slices. Our experimental results on a benchmark brain tumor MRI dataset demonstrate that the proposed hybrid model achieves a significantly higher classification accuracy compared to state-of-the-art CNN-based methods, with an average accuracy of 96.2% and improved sensitivity and specificity. This improved accuracy in brain tumor classification can lead to more effective clinical decision-making and improved patient outcomes."
http://arxiv.org/abs/2508.01223v1,ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference,"Spiking Neural Networks (SNNs) offer potential advantages in energy efficiency compared to traditional Artificial Neural Networks (ANNs), particularly when deployed on neuromorphic hardware. However, training deep SNNs remains a significant challenge due to the non-differentiable nature of the spiking mechanism. To address this, we introduce ParaRevSNN, a novel Parallel Reversible Spiking Neural Network architecture designed for efficient training and inference. ParaRevSNN leverages reversible layers within each block to enable direct gradient computation during backpropagation, circumventing the need for surrogate gradients. Additionally, the network employs parallel processing of membrane potentials, allowing for increased throughput and reduced latency. Experimental results on neuromorphic datasets demonstrate that ParaRevSNN achieves comparable or superior accuracy to state-of-the-art SNNs while exhibiting significantly faster training convergence and improved energy efficiency during inference on simulated neuromorphic hardware. This work paves the way for deploying deep, high-performance SNNs in resource-constrained environments."
http://arxiv.org/abs/2508.01112v1,MASIV: Toward Material-Agnostic System Identification from Videos,"System identification, the process of building mathematical models from observed data, is crucial for understanding and predicting the behavior of physical systems. However, current video-based system identification techniques often rely on material-specific knowledge or require extensive calibration, limiting their applicability to novel materials or complex environments. This paper addresses the challenge of material-agnostic system identification from videos by introducing MASIV, a novel framework that leverages differentiable physics and neural radiance fields (NeRFs) to simultaneously infer material properties, system parameters, and the underlying dynamics. MASIV reconstructs a dynamic 3D scene from video using a NeRF, estimates forces acting on objects through differentiable rendering and physics simulation, and optimizes material properties and system parameters to minimize the discrepancy between predicted and observed motion. Experiments on both synthetic and real-world datasets demonstrate MASIV's ability to accurately identify system parameters and material properties, even with limited data and unknown material composition, surpassing existing methods that require material-specific calibration. MASIV represents a significant step towards developing robust and generalizable video-based system identification techniques applicable across a wide range of materials and dynamic systems."
http://arxiv.org/abs/2508.00506v1,Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool,"Remote sensing image analysis relies heavily on accurately labelled datasets, but manual annotation is time-consuming and expensive, especially for large-scale imagery. This paper addresses the challenge of automatically generating pseudo-labels for remote sensing data in an unsupervised manner, reducing the reliance on human annotation. We propose a novel framework integrating Convolutional Neural Networks (CNNs) for feature extraction with Graph Neural Networks (GNNs) for contextual reasoning and label propagation. The CNN learns a rich feature representation of the remote sensing imagery, which is then used to construct a graph where nodes represent image patches and edges represent spatial proximity and feature similarity. The GNN refines initial cluster assignments based on these relationships, iteratively propagating labels across the graph to generate consistent and spatially coherent pseudo-labels. Experiments on multiple remote sensing datasets demonstrate that our method achieves significantly higher accuracy compared to traditional unsupervised clustering techniques, approaching the performance of weakly supervised methods. This unsupervised labelling tool provides a valuable resource for training deep learning models in remote sensing applications, unlocking the potential for large-scale automated analysis."
http://arxiv.org/abs/2508.00381v1,Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis,"Maritime operations heavily rely on the structural integrity of welded joints, making automated defect detection crucial for safety and efficiency. However, the variability in weld appearance due to differing welding techniques and environmental conditions presents a significant challenge for robust defect identification. This paper introduces Adapt-WeldNet, a novel deep learning framework designed to improve welding defect detection in maritime applications through domain adaptation and enhanced interpretability. Adapt-WeldNet leverages a multi-stage training process, initially pre-trained on a large synthetic welding dataset before fine-tuning on limited real-world maritime weld images using an adversarial domain adaptation module to bridge the domain gap. Furthermore, we integrate a Defect Detection Interpretability (DDI) module, utilizing attention mechanisms and Grad-CAM visualization techniques, to provide insights into the network's decision-making process. Experimental results demonstrate that Adapt-WeldNet achieves a 15% improvement in mean Average Precision (mAP) compared to state-of-the-art object detection models when evaluated on a challenging maritime welding defect dataset. The DDI module provides valuable explanations for defect classifications, increasing trust and facilitating human-in-the-loop validation. This research contributes a practical and interpretable solution for automated welding defect detection, enhancing the reliability and safety of maritime infrastructure."
http://arxiv.org/abs/2508.00366v1,SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies,"Neural implicit surface reconstruction has shown promising results in creating high-quality 3D models from multi-view images. However, performance degrades significantly when the number of input views is sparse, due to insufficient geometric constraints and feature ambiguities. This paper addresses the problem of reconstructing accurate and complete 3D surfaces from sparse views using neural implicit representations. We propose SparseRecon, a novel approach that enforces both feature and depth consistencies to guide the learning of the implicit surface. Specifically, we introduce a feature consistency loss that encourages the learned feature embeddings of the implicit surface to align with the features extracted from input images, facilitating cross-view information propagation. Furthermore, we incorporate a depth consistency loss that leverages sparse depth measurements, when available, to regularize the geometry of the reconstructed surface. Experiments on both synthetic and real-world datasets demonstrate that SparseRecon significantly outperforms state-of-the-art methods in terms of reconstruction accuracy, completeness, and robustness, especially under sparse view conditions. This work provides a valuable solution for 3D reconstruction in scenarios where acquiring dense multi-view images is challenging."
http://arxiv.org/abs/2508.00265v2,Multimodal Referring Segmentation: A Survey,"Referring segmentation, the task of segmenting an object in an image based on a natural language expression, has gained increasing attention due to its role in vision-language understanding. However, the rapid development of this field has resulted in a fragmented landscape of approaches, making it challenging to grasp the current state-of-the-art and identify promising research directions. This survey provides a comprehensive overview of the multimodal referring segmentation field, systematically categorizing existing methods based on their architectural designs, fusion strategies, and training paradigms. We analyze the evolution of these techniques, highlighting key advancements in visual and linguistic feature extraction, cross-modal interaction, and segmentation refinement. Furthermore, we critically examine commonly used datasets and evaluation metrics, pointing out their limitations and suggesting potential improvements for benchmarking future research. Finally, we identify open challenges and outline promising future research directions, such as exploring few-shot learning, incorporating contextual reasoning, and developing more robust and generalizable models. This survey serves as a valuable resource for researchers and practitioners seeking to understand the current landscape and future trends in multimodal referring segmentation."
http://arxiv.org/abs/2508.00248v1,Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network,"Depth map super-resolution (SR) aims to enhance the resolution of low-resolution depth maps, crucial for various applications like 3D reconstruction and autonomous navigation. Existing methods often struggle with preserving fine details and handling complex scene structures, particularly when relying solely on guidance images. This paper introduces a novel Guided Depth Map Super-Resolution framework leveraging a Multi-Scale Fusion U-shaped Mamba Network (MSF-UMamba). Our approach exploits the strengths of the Mamba architecture, known for its efficient long-range dependency modeling, within a U-shaped structure to capture both global context and local details. Furthermore, we incorporate multi-scale feature fusion to effectively integrate information from different resolutions, guided by the high-resolution guidance image. Experimental results on benchmark datasets demonstrate that our MSF-UMamba network achieves state-of-the-art performance in terms of quantitative metrics and qualitative visual quality, outperforming existing convolutional and transformer-based methods. The proposed method significantly improves the accuracy and visual fidelity of depth map super-resolution, enabling more robust and reliable 3D scene understanding."
http://arxiv.org/abs/2508.00205v1,Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition,"Human personality recognition is a crucial aspect of social intelligence, with applications ranging from personalized human-computer interaction to mental health assessment. Existing approaches often rely on direct self-reports or assume a universal mapping between external behaviors and personality traits, neglecting the significant influence of individual internal cognitive processes on expressive behavior. This paper addresses the problem of learning personalized mappings between observed external expressive behaviors (e.g., facial expressions, body language, speech patterns) and underlying internal cognitive states to improve the accuracy and nuance of personality recognition. We propose a novel framework that leverages a multi-modal deep learning architecture incorporating attention mechanisms to model the relationship between external behaviors and inferred internal cognitive representations. Specifically, we train personalized cognitive 'decoders' for each individual, mapping their unique behavioral patterns to latent cognitive states, which are then used to predict personality traits. Experimental results on a benchmark dataset demonstrate a significant improvement in personality recognition accuracy compared to state-of-the-art methods, particularly in capturing subtle individual differences. This work offers a promising avenue for developing more accurate and personalized personality recognition systems by explicitly modeling the influence of individual internal cognition on external expressive behaviors."
http://arxiv.org/abs/2508.00135v2,Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images,"Accurate gender classification plays a crucial role in various applications, ranging from demographic analysis to personalized human-computer interaction. While traditionally relying on full facial images, this work explores the feasibility of gender classification using only periocular regions, specifically eye images, offering potential advantages in scenarios with partial occlusion or low resolution. This paper addresses the challenge of achieving high accuracy gender classification from eye images, which inherently contain less discriminative information compared to full faces. We propose a deep learning framework leveraging Convolutional Neural Networks (CNNs) pre-trained on large-scale image datasets, followed by fine-tuning on a newly curated dataset of eye images. Furthermore, we investigate the impact of different CNN architectures, including ResNet and EfficientNet variants, and explore the effectiveness of transfer learning and data augmentation techniques to enhance performance. Our experiments demonstrate that the fine-tuned EfficientNet-B0 model achieves a promising accuracy of 89.2% on the held-out test set, indicating the potential of deep learning for gender classification from eye images. This study provides a valuable contribution towards developing robust and privacy-preserving gender recognition systems."
http://arxiv.org/abs/2508.03738v1,Improve Retinal Artery/Vein Classification via Channel Couplin,"Retinal artery/vein (A/V) classification is crucial for diagnosing various systemic and ocular diseases. However, accurate A/V classification remains challenging due to subtle structural differences between arteries and veins, variations in image quality, and the inherent ambiguity in distinguishing them, especially in complex vascular networks. This paper introduces a novel channel coupling approach to enhance A/V classification performance. Our method leverages a Siamese network architecture incorporating a channel attention mechanism designed to explicitly model the interdependencies between feature channels extracted from paired retinal images. Furthermore, we introduce a novel channel coupling loss function that encourages the network to learn discriminative features by maximizing the feature similarity between corresponding A/V pairs while minimizing similarity between non-corresponding pairs. Experimental results on benchmark datasets demonstrate that our proposed method achieves state-of-the-art A/V classification accuracy, surpassing existing methods by a significant margin. This improved A/V classification accuracy has the potential to enhance automated diagnostic systems for early detection and monitoring of retinal and cardiovascular diseases."
http://arxiv.org/abs/2507.23643v2,FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free Training Framework for Spiking Neural Networks,"Spiking Neural Networks (SNNs) offer the potential for energy-efficient computation on neuromorphic hardware but remain challenging to train effectively due to the non-differentiable nature of spiking activity. Traditional training methods often rely on surrogate gradients to approximate the derivative of the spiking function, introducing inaccuracies and limiting performance. This paper addresses the limitations of surrogate gradient methods by proposing FFGAF-SNN, a novel training framework for SNNs based on the Forward-Forward (FF) algorithm, eliminating the need for explicit gradient calculation or approximation. Our approach utilizes the FF algorithm to train each layer of the SNN independently by maximizing a goodness score that reflects the consistency of spiking activity with the input. We demonstrate that FFGAF-SNN achieves competitive or superior performance compared to state-of-the-art surrogate gradient methods on benchmark datasets such as MNIST, Fashion-MNIST, and CIFAR-10, while avoiding the complexities and potential inaccuracies associated with gradient approximation. These results highlight the promise of gradient-free methods for training high-performing and energy-efficient SNNs, paving the way for their wider adoption in real-world applications."
http://arxiv.org/abs/2507.23521v1,JPEG Processing Neural Operator for Backward-Compatible Coding,"JPEG remains a ubiquitous image compression standard, owing to its widespread adoption and hardware support. However, modern deep learning-based image compression methods often outperform JPEG but lack backward compatibility, hindering their seamless integration into existing systems. This paper addresses the challenge of improving JPEG coding efficiency while maintaining backward compatibility with legacy JPEG decoders. We introduce the JPEG Processing Neural Operator (JPNO), a learnable module that operates directly on the quantized Discrete Cosine Transform (DCT) coefficients within the JPEG encoding pipeline. JPNO leverages a neural network to refine the quantized DCT coefficients before entropy coding, effectively reducing redundancy and improving compression. Experiments demonstrate that JPNO significantly enhances the rate-distortion performance of standard JPEG, achieving substantial bit-rate savings at comparable or improved image quality. This approach allows for incremental improvements to existing JPEG infrastructure without requiring modifications to decoders, facilitating a smooth transition towards more efficient image compression."
http://arxiv.org/abs/2507.23479v1,Seeing More with Less: Video Capsule Endoscopy with Multi-Task Learning,"Video Capsule Endoscopy (VCE) is a minimally invasive procedure for visualizing the small intestine, generating vast amounts of data that require extensive manual review by clinicians. This paper addresses the challenge of efficiently and accurately identifying clinically relevant findings within VCE videos while minimizing the computational burden for real-time or near real-time applications. We propose a novel multi-task learning framework that simultaneously predicts multiple attributes of VCE frames, including the presence of lesions (e.g., bleeding, ulcers), anatomical location (e.g., duodenum, jejunum, ileum), and image quality (e.g., blur, artifacts). Our model leverages a shared convolutional backbone coupled with task-specific prediction heads, enabling efficient feature extraction and knowledge sharing across tasks. Experimental results on a large, diverse VCE dataset demonstrate that our multi-task approach achieves state-of-the-art performance in lesion detection, exceeding existing single-task models by a significant margin (e.g., a 5% increase in mAP), while also providing valuable contextual information about anatomical location and image quality. This holistic and efficient approach has the potential to significantly reduce the burden on clinicians and improve the accuracy of VCE diagnosis."
http://arxiv.org/abs/2507.23455v1,Machine learning and machine learned prediction in chest X-ray images,"Chest X-rays (CXRs) are a widely accessible and cost-effective diagnostic tool in healthcare. However, the interpretation of CXRs is often subjective and can be time-consuming, leading to potential delays in diagnosis and treatment. This paper addresses the problem of developing automated, machine learning-based methods for accurate and efficient prediction of various thoracic diseases from CXR images. We propose a novel framework that combines convolutional neural networks (CNNs) for feature extraction with a multi-label classification architecture trained on a large, publicly available CXR dataset. Specifically, we explore different CNN backbones, including DenseNet and ResNet variants, and incorporate attention mechanisms to highlight disease-relevant regions in the images. Our experiments demonstrate that the proposed model achieves state-of-the-art performance in predicting multiple thoracic diseases, surpassing existing methods in terms of both accuracy and area under the ROC curve (AUC). This work offers a promising approach for developing clinically useful tools that can assist radiologists in interpreting CXRs and improving patient outcomes."
http://arxiv.org/abs/2507.23374v1,NeRF Is a Valuable Assistant for 3D Gaussian Splatting,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, offering impressive realism but often at the cost of significant rendering time. 3D Gaussian Splatting (3D-GS) emerges as a compelling alternative, achieving real-time rendering speeds while maintaining comparable visual quality. However, 3D-GS relies on a challenging optimization process that can be sensitive to initialization and prone to local minima, leading to artifacts and incomplete scene reconstruction. This paper investigates how NeRF can be leveraged to assist and improve the training of 3D Gaussian Splatting. We propose a novel two-stage training pipeline: first, a NeRF model is trained on the input images to provide a robust, albeit slow, scene representation. Then, this NeRF model acts as a teacher, providing supervision signals in the form of density and color priors to guide the initialization and subsequent optimization of the 3D-GS model. Our experiments on diverse datasets demonstrate that NeRF-guided 3D-GS consistently achieves faster convergence, reduces artifacts, and improves overall reconstruction quality, as measured by PSNR, SSIM, and LPIPS, compared to training 3D-GS from scratch. This work highlights the synergistic potential of combining implicit and explicit scene representations, paving the way for more efficient and robust 3D reconstruction and rendering techniques."
http://arxiv.org/abs/2507.23268v2,PixNerd: Pixel Neural Field Diffusion,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, yet their reliance on per-scene optimization limits their applicability to generalizable scenarios. Existing diffusion models offer impressive generative capabilities, but often struggle to produce high-resolution, geometrically consistent 3D content. This work addresses the challenge of directly generating high-resolution, multi-view consistent images from a single latent code by introducing PixNerd: Pixel Neural Field Diffusion. PixNerd leverages a diffusion model conditioned on camera poses to generate a neural radiance field directly in pixel space. Specifically, we train a denoising diffusion probabilistic model (DDPM) to iteratively refine a noisy pixel neural field, guided by a learned prior over multi-view consistent scenes. Our experiments demonstrate that PixNerd achieves state-of-the-art results in novel view synthesis compared to existing generative NeRF models, producing significantly sharper images and more accurate geometry, particularly at higher resolutions. This approach unlocks the potential for generating complex and realistic 3D scenes with unprecedented detail and consistency."
http://arxiv.org/abs/2507.23174v1,CNN-based solution for mango classification in agricultural environments,"Mango fruit classification is crucial for automated harvesting, yield estimation, and quality control in modern agriculture. However, accurate and robust classification in uncontrolled agricultural environments presents significant challenges due to variations in lighting, fruit occlusion, and complex backgrounds. This paper addresses the problem of accurate mango classification (ripe, unripe, diseased) directly from images acquired in real-world orchard settings. We propose a Convolutional Neural Network (CNN)-based solution, MangoNet, leveraging a transfer learning approach with a pre-trained ResNet50 architecture, fine-tuned with a novel data augmentation strategy incorporating adversarial examples to improve robustness to image variations. Furthermore, we introduce a custom loss function that balances inter-class discrimination and intra-class compactness. Experimental results on a newly collected dataset of mango images demonstrate that MangoNet achieves a classification accuracy of 94.7%, outperforming existing state-of-the-art object detection and classification models by a significant margin (average improvement of 5.2% in F1-score). This research provides a practical and accurate solution for mango fruit classification, paving the way for efficient and automated agricultural practices."
http://arxiv.org/abs/2507.23162v1,Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues,"Photometric stereo (PS) enables 3D reconstruction by analyzing shading variations under different lighting conditions. Traditional PS methods require calibrated light directions and known surface albedo, presenting a significant challenge in uncontrolled environments. This paper addresses the problem of recovering accurate 3D geometry and albedo from multi-view images under unknown lighting and without relying on any explicit photometric stereo cues like light direction calibration objects. We introduce a novel neural multi-view photometric stereo framework that jointly optimizes for shape, albedo, per-view lighting, and camera poses in a self-supervised manner. Our method leverages differentiable rendering and multi-view consistency to learn these parameters directly from image observations, incorporating a novel lighting representation that enforces physically plausible constraints. Experiments on synthetic and real-world datasets demonstrate that our approach achieves state-of-the-art reconstruction accuracy compared to existing multi-view stereo and photometric stereo techniques, even in the absence of photometric stereo calibration data. This work significantly broadens the applicability of photometric stereo to scenarios where traditional calibration is impractical, enabling robust 3D reconstruction from readily available multi-view image data."
http://arxiv.org/abs/2507.23033v1,Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, achieving impressive realism by representing scenes as continuous volumetric functions. However, efficient rendering of NeRFs remains a challenge, and spiking neural networks (SNNs) offer a promising avenue due to their energy efficiency and event-driven nature. A key obstacle in training spike-based NeRFs (Spike-NeRFs) is determining the optimal time-step size, balancing accuracy and computational cost. We address this by introducing an adaptive time-step training strategy for Spike-NeRFs. Our method dynamically adjusts the time-step size during training based on the magnitude of the error gradients, allocating finer time resolutions to regions with higher reconstruction error and coarser resolutions to less critical areas. This allows for efficient exploration of the solution space and improved convergence. Experiments demonstrate that our adaptive time-step approach significantly enhances the performance of Spike-NeRFs, achieving comparable or superior rendering quality to fixed time-step methods with reduced computational overhead. This work paves the way for deploying energy-efficient and high-fidelity NeRFs on resource-constrained platforms."
http://arxiv.org/abs/2507.22873v1,LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content,"Real-time super-resolution (SR) is crucial for enhancing the visual fidelity of games, particularly on resource-constrained devices. However, deploying deep learning-based SR models often incurs significant computational overhead, limiting their applicability in power-sensitive gaming scenarios. This paper introduces LCS, a novel AI-based Low-Complexity Scaler designed for power-efficient super-resolution of game content. LCS leverages a lightweight convolutional neural network (CNN) architecture coupled with a carefully optimized scaling factor prediction network. This architecture predicts optimal scaling factors for different image regions, allowing for adaptive upscaling with minimal computational cost. Experimental results demonstrate that LCS achieves comparable visual quality to state-of-the-art SR methods while significantly reducing computational complexity and power consumption, achieving a 40% reduction in inference time compared to traditional CNN-based SR approaches on mobile GPUs. LCS offers a practical and efficient solution for high-quality, real-time super-resolution in gaming, enabling enhanced visual experiences on a wider range of devices."
http://arxiv.org/abs/2507.22567v2,Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination,"Radar-based human activity recognition offers privacy-preserving and illumination-invariant sensing capabilities, making it suitable for diverse applications, including elderly care and smart environments. However, achieving accurate human motion direction determination using low-cost radar systems remains a significant challenge due to limitations in range resolution and angular coverage. This paper addresses the problem of accurately inferring human motion direction (e.g., approaching, receding, left, right) using a single, low-cost Frequency Modulated Continuous Wave (FMCW) radar sensor. We propose a novel approach that combines micro-Doppler analysis with a carefully designed feature extraction and selection process, leveraging both time-frequency representations and statistical descriptors of the radar signal. Furthermore, we employ machine learning classifiers, specifically Support Vector Machines (SVMs) and Random Forests, trained on the extracted features to classify motion directions. Experimental results demonstrate that our proposed method achieves an average accuracy of over 90% in distinguishing between four cardinal motion directions, significantly outperforming baseline approaches relying solely on Doppler frequency shifts. This work provides a cost-effective and accurate solution for radar-based human motion direction determination, enabling wider adoption of radar technology in various human-computer interaction and monitoring applications."
http://arxiv.org/abs/2508.04233v1,DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification,"Document Image Classification (DIC) plays a crucial role in automating document processing workflows across various domains. However, the lack of interpretability in complex DIC models hinders trust and adoption, especially in high-stakes applications. We address the problem of generating visual counterfactual explanations for DIC, providing insights into how altering specific visual elements in a document image can change the model's prediction. Our proposed method, DocVCE, leverages a diffusion model conditioned on both the original image and the desired counterfactual class. DocVCE iteratively modifies the input image guided by the diffusion process, generating realistic and plausible counterfactual examples that lead to the specified change in classification. Through quantitative and qualitative evaluations on benchmark datasets, we demonstrate that DocVCE generates high-quality counterfactual explanations that are both effective in changing model predictions and visually coherent. This approach offers a valuable tool for understanding and debugging DIC models, fostering transparency and trust in their deployment."
http://arxiv.org/abs/2508.04211v1,What Holds Back Open-Vocabulary Segmentation?,"Open-vocabulary segmentation aims to identify and segment image regions corresponding to arbitrary textual descriptions, bypassing the limitations of closed-set segmentation methods. However, significant performance gaps persist between open-vocabulary segmentation models and their closed-set counterparts, hindering real-world applicability. This paper investigates the factors limiting the performance of current open-vocabulary segmentation approaches, focusing on the impact of image-text alignment, contextual reasoning, and the generalization capability of visual and textual embeddings. We propose a novel framework incorporating a multi-granularity alignment module that explicitly learns correspondences between image regions and textual concepts at different levels of abstraction. Furthermore, we introduce a contextual reasoning module to leverage surrounding visual information for improved segmentation accuracy, along with a curriculum learning strategy to enhance the generalization of the visual-textual embeddings. Experiments on multiple open-vocabulary segmentation benchmarks demonstrate significant improvements over existing state-of-the-art methods, closing a substantial portion of the performance gap with closed-set segmentation. This work offers valuable insights into the challenges of open-vocabulary segmentation and paves the way for more robust and adaptable segmentation systems."
http://arxiv.org/abs/2508.04181v1,Deeper Inside Deep ViT,"Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks, often rivaling or surpassing Convolutional Neural Networks (CNNs). However, the internal representations and operational mechanisms within deep ViTs remain relatively opaque, hindering our understanding of their superior performance. This paper addresses the challenge of interpreting the inner workings of deep ViTs by proposing a novel analysis framework that combines attention visualization, feature map analysis, and layer-wise relevance propagation (LRP). Our approach dissects the information flow within ViTs, revealing the specific roles of individual attention heads and the evolution of feature representations across different layers. We demonstrate that early layers primarily focus on low-level features like edges and textures, while deeper layers progressively aggregate these features into more complex, semantically meaningful representations. Furthermore, LRP analysis identifies crucial image regions that significantly influence the final classification decision, highlighting the network's focus on discriminative features. Our findings provide valuable insights into the internal decision-making processes of deep ViTs, paving the way for more interpretable and potentially more efficient architectures."
http://arxiv.org/abs/2508.04176v1,Uncertainty-Aware Spatial Color Correlation for Low-Light Image Enhancement,"Low-light image enhancement aims to improve the visibility and perceptual quality of images captured in dimly lit environments. Existing methods often struggle with noise amplification and color distortion due to the inherent uncertainty associated with low-light conditions. This paper introduces an Uncertainty-Aware Spatial Color Correlation (UASCC) method for robust low-light image enhancement. Our approach explicitly models the pixel-wise uncertainty in low-light images using a learned variance map. This uncertainty map is then used to guide a novel spatial color correlation module, which adaptively aggregates information from neighboring pixels, prioritizing reliable regions while mitigating the influence of noisy pixels. We further incorporate a perceptual loss function to preserve natural color rendition and enhance visual appeal. Experimental results on benchmark datasets demonstrate that UASCC achieves state-of-the-art performance, exhibiting superior noise suppression, color fidelity, and overall visual quality compared to existing methods. By explicitly addressing uncertainty, our method offers a more robust and effective solution for enhancing images captured in challenging low-light scenarios."
http://arxiv.org/abs/2508.04147v1,IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control,"Generating realistic and controllable 3D scenes from text prompts remains a significant challenge in computer vision. Existing diffusion-based approaches often struggle with maintaining metric consistency and precise camera control, leading to geometrically implausible and difficult-to-manipulate scenes. To address this, we introduce IDCNet, a novel guided video diffusion framework for generating metric-consistent RGBD scenes with precise camera control. IDCNet leverages a conditional video diffusion model trained on RGBD videos, guided by a novel ""Inverse Depth Consistency"" loss. This loss encourages temporal consistency in the predicted inverse depth maps across video frames, thereby enforcing geometric coherence and allowing for explicit control over camera trajectories. Experiments demonstrate that IDCNet significantly improves the metric consistency and visual quality of generated RGBD scenes compared to state-of-the-art baselines, enabling precise camera navigation and manipulation within the generated environments. IDCNet represents a significant step towards generating realistic and controllable 3D environments from text, opening new avenues for applications in virtual reality, robotics, and content creation."
http://arxiv.org/abs/2508.04136v1,UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval,"Fine-grained visual classification (FGVC) poses a significant challenge due to subtle inter-class variations and limited labeled data. Current few-shot FGVC methods often rely on extensive meta-training or fine-tuning, incurring substantial computational costs and limiting their adaptability to novel unseen categories. This paper introduces UniFGVC, a novel universal training-free few-shot FGVC framework leveraging attribute-aware multimodal retrieval. UniFGVC constructs a unified embedding space by aligning visual features with textual attribute descriptions, enabling zero-shot generalization to unseen classes. Specifically, we use pre-trained vision-language models to extract image and attribute embeddings, followed by an attribute-guided retrieval mechanism to identify the most relevant support samples for each query image, which are then aggregated to predict the class. Extensive experiments on diverse FGVC datasets demonstrate that UniFGVC achieves state-of-the-art performance compared to existing training-based few-shot methods, while eliminating the need for task-specific training. This approach provides a practical and efficient solution for few-shot FGVC, facilitating rapid adaptation to new categories without the burden of extensive training."
http://arxiv.org/abs/2508.04123v1,Excavate the potential of Single-Scale Features: A Decomposition Network for Water-Related Optical Image Enhancement,"Water-related optical images often suffer from severe degradation due to light absorption and scattering, hindering the performance of many vision applications. Existing underwater image enhancement methods predominantly rely on multi-scale feature aggregation to capture contextual information, inadvertently introducing redundant information and increasing computational complexity. This paper addresses the challenge of effectively utilizing single-scale features for water-related optical image enhancement by proposing a novel Decomposition Network (DecompNet). DecompNet decomposes the degraded image into reflectance, transmission, and ambient light components using a specifically designed decomposition module. Subsequently, a refinement module leverages single-scale features to enhance each component individually before recombining them to generate the enhanced image. Experimental results on both synthetic and real-world datasets demonstrate that DecompNet achieves superior performance compared to state-of-the-art methods in terms of both quantitative metrics and qualitative visual quality, while maintaining a relatively low computational cost. This work highlights the potential of single-scale feature exploitation for efficient and effective water-related optical image enhancement."
http://arxiv.org/abs/2508.04122v1,Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation,"Instance segmentation, the task of simultaneously detecting and segmenting individual objects in an image, typically requires large amounts of labeled data. This paper addresses the challenge of zero-shot instance segmentation, where the goal is to segment instances of novel object categories not seen during training. We propose a novel Conditional Latent Diffusion Model (CLDM) framework for this task. Our CLDM leverages a pre-trained text-to-image diffusion model and introduces a conditioning mechanism based on textual descriptions of the target object category. Specifically, we fine-tune the diffusion model to generate latent representations corresponding to instance masks, conditioned on both the input image and the textual description. An instance mask decoder then transforms these latent representations into pixel-level instance segmentation masks. Experiments on challenging benchmark datasets demonstrate that our CLDM achieves state-of-the-art zero-shot instance segmentation performance, surpassing existing methods by a significant margin. This work presents a promising direction for leveraging generative models for open-vocabulary instance segmentation, reducing the reliance on expensive annotated data."
http://arxiv.org/abs/2508.04107v2,Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder,"Referring expression segmentation (RES) aims to segment the object in an image that is described by a natural language expression. Recent advancements in multi-modal large language models (MLLMs) have shown promising results in vision-language tasks; however, their application to RES often suffers from high computational costs and struggles with precise pixel-level localization. To address these limitations, we propose a novel framework that leverages the semantic understanding of MLLMs while incorporating a lightweight mask decoder for efficient and accurate segmentation. Our approach uses the MLLM to extract contextualized visual and linguistic features, which are then fed into a compact transformer-based decoder to predict the segmentation mask. We introduce a novel cross-modal attention mechanism within the decoder to effectively fuse the visual and linguistic cues, guiding the mask prediction process. Experiments on multiple benchmark datasets demonstrate that our method achieves competitive segmentation accuracy with significantly reduced computational overhead compared to existing MLLM-based RES methods. This work unlocks the potential of MLLMs for precise RES, paving the way for efficient and scalable vision-language understanding."
http://arxiv.org/abs/2508.04101v1,NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding,"Vision-Language Pre-training (VLP) has shown promise in medical image understanding by leveraging large-scale image-text data. However, directly applying pre-trained VLP models to specialized medical tasks often suffers from a domain gap and suboptimal adaptation to nuanced medical concepts. To address this, we propose NEARL-CLIP, a novel approach for medical Vision-Language Understanding that incorporates Interacted Query Adaptation with Orthogonal Regularization. NEARL-CLIP refines text query representations through iterative interaction with visual features, enabling precise alignment with medical image content. Furthermore, we introduce an orthogonal regularization term that encourages diverse and complementary feature extraction within the query adaptation process, mitigating redundancy and promoting robust representation learning. Experimental results on multiple medical VQA and image retrieval datasets demonstrate that NEARL-CLIP achieves significant performance gains compared to existing VLP adaptation methods, particularly in handling fine-grained medical concepts. This highlights the potential of our approach for enhancing the accuracy and reliability of medical image analysis through improved vision-language understanding."
http://arxiv.org/abs/2508.04090v1,Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework,"Diffusion models have shown impressive capabilities in image super-resolution, yet often struggle to maintain consistency when applied to multi-view or 3D data. This paper addresses the challenge of generating high-resolution 3D structures from low-resolution inputs while ensuring geometric and textural consistency across different viewpoints. We propose a novel 3D Consistent Super-Resolution framework that leverages a score-based diffusion model conditioned on both the low-resolution input and a learned 3D representation. Specifically, we introduce a neural radiance field (NeRF) as an intermediate representation to enforce 3D consistency and guide the diffusion process towards generating detailed and coherent high-resolution volumes. Our experiments on synthetic and real-world datasets demonstrate that our method significantly outperforms existing super-resolution techniques in terms of both image quality metrics and 3D consistency, achieving state-of-the-art results. This work offers a promising approach for high-fidelity 3D reconstruction and manipulation, facilitating applications in virtual reality, content creation, and scientific visualization."
http://arxiv.org/abs/2508.04062v1,PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography,"Automated radiology report generation (ARRG) aims to alleviate the workload of radiologists by automatically generating descriptive reports from medical images. While ARRG has seen progress in modalities like X-ray and CT, its application to Positron Emission Tomography (PET), which provides crucial functional information, remains relatively unexplored. This paper addresses the challenge of generating accurate and comprehensive radiology reports from PET scans by leveraging the power of Vision-Language Models (VLMs). We introduce PET2Rep, a novel framework that utilizes a pre-trained VLM, specifically fine-tuned for medical imaging, and incorporates a multi-modal fusion strategy to effectively integrate visual features extracted from PET images with learned semantic embeddings of relevant medical concepts. The fusion module allows the model to attend to specific regions within the PET scan while simultaneously incorporating domain-specific knowledge, ultimately guiding the report generation process. Experimental results on a large clinical PET dataset demonstrate that PET2Rep significantly outperforms existing state-of-the-art ARRG methods, achieving superior performance in terms of both textual quality metrics (e.g., BLEU, ROUGE) and clinical relevance (e.g., disease mention accuracy). This work presents a significant step towards automating the generation of high-quality radiology reports for PET scans, potentially improving diagnostic efficiency and reducing radiologist fatigue."
http://arxiv.org/abs/2508.04059v1,Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models,"Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in visual understanding, yet their robustness to real-world challenges like occlusion remains largely unexplored. This paper addresses the critical gap in evaluating occlusion perception within MLLMs, specifically focusing on their ability to infer and reason about objects partially hidden from view. We introduce a novel benchmark, Occlusion Perception Evaluation Suite (OPES), comprising synthetic and real-world images with varying degrees and types of occlusion. OPES includes carefully designed question-answering prompts that probe the MLLMs' understanding of occluded objects' attributes, spatial relationships, and potential functionalities. Our experiments across several state-of-the-art MLLMs reveal significant performance degradation in the presence of occlusion, particularly with increased occlusion levels and complex scenes. These results highlight the limitations of current MLLMs in handling occlusion and underscore the need for developing more robust vision-language models capable of inferring information beyond visible pixels, ultimately paving the way for more reliable and human-like visual understanding in complex environments."
http://arxiv.org/abs/2508.04058v1,TCSAFormer: Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation,"Medical image segmentation is a crucial task for computer-aided diagnosis and treatment planning, often requiring high accuracy and computational efficiency. However, the high resolution and complex structures of medical images pose significant challenges for existing vision transformers, demanding substantial computational resources and memory. To address this, we propose TCSAFormer, an efficient vision transformer architecture for medical image segmentation that leverages Token Compression and Sparse Attention. TCSAFormer introduces a token compression module that adaptively reduces the number of tokens passed to subsequent transformer layers, thereby decreasing computational complexity. Furthermore, we incorporate a novel sparse attention mechanism that focuses on the most relevant regions in the feature maps, further reducing computational cost while preserving crucial spatial information. Experimental results on multiple medical image segmentation datasets demonstrate that TCSAFormer achieves comparable or superior segmentation performance to state-of-the-art methods while significantly reducing computational cost and memory consumption. TCSAFormer offers a practical and efficient solution for deploying vision transformers in medical image analysis, facilitating faster and more accessible diagnostic workflows."
http://arxiv.org/abs/2508.04050v1,DOMR: Establishing Cross-View Segmentation via Dense Object Matching,"Cross-view semantic segmentation aims to transfer knowledge learned from a source view (e.g., synthetic images) to a target view (e.g., real-world images) for pixel-level classification. A major challenge lies in bridging the domain gap and establishing reliable correspondences between the two views. We address this challenge by proposing Dense Object Matching Refinement (DOMR), a novel approach that explicitly establishes dense object-level correspondences to facilitate cross-view segmentation. DOMR utilizes a pre-trained object detector to extract object-level features in both views and then employs a differentiable matching module to find dense, soft correspondences between objects. These correspondences are subsequently used to warp features from the source domain to the target domain, thereby reducing the domain gap. Finally, a refinement network further improves the quality of the warped features and the final segmentation results. Experiments on the challenging SYNTHIA-to-Cityscapes and GTA5-to-Cityscapes benchmarks demonstrate that DOMR significantly outperforms existing methods, achieving state-of-the-art performance. This highlights the effectiveness of dense object matching as a powerful intermediate representation for cross-view semantic segmentation."
http://arxiv.org/abs/2508.04044v1,Iterative pseudo-labeling based adaptive copy-paste supervision for semi-supervised tumor segmentation,"Semi-supervised learning offers a promising avenue to reduce the annotation burden in medical image segmentation by leveraging both labeled and unlabeled data. However, effectively utilizing unlabeled data, particularly for complex tasks like tumor segmentation, remains a challenge. This paper addresses the problem of improving semi-supervised tumor segmentation by generating more robust and informative pseudo-labels for unlabeled data. We propose an iterative pseudo-labeling framework that adaptively combines copy-paste augmentation with consistency regularization. In each iteration, we generate pseudo-labels using an ensemble of models trained on labeled and pseudo-labeled data from previous iterations. These pseudo-labels are then refined using a novel adaptive copy-paste strategy, which selectively transfers tumor regions from high-confidence pseudo-labels to augment the training data. The resulting augmented data is used to train the next generation of models, fostering a cycle of improving pseudo-label quality and segmentation accuracy. Our experiments on the publicly available BraTS 2021 dataset demonstrate that our method significantly outperforms existing semi-supervised segmentation techniques, achieving state-of-the-art performance with limited labeled data. This work provides a practical and effective approach for leveraging unlabeled data to improve tumor segmentation, potentially reducing the need for extensive manual annotation in clinical settings."
http://arxiv.org/abs/2508.04043v1,VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning,"Visual transformation reasoning, the ability to infer how an object changes its appearance under different conditions, is a crucial aspect of visual intelligence. Current benchmarks often rely on synthetic data or constrained environments, failing to capture the complexities of real-world visual transformations. To address this, we introduce VisualTrans, a novel benchmark designed to evaluate visual transformation reasoning in unconstrained real-world scenarios. VisualTrans comprises a diverse collection of image pairs depicting the same object instance undergoing transformations in viewpoint, illumination, weather, and occlusion, sourced from publicly available datasets and annotated with detailed transformation descriptions. We evaluate several state-of-the-art models, including image retrieval, few-shot learning, and visual reasoning architectures, on VisualTrans. Our results demonstrate a significant performance gap between these models and human-level performance, highlighting the challenges of real-world visual transformation reasoning. VisualTrans provides a valuable resource for advancing research in robust and generalizable visual understanding systems."
http://arxiv.org/abs/2508.04041v1,SPJFNet: Self-Mining Prior-Guided Joint Frequency Enhancement for Ultra-Efficient Dark Image Restoration,"Low-light image enhancement is crucial for various computer vision applications, yet existing deep learning methods often demand substantial computational resources. This work addresses the challenge of restoring high-quality images from extremely dark scenes while maintaining ultra-efficient performance. We introduce SPJFNet, a Self-Mining Prior-Guided Joint Frequency Enhancement Network designed for this purpose. SPJFNet leverages self-mining prior knowledge to guide the learning process, enabling more effective feature extraction from noisy and underexposed regions. Furthermore, it employs a novel joint frequency enhancement module that operates in both the spatial and frequency domains, allowing for comprehensive noise suppression and detail recovery. Our experiments on benchmark datasets demonstrate that SPJFNet achieves state-of-the-art performance in terms of both image quality and computational efficiency, outperforming existing methods with significantly fewer parameters and faster processing speeds. SPJFNet offers a practical solution for deploying dark image restoration in resource-constrained environments."
http://arxiv.org/abs/2508.04038v1,ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents,"Motion time-series analysis is critical for understanding human activities in diverse fields like healthcare and sports. However, existing methods often require substantial labeled data for specific tasks, hindering their applicability to novel or rare activities. We address the challenge of zero-shot motion time-series analysis by introducing ZARA, a novel framework leveraging Large Language Model (LLM) agents driven by external knowledge and retrieval mechanisms. ZARA employs a multi-agent system where specialized agents, equipped with task-relevant knowledge retrieved from a knowledge base and contextual examples from a motion time-series retrieval module, collaborate to analyze and interpret motion sequences. The LLM agents perform tasks such as activity recognition, anomaly detection, and motion quality assessment, without requiring task-specific training. Our experiments demonstrate that ZARA achieves state-of-the-art performance in zero-shot settings across various motion analysis tasks, significantly outperforming existing few-shot learning methods. ZARA offers a promising approach for adapting motion analysis to new scenarios with minimal effort by leveraging the power of LLMs and external knowledge."
http://arxiv.org/abs/2508.04036v1,CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion,"Object re-identification (ReID) aims to identify and retrieve specific objects across different camera views, a crucial task for various applications like video surveillance and autonomous driving. Domain adaptation in ReID becomes essential when deploying models in new environments with different camera characteristics and data distributions. This paper addresses the challenge of effectively adapting ReID models from a labeled source domain to an unlabeled target domain. We introduce CORE-ReID V2, an improved domain adaptation framework building upon our previous work. CORE-ReID V2 leverages optimized training strategies, including a novel curriculum learning approach based on sample uncertainty, and an ensemble fusion technique that combines multiple domain-adapted models trained with different pseudo-labeling methods. Experimental results on several challenging ReID datasets demonstrate that CORE-ReID V2 significantly outperforms existing state-of-the-art domain adaptation methods, achieving a relative improvement of up to 5% in mAP and Rank-1 accuracy. The proposed method offers a robust and effective solution for deploying ReID systems in real-world scenarios with domain shift."
http://arxiv.org/abs/2508.04033v1,Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation,"Accurate pedestrian localization is crucial for advanced driver-assistance systems (ADAS), especially in challenging non-line-of-sight (NLoS) urban environments. This paper addresses the problem of localizing pedestrians darting-out from behind parked vehicles, a scenario where traditional vision-based methods often fail due to occlusion. We propose a novel radar-based pedestrian localization framework that leverages the penetration capabilities of radar and enhances its performance through camera-assisted point cloud interpretation. Our method first utilizes a deep learning model to estimate the occupancy grid around parked vehicles from camera images, which is then used to filter out radar reflections originating from static objects. Remaining radar detections are clustered and associated with potential pedestrian locations using a probabilistic model that considers radar cross-section and motion characteristics. Experimental results on a real-world dataset demonstrate that our approach significantly improves the accuracy and robustness of pedestrian localization in NLoS scenarios compared to radar-only and camera-only baselines, achieving a 35% reduction in localization error. This work provides a promising solution for enhancing pedestrian safety in complex urban driving environments."
http://arxiv.org/abs/2508.04028v1,Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval,"Vision-Language Pre-trained Models (VLPMs) have demonstrated remarkable capabilities in various cross-modal tasks. However, adapting these models to specific downstream tasks like image-text retrieval often requires computationally expensive fine-tuning of the entire model. This paper addresses the challenge of efficiently adapting VLPMs for image-text retrieval by introducing a novel Dual Prompt Learning (DPL) approach. DPL learns separate prompt embeddings for both the image and text encoders of a VLPM, enabling task-specific adaptation while keeping the original model parameters frozen. Furthermore, we introduce a cross-modal interaction module within the prompt learning framework to explicitly model the relationships between the visual and textual prompts, facilitating enhanced cross-modal alignment. Experiments on several benchmark datasets, including MSCOCO and Flickr30K, demonstrate that DPL achieves comparable or superior performance to full fine-tuning methods with significantly fewer trainable parameters. This efficient adaptation strategy unlocks the potential of VLPMs for broader application in resource-constrained settings and facilitates rapid prototyping for new image-text retrieval tasks."
http://arxiv.org/abs/2508.04022v1,Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation,"Remote sensing image segmentation plays a vital role in various applications, including urban planning, environmental monitoring, and disaster assessment. However, the inherent complexity of remote sensing imagery, characterized by intricate object boundaries and substantial intra-class variations, poses significant challenges for accurate segmentation. This paper addresses the problem of effectively capturing and leveraging structural information and class prototypes to improve remote sensing image segmentation accuracy. We propose a Prototype-Driven Structure Synergy Network (PSSN) that synergistically integrates structural cues with prototype learning. PSSN employs a novel Structure Extraction Module (SEM) to explicitly model structural relationships between pixels, enhancing boundary delineation. Simultaneously, a Prototype Generation Module (PGM) learns representative prototypes for each class, guiding the segmentation process by minimizing the distance between feature embeddings and their corresponding prototypes. Experimental results on benchmark datasets demonstrate that PSSN achieves state-of-the-art performance, outperforming existing methods in terms of both segmentation accuracy and boundary precision. The proposed PSSN offers a robust and effective framework for remote sensing image segmentation by explicitly modeling structural relationships and leveraging class prototypes."
http://arxiv.org/abs/2508.03997v1,JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation,"Accurate segmentation of multiple organs from 3D medical images is crucial for computer-aided diagnosis and treatment planning. However, acquiring large, fully labeled datasets for training deep learning models remains a significant bottleneck. This paper addresses the challenge of 3D multi-organ segmentation with limited labeled data by introducing JanusNet, a novel semi-supervised learning framework. JanusNet leverages a hierarchical slice-block shuffle strategy to generate diverse pseudo-labels from unlabeled data, promoting robust feature learning. Furthermore, a displacement-aware consistency regularization is incorporated to enforce consistent predictions under spatial perturbations, enhancing the model's generalization capability. Experiments on the publicly available NIH Pancreas dataset and a private liver dataset demonstrate that JanusNet significantly outperforms state-of-the-art semi-supervised segmentation methods, achieving Dice scores of 85.2% and 89.7% respectively, with only 20% labeled data. These results highlight the effectiveness of JanusNet in leveraging unlabeled data for accurate and efficient 3D multi-organ segmentation, paving the way for broader applications in medical image analysis."
http://arxiv.org/abs/2508.03967v1,RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification,"The proliferation of AI-generated images poses a significant challenge to distinguishing authentic visual content from synthetic forgeries. Current detection methods often struggle with the subtle artifacts and evolving generation techniques employed in sophisticated AI models. This paper introduces Retrieval-Augmented Visual Detection (RAVID), a novel knowledge-driven approach for identifying AI-generated images. RAVID leverages a retrieval mechanism to access a vast database of known AI-generated image characteristics and associated metadata. Specifically, given a query image, RAVID retrieves semantically similar images and their corresponding generation parameters from the database. This retrieved information is then fused with features extracted from the query image using a transformer-based architecture to enhance the detection process. Experimental results on a diverse dataset of real and AI-generated images demonstrate that RAVID significantly outperforms state-of-the-art detection methods, achieving an average improvement of 8% in F1-score and demonstrating robustness against adversarial attacks. These findings highlight the potential of knowledge-augmented approaches for improving the reliability and accuracy of AI-generated image detection."
http://arxiv.org/abs/2508.03960v1,Fast Magnetic Resonance Simulation Using Combined Update with Grouped Isochromats,"Magnetic Resonance Imaging (MRI) simulation is crucial for pulse sequence development and quantitative MRI research, but remains computationally expensive, particularly for complex sequences and large volumes. This paper addresses the problem of accelerating MRI simulation while maintaining high accuracy, especially for simulations involving stimulated echoes and off-resonance effects. We introduce a novel approach, Combined Update with Grouped Isochromats (CUGI), that combines Bloch equation solutions using both matrix formalism and analytical approximations, strategically selecting the most efficient method based on local field homogeneity. Furthermore, CUGI groups isochromats experiencing similar magnetic fields, reducing the number of individual calculations required. Our results demonstrate a significant speedup compared to conventional Bloch equation solvers, achieving up to a 20x acceleration in simulations with realistic off-resonance distributions, while maintaining a signal fidelity error of less than 1%. This accelerated and accurate simulation method enables faster prototyping of advanced MRI techniques and more efficient quantitative MRI parameter estimation."
http://arxiv.org/abs/2508.03953v1,Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation,"Prostate cancer localisation using multi-parametric MRI is a challenging task due to the subtle and heterogeneous appearance of tumours. Existing methods often struggle with accurately delineating tumour boundaries, particularly in the presence of anatomical variations and image noise. We address the problem of efficiently guiding iterative segmentation by optimising the selection of both imaging modality and spatial location for subsequent user interaction. Our proposed Policy to Assist Iteratively Local Segmentation (PAILS) employs a reinforcement learning framework. PAILS learns a policy that suggests the most informative modality (T2W, ADC, or DWI) and location (within the current segmentation boundary) for the user to refine the segmentation. The reward function is designed to encourage accurate and efficient segmentation, penalizing redundant interactions. Experiments on a clinical dataset of prostate MRI scans demonstrate that PAILS significantly reduces the number of user interactions required to achieve expert-level segmentation accuracy, outperforming baseline strategies that randomly select modality and location. This approach offers a practical solution for improving the efficiency and reliability of prostate cancer localisation in clinical settings."
http://arxiv.org/abs/2508.03690v1,Veila: Panoramic LiDAR Generation from a Monocular RGB Image,"Generating LiDAR point clouds from readily available RGB images offers a cost-effective alternative to expensive LiDAR sensors, enabling applications such as autonomous navigation and 3D scene understanding in resource-constrained environments. However, existing methods struggle to produce accurate and dense panoramic LiDAR representations from a single monocular image, often resulting in incomplete or geometrically inconsistent point clouds. We introduce Veila, a novel deep learning framework for generating high-quality, panoramic LiDAR point clouds from a single RGB image. Veila employs a two-stage architecture: first, a transformer-based network predicts a dense, perspective-view depth map from the input image. Second, a novel spherical projection module transforms the perspective depth map into a panoramic LiDAR point cloud, explicitly addressing the geometric distortions inherent in monocular depth estimation and panoramic projections. Experiments on the nuScenes dataset demonstrate that Veila significantly outperforms state-of-the-art methods in terms of point cloud density, geometric accuracy, and overall visual fidelity, achieving a substantial improvement in LiDAR-based perception tasks. This advancement opens up new possibilities for deploying 3D perception systems in scenarios where LiDAR data is unavailable or cost-prohibitive."
http://arxiv.org/abs/2508.03789v1,HPSv3: Towards Wide-Spectrum Human Preference Score,"Human preference is increasingly important for aligning AI systems with human values, especially in generative tasks. However, current Human Preference Score (HPS) models often exhibit limited generalization across diverse data distributions and struggle to accurately capture nuanced human judgments in complex scenarios. To address these limitations, we introduce HPSv3, a novel framework for learning a wide-spectrum Human Preference Score. HPSv3 leverages a multi-stage training paradigm incorporating both synthetic and real-world preference data, coupled with a novel contrastive loss function designed to enhance robustness to distributional shifts. Furthermore, we integrate an uncertainty-aware mechanism to dynamically adjust the influence of individual preferences during training, mitigating the impact of noisy or inconsistent human feedback. Experimental results across various generative tasks, including image generation, text summarization, and code completion, demonstrate that HPSv3 significantly outperforms existing HPS models in terms of correlation with human judgments, generalization to unseen datasets, and robustness to adversarial examples. This advancement enables more reliable and accurate alignment of AI systems with human preferences across a wider range of applications."
http://arxiv.org/abs/2508.03643v2,Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images,"Reconstructing and semantically understanding 3D scenes from multi-view images is a fundamental yet challenging problem in computer vision. Existing methods often struggle with generalization to novel scenes and the integration of reconstruction and semantic segmentation into a unified framework. We introduce Uni3R, a novel approach that leverages generalizable Gaussian splatting to simultaneously achieve high-quality 3D reconstruction and semantic understanding from unposed multi-view images. Uni3R employs a view-conditioned encoder-decoder architecture to predict Gaussian parameters and semantic logits directly from input images, enabling end-to-end optimization without explicit pose estimation. Furthermore, we incorporate a differentiable rendering module that allows for joint refinement of both geometry and semantics, promoting consistency between the two tasks. Experiments on synthetic and real-world datasets demonstrate that Uni3R achieves state-of-the-art performance in both 3D reconstruction quality and semantic segmentation accuracy, outperforming existing methods in generalization ability and overall scene understanding. This unified framework provides a promising direction for developing robust and generalizable 3D scene understanding systems."
http://arxiv.org/abs/2508.03608v1,CloudBreaker: Breaking the Cloud Covers of Sentinel-2 Images using Multi-Stage Trained Conditional Flow Matching on Sentinel-1,"Optical satellite imagery, like that from Sentinel-2, is crucial for environmental monitoring and resource management. However, persistent cloud cover significantly limits the usability of Sentinel-2 data, hindering timely and accurate analysis. This paper addresses the challenge of reconstructing cloud-occluded regions in Sentinel-2 imagery by leveraging the complementary information provided by Sentinel-1 radar data. We introduce CloudBreaker, a novel approach utilizing multi-stage trained Conditional Flow Matching (CFM) to generate cloud-free Sentinel-2 images from corresponding Sentinel-1 data. The model is trained in stages, first learning a coarse reconstruction of the underlying land cover and then refining the details in subsequent stages conditioned on both Sentinel-1 and the partially observed Sentinel-2 data. Extensive experiments on diverse geographical regions demonstrate that CloudBreaker significantly outperforms existing cloud removal techniques, achieving superior performance in terms of both quantitative metrics (e.g., PSNR, SSIM) and qualitative visual assessment. The proposed method offers a robust and effective solution for generating high-quality, cloud-free Sentinel-2 imagery, unlocking the full potential of optical satellite data for a wide range of applications."
http://arxiv.org/abs/2508.03598v1,DyCAF-Net: Dynamic Class-Aware Fusion Network,"Multi-object segmentation is a fundamental task in computer vision, enabling fine-grained scene understanding. However, effectively fusing multi-scale features extracted by deep neural networks remains a challenge, particularly in handling inter-class similarity and intra-class variance, often leading to inaccurate segmentation boundaries and misclassification. To address this, we propose DyCAF-Net, a Dynamic Class-Aware Fusion Network for multi-object segmentation. DyCAF-Net introduces a novel dynamic fusion module that generates class-specific attention weights to adaptively aggregate multi-scale features based on the semantic context of each class. This module leverages a class-aware embedding to learn distinct feature representations for different classes, enabling the network to better distinguish between similar objects and handle variations within the same object category. Experimental results on benchmark datasets such as Cityscapes and ADE20K demonstrate that DyCAF-Net achieves state-of-the-art performance, outperforming existing methods by a significant margin in terms of mean Intersection-over-Union (mIoU) and boundary F-score. The proposed DyCAF-Net offers a powerful and efficient solution for multi-object segmentation, advancing the state-of-the-art in scene understanding."
http://arxiv.org/abs/2508.03594v1,CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models,"Brain disease diagnosis often relies on identifying deviations from a normative, healthy state, typically assessed through visual inspection or quantitative analysis against population-based norms. However, subtle or complex disease-related changes can be challenging to detect due to inter-subject variability and the absence of explicit contextual information. This paper addresses the problem of enhancing disease deviation detection by leveraging normative modeling principles within a conditional diffusion framework. We propose Context-Aware Disease Deviations (CADD), a novel approach that trains a conditional diffusion model on healthy brain images, enabling the restoration of diseased brains towards a normative state conditioned on subject-specific clinical context (e.g., age, sex). By analyzing the difference between the original diseased image and its normative restoration, we generate a deviation map highlighting regions exhibiting significant disease-related abnormalities. Experiments on Alzheimer's Disease and Multiple Sclerosis datasets demonstrate that CADD effectively identifies disease-specific regions with superior accuracy compared to existing methods, achieving state-of-the-art performance in disease detection and staging. CADD provides a powerful tool for clinicians and researchers to improve diagnostic accuracy and deepen our understanding of disease-related brain alterations."
http://arxiv.org/abs/2508.03566v1,SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks,"Foundation models like the Segment Anything Model (SAM) have demonstrated remarkable zero-shot capabilities; however, adapting them effectively to downstream segmentation tasks, particularly those requiring high-resolution outputs, remains a challenge. This paper addresses the limitations of directly fine-tuning SAM for high-resolution semantic and instance segmentation by proposing SAM2-UNeXt, a novel architecture built upon the UNeXt framework with enhanced SAM integration. Specifically, we incorporate SAM's image encoder to extract multi-scale features and fuse them with UNeXt's encoder features via cross-attention mechanisms, while also leveraging SAM's mask decoder to refine initial segmentation predictions. Experimental results on several benchmark datasets, including Cityscapes, ADE20K, and COCO, demonstrate that SAM2-UNeXt significantly outperforms existing SAM fine-tuning strategies and other state-of-the-art segmentation models, achieving substantial improvements in mIoU and PQ metrics, particularly at high resolutions. Our work establishes a new, high-performance baseline for adapting foundation models to downstream segmentation tasks, showcasing the potential of synergistic architectures."
http://arxiv.org/abs/2508.03562v1,Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching,"Memes have become a dominant form of online communication, relying heavily on visual cues and contextual understanding for humor and virality. Current approaches to meme matching often rely on visual similarity measures applied to meme templates, overlooking the nuanced ways memes evolve through subtle image modifications and semantic shifts. This paper addresses the limitations of relying solely on visual similarity for accurate meme matching across variations. We propose a novel framework that integrates visual features with contextual information extracted from meme text and associated metadata. Our method employs a multi-modal embedding space, trained using a contrastive loss function, to capture both visual and semantic relationships between memes. Evaluation on a large-scale dataset of diverse meme variations demonstrates that our approach significantly outperforms state-of-the-art visual similarity measures in accurately identifying semantically related memes, achieving a 25% improvement in mean Average Precision. These findings highlight the critical need to move beyond purely visual approaches and incorporate contextual understanding for effective meme matching, enabling more robust meme retrieval, analysis, and understanding."
http://arxiv.org/abs/2508.03539v1,Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection,"Anomaly detection in images is crucial for various applications, but the scarcity of anomalous data poses a significant challenge. Existing anomaly synthesis methods often lack control over the type and quality of generated anomalies, limiting their effectiveness in training robust detectors. This paper introduces a novel Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis (Q-LARA) framework. Q-LARA utilizes a pre-trained language model to embed descriptive anomaly captions and integrates this information into a local auto-regressive generation process conditioned on image patches. A quality discriminator, trained adversarially, further ensures the realism and relevance of the synthesized anomalies based on both visual and textual cues, allowing for fine-grained control over anomaly severity. Experiments on diverse datasets demonstrate that Q-LARA significantly improves the performance of anomaly detection models, achieving state-of-the-art results, particularly in scenarios with complex and subtle anomalies. The proposed framework offers a practical and effective solution for generating high-quality, controllable anomalous data, thereby advancing the field of anomaly detection by enabling the training of more robust and reliable systems."
http://arxiv.org/abs/2508.03535v1,CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation,"Generating images conditioned on specific emotions is a challenging task, often resulting in semantically incoherent or unrealistic content. Existing methods struggle to maintain semantic consistency across generated images while scaling to a diverse range of emotional expressions. To address this, we introduce CoEmoGen, a novel framework for semantically-coherent and scalable emotional image content generation. CoEmoGen leverages a two-stage approach: first, a transformer-based emotion encoder maps emotional text prompts to a latent space, guiding the subsequent image generation. Second, a diffusion model, conditioned on both the encoded emotion and a semantic layout derived from a pre-trained object detection model, generates images that adhere to the specified emotional tone while maintaining semantic plausibility. Experimental results demonstrate that CoEmoGen significantly improves the quality and coherence of generated images compared to state-of-the-art methods, as evidenced by both quantitative metrics and qualitative evaluations. CoEmoGen offers a promising avenue for creating emotionally expressive and semantically meaningful visual content, enabling applications in areas like personalized art creation and affective computing."
http://arxiv.org/abs/2508.03511v1,MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation,"Medical image segmentation is crucial for computer-aided diagnosis, but its performance often suffers from domain shifts and limited labeled data in new clinical environments. Existing few-shot segmentation methods struggle to generalize across diverse medical centers due to variations in imaging protocols and patient populations. To address this, we propose a novel Training-free Multi-center Adaptive Uncertainty-aware Prompting (MAUP) framework for cross-domain few-shot medical image segmentation. MAUP leverages a pre-trained vision-language model and introduces a multi-center prompt pool, where each prompt is associated with a specific source domain. During inference, we estimate the uncertainty of the target domain image using a Monte Carlo dropout-based approach and adaptively select and fuse prompts from the multi-center pool based on their relevance, weighted by the estimated uncertainty. Experiments on multiple cross-domain medical image segmentation benchmarks demonstrate that MAUP achieves significant improvements in segmentation accuracy compared to state-of-the-art few-shot and domain adaptation methods, particularly in challenging scenarios with substantial domain gaps. MAUP offers a practical and effective solution for deploying medical image segmentation models in new clinical settings without requiring additional training or fine-tuning."
http://arxiv.org/abs/2508.03497v1,EditGarment: An Instruction-Based Garment Editing Dataset Constructed with Automated MLLM Synthesis and Semantic-Aware Evaluation,"Garment editing in images is a complex task requiring precise manipulation based on user instructions. Existing datasets often lack the diversity and complexity needed to effectively train and evaluate instruction-based garment editing models. This paper addresses the scarcity of high-quality, instruction-driven garment editing data by introducing EditGarment, a novel dataset synthesized using automated Multi-Modal Large Language Model (MLLM) prompting. Our approach leverages MLLMs to generate diverse and realistic editing instructions paired with corresponding image manipulations. We further introduce a semantic-aware evaluation metric that assesses the faithfulness of edits to the provided instructions, moving beyond simple pixel-level comparisons. Experiments demonstrate the effectiveness of EditGarment in training and evaluating garment editing models, revealing significant improvements in instruction following and visual fidelity compared to models trained on existing datasets. EditGarment provides a valuable resource for advancing research in instruction-based image editing and fashion-related computer vision tasks."
http://arxiv.org/abs/2508.03494v1,Prototype-Enhanced Confidence Modeling for Cross-Modal Medical Image-Report Retrieval,"Cross-modal medical image-report retrieval aims to bridge the semantic gap between visual features of medical images and textual features of radiology reports. A critical challenge lies in accurately modeling the confidence of the learned cross-modal representations, especially when dealing with the inherent ambiguity and complexity of medical data. In this paper, we propose a Prototype-Enhanced Confidence Modeling (PECM) framework for improved cross-modal retrieval. PECM leverages a prototype learning mechanism to capture representative features of different medical conditions within both image and text modalities. These prototypes are then used to refine the confidence scores of learned embeddings, allowing the model to better distinguish between relevant and irrelevant image-report pairs. Experiments on two benchmark datasets, MIMIC-CXR and IU X-Ray, demonstrate that PECM achieves significant improvements over state-of-the-art methods, increasing Recall@1 by up to 5% and 3% respectively. The proposed method provides a more robust and reliable approach for cross-modal retrieval in the medical domain, ultimately facilitating clinical decision-making and research."
http://arxiv.org/abs/2508.03492v1,Quality Versus Sparsity in Image Recovery by Dictionary Learning Using Iterative Shrinkage,"Dictionary learning has emerged as a powerful technique for image recovery, offering the potential to represent images sparsely using learned bases. However, balancing the trade-off between reconstruction quality and the sparsity of the representation remains a significant challenge. This paper addresses the problem of achieving high-quality image recovery while maintaining a sparse representation through dictionary learning with iterative shrinkage. We propose a novel iterative shrinkage algorithm that dynamically adjusts the shrinkage threshold based on both the reconstruction error and the sparsity level achieved in the previous iteration. This adaptive thresholding strategy allows the algorithm to prioritize either reconstruction accuracy or sparsity, depending on the specific needs of the image recovery task. Experimental results on benchmark image datasets demonstrate that our approach achieves superior performance compared to state-of-the-art dictionary learning methods, yielding both higher PSNR values and sparser representations. The proposed method offers a flexible framework for image recovery, enabling users to fine-tune the quality-sparsity trade-off to suit their specific application requirements."
http://arxiv.org/abs/2508.03490v1,ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes,"Accurate segmentation of small particles in recycled materials is crucial for effective quality monitoring and sorting. However, existing segmentation methods often struggle with the challenges posed by high object density, irregular particle shapes, and variations in illumination and material properties commonly found in recycling streams. This paper introduces ParticleSAM, a novel approach leveraging the Segment Anything Model (SAM) to address the specific challenges of small particle segmentation for material quality monitoring. ParticleSAM incorporates a pre-processing step involving adaptive contrast enhancement and noise reduction, followed by a SAM-based segmentation pipeline fine-tuned with a custom dataset of annotated particle images. A post-processing module refines the segmentation masks by merging fragmented regions and eliminating spurious detections based on size and shape criteria. Experimental results on a real-world recycling dataset demonstrate that ParticleSAM significantly outperforms existing methods, achieving a 15% improvement in average precision and a 12% increase in F1-score for small particle segmentation. This improved segmentation accuracy directly translates to more reliable material classification and optimized recycling processes."
http://arxiv.org/abs/2508.03485v1,LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation,"Diffusion models, particularly Diffusion Transformers (DiTs), have achieved remarkable success in text-to-image generation, but their high computational cost and large memory footprint hinder deployment on resource-constrained devices. Post-training quantization (PTQ) offers a promising solution for model compression, yet directly applying existing PTQ methods to DiTs often leads to significant performance degradation. In this work, we introduce LRQ-DiT, a novel Log-Rotation Quantization technique tailored for DiTs. LRQ-DiT leverages the inherent rotational structure within DiT's attention mechanism by quantizing the logarithmic representation of attention weights, followed by a rotation-based compensation scheme to minimize quantization errors. Furthermore, we employ a novel quantization-aware scaling factor calibration to mitigate the impact of quantization on feature distributions. Experiments on the ImageNet dataset demonstrate that LRQ-DiT achieves substantial model compression (up to 4x) with minimal impact on image quality, outperforming state-of-the-art PTQ methods in terms of FID score and visual fidelity. This makes high-quality text-to-image generation via DiTs more accessible on edge devices and facilitates broader adoption."
http://arxiv.org/abs/2508.03483v1,When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models,"Text-to-image (TTI) models have demonstrated impressive capabilities in generating realistic images from textual descriptions. However, concerns arise regarding the potential for these models to perpetuate and amplify existing societal biases, particularly in visual representations of demographic groups. This paper investigates the presence and nature of demographic bias in TTI models, specifically focusing on how these models associate object categories, such as cars, with different demographic groups based on textual prompts. We introduce a novel auditing framework that leverages carefully constructed prompts combining demographic descriptors (e.g., ""a car owned by a woman"") with common object categories. By analyzing the generated images, we quantify biases in terms of attributes like car model, color, and style, and assess the consistency of these biases across different TTI models. Our analysis reveals significant biases, demonstrating that TTI models frequently associate particular car types (e.g., minivans) with specific demographic groups, and these associations often reflect and reinforce harmful stereotypes. This work highlights the urgent need for bias mitigation strategies in TTI models to ensure fair and equitable visual representation."
http://arxiv.org/abs/2508.03480v1,VideoGuard: Protecting Video Content from Unauthorized Editing,"Video content is increasingly susceptible to unauthorized manipulation and malicious alterations, posing significant risks to authenticity and trust. This paper addresses the critical problem of verifying video integrity and detecting unauthorized edits, even in the presence of sophisticated tampering techniques. We introduce VideoGuard, a novel framework that leverages a multi-faceted approach combining deep learning-based content analysis with robust watermarking. VideoGuard first extracts spatio-temporal features from the video using a 3D Convolutional Neural Network (CNN) pre-trained for action recognition. A robust and imperceptible watermark, encoded with information about the video's origin and content hash, is then embedded within these features. Experimental results demonstrate that VideoGuard effectively detects a wide range of video editing attacks, including frame insertion, deletion, and object manipulation, while maintaining high perceptual quality and watermark robustness. The proposed method achieves an average AUC score of 0.95 on a diverse set of video tampering scenarios, significantly outperforming existing state-of-the-art techniques. VideoGuard provides a practical and effective solution for ensuring the integrity and authenticity of video content, contributing to a safer and more trustworthy digital media landscape."
http://arxiv.org/abs/2508.03469v1,IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and generating image-related text. However, with increasing model size, visual attention mechanisms within these models often suffer from degradation, leading to a focus on irrelevant image regions and hindering overall performance. This paper addresses the problem of visual attention degradation in LVLMs, particularly in high-resolution image scenarios. We introduce IKOD (Iterative Key-patch Online Distillation), a novel training strategy that leverages a smaller, well-trained teacher model to guide the visual attention of a larger student model. IKOD iteratively identifies key image patches based on the teacher's attention map and then distills the teacher's attention knowledge onto the student, focusing the student's attention on the most relevant regions. Our experiments across multiple benchmarks, including visual question answering and image captioning, demonstrate that IKOD significantly improves the visual attention quality of LVLMs, leading to substantial performance gains, especially when processing high-resolution images. These results highlight the effectiveness of IKOD as a practical and scalable solution for mitigating visual attention degradation and enhancing the performance of large vision-language models."
http://arxiv.org/abs/2508.03458v1,AVPDN: Learning Motion-Robust and Scale-Adaptive Representations for Video-Based Polyp Detection,"Video-based polyp detection offers a promising avenue for improving the sensitivity and reducing the invasiveness of colonoscopies. However, existing methods often struggle with significant motion artifacts and scale variations inherent in endoscopic videos, leading to suboptimal detection performance. To address these challenges, we propose AVPDN, a novel Attentive Video Polyp Detection Network that learns motion-robust and scale-adaptive representations. AVPDN incorporates a deformable convolutional attention module to selectively focus on polyp regions while mitigating the impact of motion blur and camera shake. Furthermore, a scale-aware feature fusion module dynamically aggregates multi-scale features, enabling robust detection across varying polyp sizes. Experiments on benchmark datasets demonstrate that AVPDN achieves state-of-the-art performance, surpassing existing methods by a significant margin in terms of both precision and recall. The proposed approach offers a valuable advancement towards reliable and accurate video-based polyp detection, potentially improving the efficacy of colonoscopy screening."
http://arxiv.org/abs/2508.03449v1,Video Demoireing using Focused-Defocused Dual-Camera System,"Moire patterns are a common artifact in videos captured by digital cameras due to the interference between the camera's sensor and the periodic structures in the scene, such as displays or textiles. Existing video demoireing methods often struggle with complex, dynamic moire patterns and can introduce blurring or artifacts. This paper proposes a novel video demoireing approach leveraging a focused-defocused dual-camera system. Our method simultaneously captures a sharp, moire-affected video and a blurred, moire-free video. We then employ a convolutional neural network (CNN) that is trained to learn the mapping between the moire-affected input and the clean target, guided by the defocused image as an auxiliary input to provide frequency priors. Specifically, the network architecture incorporates a cross-attention mechanism to effectively fuse information from both the focused and defocused streams. Experimental results on both synthetic and real-world datasets demonstrate that our method significantly reduces moire artifacts while preserving image details, outperforming state-of-the-art video demoireing techniques in terms of PSNR, SSIM, and visual quality. This work offers a practical and effective solution for video demoireing, enabling higher quality video capture in scenarios prone to moire interference."
http://arxiv.org/abs/2508.03447v1,CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection,"Zero-shot anomaly detection aims to identify anomalous data instances without requiring any anomalous training examples. Current approaches often leverage pre-trained vision-language models (VLMs), relying on hand-crafted text prompts to guide the VLM in distinguishing between normal and anomalous regions. However, these manually designed prompts are often sub-optimal and lack adaptability to diverse anomaly types. We introduce Conditional Prompt Synthesis (CoPS), a novel framework that automatically generates anomaly-aware text prompts conditioned on the visual context of the image. CoPS leverages a prompt generator network, trained using only normal data, to synthesize prompts that describe the expected appearance of normal regions. Anomaly scores are then computed based on the discrepancy between the CLIP embeddings of image regions and their corresponding synthesized prompts. Experiments on benchmark anomaly detection datasets demonstrate that CoPS significantly outperforms existing zero-shot methods, achieving state-of-the-art performance with substantial gains in AUROC. This highlights the effectiveness of dynamically generated, context-aware prompts for enhancing the sensitivity and accuracy of zero-shot anomaly detection systems."
http://arxiv.org/abs/2508.03442v1,RAAG: Ratio Aware Adaptive Guidance,"Deep convolutional neural networks (CNNs) have achieved remarkable success in various computer vision tasks. However, effectively guiding feature learning across different network layers and adapting to varying input characteristics remains a challenge, particularly for tasks requiring fine-grained feature discrimination. We address this by proposing Ratio Aware Adaptive Guidance (RAAG), a novel module that dynamically modulates feature maps based on the ratio of high-to-low frequency components within each channel. RAAG leverages this ratio to generate adaptive scaling factors, allowing the network to emphasize informative features while suppressing irrelevant ones. The module is lightweight and can be easily integrated into existing CNN architectures. Extensive experiments on image classification, semantic segmentation, and object detection tasks demonstrate that RAAG consistently improves performance compared to baseline models and other attention mechanisms. RAAG offers a computationally efficient and effective approach to enhance feature representation and improve the performance of CNNs across a range of computer vision applications."
http://arxiv.org/abs/2508.03441v1,MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis,"Foundation models have shown promise in medical image analysis, but their deployment in specialized clinical tasks often requires fine-tuning on limited labeled data, a scenario where active learning (AL) can be particularly beneficial. However, existing AL benchmarks rarely address the cold-start problem inherent in medical imaging, where initial labeled sets are extremely small and potentially biased, nor do they adequately explore the integration of foundation models. This paper introduces MedCAL-Bench, a comprehensive benchmark specifically designed to evaluate cold-start AL strategies leveraging foundation models for medical image analysis. MedCAL-Bench comprises diverse medical imaging datasets, covering various modalities and anatomical regions, and provides standardized evaluation protocols for assessing AL performance under different cold-start conditions. We evaluate several state-of-the-art AL methods, including uncertainty-based, diversity-based, and hybrid approaches, both with and without fine-tuning vision-language foundation models like CLIP. Our results demonstrate significant performance variations across different AL strategies and datasets, highlighting the challenges of cold-start AL in medical imaging and the potential benefits of foundation model integration, particularly with carefully chosen acquisition functions. MedCAL-Bench provides a valuable resource for researchers to develop and evaluate novel AL algorithms tailored for the unique demands of medical image analysis with foundation models, ultimately accelerating the development of efficient and reliable clinical AI solutions."
http://arxiv.org/abs/2508.03426v1,R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation,"Radiology report generation aims to automatically create descriptive and diagnostic reports from medical images, aiding radiologists in their clinical workflow. Current approaches often struggle to capture the intricate relationships between visual observations and medical knowledge, leading to reports lacking in comprehensive clinical understanding. To address this, we introduce R2GenKG, a hierarchical multi-modal knowledge graph that integrates visual features extracted from medical images with structured medical concepts derived from radiology reports. This knowledge graph is then leveraged to guide a Large Language Model (LLM) for report generation, enabling the model to access and reason about relevant medical knowledge during the decoding process. Experiments on the benchmark MIMIC-CXR dataset demonstrate that R2GenKG significantly improves the factual correctness and clinical relevance of generated reports, achieving state-of-the-art performance in terms of both automated metrics and expert evaluations. Our work highlights the potential of structured knowledge integration with LLMs for enhancing the quality and reliability of automated radiology report generation."
http://arxiv.org/abs/2508.03415v1,Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN,"Image-to-image translation seeks to learn a mapping between two visual domains, enabling tasks such as style transfer and image synthesis. However, existing methods often struggle to preserve fine-grained details and handle significant domain shifts, leading to artifacts and unrealistic outputs. To address these limitations, we propose Frequency Distributed CycleGAN (FD-CycleGAN), a novel approach that leverages frequency domain information to learn robust latent representations for image translation. FD-CycleGAN incorporates a frequency decomposition module within the CycleGAN framework, separating the input image into low-frequency (structure) and high-frequency (texture) components. These components are then processed by separate encoders to learn domain-invariant latent representations that explicitly capture structural and textural information. We demonstrate that FD-CycleGAN achieves superior performance compared to state-of-the-art methods on various image translation tasks, including photo-to-painting and style transfer, as measured by quantitative metrics like PSNR and SSIM, and qualitative visual assessment. By effectively disentangling and translating frequency components, FD-CycleGAN generates more realistic and detail-preserving image translations, advancing the state-of-the-art in unsupervised image transformation."
http://arxiv.org/abs/2508.03397v1,DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition,"Gait recognition, the identification of individuals by their walking style, holds promise for surveillance and security applications. However, performance suffers under variations in clothing, carrying conditions, and viewpoint. To address this, we introduce DepthGait, a novel gait recognition framework that leverages both RGB-derived depth maps and silhouette sequences to achieve robust and view-invariant performance. DepthGait employs a multi-scale cross-level feature fusion strategy. Specifically, we extract features from both modalities at multiple scales using convolutional neural networks and then fuse these features across different levels of abstraction, enabling the model to capture both fine-grained and coarse-grained gait characteristics. Experiments on the CASIA-B and OU-MVLP datasets demonstrate that DepthGait achieves state-of-the-art results, significantly outperforming existing methods, particularly under challenging conditions involving clothing variations and large viewpoint changes. This improved robustness highlights the potential of multi-modal and multi-scale feature fusion for real-world gait recognition deployments."
http://arxiv.org/abs/2508.03373v1,Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration,"Image restoration encompasses a wide range of tasks, including denoising, deblurring, and super-resolution, each typically addressed by specialized models. However, training and deploying multiple task-specific models is computationally expensive and inefficient. We address the challenge of building a single, efficient diffusion model capable of handling diverse image degradations with a single set of weights. Our approach, ""Diffusion Once and Done"" (DOD), leverages degradation-aware Low-Rank Adaptation (LoRA) to condition a pre-trained diffusion model on specific degradation types and levels. By injecting task-specific LoRA parameters into the diffusion process, DOD enables precise control over the restoration process without modifying the original model weights, leading to significant parameter efficiency. Experiments on benchmark datasets demonstrate that DOD achieves state-of-the-art or competitive performance across multiple restoration tasks, using significantly fewer parameters compared to task-specific fine-tuning or training from scratch. This work provides a practical and scalable solution for real-world image restoration applications requiring versatility and efficiency."
http://arxiv.org/abs/2508.03338v1,CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement,"Low-light image enhancement (LLIE) aims to improve the visibility and perceptual quality of images captured in dimly lit environments. However, existing LLIE methods often struggle with color distortion and noise amplification due to the ill-posed nature of the problem and limited understanding of the underlying causal relationships. To address these limitations, we propose Causal Intervention with Vector Quantization for Low-Light Image Enhancement (CIVQLLIE), a novel framework that leverages causal inference and vector quantization to achieve robust and high-quality enhancement. CIVQLLIE employs a causal graph to explicitly model the relationships between illumination, reflectance, and noise, enabling targeted interventions for effective enhancement. Furthermore, we introduce a vector quantization module to discretize the latent space, facilitating the disentanglement of illumination and reflectance components and enabling more controlled and realistic manipulation. Experimental results on benchmark datasets demonstrate that CIVQLLIE outperforms state-of-the-art LLIE methods in terms of both quantitative metrics and visual quality, exhibiting superior performance in color reproduction, noise suppression, and detail preservation. Our approach provides a novel perspective on LLIE by incorporating causal reasoning and discrete representation learning, leading to more interpretable and controllable image enhancement."
http://arxiv.org/abs/2508.03336v1,Beyond Illumination: Fine-Grained Detail Preservation in Extreme Dark Image Restoration,"Low-light image enhancement aims to improve visibility in dark scenes, but often struggles with noise amplification and loss of fine-grained details. This paper addresses the critical challenge of restoring extreme dark images while simultaneously preserving intricate details and suppressing noise. We introduce a novel Detail-Aware Restoration Network (DARN) that leverages a multi-scale feature fusion strategy coupled with an adaptive detail enhancement module. DARN first employs a deep convolutional network to extract multi-scale contextual information, which is then fused to capture both global structure and local details. Subsequently, the adaptive detail enhancement module selectively amplifies high-frequency components based on local image characteristics, mitigating noise amplification and preserving fine textures. Experimental results on benchmark datasets demonstrate that DARN significantly outperforms state-of-the-art methods in terms of both quantitative metrics (PSNR, SSIM) and qualitative visual assessment, particularly in preserving fine details and suppressing noise artifacts in extremely dark conditions. This work advances the field of low-light image restoration by enabling the recovery of visually rich and detailed images from severely underexposed scenes."
http://arxiv.org/abs/2508.03320v1,Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation,"Autoregressive models have demonstrated impressive capabilities in both natural language processing and image generation. However, a unified framework that seamlessly integrates visual understanding and generation within a single autoregressive model remains a significant challenge. This paper introduces Skywork UniPic, a novel unified autoregressive model designed for both visual understanding and generation tasks. UniPic leverages a shared transformer architecture to model images as a sequence of discrete visual tokens, learned via Vector Quantized Variational Autoencoders (VQ-VAE). We introduce a novel attention masking strategy that allows the model to be conditioned on arbitrary combinations of input tokens, enabling diverse tasks such as image completion, inpainting, conditional generation, and visual reasoning through a single model. Our experiments demonstrate that UniPic achieves state-of-the-art or competitive performance on a range of benchmark datasets for image generation and understanding. This unified approach offers a more efficient and versatile solution compared to task-specific models, paving the way for more general-purpose visual intelligence systems."
http://arxiv.org/abs/2508.03277v1,Efficient Multi-Slide Visual-Language Feature Fusion for Placental Disease Classification,"Placental pathology is critical for understanding pregnancy complications, with visual and textual information playing complementary roles in diagnosis. However, existing computational pathology approaches often struggle to efficiently integrate multi-slide whole slide images (WSIs) with clinical reports for comprehensive placental disease classification. This paper addresses the challenge of efficiently fusing visual features from multiple WSIs with textual features from clinical reports to improve placental disease classification accuracy. We propose a novel Multi-Slide Visual-Language Fusion Network (MS-VLF), which employs a hierarchical attention mechanism to select and aggregate relevant visual features from multiple WSIs. Specifically, MS-VLF first uses a WSI-level attention module to identify informative regions within each WSI and then aggregates these region-level features using a slide-level attention module. Finally, the fused visual representation is combined with textual features via a cross-modal attention mechanism for joint representation learning. Our experiments on a large-scale placental pathology dataset demonstrate that MS-VLF achieves state-of-the-art performance, outperforming existing methods by a significant margin in terms of classification accuracy and F1-score. The proposed method offers a promising solution for automated placental disease diagnosis, potentially improving clinical workflow and patient outcomes."
http://arxiv.org/abs/2508.03241v2,FFHQ-Makeup: Paired Synthetic Makeup Dataset with Facial Consistency Across Multiple Styles,"Makeup transfer, the task of applying the makeup style from a reference image to a source face, has seen significant progress with deep learning. However, existing methods are often limited by the lack of large-scale, high-quality paired datasets that ensure facial identity consistency across diverse makeup styles. To address this, we introduce FFHQ-Makeup, a novel paired synthetic makeup dataset generated using a StyleGAN2-based makeup application pipeline. Our pipeline leverages disentangled latent space manipulations to apply diverse makeup styles while preserving facial identity. Specifically, we manipulate the makeup-related latent codes learned through a combination of self-supervised and weakly-supervised training strategies, ensuring the creation of high-fidelity and consistently paired images. Experiments demonstrate that models trained on FFHQ-Makeup achieve superior performance in makeup transfer tasks compared to those trained on existing datasets, exhibiting improved realism and identity preservation. This dataset serves as a valuable resource for training and evaluating makeup transfer algorithms, facilitating advancements in the field by offering a controlled environment for studying makeup style manipulation and facial consistency."
http://arxiv.org/abs/2508.03209v1,GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about multimodal data, yet their potential to infer sensitive geolocation information from seemingly innocuous images poses a significant privacy risk. This paper addresses the vulnerability of VLMs to geolocation inference attacks based on visual cues present in images, even when explicit location metadata is absent. We introduce GeoShield, a novel framework for safeguarding geolocation privacy by generating targeted adversarial perturbations to images. GeoShield leverages a multi-objective optimization strategy to craft imperceptible perturbations that simultaneously minimize the VLMs' confidence in predicting the true location while preserving image fidelity and perceptual quality. Experimental results demonstrate that GeoShield effectively reduces the accuracy of state-of-the-art VLMs in geolocation prediction tasks by a significant margin, achieving up to a 60% reduction in top-1 accuracy, without substantially degrading image quality as measured by PSNR and SSIM. GeoShield offers a practical and effective defense mechanism against geolocation inference attacks, contributing to the development of privacy-preserving vision-language technologies."
http://arxiv.org/abs/2508.03207v1,Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration,"Human-Object Interaction (HOI) detection aims to identify interactions between humans and objects within an image, a crucial task for scene understanding. However, existing methods often struggle with open-vocabulary scenarios where novel HOI triplets, unseen during training, must be recognized. To address this limitation, we propose a novel Open-Vocabulary HOI detection framework with Interaction-aware Prompt and Concept Calibration (IPCC). Our approach leverages visual-language models (VLMs) and introduces interaction-aware prompts to guide the VLM in understanding the specific relationship between humans and objects. Furthermore, we incorporate a concept calibration module that refines the visual features of humans and objects by aligning them with relevant semantic concepts, thereby mitigating the ambiguity introduced by novel compositions. Experiments on the challenging HICO-DET and V-COCO datasets demonstrate that IPCC significantly outperforms existing open-vocabulary HOI detection methods, achieving state-of-the-art performance, especially on unseen HOI triplets. This demonstrates the effectiveness of interaction-aware prompting and concept calibration in enhancing the generalization capabilities of VLMs for open-vocabulary HOI detection, paving the way for more robust and adaptable scene understanding systems."
http://arxiv.org/abs/2508.03201v1,AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding,"Weakly-supervised visual grounding aims to localize image regions corresponding to a given textual query using only image-text paired data. A key challenge lies in effectively aligning visual features with linguistic representations, particularly when dealing with complex queries involving both object categories and their attributes. This paper addresses the problem of improving visual-linguistic alignment in weakly-supervised visual grounding by explicitly modeling the relationships between categories and attributes. We introduce AlignCAT, a novel framework that learns separate visual representations for categories and attributes and then aligns them with their corresponding linguistic embeddings through a contrastive loss function. AlignCAT further incorporates a cross-modal attention mechanism to dynamically weigh the contribution of category and attribute features based on the query. Experiments on benchmark datasets demonstrate that AlignCAT significantly outperforms existing state-of-the-art methods, achieving substantial improvements in grounding accuracy and demonstrating a better understanding of complex queries. This improved alignment between visual and linguistic features paves the way for more robust and accurate weakly-supervised visual grounding systems."
http://arxiv.org/abs/2508.03197v1,Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network,"Neovascularization, the abnormal formation of new blood vessels, is a hallmark of many ocular diseases, including diabetic retinopathy and age-related macular degeneration. Accurate segmentation of neovascularization is crucial for diagnosis, monitoring disease progression, and treatment planning. However, the complex and irregular morphology of neovascularization, coupled with low contrast in medical images, poses significant challenges for existing segmentation methods. This paper introduces a novel Multilateral Interaction-Enhanced Graph Convolutional Network (MIE-GCN) for robust and accurate neovascularization segmentation. Our approach leverages graph convolutional networks to model the intricate spatial relationships between image regions. Specifically, we introduce a multilateral interaction module that facilitates information exchange between nodes representing different image features (e.g., color, texture, and contextual information), thereby enriching node representations and improving segmentation accuracy. Experimental results on publicly available retinal image datasets demonstrate that MIE-GCN achieves state-of-the-art performance, surpassing existing methods in terms of Dice coefficient, sensitivity, and specificity. This improved segmentation performance has the potential to significantly improve the clinical management of neovascularization-related diseases."
http://arxiv.org/abs/2508.03189v1,Unifying Locality of KANs and Feature Drift Compensation for Data-free Continual Face Forgery Detection,"Continual Face Forgery Detection (CFFD) aims to detect increasingly sophisticated manipulated faces in a sequential learning manner, where catastrophic forgetting and limited access to previous data are significant challenges. Existing data-free CFFD methods often struggle to balance plasticity and stability, leading to performance degradation over time due to feature drift and the inability to effectively capture local forgery patterns. To address these issues, we propose a novel framework that unifies the locality of Kolmogorov-Arnold Networks (KANs) with a feature drift compensation strategy. Specifically, we replace the traditional Multi-Layer Perceptron (MLP) layers in the classifier with KANs, leveraging their inherent ability to approximate complex functions through localized basis functions, thereby enhancing the model's capacity to capture subtle forgery artifacts. Furthermore, we introduce a feature drift compensation module based on knowledge distillation, which dynamically aligns the feature distributions of the current and previous tasks, mitigating the negative impact of feature drift. Extensive experiments on benchmark datasets demonstrate that our method significantly outperforms existing data-free CFFD approaches in terms of average accuracy and forgetting rate. This work provides a promising direction for developing robust and adaptable face forgery detection systems in dynamic environments."
http://arxiv.org/abs/2508.03177v1,SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision,"Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in understanding and generating content related to visual inputs, yet they often suffer from hallucinations, generating text that contradicts or is unsupported by the provided image. These hallucinations significantly limit the trustworthiness and applicability of LVLMs in real-world scenarios. To address this issue, we propose Style-Aware Visual Early Revision (SAVER), a novel framework designed to mitigate hallucinations by proactively refining the visual features extracted from the input image before they are fed into the language model. SAVER employs a style classification network to identify the artistic style of the image and subsequently leverages a style-conditioned visual revision module to correct potentially misleading or ambiguous visual features, thereby creating a more accurate visual representation. Experiments on benchmark datasets demonstrate that SAVER significantly reduces hallucination rates and improves the factual consistency of generated text, achieving state-of-the-art performance compared to existing methods. SAVER provides a promising direction for enhancing the reliability and trustworthiness of LVLMs by focusing on early visual feature refinement."
http://arxiv.org/abs/2508.03164v1,ChartCap: Mitigating Hallucination of Dense Chart Captioning,"Dense chart captioning, aiming to describe fine-grained information within visualizations, has emerged as a crucial task for enhancing data accessibility. However, existing models often suffer from hallucination, generating descriptions that are factually inconsistent with the chart's visual content, particularly when dealing with densely packed or complex charts. To address this, we introduce ChartCap, a novel framework that mitigates hallucination by incorporating a multi-modal verification module. ChartCap leverages a contrastive learning objective to align the textual caption embedding with the corresponding visual regions, followed by a verification network that evaluates the consistency between the generated caption and the chart image, penalizing hallucinated content through a reinforcement learning reward. Experiments on benchmark datasets demonstrate that ChartCap significantly reduces hallucination, improving factual consistency metrics by an average of 15% compared to state-of-the-art methods, while maintaining competitive caption quality. This work provides a significant step towards building more reliable and trustworthy AI systems for chart understanding and accessibility."
http://arxiv.org/abs/2508.03144v1,LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing,"Rectified flow models have emerged as powerful generative tools, enabling high-fidelity image synthesis and editing through deterministic transformations between noise and data distributions. However, achieving precise semantic control during editing remains a challenge, often requiring complex inversion and manipulation strategies in the latent space that lack intuitive correspondence with desired semantic changes. We introduce LORE: Latent Optimization for Rectified Editing, a novel approach that optimizes latent vectors along the rectified flow trajectory to precisely align edited images with user-defined semantic targets. LORE leverages a differentiable semantic loss function computed directly on the generated images to guide latent vector adjustments at each step of the flow, ensuring consistent and controllable semantic modifications. Experimental results demonstrate that LORE significantly improves the accuracy and faithfulness of semantic edits compared to existing methods, achieving state-of-the-art performance in tasks such as object replacement and attribute manipulation. This precise and controllable editing framework unlocks new possibilities for creative image manipulation and content creation with rectified flow models."
http://arxiv.org/abs/2508.03142v1,"UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying","Vision-Language Models (VLMs) have shown remarkable capabilities in visual understanding and generation, yet their application to complex image editing tasks remains challenging. Existing methods often require extensive training or fine-tuning to align textual instructions with pixel-level manipulations, limiting their generalization ability and practicality. To address this, we introduce UniEdit-I, a novel training-free image editing framework for unified VLMs, leveraging an iterative process of understanding, editing, and verifying. UniEdit-I first decomposes complex editing instructions into simpler, actionable sub-goals. Then, it iteratively utilizes the VLM to understand the current image, perform targeted edits based on the sub-goals, and verify the outcome against the original instruction. This iterative refinement loop ensures that the final image accurately reflects the user's intent without requiring any task-specific training. Experiments on a diverse set of image editing tasks demonstrate that UniEdit-I significantly outperforms existing zero-shot and few-shot methods in terms of both edit fidelity and instruction adherence, achieving comparable or superior results to training-based approaches. UniEdit-I offers a practical and versatile solution for image editing by harnessing the power of pre-trained VLMs, paving the way for more intuitive and accessible image manipulation tools."
http://arxiv.org/abs/2508.03139v1,Uint: Building Uint Detection Dataset,"Object detection has achieved remarkable progress in recent years, driven by deep learning techniques. However, performance still lags in specialized domains due to the scarcity of labeled data for training robust models. This paper addresses the lack of a publicly available, high-quality dataset for detecting user interface (UI) elements in screenshots, a critical task for automation and accessibility. We introduce Uint, a novel dataset comprising over 10,000 annotated UI screenshot images sourced from diverse platforms and applications. Our annotation methodology incorporates a rigorous quality control pipeline, ensuring high precision bounding box annotations for 15 common UI element categories. Experiments using state-of-the-art object detection models trained on Uint demonstrate significant improvements in detection accuracy compared to models trained on existing, smaller datasets, achieving a mean Average Precision (mAP) of 75.2% on a held-out test set. The release of Uint provides a valuable resource for the computer vision and human-computer interaction communities, enabling the development of more accurate and reliable UI detection systems."
http://arxiv.org/abs/2508.03127v2,Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery,"Vision-language pre-training has achieved remarkable success in various domains, yet its application to remote sensing, particularly for regional-specific landscapes, remains limited by the scarcity of suitable datasets. This paper addresses the lack of a large-scale vision-language dataset tailored for Australian Landsat imagery, hindering the development of effective models for understanding and interpreting this critical data source. We introduce Landsat30-AU, a novel dataset comprising over 30,000 Landsat-8 and Landsat-9 satellite image patches covering diverse Australian landscapes, each paired with human-annotated descriptive captions. The dataset includes images from all Australian states and territories, capturing a wide range of land cover types, seasonal variations, and environmental phenomena. We demonstrate the utility of Landsat30-AU by pre-training a CLIP-based model and evaluating its performance on downstream tasks, including image-text retrieval and zero-shot land cover classification, achieving competitive results compared to models trained on generic datasets. Landsat30-AU facilitates the development of more accurate and context-aware vision-language models for Australian remote sensing applications, enabling improved environmental monitoring, agricultural management, and disaster response."
http://arxiv.org/abs/2508.03094v1,Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts,"Continual learning (CL) in medical imaging faces the challenge of catastrophic forgetting when adapting to new diseases. Existing CL methods often struggle to generalize to unseen disease manifestations due to limited data and the subtle visual differences between diseases. This paper addresses the problem of improving continual learning of new diseases by incorporating richer visual knowledge beyond the pixel level. We propose a novel framework that leverages Large Language Models (LLMs) to generate textual descriptions of visual concepts associated with each disease. These descriptions are then used to synthesize visual concept embeddings, which are incorporated as auxiliary information to augment the image embeddings during training. This allows the model to learn more robust and transferable representations. Experimental results on a diverse set of medical imaging datasets demonstrate that our LLM-augmented continual learning approach significantly outperforms state-of-the-art CL methods, achieving up to 15% improvement in average accuracy while effectively mitigating catastrophic forgetting. This work highlights the potential of integrating LLM-generated knowledge to enhance the performance and robustness of continual learning systems in medical image analysis."
http://arxiv.org/abs/2508.04260v1,Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark,"Vehicle segmentation is a crucial task for autonomous driving and intelligent transportation systems, often relying on expensive and specialized datasets. Recently, Segment Anything Model (SAM) has demonstrated impressive zero-shot segmentation capabilities, but its performance on complex, diverse vehicle datasets remains largely unexplored and often insufficient due to lack of contextual understanding. This paper introduces ""Segment Any Vehicle"" (SAV), a novel framework that leverages both semantic and visual context to enhance SAM's vehicle segmentation performance. SAV incorporates a semantic context module that utilizes scene parsing information to guide SAM's attention, and a visual context module that refines segmentation boundaries based on local image features and vehicle part priors. We further present a new, challenging benchmark dataset comprising diverse vehicle types and environmental conditions to rigorously evaluate SAV and other segmentation models. Experimental results demonstrate that SAV significantly outperforms the original SAM and other state-of-the-art segmentation methods, achieving a substantial improvement in segmentation accuracy, especially in challenging scenarios with occlusions and varying lighting conditions. This work provides a practical solution for high-quality vehicle segmentation and establishes a valuable benchmark for future research in this domain."
http://arxiv.org/abs/2508.04201v1,ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs,"Vision-Language Models (VLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet their performance is often hindered by the presence of visual false positives  incorrect visual perceptions that lead to flawed reasoning. This paper addresses the critical problem of detecting these visual false positives to improve the reliability of VLM reasoning. We introduce ViFP, a novel framework that leverages a multi-faceted approach to identify potential visual inaccuracies. ViFP employs a combination of visual anomaly detection, semantic consistency checks between the image and the textual query, and uncertainty estimation derived from the VLM itself. Specifically, we train a lightweight anomaly detection module on a curated dataset of realistic visual distortions and integrate it with a semantic similarity scorer and the VLM's attention maps to flag suspect visual regions. Experiments on multiple VLM reasoning benchmarks demonstrate that ViFP significantly reduces the impact of visual false positives, leading to an average accuracy improvement of 8-12% across various tasks. By effectively mitigating the influence of erroneous visual information, ViFP enhances the robustness and trustworthiness of VLMs in real-world applications."
http://arxiv.org/abs/2508.04192v1,From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse applications, including biomedical image analysis and report generation. However, their reliance on vast datasets, often containing sensitive patient information, raises significant security and privacy concerns due to the potential for memorization and subsequent leakage of protected health information (PHI). This paper addresses the critical challenge of mitigating biomedical security risks in MLLMs by developing a novel unlearning framework tailored for multimodal data. Our approach, termed ""Multimodal Biomedical Unlearning"" (MBU), combines gradient-based unlearning techniques with adversarial training to selectively erase the influence of specific PHI-related data points from the model's memory. Specifically, MBU identifies and minimizes the contribution of sensitive image-text pairs during fine-tuning while simultaneously training a discriminator to distinguish between unlearned and original model outputs, ensuring effective privacy preservation. Experimental results on a curated biomedical dataset demonstrate that MBU significantly reduces the model's ability to recall PHI without substantially compromising performance on downstream clinical tasks. This work offers a practical and effective strategy for enhancing the security and privacy of MLLMs deployed in sensitive biomedical applications."
http://arxiv.org/abs/2508.04175v1,AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization,"Anomaly detection is crucial for ensuring the reliability and safety of various real-world systems, yet current methods often struggle with complex, nuanced anomalies requiring reasoning across multiple modalities. This paper addresses the limitations of existing anomaly detection approaches by leveraging the reasoning capabilities of Multimodal Large Language Models (MLLMs). We introduce AD-FM, a novel framework for anomaly detection that employs a multi-stage reasoning process within an MLLM, guiding it to first identify potential anomalies based on visual and textual inputs, then to refine these initial hypotheses through iterative analysis and contextual understanding. Furthermore, we introduce a fine-grained reward optimization strategy that trains the MLLM to provide more accurate and reliable anomaly assessments by rewarding specific reasoning steps and penalizing common failure modes. Experiments on diverse anomaly detection datasets demonstrate that AD-FM significantly outperforms state-of-the-art methods, achieving substantial improvements in F1-score and precision, particularly in scenarios requiring complex reasoning and multimodal integration. This work opens new avenues for leveraging the power of MLLMs in anomaly detection, leading to more robust and interpretable systems."
http://arxiv.org/abs/2508.04166v1,ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations,"The proliferation of toxic content online, particularly in the form of memes, poses a significant threat to online discourse and mental well-being. Identifying and mitigating toxic memes requires robust methods capable of understanding their multimodal nature, which current approaches often struggle with. This paper introduces ToxicTAGS, a novel dataset of memes annotated with rich, fine-grained tags capturing both the visual and textual components, as well as their interplay in conveying toxic meanings. We propose a multimodal transformer-based architecture that leverages these tags to predict toxicity levels, incorporating attention mechanisms to focus on relevant image regions and textual phrases. Experimental results demonstrate that our tag-assisted model significantly outperforms baseline models in predicting meme toxicity, achieving a 15% improvement in F1-score. This work underscores the importance of rich semantic annotations for understanding complex multimodal toxicity and provides a valuable resource and methodology for future research in this critical area."
http://arxiv.org/abs/2508.04120v1,CLIPVehicle: A Unified Framework for Vision-based Vehicle Search,"Vision-based vehicle search is crucial for various applications, including intelligent transportation systems and law enforcement. Current methods often rely on specific vehicle attributes or pre-defined categories, limiting their generalization ability across diverse datasets and complex real-world scenarios. This paper addresses the challenge of building a unified and flexible vehicle search framework that can handle diverse queries without requiring extensive re-training for new attributes or categories. We introduce CLIPVehicle, a novel framework leveraging the power of Contrastive Language-Image Pre-training (CLIP) to bridge the semantic gap between textual descriptions and visual vehicle representations. CLIPVehicle employs a two-stage approach: first, CLIP is used to encode both vehicle images and text descriptions into a shared embedding space; second, efficient nearest neighbor search within this space allows retrieval of vehicles matching the query. Experiments on multiple benchmark datasets demonstrate that CLIPVehicle achieves state-of-the-art performance in zero-shot vehicle search and outperforms existing methods in handling complex, attribute-based queries. The proposed framework offers a significant advancement in vehicle search by providing a more adaptable and scalable solution for real-world applications."
http://arxiv.org/abs/2508.04099v1,DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting,"3D Gaussian Splatting (3D-GS) has emerged as a powerful technique for novel view synthesis, offering state-of-the-art rendering quality and real-time performance. However, 3D-GS often suffers from geometric inaccuracies and floating artifacts, particularly in regions with sparse or noisy input data, leading to suboptimal rendering quality. This paper introduces DET-GS, a novel depth- and edge-aware regularization framework designed to enhance the geometric fidelity of 3D Gaussian Splatting. DET-GS leverages depth priors from multi-view stereo to constrain the Gaussian positions along the viewing rays and incorporates an edge-aware smoothness term that encourages local consistency while preserving sharp features. Furthermore, we introduce a dynamic regularization scheme that adapts the regularization strength based on the confidence of the depth priors. Experimental results on challenging benchmark datasets demonstrate that DET-GS significantly reduces floating artifacts and improves geometric accuracy, leading to superior rendering quality compared to existing 3D-GS methods. DET-GS provides a robust and effective approach for generating high-fidelity 3D Gaussian Splatting models, advancing the state-of-the-art in novel view synthesis."
http://arxiv.org/abs/2508.04064v1,FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning,"Federated learning (FL) is vulnerable to backdoor attacks, where malicious clients inject poisoned data to manipulate the global model's behavior on specific trigger inputs. Existing backdoor attacks in FL often focus on targeted attacks, where the backdoor aims to misclassify poisoned inputs to a pre-defined target class. However, crafting backdoors capable of inducing arbitrary, user-defined target outputs remains a significant challenge.

This paper addresses the problem of arbitrary-target backdoor attacks in FL, where the adversary aims to control the model's output on poisoned inputs to match any desired target, not just a specific class label. We propose FLAT, a novel Latent-driven Arbitrary-Target backdoor attack framework in Federated Learning. FLAT leverages a conditional variational autoencoder (CVAE) to learn a latent space representation of the training data and then manipulates the latent codes associated with poisoned inputs. By guiding these latent codes towards a target point in the latent space corresponding to the desired output, we effectively steer the model's behavior on poisoned inputs towards arbitrary targets.

Our experiments on benchmark datasets demonstrate that FLAT achieves significantly higher attack success rates compared to existing targeted and untargeted backdoor attacks, while maintaining comparable or even superior clean accuracy. Furthermore, FLAT exhibits strong robustness against various defense mechanisms, including anomaly detection and gradient clipping. This work highlights a critical vulnerability in FL systems and paves the way for developing more robust and secure federated learning algorithms."
http://arxiv.org/abs/2508.04017v1,Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability,"Large Multimodal Models (LMMs) have demonstrated impressive capabilities across various vision-language tasks, but their robustness to noisy or adversarial inputs remains largely unexplored. This paper addresses the critical problem of evaluating whether LMMs can actively recognize and flag potentially faulty inputs before processing them. We introduce a systematic evaluation framework comprising diverse input corruptions, including image perturbations, text paraphrasing, and modality conflicts, combined with a novel prompting strategy that encourages LMMs to explicitly assess input reliability alongside task execution. Our experiments with state-of-the-art LMMs reveal a significant gap between their performance on clean data and their ability to detect faulty inputs, particularly in scenarios involving subtle semantic inconsistencies or adversarial attacks. Furthermore, we find that explicit prompting can improve input scrutiny to some extent, but the effectiveness is highly dependent on the type and severity of the input corruption. These findings highlight the need for future research focused on enhancing the input scrutiny ability of LMMs to ensure trustworthy and reliable performance in real-world applications."
http://arxiv.org/abs/2508.03654v1,Can Large Vision-Language Models Understand Multimodal Sarcasm?,"Sarcasm detection is a challenging task in natural language processing, further complicated when relying on multimodal cues. While Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in various vision-language tasks, their ability to discern subtle forms of multimodal communication like sarcasm remains largely unexplored. This paper investigates the extent to which current LVLMs can understand and accurately identify sarcasm conveyed through the interplay of visual and textual information. We propose a novel fine-tuning approach that incorporates contrastive learning with hard negative mining to enhance the model's sensitivity to subtle discrepancies between visual and textual content indicative of sarcasm. Specifically, we train the LVLM to distinguish between sarcastic and non-sarcastic image-text pairs, using carefully selected hard negatives that exhibit similar semantic content but lack the sarcastic undertones. Our experiments on a publicly available multimodal sarcasm dataset demonstrate a significant improvement in sarcasm detection accuracy compared to existing baselines and zero-shot performance of pre-trained LVLMs. These findings suggest that while LVLMs possess a foundational understanding of vision-language relationships, targeted fine-tuning is crucial for unlocking their potential in nuanced multimodal understanding, specifically in the complex task of sarcasm detection."
http://arxiv.org/abs/2508.03545v1,Advancing Wildlife Monitoring: Drone-Based Sampling for Roe Deer Density Estimation,"Accurate and efficient wildlife monitoring is crucial for effective conservation management. Traditional methods for estimating Roe deer (Capreolus capreolus) populations are often labor-intensive, costly, and can be invasive, leading to potential disturbance of the animals and inaccuracies in data collection. This paper addresses the challenge of obtaining reliable Roe deer density estimates across large and varied landscapes by leveraging drone-based thermal infrared (TIR) imagery. We propose a novel workflow involving automated drone flights to capture high-resolution TIR imagery, followed by a custom-trained convolutional neural network (CNN) for robust Roe deer detection even under challenging environmental conditions such as dense vegetation and varying thermal signatures. The CNN architecture incorporates a multi-scale feature extraction module to improve detection accuracy. Our results demonstrate a significant improvement in detection rates compared to manual counting methods, achieving a precision of 85% and a recall of 80% on a held-out validation dataset. This approach allows for rapid and non-invasive assessment of Roe deer populations, providing valuable data for informed wildlife management decisions."
http://arxiv.org/abs/2508.03252v1,Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion,"Single-stage 3D object detection offers efficiency for real-time applications, but struggles with robustness in sparse point cloud scenarios. Existing single-stage methods often rely on dense voxelization or point-based feature propagation, leading to computational bottlenecks and sensitivity to noise in sparse point clouds. This paper addresses the challenge of achieving robust and efficient single-stage 3D object detection directly from highly sparse point clouds. We propose a novel Detachable Latent Diffusion (DLD) framework that decouples feature learning and object localization. DLD leverages a sparse convolutional network for initial feature extraction, followed by a latent diffusion module that enriches features with contextual information learned through a diffusion process in a lower-dimensional latent space. This latent diffusion is then detached during inference, allowing for efficient single-stage prediction using only the learned contextual features. Experimental results on the Waymo Open Dataset demonstrate that DLD achieves state-of-the-art performance in sparse point cloud settings, significantly outperforming existing single-stage methods in terms of detection accuracy and robustness while maintaining comparable inference speed. The DLD framework provides a novel and efficient approach to address the limitations of single-stage 3D object detection in sparse point cloud environments."
http://arxiv.org/abs/2508.03143v1,SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance,"Anomaly detection in images is crucial for various applications, but training robust models requires substantial annotated anomalous data, which is often scarce. This paper addresses the challenge of synthesizing realistic anomalies for improved anomaly detection by proposing Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance (SARD). SARD leverages a pre-trained diffusion model to generate anomalies within specified regions of an image, guided by a discriminative mask that highlights areas where anomalies are likely to occur. The region constraint ensures that generated anomalies are contextually relevant and do not disrupt the overall image structure, while the discriminative mask focuses the anomaly generation process on semantically meaningful areas. Experimental results demonstrate that training anomaly detection models with SARD-synthesized data significantly improves their performance, achieving state-of-the-art results on several benchmark datasets compared to existing anomaly synthesis techniques. This approach offers a powerful and flexible solution for generating high-quality, realistic anomalous data, thereby improving the robustness and accuracy of anomaly detection systems."
http://arxiv.org/abs/2508.03006v1,Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models,"Diffusion-based text-to-image models have achieved remarkable success in generating high-fidelity images from textual descriptions, but also raise concerns about the potential generation of harmful or inappropriate content (NSFW). Existing NSFW detection methods primarily operate on completed images, representing a reactive approach that wastes computational resources and potentially exposes users to undesirable content. This paper introduces an ""in-generation"" NSFW detection strategy that proactively identifies and mitigates the generation of inappropriate content directly within the diffusion process. Our method leverages a lightweight classifier trained to predict NSFW scores from intermediate latent representations produced during the diffusion process. By monitoring these scores at each denoising step, we can either terminate the generation early or steer the diffusion trajectory away from NSFW content through a novel guidance mechanism. Experiments demonstrate that our approach achieves high accuracy in identifying potentially NSFW content early in the generation process, significantly reducing computational costs and minimizing the production of unwanted images, while maintaining image quality for safe prompts. This proactive approach offers a more efficient and responsible way to deploy text-to-image models."
http://arxiv.org/abs/2508.02957v1,AMD-Mamba: A Phenotype-Aware Multi-Modal Framework for Robust AMD Prognosis,"Age-related macular degeneration (AMD) is a leading cause of irreversible vision loss worldwide, and accurate prognosis is crucial for timely intervention. Existing AMD prognosis methods often struggle with the heterogeneity of the disease and the effective integration of multi-modal data. We address this challenge by introducing AMD-Mamba, a novel phenotype-aware multi-modal framework for robust AMD prognosis. AMD-Mamba leverages a Mamba-based architecture to effectively model long-range dependencies within and across optical coherence tomography (OCT) volumes and fundus photographs. Furthermore, we incorporate phenotype awareness by using a pre-trained classification network to embed phenotypic information directly into the multi-modal feature space, guiding the prognosis model to focus on relevant disease characteristics. Experimental results on a large, multi-center dataset demonstrate that AMD-Mamba significantly outperforms state-of-the-art methods in predicting disease progression, achieving a substantial improvement in prognostic accuracy and calibration. This advancement facilitates personalized AMD management and enables more effective clinical trial design."
http://arxiv.org/abs/2508.02927v1,Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?,"Infrared object detection is crucial for various applications, including autonomous driving and surveillance, particularly in adverse lighting conditions. However, deploying deep learning models for such tasks on resource-constrained devices necessitates ultra-small convolutional neural networks (ConvNets). This paper investigates the effectiveness of ImageNet pretraining for infrared object detection when using extremely small ConvNets, a question that has not been thoroughly explored. We propose a systematic evaluation of different pretraining strategies, including training from scratch, full ImageNet pretraining, and layer-wise fine-tuning of pretrained weights, across a range of ultra-small architectures designed specifically for infrared imagery. Our experiments on a challenging infrared object detection dataset demonstrate that, surprisingly, training from scratch or employing a carefully tuned layer-wise fine-tuning strategy can outperform full ImageNet pretraining for several ultra-small ConvNet architectures. These findings challenge the conventional wisdom regarding ImageNet pretraining and highlight the importance of task-specific optimization strategies for achieving optimal performance with ultra-small models in specialized domains like infrared object detection. This work provides valuable insights into efficient model design and training methodologies for deploying computer vision applications on resource-limited platforms."
http://arxiv.org/abs/2508.02889v1,REFLECT: Rectified Flows for Efficient Brain Anomaly Correction Transport,"Brain anomaly correction in magnetic resonance imaging (MRI) is crucial for accurate diagnosis and subsequent treatment planning. However, current methods often struggle with efficiently transporting complex anatomical structures while preserving intricate details, leading to suboptimal correction results. To address this, we introduce REFLECT: Rectified Flows for Efficient Brain Anomaly Correction Transport. REFLECT leverages rectified flows to learn a smooth, invertible mapping between anomalous and healthy brain MRI volumes, enabling efficient and accurate anomaly correction. Specifically, we incorporate a novel rectification loss that encourages the flow to follow a straighter path, minimizing unnecessary deformations and improving transport efficiency. Experimental results on a large multi-center dataset of brain MRI scans demonstrate that REFLECT achieves superior anomaly correction performance compared to state-of-the-art methods, exhibiting improved structural similarity and reduced anomaly burden, while significantly reducing computational cost. This efficient and accurate anomaly correction framework has the potential to improve the reliability and efficiency of clinical workflows in neuroradiology."
http://arxiv.org/abs/2508.02858v1,MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model,"LiDAR provides accurate 3D information crucial for robust perception in autonomous driving and intelligent transportation systems. However, the high cost and computational demands of LiDAR sensors hinder their widespread deployment. This paper addresses the challenge of achieving LiDAR-level detection performance using solely camera data, thereby reducing hardware costs and enabling broader accessibility. We propose MIDAR, a lightweight, plug-and-play module designed to mimic LiDAR detection capabilities. MIDAR leverages a novel depth estimation network trained with a synthetic LiDAR-camera dataset and integrates seamlessly into existing camera-based object detection pipelines. It enhances image features with pseudo-LiDAR point clouds, allowing the detector to reason in 3D space. Experimental results on the challenging KITTI dataset demonstrate that MIDAR significantly improves the performance of various 2D object detectors, achieving comparable or even superior results to some LiDAR-based methods, while maintaining real-time processing speeds. This work presents a cost-effective and efficient solution for enabling accurate 3D object detection from camera data, facilitating the development of advanced traffic applications."
http://arxiv.org/abs/2508.02533v1,Precision-Aware Video Compression for Reducing Bandwidth Requirements in Video Communication for Vehicle Detection-Based Applications,"Video communication for vehicle detection-based applications, such as autonomous driving and traffic monitoring, demands high bandwidth due to the need for high-resolution and high-frame-rate video streams. Compressing these videos is crucial, but traditional compression methods often introduce artifacts that can negatively impact the performance of vehicle detection algorithms. This paper addresses the problem of reducing bandwidth requirements for vehicle detection applications while preserving the precision of vehicle detection performance. We propose a precision-aware video compression framework that prioritizes the preservation of image regions and features critical for accurate vehicle detection. This is achieved by integrating a vehicle detection module within the compression loop, allowing for adaptive quantization and bit allocation strategies that minimize distortion in regions containing vehicles and their key features. Experimental results on benchmark datasets demonstrate that our method achieves significant bandwidth reduction compared to standard video codecs, while maintaining comparable or even improved vehicle detection accuracy. The proposed framework offers a practical solution for efficient and reliable video communication in vehicle detection-based applications, enabling deployment in bandwidth-constrained environments."
http://arxiv.org/abs/2508.02530v1,Understanding the Risks of Asphalt Art on the Reliability of Surveillance Perception Systems,"Surveillance perception systems increasingly rely on computer vision algorithms for tasks such as object detection, tracking, and scene understanding. The growing trend of large-scale asphalt art installations in urban environments presents a novel challenge to the robustness of these systems. This paper investigates the potential risks that asphalt art poses to the reliability of surveillance perception, specifically focusing on how these visually complex and often geometrically distorted patterns impact the performance of common computer vision tasks. We introduce a synthetic data generation pipeline to simulate asphalt art with varying levels of complexity and then evaluate the performance of several state-of-the-art object detection models on both synthetic and real-world asphalt art datasets. Our results demonstrate a significant decrease in object detection accuracy, particularly for smaller objects and in scenes with high art complexity, with performance drops of up to 30% observed in certain scenarios. Furthermore, we analyze the types of errors introduced, revealing a tendency for false positives driven by the arts textural and color similarities to target objects. This research highlights the critical need to consider the impact of environmental art installations on the reliability of computer vision-based surveillance systems and motivates the development of more robust and adaptable algorithms."
http://arxiv.org/abs/2508.03762v1,"Scaling Artificial Intelligence for Prostate Cancer Detection on MRI towards Population-Based Screening and Primary Diagnosis in a Global, Multiethnic Population (Study Protocol)","Prostate cancer (PCa) is a leading cause of cancer-related mortality in men, with early detection crucial for improved outcomes. Current PCa detection methods lack the necessary accuracy and scalability for effective population-based screening, particularly in diverse, multiethnic populations where disease prevalence and presentation can vary significantly. This study protocol outlines a comprehensive approach to develop and validate a scalable artificial intelligence (AI) system for PCa detection on multi-parametric MRI (mpMRI) capable of supporting both screening and primary diagnosis. Our proposed method involves training deep learning models on a large, globally sourced, multiethnic dataset of mpMRI scans with corresponding clinical and pathological data. We will employ federated learning techniques to address data privacy concerns and enable collaborative model development across multiple international sites. The resulting AI system will be rigorously validated on independent datasets, assessing its performance in detecting clinically significant PCa across diverse demographic subgroups and imaging protocols. This research has the potential to transform PCa diagnosis by providing an accurate, scalable, and equitable tool for early detection and personalized management, ultimately improving patient outcomes worldwide."
http://arxiv.org/abs/2508.02521v2,Towards Reliable Audio Deepfake Attribution and Model Recognition: A Multi-Level Autoencoder-Based Framework,"The proliferation of audio deepfakes poses a significant threat to information integrity and trust in digital media. Existing audio deepfake detection methods often struggle with generalization across different deepfake generation models and are susceptible to adversarial attacks. This paper addresses the challenge of reliable audio deepfake attribution and model recognition by proposing a novel multi-level autoencoder-based framework. Our approach leverages a hierarchical autoencoder architecture to extract both global acoustic features and fine-grained model-specific artifacts from audio signals. Specifically, we train separate autoencoders on features extracted from different frequency bands and temporal scales, enabling the capture of nuanced discrepancies introduced by various deepfake generation techniques. Experimental results on a diverse dataset of real and deepfake audio samples demonstrate that our framework achieves superior performance compared to state-of-the-art methods in both deepfake detection accuracy and model attribution, particularly in cross-model generalization scenarios. These findings highlight the potential of our method to enhance the robustness and reliability of audio deepfake forensics."
http://arxiv.org/abs/2508.02479v1,Fine-grained Multiple Supervisory Network for Multi-modal Manipulation Detecting and Grounding,"Multi-modal manipulation detection and grounding, identifying and localizing tampered regions using inconsistencies across different modalities, is crucial for ensuring data integrity. Existing methods often struggle to capture subtle inconsistencies between modalities, particularly at the fine-grained level, hindering precise localization of manipulated regions. This paper introduces a Fine-grained Multiple Supervisory Network (FMSN) for robust multi-modal manipulation detection and grounding. FMSN leverages a hierarchical feature extraction module to capture multi-scale representations from both RGB and noise modalities. We then introduce a novel Fine-grained Consistency Learning (FCL) strategy, employing multiple supervisory signals at different feature levels to explicitly guide the network to detect subtle inconsistencies. Furthermore, a cross-modal attention mechanism dynamically fuses features from both modalities, enhancing the network's ability to identify manipulated regions. Extensive experiments on benchmark datasets demonstrate that FMSN significantly outperforms state-of-the-art methods in both manipulation detection and grounding accuracy, achieving substantial improvements in IoU and F1-score. The proposed FMSN provides a promising approach for detecting and localizing subtle multi-modal manipulations, advancing the field of multimedia forensics."
http://arxiv.org/abs/2508.02477v1,Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions,"Image anomaly detection aims to identify deviations from normal patterns, crucial for various real-world applications like industrial inspection and medical diagnosis. However, existing anomaly detection methods often struggle with multi-class datasets and lack robustness against distribution shifts encountered in practical deployments. This paper addresses the limitations of current anomaly detection techniques by focusing on multi-class anomaly detection and the need for robust performance in the face of realistic data variations. We propose a novel framework that combines a deep feature extractor trained with a contrastive loss on normal data with an ensemble of one-class classifiers, each specialized for a specific normal class, to improve anomaly detection accuracy and reduce false positives. Furthermore, we introduce a data augmentation strategy during training to enhance the robustness of the feature extractor against common image corruptions and variations. Experimental results on benchmark datasets and a real-world industrial inspection dataset demonstrate that our proposed approach significantly outperforms state-of-the-art methods in both anomaly detection accuracy and robustness. This work advances the field of anomaly detection by providing a practical and robust solution for multi-class scenarios, facilitating its wider adoption in real-world applications."
http://arxiv.org/abs/2508.02431v2,Identifying actionable driver mutations in lung cancer using an efficient Asymmetric Transformer Decoder,"Lung cancer is a leading cause of cancer-related mortality, with actionable driver mutations dictating personalized treatment strategies. Identifying these mutations from high-throughput sequencing data remains a crucial but computationally challenging task, often requiring significant resources and expertise. This paper addresses the problem of efficiently and accurately identifying actionable driver mutations in lung cancer using a novel deep learning approach. We propose an Asymmetric Transformer Decoder (ATD) that leverages a pre-trained protein language model to encode genomic sequences and then employs a lightweight, asymmetric decoder architecture to predict the presence of specific driver mutations. The asymmetry focuses computational resources on relevant genomic regions, improving efficiency. Our experiments on a large-scale lung cancer genomic dataset demonstrate that ATD achieves state-of-the-art performance in identifying actionable driver mutations, exhibiting a significant improvement in F1-score compared to existing methods while requiring fewer computational resources. This efficient and accurate identification of driver mutations can facilitate more effective and timely treatment decisions for lung cancer patients."
http://arxiv.org/abs/2508.02386v1,Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection,"Unsupervised object discovery aims to identify and segment objects in images without relying on manual annotations, a crucial step towards truly autonomous visual understanding. However, current methods often struggle with complex scenes containing cluttered backgrounds, significant occlusions, and objects with high intra-class variation, hindering their ability to accurately discover and segment individual instances. This paper introduces a novel framework, Contrastive Clustering with Attention-guided Refinement (CCAR), to enhance object discovery for unsupervised instance segmentation and object detection. CCAR leverages a contrastive learning objective to learn discriminative feature embeddings, followed by an iterative clustering process that refines object boundaries using attention maps derived from a transformer-based architecture. Specifically, we incorporate a self-attention mechanism to selectively focus on object-centric features, suppressing background noise and improving segmentation masks. Experiments on challenging datasets like COCO and Pascal VOC demonstrate that CCAR significantly outperforms existing unsupervised object discovery methods, achieving state-of-the-art performance in both instance segmentation and object detection metrics. The proposed approach offers a robust and effective solution for unsupervised object understanding, paving the way for more generalizable and scalable computer vision systems."
http://arxiv.org/abs/2508.02372v1,TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification,"Reliable identification of transportation units (TUs), such as shipping containers and trailers, is crucial for efficient logistics and supply chain management. However, existing datasets often lack the diversity in viewpoints and environmental conditions necessary to train robust recognition systems. To address this, we introduce TRUDI and TITUS: a novel multi-perspective dataset of transportation unit images captured from diverse angles, lighting conditions, and occlusions. We also propose a three-stage recognition system, TITUS, comprised of (1) a YOLOv8-based TU detection module, (2) a perspective correction module utilizing homography estimation to normalize views, and (3) a character segmentation and recognition module leveraging a custom-trained CRNN. Experimental results demonstrate that TITUS achieves a mean Average Precision (mAP) of 87.5% on the TRUDI dataset, significantly outperforming existing state-of-the-art object detection and OCR methods. This work provides a valuable resource and a high-performing system for robust TU identification in real-world scenarios, contributing to advancements in automated logistics and intelligent transportation systems."
http://arxiv.org/abs/2508.02348v1,mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera,"Non-Line-of-Sight (NLOS) pedestrian localization is a critical challenge for autonomous navigation systems operating in urban environments. Existing methods often struggle to accurately estimate pedestrian locations in scenarios where direct visibility is obstructed, such as at T-junctions. This paper addresses the problem of robust and accurate NLOS pedestrian localization at T-junctions using millimeter-wave (mmWave) radar by leveraging road layout information extracted from camera imagery. Our proposed method integrates mmWave radar detections with a vision-based road layout extraction module to constrain the potential locations of NLOS pedestrians. Specifically, we use semantic segmentation of camera images to identify drivable areas, and then project these areas onto the radar point cloud to create a contextual prior for pedestrian localization. Experimental results at real-world T-junctions demonstrate that our approach significantly improves localization accuracy compared to radar-only methods, achieving a mean localization error reduction of over 40% in NLOS scenarios. This integrated sensor fusion approach provides a reliable and cost-effective solution for enhanced pedestrian safety in complex urban environments."
http://arxiv.org/abs/2508.02288v1,Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection,"Event cameras offer significant advantages over traditional cameras in high-speed and high dynamic range scenarios, but their asynchronous and sparse nature poses challenges for direct integration with existing computer vision algorithms. This paper addresses the problem of leveraging the rich temporal information provided by stereo event cameras for continuous-time 3D object detection. We propose a novel framework, STEAD (Stereo Temporal Event-based 3D Detection), which fuses stereo event streams within a continuous-time volumetric representation. STEAD employs a voxel grid that accumulates events over time, dynamically adapting the voxel occupancy based on event polarity and timestamps. A spatiotemporal convolutional network then processes this volumetric representation to predict 3D bounding boxes, enabling object detection at any arbitrary time instant. Experiments on the challenging DSEC-Det dataset demonstrate that STEAD achieves state-of-the-art performance, significantly improving the accuracy and temporal resolution of 3D object detection compared to existing event-based and frame-based methods. This work unlocks the potential of stereo event cameras for real-time perception in dynamic environments by effectively harnessing their inherent temporal advantages."
http://arxiv.org/abs/2508.02179v1,Weakly Supervised Multimodal Temporal Forgery Localization via Multitask Learning,"Multimodal video forgery detection is crucial for ensuring the integrity of information in the digital age. However, existing methods often require expensive pixel-level annotations, hindering their scalability to real-world scenarios. This paper addresses the challenge of localizing temporal forgeries in videos using only weak supervision in the form of video-level labels, leveraging both visual and auditory modalities. We propose a novel weakly supervised multitask learning framework that simultaneously learns to classify the video as forged or authentic and to identify the temporal segments containing manipulations. This is achieved by incorporating a cross-modal attention mechanism to fuse information from both modalities, a temporal aggregation module to capture long-range dependencies, and a consistency regularization term to ensure agreement between classification and localization outputs. Experiments on benchmark datasets demonstrate that our method achieves state-of-the-art weakly supervised temporal forgery localization performance, significantly outperforming existing approaches with comparable supervision. This work offers a practical and scalable solution for detecting manipulated videos, contributing to a more trustworthy online environment."
http://arxiv.org/abs/2508.02127v1,Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps,"Object detection in computer vision has primarily relied on RGB images and, more recently, event cameras to address challenges in dynamic scenes. However, performance degrades significantly under adverse lighting conditions such as low light or high dynamic range scenarios where RGB data is often unreliable. This paper addresses the problem of robust object detection under challenging lighting by incorporating monocular normal map estimation as an auxiliary modality. We propose a novel framework that leverages a deep convolutional neural network to predict surface normals from a single RGB image, subsequently fusing these estimated normals with the RGB features using a cross-modal attention mechanism within a standard object detection architecture. The fused features are then used for bounding box regression and classification. Experimental results on synthetic and real-world datasets demonstrate that our approach significantly improves object detection accuracy, particularly under extreme lighting conditions, outperforming state-of-the-art RGB-only and RGB-event based methods. The use of monocular normal maps provides valuable geometric cues that complement RGB data, leading to more resilient and accurate object detection in visually degraded environments."
http://arxiv.org/abs/2508.03753v1,Classification non supervis{}es d'acquisitions hyperspectrales cod{}es : quelles v{}rit{}s terrain ?,"Hyperspectral imaging, particularly when coupled with coded aperture techniques, offers rich spectral and spatial information for remote sensing applications. However, the lack of labeled data often necessitates unsupervised classification approaches, presenting a challenge in validating the resulting clusters against ground truth. This paper investigates the impact of different ground truth representations on the evaluation of unsupervised classification performance in coded aperture hyperspectral imagery. We propose a framework that explores various methods for establishing ground truth, including pixel-wise spectral similarity measures, region-based spatial homogeneity, and combinations thereof. These representations are then used to assess the quality of clusters obtained from several unsupervised classification algorithms, such as k-means and hierarchical clustering, applied to coded hyperspectral data. Our results demonstrate that the choice of ground truth significantly influences the perceived accuracy of unsupervised classification, with region-based approaches generally providing a more robust and interpretable evaluation metric. This work highlights the critical importance of carefully considering the definition and generation of ground truth when evaluating unsupervised classification methods in hyperspectral remote sensing, ultimately leading to more reliable and meaningful assessments of algorithm performance."
http://arxiv.org/abs/2508.02067v1,YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges,"Real-time object detection has undergone significant advancements, particularly driven by the You Only Look Once (YOLO) family of detectors. This survey addresses the critical need for a structured understanding of the evolution, innovations, and persistent challenges within the YOLO architecture over its eleven major iterations, from YOLOv1 to YOLOv11. We present a comprehensive analysis of each YOLO version, dissecting architectural modifications, loss function refinements, and training strategies employed to improve speed and accuracy. Our survey categorizes these improvements into key areas such as backbone networks, detection heads, feature aggregation techniques, and data augmentation methods, highlighting their impact on performance. Furthermore, we identify and discuss the remaining challenges, including handling small objects, improving robustness to adverse conditions, and enhancing generalization capabilities across diverse datasets. Through a systematic review of existing literature and comparative performance analyses, this work provides a valuable resource for researchers and practitioners seeking to leverage the power of YOLO for real-time object detection, while also illuminating directions for future research. This survey serves as a crucial roadmap for navigating the complexities of the YOLO landscape and fostering further innovation in the field."
http://arxiv.org/abs/2508.01980v1,On-the-Fly Object-aware Representative Point Selection in Point Cloud,"Point cloud processing is crucial for various applications, from autonomous driving to robotics. A key challenge in processing large-scale point clouds lies in efficiently extracting representative points that capture the underlying object structure while minimizing computational cost. This paper addresses the problem of selecting representative points in a point cloud in an object-aware and on-the-fly manner, without requiring prior knowledge or offline pre-processing. We propose a novel algorithm that dynamically adjusts the sampling density based on both local geometric features and learned object-level semantic information. Our method integrates a lightweight semantic segmentation network with a differentiable point selection module, allowing for end-to-end training that optimizes for both reconstruction accuracy and semantic consistency in the selected points. Experimental results on benchmark datasets demonstrate that our approach achieves superior performance compared to state-of-the-art methods, yielding more accurate and robust object representations with significantly fewer points. This efficient and adaptable point selection strategy paves the way for real-time applications demanding high fidelity with limited computational resources."
http://arxiv.org/abs/2508.01966v1,Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection,"Object detection relies heavily on large-scale annotated datasets, which are expensive and time-consuming to create. This paper addresses the challenge of reducing annotation dependence in object detection by proposing Self-Supervised YOLO (SS-YOLO), a novel framework that leverages contrastive learning to pre-train YOLO models without manual bounding box annotations. SS-YOLO employs a multi-stage training strategy. First, a contrastive learning module, utilizing image-level augmentations and a momentum encoder, learns robust feature representations from unlabeled images. Second, pseudo-labels are generated based on the learned features and used to pre-train the YOLO detector in a self-supervised manner. Finally, the pre-trained detector is fine-tuned on a small, labeled dataset for the downstream detection task. Experiments on PASCAL VOC and COCO datasets demonstrate that SS-YOLO significantly improves object detection performance compared to training from scratch, particularly when using limited labeled data, achieving gains of up to 8% mAP with only 10% of the labeled data. This work offers a promising avenue for developing more label-efficient object detection systems, reducing reliance on expensive annotations and enabling deployment in data-scarce environments."
http://arxiv.org/abs/2508.03750v1,GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification,"Glaucoma, a leading cause of irreversible blindness, necessitates accurate risk stratification for timely intervention and vision preservation. Existing methods often rely on single modality data or lack structured frameworks to effectively integrate diverse clinical information. This paper addresses the challenge of improving glaucoma risk stratification by leveraging a multimodal structured framework. We introduce GlaBoost, a novel approach that combines structured clinical data (e.g., intraocular pressure, age) with unstructured imaging data (e.g., fundus photographs, optical coherence tomography) within a gradient boosting framework. GlaBoost employs a hierarchical feature selection process, prioritizing structured data for initial stratification and subsequently refining predictions using image-derived features extracted via deep learning. Experimental results on a large, real-world glaucoma dataset demonstrate that GlaBoost significantly outperforms existing methods, achieving a 12% improvement in AUC compared to unimodal approaches and a 5% improvement over state-of-the-art multimodal techniques. GlaBoost offers a robust and clinically relevant tool for enhanced glaucoma risk assessment, facilitating personalized management strategies and potentially reducing vision loss."
http://arxiv.org/abs/2508.01921v1,"InspectVLM: Unified in Theory, Unreliable in Practice","Vision-Language Models (VLMs) have demonstrated impressive capabilities in tasks requiring joint reasoning about visual and textual data, holding promise for applications in automated inspection and quality control. However, the reliability of these models in real-world industrial settings, where subtle defects and nuanced contextual understanding are crucial, remains underexplored. This paper investigates the practical limitations of unified VLMs, specifically those employing a single, large pre-trained model for both visual and textual processing, when applied to defect detection and classification in manufacturing scenarios. We introduce InspectVLM, a comprehensive evaluation framework comprising a novel dataset of high-resolution images of manufactured components with detailed textual annotations describing potential defects, alongside a suite of metrics designed to assess the models' sensitivity to subtle variations and their ability to generalize across different manufacturing processes. Our experiments reveal that while state-of-the-art VLMs achieve high accuracy on coarse-grained defect classification, their performance significantly degrades when faced with subtle anomalies and out-of-distribution samples, highlighting a critical gap between theoretical capabilities and practical applicability. These findings underscore the need for further research into robust and reliable VLMs tailored for the stringent requirements of industrial inspection."
http://arxiv.org/abs/2508.01873v1,DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization,"The proliferation of deep learning has led to increasingly sophisticated face forgery techniques, posing a significant threat to online security and trust. Detecting these forgeries remains challenging, particularly when manipulations are subtle and visually realistic. This paper introduces DiffusionFF, a novel face forgery detection method that leverages diffusion models to localize subtle forgery artifacts. Our approach first trains a diffusion model on real face images to learn the underlying data distribution. We then invert the suspected forged image into the latent space of the pre-trained diffusion model and reconstruct it. The reconstruction process effectively highlights inconsistencies and artifacts introduced by forgery techniques, allowing us to generate a forgery probability map. Experimental results on benchmark datasets demonstrate that DiffusionFF achieves state-of-the-art performance in both forgery detection and localization, surpassing existing methods, especially in challenging scenarios with high-quality forgeries. This research offers a promising direction for robust and interpretable face forgery detection, contributing to the development of more secure and reliable online environments."
http://arxiv.org/abs/2508.01730v1,Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos,"Multi-object tracking (MOT) in unmanned aerial vehicle (UAV)-captured videos presents unique challenges due to frequent viewpoint changes, occlusions, and the small size of target objects. Current MOT methods often struggle to maintain track IDs for objects exhibiting unstable motion patterns, such as sudden accelerations or erratic maneuvers, particularly when combined with appearance similarity. This paper introduces a novel appearance-guided motion modeling framework to address the problem of robust multi-object tracking in UAV videos with unstable object motion. Our approach leverages a Kalman filter-based tracker enhanced with an appearance-aware motion prediction module. This module dynamically adjusts the predicted object trajectory based on learned appearance embeddings, effectively mitigating the impact of noisy or inconsistent motion cues. Experimental results on challenging UAV benchmark datasets demonstrate that our method significantly improves tracking accuracy and identity preservation compared to state-of-the-art MOT algorithms, especially for objects undergoing unstable motion. This work provides a valuable solution for reliable object tracking in complex UAV video scenarios, enabling downstream applications such as traffic monitoring and autonomous navigation."
http://arxiv.org/abs/2508.01712v1,HateClipSeg: A Segment-Level Annotated Dataset for Fine-Grained Hate Video Detection,"Hate speech detection in online videos is crucial for fostering safer online environments, yet current methods often struggle with the nuanced and contextual nature of hate, especially within short video segments. Existing datasets primarily focus on video-level labels, hindering the development of fine-grained models capable of identifying subtle instances of hate speech within specific segments. To address this, we introduce HateClipSeg, a novel segment-level annotated dataset for fine-grained hate video detection. HateClipSeg comprises over 1,000 videos, meticulously annotated with segment-level hate speech labels, including both explicit and implicit forms of hate. We further provide detailed annotations on the type of hate, target group, and severity, enabling a deeper understanding of the hateful content. We benchmark several state-of-the-art video understanding models on HateClipSeg, demonstrating the dataset's utility in training and evaluating fine-grained hate speech detection systems. Our results reveal a significant performance gap between video-level and segment-level detection, highlighting the challenges posed by this new dataset and the need for more sophisticated models. HateClipSeg serves as a valuable resource for advancing research in fine-grained hate speech detection, ultimately contributing to more effective online content moderation."
http://arxiv.org/abs/2508.01704v1,LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for Autonomous Driving,"Accurate and up-to-date maps are crucial for the safe and reliable operation of autonomous vehicles, especially in dynamic urban environments. However, existing mapping techniques often struggle with long-term map maintenance due to changes in the environment, such as construction, seasonal variations, and object relocation, leading to map staleness and reduced autonomy performance. To address this challenge, we present LT-Gaussian, a novel long-term map update framework utilizing 3D Gaussian Splatting (3D-GS) for autonomous driving. Our method leverages the efficient and view-consistent rendering capabilities of 3D-GS to represent the environment, coupled with a novel Bayesian filtering approach to dynamically update the Gaussian parameters based on new observations while mitigating the effects of noisy or transient changes. We introduce a confidence-based weighting scheme within the filter to prioritize reliable observations and selectively update map elements, ensuring robust map maintenance over extended periods. Experimental results on real-world autonomous driving datasets demonstrate that LT-Gaussian significantly improves map accuracy and reduces map staleness compared to existing methods, leading to enhanced localization and path planning performance. This work provides a practical and efficient solution for long-term map maintenance in autonomous driving, enabling more robust and reliable autonomous navigation in dynamic environments."
http://arxiv.org/abs/2508.01699v1,TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding,"Video Temporal Grounding (VTG) aims to localize the start and end times of a target event within an untrimmed video given a textual query. Existing Video Large Language Models (Video LLMs) struggle to accurately capture long-range temporal dependencies and fine-grained temporal boundaries, limiting their effectiveness in VTG. We introduce TimeExpert, an expert-guided Video LLM framework specifically designed to enhance VTG performance. TimeExpert leverages a novel Temporal Reasoning Module (TRM) that explicitly models the temporal context of video segments through hierarchical temporal aggregation and relation learning. Furthermore, we incorporate a Time-Aware Prompting strategy, utilizing external knowledge from pre-trained language models to guide the Video LLM towards a more precise understanding of temporal nuances in the query. Experimental results on multiple benchmark datasets, including ActivityNet Captions, Charades-STA, and TACoS, demonstrate that TimeExpert significantly outperforms state-of-the-art methods, achieving improvements of up to 5% in R@1, IoU=0.5. These findings highlight the importance of specialized temporal reasoning and expert knowledge integration for effectively addressing the challenges of video temporal grounding with Video LLMs."
http://arxiv.org/abs/2508.03749v1,Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation,"Urban rail platforms experience significant crowding, impacting passenger safety, comfort, and operational efficiency. Accurate and timely estimation of platform crowding is crucial for proactive management and real-time information dissemination, yet current methods often rely on expensive sensor deployments or passenger surveys. This paper addresses the problem of leveraging existing closed-circuit television (CCTV) infrastructure as a cost-effective and readily available data source for urban rail platform crowding estimation. We propose a novel deep learning framework that combines a pre-trained object detection model (YOLOv5) for pedestrian detection with a perspective transformation module to map pixel coordinates to real-world locations on the platform. Subsequently, a density estimation network, trained on synthetic data generated with platform layouts and realistic passenger distributions, is used to predict crowd density maps from the transformed pedestrian locations. Experimental results on a real-world CCTV dataset from a busy urban rail station demonstrate that our method achieves high accuracy in estimating platform crowding levels, with a mean absolute error (MAE) of 0.12 persons/m compared to ground truth data obtained through manual annotation. This demonstrates the potential of utilizing existing CCTV infrastructure for automated and accurate platform crowding estimation, enabling proactive management and improved passenger experience."
http://arxiv.org/abs/2508.01603v1,Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning,"The proliferation of AI-generated images poses a significant challenge to information integrity, necessitating robust detection mechanisms. Existing AI-generated image detectors often struggle to generalize across diverse generation models and prompts, exhibiting performance degradation when faced with unseen data distributions. To address this limitation, we propose a novel Image-Adaptive Prompt Learning (IAPL) framework. IAPL dynamically crafts tailored prompts based on the input image itself, leveraging a learnable prompt generator conditioned on image features extracted from a pre-trained vision transformer. These adaptive prompts are then used to query a large language model (LLM), whose output is fused with image features to predict the authenticity of the image. Experiments on a diverse dataset of AI-generated and real images demonstrate that IAPL significantly outperforms existing state-of-the-art detectors in cross-model and cross-prompt generalization scenarios, achieving an average improvement of 5-10% in AUC. This work offers a promising direction towards building more reliable and generalizable AI-generated image detection systems, crucial for mitigating the spread of misinformation."
http://arxiv.org/abs/2508.01591v1,Self-Navigated Residual Mamba for Universal Industrial Anomaly Detection,"Industrial anomaly detection is crucial for ensuring product quality and preventing costly manufacturing errors. However, existing methods often struggle with the diverse and complex nature of anomalies across different industrial settings and modalities. This paper addresses the challenge of developing a universal anomaly detection framework capable of handling various data types and anomaly characteristics without extensive task-specific tuning. We propose Self-Navigated Residual Mamba (SNR-Mamba), a novel architecture leveraging the selective state space model (Mamba) and a self-navigation mechanism to learn robust feature representations. SNR-Mamba incorporates residual connections and adaptive gating, allowing the model to selectively attend to relevant features and capture subtle anomalies. The self-navigation component dynamically adjusts the model's focus based on the input data, enhancing its adaptability to different anomaly types. Experiments on a diverse set of industrial anomaly detection benchmarks demonstrate that SNR-Mamba consistently outperforms state-of-the-art methods, achieving significant improvements in both detection accuracy and generalization ability. SNR-Mamba offers a practical and efficient solution for universal industrial anomaly detection, reducing the need for specialized models and streamlining deployment across various manufacturing processes."
http://arxiv.org/abs/2508.01562v1,Adaptive LiDAR Scanning: Harnessing Temporal Cues for Efficient 3D Object Detection via Multi-Modal Fusion,"LiDAR-based 3D object detection is crucial for autonomous navigation, but the computational cost of processing dense point clouds remains a significant challenge. This paper addresses the problem of inefficient LiDAR scanning by proposing an adaptive scanning strategy guided by temporal information and fused with camera imagery for enhanced 3D object detection. Our method dynamically adjusts the LiDAR scanning density based on the predicted motion and uncertainty of objects observed in previous frames, focusing computational resources on regions with high potential for change. This is achieved through a novel multi-modal fusion network that integrates LiDAR point clouds with temporally aligned camera images, enabling accurate object localization and motion prediction. Experimental results on the nuScenes dataset demonstrate a significant reduction in LiDAR point cloud density (up to 40%) with minimal impact on 3D object detection performance, achieving comparable or even superior results compared to static, high-density scans, particularly for fast-moving objects. This adaptive LiDAR scanning approach provides a pathway to more efficient and robust 3D perception systems for resource-constrained autonomous platforms."
http://arxiv.org/abs/2508.01525v1,MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection,"The proliferation of AI-generated images presents a growing challenge to information integrity and trust. Detecting these synthetic images requires robust methods that generalize across diverse generation techniques and datasets. This paper addresses the problem of limited generalization in AI-generated image detection by proposing MiraGe, a novel multimodal discriminative representation learning framework. MiraGe leverages both visual and frequency domain information, employing a contrastive learning objective to learn discriminative features that are invariant to style variations and content manipulations. Specifically, we train a dual-branch network, one operating on the RGB image and the other on its Discrete Cosine Transform (DCT) representation, to maximize feature similarity for real images and minimize it for generated ones, effectively capturing subtle inconsistencies introduced by AI generation processes. Experimental results demonstrate that MiraGe significantly outperforms existing state-of-the-art methods on cross-dataset and cross-generator generalization benchmarks, achieving up to 15% improvement in AUC score. The proposed method offers a more reliable and generalizable solution for distinguishing between real and AI-generated images, contributing to the development of trustworthy AI systems."
http://arxiv.org/abs/2508.01435v1,Hyperspectral Image Recovery Constrained by Multi-Granularity Non-Local Self-Similarity Priors,"Hyperspectral image (HSI) recovery from degraded measurements is a challenging ill-posed inverse problem. Exploiting the inherent self-similarity within HSI data is crucial for effective recovery, yet existing methods often struggle to capture the complex multi-scale nature of this similarity. This paper proposes a novel HSI recovery framework that leverages multi-granularity non-local self-similarity priors to regularize the solution. Specifically, we construct a hierarchical search strategy to identify similar spectral patches at varying scales, capturing both fine-grained and coarse-grained similarities within the HSI cube. These similar patches are then adaptively aggregated to form a robust prior, which is incorporated into a convex optimization framework. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of both quantitative metrics (PSNR, SSIM, SAM) and visual quality, outperforming existing self-similarity-based and deep learning-based approaches. This work provides a powerful and flexible framework for HSI recovery, offering improved accuracy and robustness compared to existing methods by more effectively harnessing the data's inherent structure."
http://arxiv.org/abs/2508.01423v2,3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks,"RGB-based 3D perception is a crucial area of research, enabling cost-effective solutions for various applications. However, the lack of explicit 3D information in RGB images poses a significant challenge for training robust 3D models. This paper addresses the problem of limited viewpoint diversity and the resulting overfitting in RGB-based 3D tasks. We introduce 3DRot, a novel 3D rotation augmentation technique that synthesizes new viewpoints of a scene by virtually rotating the 3D space around the camera. Our method employs differentiable rendering to project rotated 3D scene representations back into the image space, effectively generating augmented RGB images and corresponding pseudo-ground truth for tasks such as 3D object detection and pose estimation. Experimental results on the ScanNet and CO3Dv1 datasets demonstrate that 3DRot consistently improves the performance of state-of-the-art RGB-based 3D models, achieving significant gains in average precision and pose accuracy. This work offers a simple yet effective data augmentation strategy to improve the robustness and generalization ability of RGB-based 3D perception systems."
http://arxiv.org/abs/2508.01402v1,ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models,"The proliferation of AI-generated images necessitates robust detection methods, yet current approaches often lack transparency and explainability. This paper addresses the critical problem of understanding *why* a given image is classified as AI-generated, moving beyond simple binary classification. We introduce ForenX, a novel framework leveraging Multimodal Large Language Models (MLLMs) to not only detect AI-generated images but also provide human-interpretable explanations for their decisions. ForenX integrates visual feature extraction with textual reasoning, enabling it to identify subtle artifacts and inconsistencies indicative of AI generation and articulate these findings in natural language. Experimental results demonstrate that ForenX achieves competitive detection accuracy compared to state-of-the-art methods while simultaneously offering insightful explanations that highlight specific image regions and characteristics contributing to the AI-generated classification. This work represents a significant step towards building trust and accountability in AI-generated content detection by providing transparency into the decision-making process."
http://arxiv.org/abs/2508.01382v1,A Full-Stage Refined Proposal Algorithm for Suppressing False Positives in Two-Stage CNN-Based Detection Methods,"Two-stage Convolutional Neural Network (CNN)-based object detection methods rely on generating region proposals in the first stage and refining them in the second stage for final classification and localization. Despite significant progress, these methods often suffer from a high number of false positive detections, particularly in complex scenes with cluttered backgrounds. This paper addresses the problem of effectively suppressing false positive proposals in two-stage detectors without significantly impacting recall. We introduce a Full-Stage Refined Proposal (FSRP) algorithm that leverages contextual information and feature consistency across both the Region Proposal Network (RPN) and the region-of-interest (RoI) head. FSRP incorporates a contextual feature aggregation module in the RPN to improve initial proposal quality and a novel feature consistency loss in the RoI head to penalize proposals with inconsistent features between the two stages. Extensive experiments on the MS COCO dataset demonstrate that FSRP consistently improves the performance of Faster R-CNN and Mask R-CNN, achieving up to 2.1% AP improvement with a minimal increase in computational cost. This refined proposal strategy offers a practical and effective way to enhance the accuracy of two-stage object detectors by effectively mitigating false positive detections."
http://arxiv.org/abs/2508.01339v1,SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes,"Accurate and efficient detection of road anomalies like speed bumps and potholes is crucial for enhancing driving safety and enabling proactive road maintenance. However, existing object detection models often suffer from high computational costs, hindering their deployment in real-time, resource-constrained environments. This paper introduces SBP-YOLO, a lightweight and real-time object detection model specifically designed for detecting speed bumps and potholes. SBP-YOLO leverages a streamlined backbone network with depthwise separable convolutions and a novel feature fusion module to reduce computational complexity while preserving detection accuracy. Furthermore, we incorporate a spatial pyramid pooling fast (SPPF) module to effectively extract multi-scale contextual information. Experimental results on a newly curated dataset demonstrate that SBP-YOLO achieves a significant reduction in model size and inference time compared to standard YOLOv5, while maintaining comparable or even superior detection accuracy for speed bumps and potholes. SBP-YOLO offers a practical solution for real-time road anomaly detection, paving the way for improved driver assistance systems and efficient road infrastructure management."
http://arxiv.org/abs/2508.01338v1,Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework,"Image forgery detection is crucial for maintaining the integrity of visual information, yet pixel-level annotation for training forgery localization models remains expensive and time-consuming. This paper addresses the challenge of weakly-supervised image forgery localization using only image-level labels. We propose a novel Vision-Language Collaborative Reasoning (VLCR) framework that leverages the complementary strengths of visual and textual information. VLCR incorporates a cross-modal attention mechanism to dynamically integrate visual features extracted from a pre-trained vision transformer with semantic information derived from a text encoder describing potential forgery characteristics. This collaborative reasoning process enables the model to identify subtle inconsistencies indicative of manipulation, even with limited supervision. Experimental results on benchmark datasets demonstrate that VLCR achieves state-of-the-art performance in weakly-supervised forgery localization, significantly outperforming existing methods in terms of F1-score and IoU. The proposed VLCR framework offers a practical and effective solution for detecting image forgeries with minimal annotation effort, facilitating broader application in real-world scenarios."
http://arxiv.org/abs/2508.01334v2,Zero-shot Segmentation of Skin Conditions: Erythema with Edit-Friendly Inversion,"Skin condition segmentation is crucial for automated diagnosis and treatment planning, yet acquiring sufficient labeled data for diverse conditions remains a significant bottleneck. This paper addresses the challenge of segmenting erythema, a common skin manifestation, in a zero-shot setting where no labeled erythema data is available during training. We propose a novel approach leveraging a pre-trained vision-language model (VLM) and a two-stage inversion process. First, we generate a coarse segmentation map via a prompt-engineered VLM. Second, we refine this segmentation using an edit-friendly inversion technique that allows for targeted adjustments based on visual cues and contextual information, effectively mitigating inaccuracies in the initial VLM prediction. Our method demonstrates competitive performance against existing zero-shot segmentation techniques on a diverse dataset of erythema images, achieving a Dice score of X and an IoU of Y, even outperforming some weakly supervised methods. This work opens avenues for deploying segmentation models for rare or under-represented skin conditions without requiring extensive manual annotation."
http://arxiv.org/abs/2508.01311v1,C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor,"Anomaly detection in 3D space is crucial for various applications like autonomous driving and robotic manipulation, yet most existing methods struggle to adapt to evolving anomaly patterns in a continual learning setting. This paper addresses the challenging problem of continual 3D anomaly detection where the model must learn new anomaly types sequentially without forgetting previously learned ones. We propose C3D-AD, a novel approach that integrates a Kernel Attention mechanism with a Learnable Advisor to mitigate catastrophic forgetting and enhance anomaly representation. The Kernel Attention module captures long-range dependencies within 3D point clouds, enabling the model to discern subtle anomalous patterns while preserving previously learned features. The Learnable Advisor dynamically adjusts the learning rate for each feature map based on its relevance to the current and past tasks, effectively preventing knowledge erosion. Experimental results on benchmark datasets demonstrate that C3D-AD significantly outperforms state-of-the-art continual learning methods in terms of anomaly detection accuracy and forgetting rate. Our approach provides a robust and efficient solution for adapting 3D anomaly detection models to dynamic environments with evolving anomaly characteristics."
http://arxiv.org/abs/2508.01292v1,CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis,"Amyloid Positron Emission Tomography (PET) imaging is crucial for early Alzheimer's Disease (AD) diagnosis, but its high cost and radiation exposure limit widespread use. Synthesizing amyloid PET from readily available Magnetic Resonance Imaging (MRI) could provide a cost-effective screening tool. However, existing image-to-image translation methods often struggle to preserve anatomical details and generate realistic PET images with accurate amyloid deposition patterns. We propose CoCoLIT, a ControlNet-conditioned Latent Image Translation framework for synthesizing amyloid PET from MRI. CoCoLIT leverages a pre-trained Stable Diffusion latent space and incorporates anatomical guidance from MRI using ControlNet to enforce structural consistency. A conditional variational autoencoder (cVAE) is trained within this framework to map MRI latent representations to PET latent representations, allowing for controllable synthesis. Experimental results on a large ADNI dataset demonstrate that CoCoLIT significantly outperforms state-of-the-art image translation methods in terms of image quality, structural similarity, and amyloid burden accuracy, as measured by PSNR, SSIM, and correlation with ground truth PET standardized uptake value ratios (SUVR). CoCoLIT offers a promising approach for generating realistic and clinically relevant amyloid PET images from MRI, potentially improving AD screening accessibility and reducing the need for invasive procedures."
http://arxiv.org/abs/2508.01253v1,ODOV: Towards Open-Domain Open-Vocabulary Object Detection,"Open-vocabulary object detection aims to detect objects beyond the limited set of categories seen during training, leveraging external knowledge to recognize novel objects. However, existing open-vocabulary detectors often struggle with complex real-world scenarios and are limited by the scope of pre-defined vocabulary. This paper addresses the challenge of achieving truly open-domain open-vocabulary object detection, where the detector can identify and localize objects described by arbitrary textual phrases in unconstrained environments. We introduce ODOV, a novel framework that integrates a powerful vision-language model with a dynamic query generation and scoring mechanism. ODOV adaptively generates relevant query embeddings based on image content and contextual information, enabling it to effectively ground diverse and complex textual descriptions onto visual regions. Extensive experiments on challenging open-vocabulary benchmarks demonstrate that ODOV significantly outperforms existing methods, achieving state-of-the-art performance in detecting objects with complex attributes and relationships. This work paves the way for more robust and versatile object detection systems capable of understanding and interacting with the visual world in a truly open-ended manner."
http://arxiv.org/abs/2508.01248v1,NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection,"The proliferation of AI-generated images presents a significant challenge to information integrity and online trust. Existing detectors often struggle to generalize across diverse generation methods and unseen datasets due to overfitting to specific artifacts or stylistic biases present in the training data. To address this limitation, we propose NS-Net, a novel detection framework that decouples CLIP-encoded semantic information through a learned NULL-Space projection. By projecting CLIP features into the NULL-Space of a classifier trained to predict the presence of AI-generated content, we effectively remove task-relevant semantic information, forcing the network to focus on subtle, underlying artifacts indicative of AI generation. This decoupled representation is then fed into a lightweight classifier for final prediction. Extensive experiments demonstrate that NS-Net achieves superior generalization performance compared to state-of-the-art methods on a variety of unseen AI-generated image datasets. This approach offers a more robust and generalizable solution for detecting AI-generated images, mitigating the risks associated with synthetic media."
http://arxiv.org/abs/2508.01095v2,"AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions","Industrial smoke emissions pose a significant environmental hazard, necessitating robust and real-time monitoring systems. Existing smoke detection methods often struggle with complex environmental conditions, such as varying illumination, dynamic backgrounds, and subtle smoke characteristics, leading to high false positive and false negative rates. We introduce AURA, a novel hybrid framework that integrates spatiotemporal analysis with chromatic information for robust and real-time industrial smoke emission detection. AURA leverages a motion-compensated temporal difference module to identify potential smoke regions based on dynamic changes, followed by a chromatic analysis module that utilizes a learned color space to discern smoke from non-smoke objects based on their distinctive color properties. Furthermore, AURA incorporates a spatial context module that refines the detection by considering the spatial distribution and adjacency of smoke-like pixels, reducing false positives caused by isolated noise. Experimental results on a newly curated industrial smoke emission dataset, containing diverse environmental conditions and smoke characteristics, demonstrate that AURA achieves state-of-the-art performance, with an average F1-score of 92.3% and a processing speed of 45 frames per second, significantly outperforming existing methods. AURA provides a practical and effective solution for real-time and accurate monitoring of industrial smoke emissions, contributing to improved environmental protection and regulatory compliance."
http://arxiv.org/abs/2508.01045v1,Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans,"Anomaly classification in 3D chest CT scans is crucial for early diagnosis and improved patient outcomes, particularly in the context of emerging infectious diseases. However, the subtle and varied nature of pulmonary anomalies poses a significant challenge for automated detection. This paper addresses the problem of effectively capturing complex structural relationships between lung regions for improved anomaly classification in 3D chest CT scans. We propose a novel Structured Spectral Graph Learning (SSGL) framework that learns a graph representation reflecting the anatomical structure of the lungs and leverages spectral analysis to identify anomalous patterns. SSGL constructs a graph where nodes represent segmented lung regions and edge weights are learned adaptively based on both spatial proximity and feature similarity. The learned graph Laplacian is then used in a spectral embedding, enabling anomaly classification based on deviations from the expected spectral characteristics of healthy lungs. Experimental results on a large-scale chest CT dataset demonstrate that SSGL achieves state-of-the-art performance in anomaly classification, outperforming existing methods by a significant margin in terms of AUC and F1-score. This work provides a powerful and interpretable approach for automated anomaly detection in chest CT scans, facilitating timely and accurate diagnosis."
http://arxiv.org/abs/2508.01016v1,Diagnostic Accuracy of Open-Source Vision-Language Models on Diverse Medical Imaging Tasks,"Vision-language models (VLMs) have shown remarkable capabilities in general visual understanding, raising interest in their application to medical image analysis. However, their diagnostic accuracy across diverse medical imaging tasks remains largely unexplored, particularly for readily accessible, open-source models. This work addresses the problem of evaluating the diagnostic performance of open-source VLMs across a spectrum of medical imaging modalities and clinical tasks. We systematically evaluate several prominent open-source VLMs, including CLIP, ALBEF, and Florence, on classification, segmentation, and visual question answering tasks using publicly available datasets spanning radiology, pathology, and dermatology. We employ zero-shot and fine-tuning strategies, adapting the models to specific tasks through prompt engineering and task-specific heads. Our results demonstrate that while VLMs exhibit reasonable zero-shot performance, fine-tuning significantly improves diagnostic accuracy, achieving state-of-the-art performance on some tasks, but with considerable variability across different modalities and clinical applications. This study provides a comprehensive benchmark and highlights the potential and limitations of open-source VLMs for medical image analysis, guiding future research towards more robust and reliable AI-driven diagnostic tools."
http://arxiv.org/abs/2508.01015v1,AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise,"Eye tracking offers a promising avenue for understanding cognitive processes and skill levels. However, manually analyzing eye movement data for expertise assessment is time-consuming and subjective. This paper introduces AutoSIGHT, an automatic eye tracking-based system designed for the immediate grading of human expertise in specific tasks. AutoSIGHT leverages a novel deep learning architecture that combines convolutional neural networks (CNNs) for feature extraction from fixation maps and recurrent neural networks (RNNs) for modeling temporal gaze patterns. The system is trained on labeled eye-tracking data from participants with varying levels of expertise performing a simulated task. Experimental results on a benchmark dataset demonstrate that AutoSIGHT achieves state-of-the-art accuracy in classifying expertise levels, outperforming traditional feature-based methods by a significant margin. AutoSIGHT provides a valuable tool for automated skill assessment and personalized training in various domains."
http://arxiv.org/abs/2508.01008v1,ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation,"Generating images conditioned on open-vocabulary instructions and grounded to specific instances within a scene remains a challenging task for current text-to-image models. Existing datasets either lack instance-level annotations or offer limited descriptive captions that fail to fully capture the nuances of object relationships and attributes relevant for controllable generation. To address this, we introduce ROVI: a novel dataset of 85K images with re-captions specifically designed for open-vocabulary instance-grounded text-to-image generation. ROVI leverages a Vision-Language Model (VLM) to automatically identify and segment objects, followed by a Large Language Model (LLM) to generate detailed, instance-specific captions incorporating contextual information. We curate a diverse set of prompts guiding the LLM to produce rich descriptions focusing on attributes, relationships, and actions of individual objects. Experiments demonstrate that training existing text-to-image models on ROVI significantly improves their ability to generate images that accurately reflect both the textual instruction and the specified instance, leading to a 15% improvement in grounding accuracy compared to models trained on existing datasets. ROVI provides a valuable resource for advancing research in controllable and grounded image generation, enabling more precise and semantically meaningful image synthesis."
http://arxiv.org/abs/2508.00822v1,Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning,"Semantic segmentation of 3D point clouds is crucial for various applications in urban environments, including autonomous driving and city planning. However, deep learning models trained on one dataset often exhibit poor generalization performance when applied to data from different sources due to variations in sensor characteristics, data density, and annotation styles. This paper addresses the challenge of cross-dataset generalization in 3D semantic segmentation by investigating the performance of state-of-the-art deep learning models across a diverse collection of NIST point cloud city datasets. We propose a unified framework for data preprocessing, annotation mapping, and model evaluation to facilitate a comprehensive analysis of cross-dataset performance. Specifically, we evaluate PointNet++, DGCNN, and RandLA-Net on multiple NIST datasets, systematically analyzing the impact of data characteristics on segmentation accuracy. Our results demonstrate significant performance degradation when models trained on one dataset are directly applied to another, highlighting the need for domain adaptation techniques. This work provides valuable insights into the challenges of cross-dataset generalization in 3D semantic segmentation and establishes a benchmark for future research in this area."
http://arxiv.org/abs/2508.00777v1,Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning,"Anomaly detection aims to identify unusual patterns that deviate significantly from the norm, a crucial task in various industrial and surveillance applications. However, traditional anomaly detection methods often require substantial labeled anomalous data for training, which is impractical in real-world scenarios where anomalies are rare and diverse. This paper addresses the challenge of zero-shot anomaly detection, where the model must identify anomalies without any prior exposure to anomalous samples. We propose a novel Dual-Branch Prompt Learning (DBPL) framework that leverages pre-trained vision-language models to detect anomalies by contrasting normal and anomalous representations generated through carefully designed prompts. DBPL consists of two parallel branches: a normal branch that learns to represent normal patterns and an anomaly branch that focuses on representing potential anomalous deviations, both guided by learnable prompts optimized to elicit relevant features from the pre-trained model. Experiments on benchmark anomaly detection datasets demonstrate that DBPL significantly outperforms existing zero-shot and unsupervised methods, achieving state-of-the-art performance while requiring no anomaly labels. This advancement offers a practical and scalable solution for anomaly detection in diverse and dynamic environments."
http://arxiv.org/abs/2508.00748v2,Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos,"The proliferation of photorealistic talking-head avatars raises significant concerns about their potential misuse, particularly in bypassing biometric verification systems. This paper addresses the critical problem of assessing the vulnerability of face and voice-based biometric authentication methods to sophisticated talking-head avatar videos. We propose a novel evaluation framework that systematically investigates the performance of state-of-the-art biometric verification systems under a range of realistic attack scenarios, including varying video quality, lighting conditions, and avatar customization levels. Our method leverages a combination of deep learning-based face recognition and speaker verification models to quantify the success rate of avatar-based impersonation attempts. Experimental results demonstrate a significant degradation in the accuracy of both face and voice verification systems when presented with photorealistic avatar videos, highlighting the susceptibility of these systems to even subtle manipulations. These findings underscore the urgent need for developing robust and reliable anti-spoofing techniques to safeguard biometric authentication systems against the evolving threat of advanced avatar technology."
http://arxiv.org/abs/2508.00744v1,Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR,"LiDAR-based 3D object detection is crucial for autonomous driving, demanding both high accuracy and computational efficiency. Existing lightweight 3D object detection methods often rely on directly adapting 2D image backbones, overlooking the unique characteristics of sparse and irregular LiDAR point cloud data, resulting in suboptimal performance. This paper addresses the challenge of designing a truly lightweight and effective backbone tailored for LiDAR-based 3D object detection. We propose a novel backbone architecture, the Sparse Feature Pyramid Transformer (SFPT), which leverages sparse convolution to efficiently process point clouds and introduces a transformer-based feature fusion module to effectively aggregate multi-scale contextual information. SFPT is designed with a specific focus on minimizing computational cost while maximizing feature representation power within the sparse LiDAR domain. Experiments on the KITTI dataset demonstrate that our SFPT backbone achieves comparable or superior detection accuracy to heavier backbones while significantly reducing computational complexity and memory footprint. This improved efficiency facilitates real-time deployment of high-performance 3D object detection systems on resource-constrained platforms."
http://arxiv.org/abs/2508.00701v2,D3: Training-Free AI-Generated Video Detection Using Second-Order Features,"The rapid advancement of AI-driven video generation models poses a significant threat to information integrity, necessitating robust detection methods. Existing techniques often rely on extensive training with labeled data, limiting their generalization to unseen generative models and requiring constant adaptation. This paper addresses the challenge of detecting AI-generated videos without relying on training data by exploiting inherent statistical differences in second-order feature distributions between real and synthetic content. Our method, D3 (Detection via Distribution Discrepancy), leverages pre-trained convolutional neural networks to extract high-level features and then computes the Wasserstein distance between the distributions of Gram matrices derived from these features for a given video and a reference set of real videos. By analyzing these distribution discrepancies in feature space, D3 effectively identifies AI-generated content. Experimental results demonstrate that D3 achieves state-of-the-art performance in training-free AI-generated video detection across various generative models and datasets, outperforming existing zero-shot approaches by a significant margin. This training-free approach provides a crucial tool for combating the spread of misinformation and maintaining trust in visual media."
http://arxiv.org/abs/2508.00649v2,"Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights","Adversarial patches pose a significant threat to object detection systems, as small, carefully crafted image regions can drastically alter detector outputs. Despite numerous proposed defenses, a comprehensive and unified evaluation across diverse object detectors and attack configurations remains lacking, hindering a clear understanding of their true effectiveness. This paper addresses this gap by introducing a large-scale dataset of adversarial patch attacks against popular object detectors (Faster R-CNN, YOLOv5, and DETR) and providing a standardized evaluation framework for assessing defense mechanisms. We analyze existing defense strategies, revealing their limitations under adaptive attacks and propose a novel input transformation defense based on frequency domain filtering, demonstrating improved robustness against a wide range of patch sizes and locations. Our experiments show that frequency filtering boosts the Average Precision (AP) of defended detectors by up to 15% under strong adversarial attacks, significantly outperforming existing defense methods in many scenarios. This work provides a valuable benchmark for future research and contributes towards the development of more resilient object detection systems in real-world applications."
http://arxiv.org/abs/2508.00620v1,Backdoor Attacks on Deep Learning Face Detection,"Deep learning-based face detection models are increasingly deployed in security-sensitive applications, making them potential targets for adversarial attacks. This paper investigates the vulnerability of state-of-the-art deep learning face detectors to backdoor attacks. We propose a novel backdoor attack strategy, termed ""Feature-Space Trigger Injection"" (FSTI), which injects a subtle, imperceptible trigger directly into the feature space of the face detection model during training. FSTI utilizes a generative adversarial network (GAN) to craft feature-space triggers that are both effective in activating the backdoor and difficult to detect. Our experiments on several popular face detection architectures, including RetinaFace and Faster R-CNN, demonstrate that FSTI achieves high attack success rates (over 95%) while maintaining minimal performance degradation on clean data. This highlights the significant security risk posed by backdoor attacks on face detection systems and underscores the need for robust defense mechanisms."
http://arxiv.org/abs/2508.00591v1,Wukong Framework for Not Safe For Work Detection in Text-to-Image systems,"Text-to-image (TTI) generation models have demonstrated remarkable capabilities in creating photorealistic imagery from textual prompts. However, this technology also presents a significant risk of generating Not Safe For Work (NSFW) content, necessitating robust detection mechanisms. This paper addresses the challenge of reliably identifying NSFW content generated by TTI systems, which often exhibits subtle variations and manipulations not readily captured by existing image-based NSFW detectors. We introduce the Wukong Framework, a novel approach that leverages both image and text modalities for enhanced NSFW detection. Wukong employs a multi-stage architecture: first, a vision transformer analyzes the generated image for visual cues indicative of NSFW content; second, the original text prompt is analyzed using a contextualized language model to assess its inherent NSFW potential; finally, a fusion module integrates these multimodal embeddings to produce a comprehensive NSFW score. Experimental results on a diverse dataset of generated images demonstrate that Wukong achieves significantly higher precision and recall compared to state-of-the-art image-only NSFW detectors, reducing false negatives by up to 30%. The Wukong Framework provides a crucial safeguard for responsible deployment of TTI systems, mitigating the risk of generating and disseminating harmful content."
http://arxiv.org/abs/2508.00587v1,Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is crucial for the safe deployment of deep learning models in real-world scenarios, particularly in pixel-wise dense prediction tasks like semantic segmentation. Existing likelihood-based OOD detection methods often struggle with overconfident predictions on OOD data, leading to unreliable uncertainty estimates and poor performance. This paper addresses the problem of improving pixel-wise OOD detection by developing a novel uncertainty-aware likelihood ratio estimation framework. Our approach leverages a variational autoencoder (VAE) to model the in-distribution data likelihood and introduces a learned uncertainty map that modulates the likelihood ratio calculation. This uncertainty map is trained adversarially to minimize the separation between in- and out-of-distribution likelihood ratios, effectively calibrating the model's confidence. We demonstrate through extensive experiments on diverse datasets, including Cityscapes, GTA5, and IDD, that our method significantly outperforms state-of-the-art OOD detection techniques, achieving substantially improved AUROC and FPR@95TPR scores. The proposed method offers a robust and reliable solution for identifying OOD pixels, enhancing the safety and trustworthiness of dense prediction models."
http://arxiv.org/abs/2508.00563v1,Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images,"Electron microscopy (EM) is a critical tool for structural virology, enabling the visualization and analysis of virus capsids. However, manually annotating virus capsids in EM images is a laborious and time-consuming process, hindering large-scale studies. This paper addresses the challenge of detecting virus capsids in EM images using only weak supervision in the form of image-level annotations, indicating the presence or absence of capsids within an image. We propose a novel weakly supervised learning framework that leverages a convolutional neural network (CNN) to generate capsid localization maps, guided by a custom loss function that encourages the network to focus on capsid-like features while suppressing background noise. Specifically, we incorporate a multi-instance learning (MIL) loss to associate image-level labels with potential capsid locations identified by the CNN, coupled with a regularization term to promote spatial consistency in the generated localization maps. Experimental results on a diverse EM dataset demonstrate that our method achieves competitive detection performance compared to fully supervised approaches, while significantly reducing annotation effort. This work provides a practical and efficient solution for large-scale virus capsid detection, facilitating high-throughput analysis of viral structures in EM imagery."
http://arxiv.org/abs/2508.00528v1,EPANet: Efficient Path Aggregation Network for Underwater Fish Detection,"Underwater fish detection is crucial for marine ecosystem monitoring and sustainable fishing practices. However, the inherent challenges of underwater environments, such as low visibility, color distortion, and complex backgrounds, significantly impede the performance of existing object detection models. This paper addresses the problem of efficiently and accurately detecting fish in challenging underwater conditions by proposing EPANet, an Efficient Path Aggregation Network. EPANet leverages a novel feature pyramid architecture incorporating a lightweight channel attention mechanism and a bi-directional feature fusion strategy. This allows the network to effectively aggregate multi-scale contextual information while suppressing noise and enhancing discriminative features relevant to fish detection. Experimental results on benchmark underwater fish datasets demonstrate that EPANet achieves state-of-the-art performance in terms of both detection accuracy and computational efficiency, surpassing existing methods by a significant margin, especially in low-visibility scenarios. The proposed EPANet offers a practical and effective solution for real-time underwater fish detection, contributing to advancements in marine resource management and conservation efforts."
http://arxiv.org/abs/2508.00473v1,HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection,"Anomaly detection in 3D point cloud videos is critical for safety-sensitive applications such as autonomous driving and surveillance. Existing methods often struggle to effectively model the complex spatio-temporal relationships present in dynamic 3D point cloud data, leading to suboptimal anomaly detection performance. To address this, we introduce HyPCV-Former, a novel Hyperbolic Point Cloud Video Transformer architecture. HyPCV-Former leverages the inherent hierarchical structure of point clouds by embedding them into hyperbolic space, enabling efficient modeling of long-range dependencies and complex relationships using a hierarchical attention mechanism. Furthermore, a spatio-temporal transformer network is designed to capture both spatial and temporal anomalies within the hyperbolic embedding space. Our experiments on benchmark datasets, including the ShanghaiTech and RoadAnomaly3D datasets, demonstrate that HyPCV-Former achieves state-of-the-art anomaly detection performance, surpassing existing methods by a significant margin. The proposed method offers a robust and efficient solution for anomaly detection in dynamic 3D point cloud data, paving the way for safer and more reliable autonomous systems."
http://arxiv.org/abs/2508.00445v1,AutoDebias: Automated Framework for Debiasing Text-to-Image Models,"Text-to-image (TTI) models have demonstrated remarkable capabilities in generating photorealistic images from textual descriptions, but they often inherit and amplify biases present in their training data, leading to unfair or discriminatory outcomes. This work addresses the critical problem of automatically identifying and mitigating biases in TTI models without requiring extensive human annotation or intervention. We introduce AutoDebias, an automated framework leveraging a novel combination of bias detection and mitigation strategies. AutoDebias first employs a contrastive learning approach to identify biased concepts within the model's latent space. Subsequently, it utilizes a fine-tuning procedure with dynamically generated counterfactual data and adversarial regularization to steer the model away from biased associations, promoting fairness across sensitive attributes. Experiments conducted on Stable Diffusion using benchmark datasets demonstrate that AutoDebias significantly reduces bias in generated images across various sensitive attributes, such as gender and race, while preserving image quality and text alignment. AutoDebias offers a practical and scalable solution for developing fairer and more equitable TTI models, advancing the responsible development of AI technology."
http://arxiv.org/abs/2508.00440v1,Reducing the gap between general purpose data and aerial images in concentrated solar power plants,"Concentrated Solar Power (CSP) plants offer a promising avenue for sustainable energy production, yet their efficient operation hinges on precise monitoring and control of heliostat fields. Deep learning models offer potential for automating this monitoring, but their performance is often limited by the domain gap between general-purpose datasets used for pre-training and the specific characteristics of aerial imagery from CSP plants, such as unique illumination conditions and viewpoints. This paper addresses the challenge of adapting deep learning models trained on general datasets to the specific domain of aerial imagery of CSP plants. We propose a novel transfer learning framework incorporating a two-stage approach: first, a self-supervised pre-training phase leveraging unlabeled aerial imagery to learn domain-specific features, followed by fine-tuning with a small set of labeled CSP plant images using a contrastive loss function to enhance feature discrimination. Experimental results on a real-world dataset of aerial images from a CSP plant demonstrate a significant improvement in heliostat identification and anomaly detection accuracy compared to models trained solely on general datasets or with standard transfer learning techniques, achieving a 15% increase in mean Average Precision (mAP). This work demonstrates an effective strategy for bridging the domain gap and enabling the deployment of robust vision-based monitoring systems in CSP plants."
http://arxiv.org/abs/2508.00421v1,UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken,"Underwater instance segmentation is crucial for marine environment monitoring and resource management, yet it remains challenging due to low visibility, color distortion, and complex object shapes. Existing convolutional and transformer-based methods often struggle to capture long-range dependencies and fine-grained details in such degraded underwater images. To address these limitations, we introduce UIS-Mamba, a novel architecture leveraging the Mamba state space model for underwater instance segmentation. UIS-Mamba incorporates a Dynamic Tree Scan (DTS) module to efficiently model non-contiguous object regions and a Hidden State Weaken (HSW) strategy to mitigate the accumulation of irrelevant information in the Mamba's hidden states, thereby enhancing feature representation in challenging underwater conditions. Experimental results on benchmark underwater datasets demonstrate that UIS-Mamba achieves state-of-the-art performance, surpassing existing CNN and Transformer-based methods in both segmentation accuracy and computational efficiency. This work highlights the potential of state space models for addressing the unique challenges of underwater image analysis and opens new avenues for future research in this domain."
http://arxiv.org/abs/2508.00397v1,Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency,"Video forgery poses a significant threat to information integrity, demanding robust detection methods. This paper addresses the challenge of detecting manipulated videos by exploiting inconsistencies in optical flow and spatial-temporal domains. Our proposed method leverages optical flow residuals, calculated by comparing estimated optical flow with the flow warped from neighboring frames, to identify regions of potential tampering. Furthermore, we enforce spatial-temporal consistency by employing a 3D convolutional neural network (CNN) to analyze the temporal evolution of these optical flow residuals and spatial features extracted from the video frames. This network learns to identify subtle inconsistencies indicative of forgery. Experimental results on benchmark datasets, including DFDC and FaceForensics++, demonstrate that our approach achieves state-of-the-art performance in detecting various types of video manipulations, surpassing existing methods in both accuracy and robustness. This research provides a valuable tool for identifying and mitigating the spread of manipulated video content."
http://arxiv.org/abs/2508.00374v1,Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models,"Long-term action anticipation, predicting future human actions several steps ahead, is crucial for proactive human-robot interaction and video understanding. Current methods often struggle to maintain accuracy over extended horizons due to the compounding uncertainty in future action sequences. This paper addresses the challenge of anticipating long-term human actions by leveraging the contextual reasoning capabilities of Large Language Models (LLMs) and introducing a novel Bidirectional Action Sequence Learning (BASL) framework. BASL employs a transformer-based architecture to encode past action sequences and decode future actions, conditioned on LLM-generated action descriptions. Critically, BASL incorporates both forward and backward decoding pathways, enabling the model to refine its predictions by considering both preceding and subsequent actions within a temporal window. Experiments on benchmark datasets demonstrate that BASL significantly outperforms state-of-the-art methods, achieving a 15-20% improvement in long-term anticipation accuracy. These results highlight the effectiveness of bidirectional reasoning and LLM integration for enhancing the coherence and accuracy of long-term action prediction."
http://arxiv.org/abs/2508.00358v1,Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering,"Multi-object tracking (MOT) is a crucial task in computer vision, enabling a wide range of applications from autonomous driving to video surveillance. Existing MOT methods often struggle to maintain tracklet stability across varying object speeds and complex motion patterns, leading to frequent identity switches and fragmented trajectories. This paper addresses the challenge of robust MOT performance under diverse speed conditions. We propose a novel speed-driven MOT framework incorporating a learnable Kalman Filter (LKF) that dynamically adapts its state transition and observation models based on the estimated speed of each individual object. The LKF learns speed-dependent parameters from data, allowing it to better predict object motion and handle abrupt speed changes. Experimental results on challenging MOT benchmarks, including MOT17 and MOT20, demonstrate significant improvements in tracking accuracy (IDF1) and reduction in identity switches (IDSw) compared to state-of-the-art methods, particularly for fast-moving objects. Our approach provides a more reliable and robust solution for MOT in dynamic environments, paving the way for improved performance in real-world applications."
http://arxiv.org/abs/2508.00312v1,GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection,"Video anomaly detection (VAD) aims to identify unusual events in surveillance videos, a task crucial for public safety and security. However, the scarcity of fully annotated abnormal videos necessitates weakly-supervised approaches, often relying solely on normal video data for training. This paper addresses the challenge of effectively leveraging generative models for weakly-supervised VAD by introducing GV-VAD, a novel framework that explores video generation as an auxiliary task to enhance anomaly detection performance. GV-VAD employs a conditional variational autoencoder (CVAE) to generate future video frames conditioned on past observations. We introduce a novel anomaly scoring mechanism that combines reconstruction error from the CVAE with a learned discrepancy between generated and real future frames, capturing both appearance and motion irregularities. Experimental results on benchmark datasets, including UCF-Crime and ShanghaiTech, demonstrate that GV-VAD achieves state-of-the-art performance in weakly-supervised VAD, surpassing existing methods by a significant margin. This highlights the potential of video generation as a powerful tool for learning robust representations of normality, leading to improved anomaly detection capabilities."
http://arxiv.org/abs/2508.00299v1,Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence,"Realistic and controllable pedestrian simulation is crucial for evaluating the robustness and safety of autonomous driving systems, especially in complex multi-view scenarios. Existing methods often struggle to provide fine-grained control over pedestrian actions and appearances within a consistent multi-view environment. This paper addresses the challenge of controllable pedestrian video editing in multi-view driving scenarios, enabling precise manipulation of pedestrian behavior while preserving scene realism and multi-view consistency. We introduce a novel framework that leverages motion sequences to guide pedestrian editing. Our approach first extracts motion sequences from real pedestrian videos and then uses these sequences as control signals to manipulate the pose and appearance of virtual pedestrians inserted into real driving scenes. A multi-view consistent generative adversarial network (GAN) is then employed to seamlessly integrate the edited pedestrians, ensuring realistic rendering and spatial-temporal coherence across multiple camera perspectives. Experimental results demonstrate our method's ability to generate highly realistic and controllable pedestrian videos, allowing for precise manipulation of pedestrian trajectories, actions, and appearances while maintaining consistent multi-view representations. This capability offers significant potential for enhancing the realism and diversity of autonomous driving simulation environments, leading to more robust and reliable autonomous vehicles."
http://arxiv.org/abs/2508.00287v1,Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning,"Driver drowsiness is a significant contributor to traffic accidents, motivating the development of robust and reliable detection systems. However, these systems often rely on sensitive driver data, raising privacy concerns. This paper addresses the challenge of driver drowsiness detection while preserving driver privacy through a novel combination of spatial self-attention and federated learning. Our approach employs a lightweight convolutional neural network enhanced with a spatial self-attention mechanism to extract relevant features from facial regions, focusing on key indicators of drowsiness such as eye closure and head pose. This model is then trained using federated learning, enabling collaborative learning across multiple vehicles without direct access to individual driver data. Experimental results on benchmark drowsiness datasets demonstrate that our privacy-preserving method achieves comparable accuracy to centralized training approaches, while significantly reducing the risk of privacy breaches. This work offers a practical and effective solution for real-world deployment of driver drowsiness detection systems that prioritizes user privacy."
http://arxiv.org/abs/2508.00169v1,Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs,"Single-Photon LiDAR (SPL) offers significant advantages in range and power efficiency compared to traditional multi-beam LiDAR, making it attractive for autonomous navigation and mapping. However, SPL data is inherently noisy, producing probabilistic point clouds with significant variations in point density and accuracy, which poses a significant challenge for robust 3D object detection. This paper addresses the problem of achieving high-performance 3D object detection directly from probabilistic point clouds generated by SPL systems. We propose a novel detection framework that explicitly models the probabilistic nature of SPL point clouds within a voxel-based architecture. Our approach incorporates a probabilistic voxelization layer that aggregates point probabilities and learns voxel-wise uncertainty estimates, followed by a 3D convolutional network trained to predict object bounding boxes and associated confidence scores. Experimental results on simulated and real-world SPL datasets demonstrate that our method significantly outperforms existing state-of-the-art 3D object detection algorithms designed for traditional LiDAR data, achieving higher detection accuracy and robustness, particularly in challenging low-light and long-range scenarios. This work demonstrates the feasibility of directly processing probabilistic point clouds from SPL LiDARs, paving the way for more efficient and reliable perception systems in resource-constrained environments."
http://arxiv.org/abs/2507.23611v1,LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora,"Infostealer malware poses a significant threat to cybersecurity, with Aurora being a prominent example known for its credential-harvesting capabilities. Identifying infection vectors from post-infection artifacts remains challenging, particularly when relying solely on visual evidence like screenshots. This paper addresses the problem of automatically identifying Aurora's infection vectors from screenshots, a task traditionally requiring manual reverse engineering and security analysis. We propose a novel approach leveraging Large Language Models (LLMs) to analyze visual cues within screenshots of infected systems. Our method combines image captioning to generate descriptive text of the screenshot content with LLM-based reasoning to infer the likely infection vector based on observed elements like browser interfaces, email clients, and specific error messages. We evaluated our method on a dataset of Aurora-infected system screenshots, achieving high accuracy in identifying common infection vectors such as malicious email attachments and drive-by downloads. This LLM-driven approach offers a scalable and efficient method for rapidly identifying infostealer infection vectors from visual data, improving incident response capabilities and threat intelligence gathering."
http://arxiv.org/abs/2507.23601v1,Mamba-based Efficient Spatio-Frequency Motion Perception for Video Camouflaged Object Detection,"Camouflaged object detection (COD) in video presents a significant challenge due to the dynamic nature of camouflage and the need to discern subtle motion cues. Existing video COD methods often struggle with computational efficiency and capturing long-range spatio-temporal dependencies crucial for distinguishing camouflaged objects from their surroundings. To address these limitations, we propose a novel Mamba-based Efficient Spatio-Frequency Motion Perception (MSFMP) network. Our method leverages the selective state space model (S6) of Mamba to efficiently model long-range spatio-temporal relationships within and across video frames. Furthermore, we introduce a frequency-domain motion enhancement module that decomposes video frames into frequency components, allowing the network to better capture subtle motion patterns indicative of camouflaged objects. Extensive experiments on benchmark video COD datasets demonstrate that MSFMP achieves state-of-the-art performance while maintaining significantly improved computational efficiency compared to existing transformer-based approaches. This work offers a promising avenue for deploying video COD systems in resource-constrained environments and advances the field towards more efficient and accurate motion-aware perception."
http://arxiv.org/abs/2507.23567v1,3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection,"Monocular 3D object detection is crucial for autonomous systems, yet its performance lags behind its stereo and LiDAR-based counterparts, especially when encountering novel, unseen object categories in open-set scenarios. This paper addresses the challenge of detecting and localizing both known and unknown 3D objects from a single RGB image, where existing methods often struggle with accurate 3D pose estimation and distinguishing between known and novel classes. We introduce 3D-MOOD, a Monocular Open-set Object Detector that leverages a novel 2D-to-3D lifting strategy combined with a class-agnostic 3D proposal generation network. Specifically, 3D-MOOD employs a learned perspective-aware lifting module to project 2D features into a pseudo-3D space, facilitating more robust 3D reasoning. This lifted 3D representation is then processed by a transformer-based network to generate and refine 3D object proposals, followed by a confidence scoring mechanism to identify potential novel objects. Our experiments on the challenging KITTI and nuScenes datasets demonstrate that 3D-MOOD significantly improves open-set 3D object detection performance, achieving state-of-the-art results in both known object detection and novel object discovery compared to existing monocular methods. This work advances the robustness and generalization capabilities of monocular 3D object detection systems in real-world, open-world environments."
http://arxiv.org/abs/2507.23543v1,ART: Adaptive Relation Tuning for Generalized Relation Prediction,"Relation prediction, the task of inferring relationships between entities in images, is crucial for scene understanding. Existing methods often struggle to generalize to novel relationships due to biases towards frequently observed relations in training data. This paper introduces Adaptive Relation Tuning (ART), a novel framework designed to improve generalization in relation prediction by dynamically adjusting feature representations based on the specific relation being predicted. ART employs a relation-aware attention mechanism that recalibrates visual features based on learned relation embeddings. Furthermore, ART incorporates a contrastive loss that encourages the model to learn distinct representations for different relations, even those with subtle semantic differences. Experiments on the Visual Genome dataset and a zero-shot relation prediction setting demonstrate that ART significantly outperforms existing state-of-the-art methods, achieving substantial gains in both seen and unseen relation prediction accuracy. These results highlight the importance of adaptive feature tuning and relation-aware learning for robust and generalizable relation prediction."
http://arxiv.org/abs/2507.23478v1,3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding,"Vision-Language Models (VLMs) have demonstrated remarkable progress in understanding 2D images, but their application to 3D scenes remains challenging due to the inherent complexity of 3D data and the difficulty in reasoning about spatial relationships. Existing 3D VLMs often struggle with complex reasoning tasks that require integrating information across different modalities and understanding intricate spatial arrangements. To address this, we introduce 3D-R1, a novel framework designed to enhance reasoning capabilities in 3D VLMs for unified scene understanding. 3D-R1 incorporates a Reasoning-enhanced Region Representation module that leverages graph neural networks to capture relationships between 3D regions, and a Cross-Modal Reasoning module that explicitly models interactions between visual and textual information through iterative attention mechanisms. Experiments on challenging 3D scene understanding benchmarks, including ScanQA and 3D Question Answering, demonstrate that 3D-R1 significantly outperforms existing state-of-the-art methods, achieving improvements of up to 8% in overall accuracy. These results highlight the effectiveness of 3D-R1 in enabling more sophisticated reasoning and paving the way for more comprehensive 3D scene understanding."
http://arxiv.org/abs/2507.23416v1,Honey Adulteration Detection using Hyperspectral Imaging and Machine Learning,"Honey adulteration, the practice of adding cheaper sweeteners to increase volume and reduce production costs, is a significant concern for consumers and the honey industry. This paper addresses the challenge of accurately and efficiently detecting honey adulteration using hyperspectral imaging (HSI) coupled with machine learning techniques. We propose a novel approach involving the acquisition of hyperspectral images of honey samples across the visible and near-infrared (VNIR) spectrum (400-1000 nm), followed by spectral feature extraction and selection using techniques like Principal Component Analysis (PCA) and Variable Importance in Projection (VIP). The selected features are then fed into various machine learning classifiers, including Support Vector Machines (SVM), Random Forest (RF), and Artificial Neural Networks (ANN), to discriminate between pure and adulterated honey samples. Our results demonstrate that the proposed HSI-based method, particularly when combined with Random Forest, achieves high classification accuracy (above 95%) in detecting adulteration with common sweeteners such as corn syrup and sucrose. This research provides a rapid, non-destructive, and accurate method for honey authentication, contributing to the fight against food fraud and ensuring the quality and integrity of honey products."
http://arxiv.org/abs/2507.23411v1,Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories,"Out-of-distribution (OOD) detection is crucial for the safe and reliable deployment of medical image analysis systems, ensuring that models abstain from making predictions on unfamiliar data. However, effectively distinguishing between in-distribution (ID) and OOD medical images remains a significant challenge, particularly when OOD data exhibits subtle anomalies or originates from previously unseen imaging modalities. We introduce a novel approach to OOD detection in medical imaging based on analyzing the trajectories of diffusion models. Our method leverages the inherent generative capabilities of diffusion models to reconstruct input images and then analyzes the divergence between the initial input and intermediate states generated during the reverse diffusion process. Specifically, we quantify trajectory divergence using a learned discriminator that is trained to distinguish between ID and OOD diffusion trajectories. Experiments on multiple medical imaging datasets, including chest X-rays and brain MRIs, demonstrate that our method achieves state-of-the-art OOD detection performance, outperforming existing techniques based on confidence scores and reconstruction errors. This improved OOD detection capability significantly enhances the reliability and trustworthiness of medical image analysis systems in real-world clinical settings."
http://arxiv.org/abs/2507.23331v1,Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision,"Traffic sign perception is crucial for autonomous driving, demanding robust performance across diverse environmental conditions. However, existing methods often struggle with challenges like occlusion, varying illumination, and adversarial attacks, hindering reliable sign recognition. To address these limitations, we propose a novel contrastive learning-driven multi-modal fusion approach for traffic sign perception, leveraging both visual and textual modalities. Our method employs a dual-branch architecture: a vision branch using a convolutional neural network to extract visual features and a text branch utilizing a transformer-based model to encode textual descriptions of traffic signs. A contrastive learning objective is then applied to align and fuse these features in a shared embedding space, encouraging the model to learn representations invariant to modality-specific noise. Experimental results on benchmark datasets, including the German Traffic Sign Recognition Benchmark (GTSRB) and a newly created multi-modal traffic sign dataset, demonstrate significant improvements in recognition accuracy, particularly under adverse conditions, surpassing state-of-the-art single-modal and early fusion methods. This highlights the potential of contrastive learning and multi-modal fusion to enhance the robustness and reliability of traffic sign perception systems for autonomous driving."
http://arxiv.org/abs/2507.23325v1,FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models,"Lane segment topology reasoning is crucial for autonomous driving, enabling safe and efficient navigation. Existing methods often struggle to balance accuracy and real-time performance, particularly in complex scenarios with occlusions and varying lane markings. This paper introduces FASTopoWM, a novel approach for fast and accurate lane segment topology reasoning leveraging a latent world model. FASTopoWM employs a fast perception module to extract preliminary lane segment proposals, followed by a slow reasoning module that refines these proposals and infers the underlying topology using a learned latent space representation of the driving environment. This latent space, trained with a contrastive learning objective, captures long-range dependencies and contextual information, enabling robust topology reasoning even with incomplete or noisy observations. Experiments on challenging real-world datasets demonstrate that FASTopoWM achieves state-of-the-art performance in lane topology accuracy while maintaining real-time processing speeds. The proposed method offers a significant advancement in robust and efficient lane understanding for autonomous driving systems."
http://arxiv.org/abs/2507.23307v1,ST-SAM: SAM-Driven Self-Training Framework for Semi-Supervised Camouflaged Object Detection,"Camouflaged object detection (COD) aims to identify objects that are visually similar to their surrounding environment, posing a significant challenge due to the inherent ambiguity in distinguishing object boundaries. The scarcity of labeled data further exacerbates this problem. To address these issues, we propose ST-SAM, a novel self-training framework for semi-supervised COD, leveraging the Segment Anything Model (SAM) for robust pseudo-label generation. ST-SAM iteratively refines COD model performance by first training on a small labeled dataset and then using the model to predict pseudo-labels on unlabeled data. These pseudo-labels are then refined using SAM, which acts as a powerful prior to correct inconsistencies and improve boundary accuracy. We introduce a novel confidence-aware filtering mechanism that leverages both the COD model's prediction confidence and SAM's segmentation confidence to selectively incorporate high-quality pseudo-labels into the training set. Experimental results on several benchmark COD datasets demonstrate that ST-SAM significantly outperforms existing semi-supervised COD methods, achieving state-of-the-art performance with limited labeled data. This work provides a practical and effective approach to improve COD performance in data-scarce scenarios by harnessing the power of foundation models."
http://arxiv.org/abs/2507.23295v1,LED Benchmark: Diagnosing Structural Layout Errors for Document Layout Analysis,"Document Layout Analysis (DLA) is a crucial step in document understanding, enabling downstream tasks like information extraction and search. Existing DLA benchmarks primarily focus on overall accuracy, often overlooking the nuanced structural layout errors that significantly impact usability in real-world applications. This paper introduces the LED Benchmark, a novel evaluation paradigm designed to specifically diagnose and quantify structural layout errors made by DLA systems. Our benchmark comprises a diverse set of document images annotated with detailed structural hierarchies, including reading order, logical regions, and their relationships. We propose a suite of metrics tailored to assess the precision and recall of hierarchical structure prediction, focusing on common errors like region splitting, merging, and incorrect ordering. Experimental results on state-of-the-art DLA models reveal significant performance variations across different structural error types, highlighting the limitations of existing methods in accurately capturing complex document layouts. The LED Benchmark provides a valuable tool for researchers to develop and evaluate DLA systems with a greater emphasis on structural correctness, leading to improved document understanding and usability."
http://arxiv.org/abs/2507.23226v1,"Toward Safe, Trustworthy and Realistic Augmented Reality User Experience","Augmented Reality (AR) promises to seamlessly blend virtual content with the real world, offering transformative user experiences across various domains. However, realizing the full potential of AR hinges on ensuring user safety, fostering trust in the system, and generating realistic augmentations that are perceptually consistent with the environment. This paper addresses the critical need for a holistic framework that tackles these intertwined challenges. We propose a novel approach that combines semantic scene understanding with uncertainty-aware rendering and a proactive safety mechanism. Our system first leverages deep learning to infer a comprehensive semantic map of the environment, including identifying potential hazards and occlusions. This semantic information is then integrated into a differentiable rendering pipeline that explicitly models uncertainty in both scene geometry and object pose, allowing for the generation of more robust and realistic AR augmentations. Furthermore, a safety module proactively monitors user proximity to identified hazards and dynamically adjusts the AR experience to mitigate potential risks. Experimental results demonstrate that our approach significantly improves the realism of AR augmentations, reduces the occurrence of visual artifacts, and effectively prevents simulated collisions, thereby enhancing both user trust and safety. This work paves the way for the development of more reliable and user-friendly AR applications."
http://arxiv.org/abs/2507.23002v1,Noise-Coded Illumination for Forensic and Photometric Video Analysis,"Forensic video analysis and photometric stereo techniques often struggle with uncontrolled illumination conditions, leading to inaccurate reconstructions and hindering reliable evidence extraction. This paper addresses the challenge of capturing reliable photometric information from video sequences acquired under unknown and potentially varying lighting environments. We propose a novel noise-coded illumination (NCI) approach, where a temporally varying pseudo-random binary pattern is projected onto the scene during video acquisition. By analyzing the temporal signal at each pixel and deconvolving the projected noise pattern, we can estimate per-pixel surface normals and albedo even in the presence of significant ambient light and shadows. Our experiments on synthetic and real-world video sequences demonstrate that NCI significantly improves the accuracy of surface normal estimation and 3D reconstruction compared to traditional photometric stereo methods under uncontrolled illumination. This technique provides a robust and practical solution for enhancing forensic video analysis and enabling reliable photometric reconstruction in challenging environments."
http://arxiv.org/abs/2507.22886v2,Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation,"Referring audio-visual segmentation (RAVS) aims to segment a target object in a video based on a free-form language description and a corresponding sound event. Current RAVS methods primarily focus on unimodal language expressions, limiting their ability to handle complex real-world scenarios where descriptions may incorporate other modalities like visual cues or acoustic properties. This paper addresses the challenge of extending RAVS to handle *omnimodal* expressions, encompassing language, visual, and acoustic references. We propose a novel Omnimodal Referring Audio-Visual Segmentation Network (ORAVSN) that leverages cross-modal attention mechanisms to fuse information from all three modalities. ORAVSN incorporates a visual grounding module to identify visually salient objects mentioned in the expression and an acoustic grounding module to attend to relevant sound events. Extensive experiments on a newly constructed RAVS benchmark with omnimodal annotations demonstrate that ORAVSN significantly outperforms existing unimodal RAVS methods, achieving a relative improvement of 8.5% in IoU. This work paves the way for more robust and versatile audio-visual perception systems that can understand and act upon richer, more human-like instructions."
http://arxiv.org/abs/2507.22827v1,ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents,"Visual-to-code generation holds immense potential for automating front-end development, but current approaches struggle with the complexity and nuances of real-world user interfaces. This paper addresses the challenge of generating accurate and maintainable code from visual representations of web and mobile applications. We introduce ScreenCoder, a novel framework employing modular multimodal agents that decompose the visual-to-code task into specialized sub-problems. ScreenCoder leverages a hierarchical architecture comprising a Layout Agent for structural analysis, a Content Agent for text and icon recognition, and a Style Agent for CSS attribute extraction, each finetuned on relevant datasets. These agents collaboratively inform a Code Generation Module, which synthesizes the final code using a rule-based system enhanced with learned code patterns. Experimental results on a diverse benchmark of UI designs demonstrate that ScreenCoder significantly outperforms existing state-of-the-art models in terms of code accuracy, structural fidelity, and execution success rate, achieving a relative improvement of 15% in end-to-end code generation accuracy. ScreenCoder represents a significant step towards democratizing front-end development and enabling automated UI creation and maintenance."
http://arxiv.org/abs/2507.22824v1,Bi-Level Optimization for Self-Supervised AI-Generated Face Detection,"The proliferation of AI-generated faces presents a significant challenge to the integrity of online information and the reliability of biometric systems. Detecting these synthetic faces is crucial, yet traditional supervised methods struggle due to the evolving nature of generation techniques and the scarcity of labeled real and fake face data. This paper addresses the problem of training robust AI-generated face detectors without relying on extensive manual annotations. We propose a novel bi-level optimization framework for self-supervised learning, where the inner level optimizes a proxy task of distinguishing between different augmentations of unlabeled face images, and the outer level optimizes the performance of a downstream AI-generated face detection task using the learned representations. Specifically, we use a contrastive loss in the inner loop to learn robust feature embeddings and then fine-tune a classifier on a small, potentially noisy, set of AI-generated faces in the outer loop. Experimental results on a diverse set of datasets demonstrate that our method significantly outperforms existing self-supervised and transfer learning approaches, achieving state-of-the-art performance in detecting AI-generated faces with minimal labeled data. This approach offers a practical solution for building robust and adaptable AI-generated face detectors, mitigating the risks associated with synthetic identity manipulation."
http://arxiv.org/abs/2507.22781v1,HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training,"Audio-visual deepfakes pose a significant threat to information integrity due to their increasingly realistic and deceptive nature. Existing deepfake detection methods often struggle to effectively capture the complex interdependencies between audio and visual modalities, hindering their ability to generalize across diverse manipulation techniques. To address this limitation, we propose HOLA: a novel deepfake detection framework that leverages Hierarchical cOntextual Aggregations and efficient pre-training. HOLA employs a hierarchical attention mechanism to capture both local and global contextual information within and across audio and visual streams, facilitating a more comprehensive understanding of cross-modal inconsistencies. Furthermore, we introduce a contrastive pre-training strategy that leverages readily available self-supervised signals to improve the model's robustness and generalization capabilities. Extensive experiments on benchmark datasets demonstrate that HOLA significantly outperforms state-of-the-art methods, achieving substantial improvements in both detection accuracy and cross-dataset generalization. Our work offers a robust and efficient solution for detecting sophisticated audio-visual deepfakes, contributing towards a safer and more trustworthy information ecosystem."
http://arxiv.org/abs/2508.03732v1,What is Beneath Misogyny: Misogynous Memes Classification and Explanation,"Misogyny, the dislike of or ingrained prejudice against women, manifests insidiously online, particularly through memes that normalize and propagate harmful stereotypes. Identifying and understanding the nuances of misogynous memes is crucial for mitigating their negative impact on society. However, existing methods often struggle to capture the subtle and multimodal nature of misogyny expressed through image and text combinations. This paper introduces a novel approach for classifying misogynous memes that leverages a multimodal transformer architecture, incorporating both visual and textual information, enhanced with attention mechanisms to highlight salient features contributing to misogynistic content. Furthermore, we propose an explanation module that identifies the specific image regions and text segments most influential in the classification decision, providing insights into the underlying reasoning. Our experiments demonstrate that the proposed method achieves state-of-the-art performance in misogynous meme classification, surpassing existing baselines by a significant margin. The explanation module provides valuable interpretations, revealing the complex interplay between visual and textual cues in conveying misogynistic messages. This work contributes to a deeper understanding of online misogyny and offers a valuable tool for automated detection and analysis, ultimately enabling proactive interventions to combat its spread."
http://arxiv.org/abs/2507.22692v1,Zero-Shot Image Anomaly Detection Using Generative Foundation Models,"Image anomaly detection aims to identify deviations from normal patterns without prior knowledge of anomalous examples, a challenging task particularly in scenarios where labeled anomalous data is scarce or unavailable. Existing methods often rely on hand-crafted features or require extensive training on normal data, limiting their generalization capabilities. This paper addresses the problem of zero-shot image anomaly detection by leveraging the rich semantic understanding and generative capabilities of pre-trained generative foundation models. Our approach utilizes a masked image reconstruction framework, where a portion of the input image is masked and the foundation model is prompted to reconstruct the missing region. We hypothesize that anomalies, being unseen during the model's pre-training, will result in less accurate reconstructions compared to normal regions. We then introduce an anomaly scoring mechanism based on the reconstruction error and semantic discrepancy between the original and reconstructed images, calculated using features extracted from a contrastively trained vision-language model. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art zero-shot anomaly detection performance, surpassing existing unsupervised and reconstruction-based approaches. The proposed method offers a robust and generalizable solution for anomaly detection, unlocking the potential of foundation models for real-world applications requiring zero-shot adaptation."
http://arxiv.org/abs/2507.22685v1,Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing,"Leaf wetness duration (LWD) is a crucial microclimatic parameter influencing plant disease development and irrigation management. Existing leaf wetness sensors often rely on single modalities, limiting their robustness and accuracy under diverse environmental conditions. This paper addresses the lack of a comprehensive, multi-modal benchmark for evaluating leaf wetness sensing algorithms. We introduce Hydra-Bench, a novel dataset comprising synchronized visual (RGB, thermal, hyperspectral), environmental (temperature, humidity, rainfall), and electrical impedance data collected from multiple leaf types under varying natural conditions. We provide baseline performance evaluations for several classical and deep learning models applied to each modality and their fusions, demonstrating the potential for improved LWD estimation through multi-modal sensing. Experiments reveal that fusing thermal and RGB imagery yields significant improvements in LWD classification accuracy compared to single-modality approaches, particularly under challenging lighting conditions. Hydra-Bench facilitates the development and validation of robust and accurate leaf wetness sensing systems, ultimately contributing to more efficient and sustainable agricultural practices."
http://arxiv.org/abs/2507.22650v1,"SpectraSentinel: LightWeight Dual-Stream Real-Time Drone Detection, Tracking and Payload Identification","Real-time drone detection and tracking are crucial for maintaining airspace security and mitigating potential threats. However, current methods often struggle to balance accuracy, computational efficiency, and the ability to identify drone payloads in real-time, particularly when deployed on resource-constrained edge devices. To address these limitations, we present SpectraSentinel, a lightweight dual-stream deep learning architecture designed for real-time drone detection, tracking, and payload identification. SpectraSentinel employs a computationally efficient object detection stream coupled with a spectral analysis stream that leverages onboard drone audio recordings. The object detection stream provides bounding box coordinates and confidence scores for drone localization, while the spectral analysis stream classifies the drone's acoustic signature to facilitate payload identification, such as distinguishing between drones carrying cameras or speakers. Experimental results demonstrate that SpectraSentinel achieves state-of-the-art detection accuracy with a significantly reduced computational footprint compared to existing methods, enabling real-time performance on embedded platforms and accurate payload identification. This novel approach facilitates rapid response to drone threats and enhances situational awareness in dynamic environments."
http://arxiv.org/abs/2507.22617v1,Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions,"The proliferation of diffusion models has enabled the creation of photorealistic images, raising concerns about their potential misuse for generating and disseminating hateful content. Current content moderation techniques primarily focus on text-based prompts and struggle to effectively identify hateful content embedded within generated images, especially when subtle visual cues are employed. This paper addresses the critical problem of detecting and mitigating AI-generated hateful illusions  images that convey hateful messages through nuanced visual elements rather than explicit textual prompts. We propose a novel framework that combines visual sentiment analysis with object co-occurrence analysis to identify subtle hateful associations within images. Specifically, we leverage pre-trained vision-language models to extract semantic relationships between objects and contextualize them with sentiment scores derived from hateful memes datasets. Our experiments demonstrate that our approach significantly outperforms existing content moderation systems in detecting hateful illusions, achieving a 20% improvement in F1-score on a newly curated dataset of AI-generated hateful images. This work highlights the urgent need for advanced content moderation strategies capable of discerning subtle visual cues to prevent the spread of AI-generated hate."
http://arxiv.org/abs/2507.22601v1,Robust Deepfake Detection for Electronic Know Your Customer Systems Using Registered Images,"Electronic Know Your Customer (eKYC) systems are increasingly vulnerable to sophisticated deepfake attacks, posing a significant threat to identity verification processes. While existing deepfake detection methods show promise, they often struggle with generalization across diverse datasets and robustness against adversarial manipulations, especially within the constraints of real-world eKYC applications. This paper addresses the challenge of creating a robust deepfake detection system for eKYC by leveraging the availability of registered images  images officially associated with an individual's identity. We propose a novel deep learning architecture, the Registered Image Verification Network (RIVaNet), which incorporates a Siamese network to compare the liveness check image with the registered image, followed by a fusion module that integrates the similarity score with features extracted from a deepfake detection network. RIVaNet is trained using a combination of real and synthetically generated deepfake videos, augmented with adversarial examples. Experimental results on a newly curated eKYC deepfake dataset demonstrate that RIVaNet outperforms state-of-the-art deepfake detection methods, achieving a significant improvement in both accuracy and robustness against adversarial attacks. This work provides a practical and effective solution for enhancing the security of eKYC systems against evolving deepfake threats."
http://arxiv.org/abs/2507.22576v1,COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP,"Out-of-distribution (OOD) detection is crucial for deploying computer vision models in open-world scenarios where unexpected inputs are common. While Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable zero-shot transfer capabilities, its OOD detection performance remains an open challenge, particularly when distinguishing between semantically similar but distributionally shifted data. This paper addresses the problem of robust OOD detection in CLIP by leveraging an ensemble-based approach. Our method, COOkeD (CLIP OOD Ensemble Detector), constructs an ensemble of CLIP models, each fine-tuned on diverse, automatically generated proxy datasets designed to introduce controlled variations in style and context. We then combine the prediction confidence scores from each ensemble member using a novel aggregation scheme that incorporates both the mean and variance, allowing us to effectively discriminate between in-distribution and OOD samples. Experiments on a diverse set of OOD benchmarks, including near-OOD scenarios, demonstrate that COOkeD significantly outperforms existing state-of-the-art OOD detection methods for CLIP, achieving an average improvement of 5-10% in AUROC. This highlights the efficacy of our ensemble-based approach for enhancing the reliability and robustness of zero-shot CLIP models in open-world environments."
http://arxiv.org/abs/2507.22498v2,Robust Adverse Weather Removal via Spectral-based Spatial Grouping,"Adverse weather conditions, such as rain, snow, and fog, significantly degrade the performance of computer vision systems. Removing these atmospheric degradations from images remains a challenging task, particularly in complex real-world scenarios. This paper addresses the problem of robust and effective adverse weather removal by exploiting spectral properties and spatial relationships within images. We propose a novel spectral-based spatial grouping method that decomposes the image into distinct layers representing the scene radiance and the adverse weather effects. Specifically, we leverage spectral clustering to group spatially connected pixels with similar spectral characteristics, followed by an adaptive fusion strategy that selectively combines information from different spectral groups to reconstruct a clean image. Our approach demonstrates superior performance compared to state-of-the-art methods on both synthetic and real-world datasets, exhibiting improved visual quality and quantitative metrics across various adverse weather types and intensities. This work offers a promising direction for enhancing the robustness of computer vision applications operating in challenging environmental conditions."
http://arxiv.org/abs/2507.22481v1,Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework,"Bitstream corruption poses a significant challenge to video transmission and storage, leading to visual artifacts and compromised viewing experiences. Existing video recovery methods often rely on explicit knowledge of the corruption type and severity, limiting their applicability in real-world scenarios where such information is unavailable. This paper introduces a novel framework for blind bitstream-corrupted video recovery leveraging the power of visual foundation models. Our approach employs a two-stage process: first, a pre-trained visual foundation model, specifically DINOv2, extracts robust and semantically meaningful features from the corrupted video frames. Second, a lightweight transformer-based network is trained to map these features to clean video frames, effectively learning a corruption-agnostic restoration function. Experimental results on benchmark datasets demonstrate that our framework achieves state-of-the-art performance in blind video recovery, significantly outperforming existing methods in terms of both quantitative metrics (PSNR, SSIM) and perceptual quality. This research offers a promising direction for robust video recovery in challenging and unpredictable real-world conditions, moving towards more reliable and user-friendly video systems."
http://arxiv.org/abs/2507.22469v1,Visual Language Models as Zero-Shot Deepfake Detectors,"The proliferation of deepfakes poses a significant threat to information integrity, necessitating robust and generalizable detection methods. Existing deepfake detectors often rely on supervised learning, limiting their effectiveness against unseen manipulation techniques. This paper investigates the potential of Visual Language Models (VLMs) as zero-shot deepfake detectors, leveraging their pre-trained knowledge of visual and textual relationships to identify inconsistencies indicative of manipulated content. We propose a novel approach that prompts VLMs with questions designed to elicit responses revealing deepfake artifacts, such as inconsistencies in identity, scene context, or object interactions. By analyzing the VLM's generated text and confidence scores, we can identify potential deepfakes without requiring any task-specific training data. Experiments on a diverse set of deepfake datasets demonstrate that our method achieves competitive performance compared to established supervised baselines in zero-shot settings, even surpassing them in certain cases. This work highlights the promise of VLMs for building adaptable and generalizable deepfake detection systems, reducing the reliance on labeled data and improving resilience against evolving manipulation strategies."
http://arxiv.org/abs/2507.22465v1,Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation,"Unsupervised Video Object Segmentation (UVOS) aims to identify and segment salient objects in videos without requiring manual annotations, a challenging task due to the lack of explicit object supervision. Existing memory-based approaches often rely on deep features and overlook the potential of shallow features for capturing low-level cues crucial for distinguishing foreground objects. To address this, we propose a novel Hierarchical Memory with Heterogeneous Interaction (HMHI) framework. HMHI constructs a hierarchical memory bank that stores both shallow and deep features, enabling multi-level object representation. Furthermore, we introduce a heterogeneous interaction module that adaptively fuses information from different levels of the hierarchy, leveraging the complementary strengths of shallow and deep features. Experiments on standard UVOS benchmarks, including DAVIS-2016, FBMS, and YouTube-Objects, demonstrate that HMHI significantly outperforms state-of-the-art methods, achieving substantial gains in segmentation accuracy and robustness, particularly in handling complex object appearances and motion patterns. This work highlights the importance of shallow features and heterogeneous feature interaction for robust and accurate unsupervised video object segmentation."
http://arxiv.org/abs/2508.03727v1,TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization,"Thermal Infrared (TIR) imaging plays a crucial role in various applications, including surveillance, medical diagnosis, and autonomous driving. However, TIR images are often corrupted by significant noise due to sensor limitations and environmental factors, hindering subsequent analysis. This paper addresses the challenge of effectively denoising TIR images, particularly in scenarios with high noise levels. We propose TIR-Diffusion, a novel diffusion-based denoising framework that leverages both latent and wavelet domain optimization. Specifically, we first map the noisy TIR image to a lower-dimensional latent space using a learned encoder. Then, we employ a diffusion model conditioned on wavelet coefficients to progressively remove noise in the latent representation, guided by a wavelet loss that preserves crucial edge and texture details. Finally, the denoised latent representation is decoded back to the image space. Experimental results demonstrate that TIR-Diffusion significantly outperforms state-of-the-art denoising methods, achieving superior Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) scores, particularly under high noise conditions. This research provides a powerful and effective approach for enhancing the quality of TIR images, enabling more reliable and accurate downstream tasks."
http://arxiv.org/abs/2508.04061v1,TNet: Terrace Convolutional Decoder Network for Remote Sensing Image Semantic Segmentation,"Semantic segmentation of remote sensing imagery is crucial for various applications, including urban planning, environmental monitoring, and disaster management. However, accurately delineating complex land cover classes with high spatial resolution remains a challenge due to the inherent scale variations and intricate contextual relationships within remote sensing scenes. To address this, we propose TNet, a novel Terrace Convolutional Decoder Network specifically designed for remote sensing image semantic segmentation. TNet introduces a Terrace Convolutional Module (TCM) within the decoder, which employs a series of progressively refined convolutional layers resembling a terraced landscape. This architecture allows the network to effectively capture multi-scale contextual information and gradually recover detailed spatial information lost during the encoding process. Furthermore, we incorporate a channel attention mechanism to selectively enhance important feature channels and suppress irrelevant ones. Experimental results on benchmark remote sensing datasets, including ISPRS Vaihingen and Potsdam, demonstrate that TNet achieves state-of-the-art performance, surpassing existing methods in terms of overall accuracy and intersection-over-union (IoU) scores. The proposed TNet offers a robust and effective solution for semantic segmentation of high-resolution remote sensing imagery, contributing to more accurate and reliable land cover mapping."
http://arxiv.org/abs/2508.03334v2,Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation,"Autoregressive models have shown promise in generating long videos, yet they often struggle with maintaining temporal coherence and exhibit limited parallelism due to their inherent sequential nature. This paper addresses the challenge of generating high-quality, temporally consistent, and long videos while mitigating the computational bottleneck of sequential autoregressive decoding. We introduce a novel ""Macro-from-Micro"" planning framework. First, a micro-level module predicts short, high-resolution video segments conditioned on preceding frames. Then, a macro-level planner aggregates these micro-level segments into a long-term plan represented by low-resolution keyframes, enforcing global coherence. Finally, the micro-level module refines and interpolates the planned keyframes to generate the final high-resolution video. Experiments on diverse video datasets demonstrate that our approach achieves state-of-the-art video quality and temporal consistency, as measured by Frchet Video Distance (FVD) and user studies, while significantly improving generation speed through parallelized micro-segment generation. This framework offers a scalable and effective solution for high-quality long video generation, opening avenues for creative applications and reducing the computational cost associated with autoregressive models."
http://arxiv.org/abs/2508.03227v1,Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing,"Lifting 2D image segmentations to 3D point clouds is crucial for generating dense 3D scene understanding. However, existing methods often suffer from inconsistencies in the lifted segmentations due to perspective distortions, occlusions, and the inherent ambiguity of associating 2D regions with 3D points. To address this, we introduce Trace3D, a novel approach that leverages Gaussian instance tracing to achieve consistent segmentation lifting. Trace3D propagates instance-aware uncertainty from 2D detections to 3D space by modeling each instance as a Gaussian distribution and tracing its trajectory through multiple camera views. This allows us to probabilistically associate 3D points with 2D instances, mitigating the effects of noisy 2D segmentations and improving the coherence of the lifted 3D segmentations. Experiments on the ScanNet and Matterport3D datasets demonstrate that Trace3D significantly outperforms state-of-the-art methods in terms of segmentation consistency and accuracy, particularly in challenging scenarios with significant occlusions. Our method provides a robust and accurate framework for lifting 2D segmentations to 3D, enabling a wide range of applications in scene understanding and 3D reconstruction."
http://arxiv.org/abs/2508.03073v1,Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution,"Multimodal medical image super-resolution (SR) is crucial for enhancing diagnostic accuracy and enabling precise analysis, particularly when high-resolution images are unavailable. However, existing methods often struggle to generate realistic and diverse high-resolution (HR) images at arbitrary scales, especially when guided by diverse modalities. To address this, we introduce Nexus-INR, a novel knowledge-guided Implicit Neural Representation (INR) framework for arbitrary-scale multimodal medical image SR. Nexus-INR leverages a hierarchical hypernetwork to modulate the INR, conditioning it on both low-resolution (LR) image features and cross-modal knowledge extracted from auxiliary modalities using a contrastive learning objective. This allows the INR to implicitly represent the continuous HR image space, enabling arbitrary scaling and facilitating the integration of diverse knowledge. Our experiments on benchmark datasets demonstrate that Nexus-INR significantly outperforms state-of-the-art SR methods in terms of quantitative metrics (PSNR, SSIM) and perceptual quality, while also showcasing its ability to generate diverse and plausible HR images conditioned on different modality combinations. Nexus-INR offers a powerful and flexible solution for multimodal medical image SR, paving the way for improved clinical decision-making and downstream analysis."
http://arxiv.org/abs/2508.03069v1,SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation,"Accurate 3D medical image segmentation is crucial for computer-aided diagnosis and treatment planning. However, capturing both local and global spatial dependencies, alongside fine-grained details, remains a significant challenge. This paper introduces SSFMamba, a novel Symmetry-driven Spatial-Frequency Feature Fusion network built upon the Mamba architecture for improved 3D medical image segmentation. SSFMamba leverages a dual-branch structure, one operating in the spatial domain and the other in the frequency domain (utilizing Discrete Cosine Transform), to capture complementary information. A symmetry-driven fusion module then adaptively integrates these features, emphasizing relevant information while suppressing noise. We evaluate SSFMamba on multiple publicly available 3D medical image segmentation datasets, including the Medical Segmentation Decathlon (MSD) and the BraTS datasets, demonstrating superior performance compared to state-of-the-art convolutional and transformer-based methods, particularly in segmenting small and complex structures. SSFMamba achieves a significant boost in segmentation accuracy and robustness, offering a promising solution for precise and reliable 3D medical image analysis."
http://arxiv.org/abs/2508.03060v2,CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation,"Semantic segmentation benefits significantly from multi-modal data, yet current approaches often struggle with missing modalities or require specific modality combinations during training and inference. This paper addresses the challenge of modality-agnostic semantic segmentation, aiming to achieve robust performance regardless of the available input modalities. We introduce CHARM, a Collaborative Harmonization framework that leverages a shared latent space to bridge the representational gaps between modalities. CHARM employs modality-specific encoders to project inputs into this shared space, followed by a collaborative harmonization module that dynamically adjusts feature representations based on the presence or absence of each modality. A modality-agnostic decoder then maps the harmonized representation to a segmentation prediction. Extensive experiments on benchmark datasets demonstrate that CHARM achieves state-of-the-art performance in various modality configurations, including unseen combinations and missing modalities during inference, surpassing existing methods by a significant margin. This work provides a flexible and robust solution for semantic segmentation in real-world scenarios with varying and potentially incomplete multi-modal data."
http://arxiv.org/abs/2508.03055v1,Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation,"Face transformation techniques have seen significant advancements, yet often struggle with occlusions, leading to artifacts and unrealistic results. This paper addresses the challenge of robust face transformation in the presence of occlusions by introducing an uncertainty-guided face matting framework. Our method leverages a novel deep learning architecture that simultaneously predicts a face matte and an associated uncertainty map. This uncertainty map is then incorporated into a matting refinement network, guiding it to focus on regions with high uncertainty, typically around occlusion boundaries, and to effectively propagate information from confident regions. Furthermore, we utilize the refined matte to composite the transformed face onto the original background, minimizing artifacts caused by occlusions. Experimental results on both synthetic and real-world datasets demonstrate that our approach significantly improves the quality of face matting, leading to more realistic and robust face transformations compared to state-of-the-art methods, especially in scenarios with significant occlusions. Our uncertainty-guided matting framework offers a crucial step towards occlusion-aware face transformation, enabling more reliable and visually appealing results in diverse and challenging scenarios."
http://arxiv.org/abs/2508.03007v1,Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation,"Domain generalization (DG) in semantic segmentation aims to train a model on multiple source domains that can generalize well to unseen target domains. A key challenge lies in mitigating domain shift arising from variations in image style and content across domains. To address this, we propose a novel Multi-Granularity Feature Calibration method via Variance-based Feature Masking (VFM) for domain generalized semantic segmentation. VFM dynamically generates feature masks based on the variance of feature activations across different spatial locations and channels, effectively suppressing domain-specific features while preserving domain-invariant ones. Furthermore, we apply VFM at multiple granularities within the network, enabling the model to calibrate features at both coarse and fine levels. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art DG methods, achieving an average mIoU improvement of 3-5% on unseen target domains. This highlights the efficacy of our multi-granularity feature calibration strategy for robust domain generalization in semantic segmentation."
http://arxiv.org/abs/2508.02903v1,RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation,"Unsupervised anomaly segmentation aims to identify irregular regions in images without relying on labeled anomalous data, a crucial task in various industrial and medical applications. However, existing methods often struggle with complex textures and subtle anomalies, leading to inaccurate segmentation. This paper introduces RDDPM, a Robust Denoising Diffusion Probabilistic Model, for unsupervised anomaly segmentation. RDDPM leverages a diffusion model trained on normal data to learn a robust representation of the data distribution. At inference, anomalous regions are identified by comparing the reconstruction error between the original image and its reconstruction obtained by inverting the diffusion process. To enhance robustness, we introduce a novel noise-aware reconstruction loss that dynamically adjusts the influence of different noise levels during the reverse diffusion process, mitigating the impact of noisy features present in anomalous regions. Experimental results on benchmark datasets such as MVTec AD and BTAD demonstrate that RDDPM achieves state-of-the-art anomaly segmentation performance, outperforming existing methods, particularly in scenarios with complex backgrounds and subtle anomalies. RDDPM provides a powerful and robust framework for unsupervised anomaly segmentation, enabling effective defect detection in real-world applications."
http://arxiv.org/abs/2508.02844v1,RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation,"Medical image segmentation is crucial for computer-aided diagnosis and treatment planning. However, achieving accurate and detailed segmentation remains challenging due to the inherent complexity and variability of medical images. This paper addresses the problem of effectively capturing both global contextual information and fine-grained details for precise medical image segmentation. We propose RefineSeg, a novel dual coarse-to-fine learning framework. RefineSeg employs a coarse segmentation branch to capture global context and generate a preliminary segmentation map. Subsequently, a fine segmentation branch refines this initial map by focusing on local details and boundary information, guided by the coarse segmentation output. A key aspect of our approach is the incorporation of a feature fusion module that adaptively combines features from both branches, enabling effective knowledge transfer between coarse and fine levels. Experimental results on multiple medical image datasets demonstrate that RefineSeg achieves state-of-the-art segmentation performance, surpassing existing methods in terms of accuracy and boundary delineation. The proposed framework offers a promising approach to improve the precision and reliability of medical image segmentation for clinical applications."
http://arxiv.org/abs/2508.02557v1,RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation,"Accurate and automated 3D whole-heart segmentation is crucial for various clinical applications, including cardiac disease diagnosis and surgical planning. However, the variability in cardiac anatomy and image quality across different modalities poses a significant challenge to robust segmentation performance. This paper introduces RL-U$^2$Net, a novel dual-branch UNet architecture with reinforcement learning (RL)-assisted multimodal feature fusion to address this challenge. RL-U$^2$Net employs two parallel UNet branches to independently extract features from different modalities. A novel RL-based fusion module dynamically learns optimal weights for combining features from both branches at multiple scales, adapting to the specific characteristics of each input image. We evaluated RL-U$^2$Net on a multi-center, multi-vendor dataset of cardiac CT and MRI scans, demonstrating superior segmentation accuracy with a mean Dice score of 0.92 and Hausdorff distance of 4.1 mm, outperforming state-of-the-art methods. Our results highlight the effectiveness of RL-guided multimodal feature fusion for robust and accurate 3D whole-heart segmentation, paving the way for improved clinical workflows."
http://arxiv.org/abs/2508.02480v1,MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding,"Reconstructing visual experiences from human brain activity, particularly fMRI, is a long-standing challenge in neuroscience and computer vision. Existing methods primarily focus on reconstructing single, static images, neglecting the temporal dynamics and narrative structure inherent in video. We address the problem of reconstructing multi-shot video sequences from fMRI data, a significantly more complex task than single-image reconstruction. Our method, MindShot, leverages the representational power of pre-trained Large Language Models (LLMs) to bridge the gap between fMRI signals and high-level semantic video content. Specifically, MindShot first decodes fMRI activity into textual descriptions using an LLM, conditioned on a learned mapping between brain activity and semantic space. These descriptions are then used to guide a text-to-video diffusion model, generating a coherent multi-shot video sequence that aligns with the subject's perceived visual experience. Experimental results demonstrate that MindShot significantly improves the fidelity and coherence of reconstructed videos compared to existing methods, as evaluated by both quantitative metrics and qualitative human evaluations. This represents a significant step towards understanding and decoding the neural basis of dynamic visual processing and opens new avenues for brain-computer interfaces and cognitive neuroscience research."
http://arxiv.org/abs/2508.02464v1,SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models,"Vision foundation models, such as the Segment Anything Model (SAM), have demonstrated remarkable zero-shot segmentation capabilities. However, directly applying these models to intent-aware segmentation, where the desired segmentation aligns with a specific user intention or objective, often yields suboptimal results due to the lack of explicit preference alignment. This paper introduces SAMPO, a novel visual preference optimization framework for adapting vision foundation models to intent-aware segmentation tasks. SAMPO leverages a learned preference model to guide the segmentation process, iteratively refining the segmentation mask by incorporating visual feedback that reflects the desired intention. This is achieved through a reinforcement learning paradigm where the agent is the segmentation model and the reward is provided by the learned preference model, trained to distinguish between segmentations that align with the specified intent and those that do not. Experiments on diverse datasets demonstrate that SAMPO significantly improves the accuracy and consistency of intent-aware segmentation compared to existing methods, achieving state-of-the-art performance while requiring minimal task-specific fine-tuning. SAMPO provides a powerful and generalizable approach for aligning vision foundation models with specific user intentions, paving the way for more intuitive and effective human-AI interaction in segmentation tasks."
http://arxiv.org/abs/2508.02307v1,Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment,"Accurate assessment of preclinical disease risk is crucial for timely intervention and improved patient outcomes. Traditional risk prediction models often rely on isolated biomarkers, neglecting the complex interplay of physiological systems. This work addresses the challenge of integrating whole-body information from diverse imaging modalities for improved preclinical disease risk assessment in a competing risks framework. We propose a novel whole-body representation learning approach that leverages contrastive learning on longitudinal imaging data (MRI, CT, PET) to generate comprehensive, multimodal embeddings capturing individual health trajectories. These embeddings are then used within a competing risks model to predict the probabilities of developing distinct diseases, accounting for their interdependencies. Our experiments on a large preclinical dataset demonstrate that our approach significantly outperforms single-modality and biomarker-based methods in predicting the onset of various diseases, including cardiovascular and neurodegenerative conditions, achieving an average C-index improvement of 0.1 compared to the strongest baseline. This holistic approach to disease risk assessment has the potential to revolutionize early disease detection and personalized preventative care."
http://arxiv.org/abs/2508.02281v1,Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation,"Medical image segmentation is a crucial task for computer-aided diagnosis, but often suffers from limited labeled data. Pre-training on large, unrelated datasets has shown promise, yet often neglects the importance of fine-grained anatomical boundaries critical for precise segmentation. This work investigates whether explicitly incorporating edge information during pre-training enhances downstream medical image segmentation performance. We propose an edge-enhanced pre-training strategy that leverages both image appearance and edge maps during contrastive learning. Specifically, we train a self-supervised model to discriminate between image patches and their corresponding edge maps, forcing the model to learn representations that are sensitive to boundary information. Experiments on multiple medical image segmentation benchmarks demonstrate that our edge-enhanced pre-training consistently outperforms standard pre-training methods, achieving significant improvements in Dice score and boundary IoU, especially in scenarios with limited labeled data. These results highlight the importance of edge information for medical image segmentation and suggest that edge-enhanced pre-training is a valuable strategy for improving the accuracy and robustness of segmentation models."
http://arxiv.org/abs/2508.02265v1,Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation,"Ultrasound imaging is a cost-effective and readily accessible modality for various clinical diagnoses, yet its interpretation remains challenging due to speckle noise and limited contrast. This necessitates automated methods for accurate classification and segmentation, particularly in scenarios with limited labeled data. We address the problem of improving ultrasound image classification and segmentation performance in a semi-supervised setting. Our proposed method, Semi-Supervised Dual-Threshold Contrastive Learning (DTCL), leverages both labeled and unlabeled data to learn robust feature representations. DTCL employs a dual-threshold mechanism to selectively enforce contrastive learning objectives: a high threshold encourages similarity between augmented views of the same image, while a low threshold promotes dissimilarity between different images, mitigating the impact of noisy pseudo-labels. Experimental results on benchmark ultrasound datasets demonstrate that DTCL significantly outperforms existing semi-supervised learning techniques, achieving state-of-the-art classification accuracy and segmentation performance with limited labeled data. This approach offers a practical solution for improving the accuracy and efficiency of ultrasound image analysis in resource-constrained environments."
http://arxiv.org/abs/2508.02254v1,Semi-Supervised Semantic Segmentation via Derivative Label Propagation,"Semantic segmentation, assigning a class label to each pixel in an image, is a fundamental task in computer vision. However, training deep learning models for semantic segmentation typically requires large amounts of pixel-wise labeled data, which is expensive and time-consuming to acquire. This paper addresses the challenge of reducing the annotation burden by exploring semi-supervised semantic segmentation. We propose Derivative Label Propagation (DLP), a novel approach that leverages unlabeled data by propagating label information from labeled pixels to unlabeled pixels based on the derivative of the segmentation network's output. Specifically, DLP utilizes the gradient of the predicted segmentation map with respect to the input image to estimate the direction of semantic consistency and propagates labels along these directions. We further introduce a confidence-aware mechanism to mitigate the impact of noisy pseudo-labels generated from unlabeled data. Experimental results on benchmark datasets, including Cityscapes and PASCAL VOC 2012, demonstrate that DLP significantly outperforms existing semi-supervised semantic segmentation methods, achieving state-of-the-art performance with limited labeled data. This work offers a promising direction for efficient and accurate semantic segmentation by effectively utilizing unlabeled data."
http://arxiv.org/abs/2508.02172v1,GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting,"Learning effective 3D representations from multi-modal data is crucial for scene understanding and downstream tasks. Existing self-supervised methods often rely on contrastive learning or reconstruction objectives that can be computationally expensive and may not fully capture the intricacies of 3D geometry and cross-modal relationships. To address these limitations, we introduce GaussianCross, a novel cross-modal self-supervised learning framework leveraging 3D Gaussian Splatting (3D-GS) as an efficient and explicit 3D representation. Our method learns by predicting cross-modal correspondences between rendered 2D views and the underlying 3D-GS representation, guided by a novel Gaussian-aware cross-attention mechanism. This involves projecting Gaussian features into 2D, and then predicting corresponding image features. A key aspect is the utilization of Gaussian properties (e.g., covariance) to weight the attention mechanism, thereby focusing on geometrically consistent regions. We demonstrate that GaussianCross significantly improves the quality of learned 3D representations, achieving state-of-the-art performance on downstream tasks such as 3D object classification and semantic segmentation, outperforming existing self-supervised methods by a significant margin. This work establishes 3D Gaussian Splatting as a powerful and efficient primitive for self-supervised 3D representation learning."
http://arxiv.org/abs/2508.02149v1,AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation,"Reference Audio-Visual Segmentation (RAVS) aims to extract the visual region corresponding to a sound event described by a free-form textual query. Existing methods often struggle with complex queries requiring multi-step reasoning and exhibit limited generalization due to the lack of explicit structural awareness. We introduce AURORA, a novel framework for RAVS that augments understanding via structured reasoning and reinforcement learning. AURORA utilizes a graph-based scene representation to explicitly model relationships between objects and sound events. A reinforcement learning agent is then trained to navigate this graph, performing iterative refinement of the segmentation mask by attending to relevant audio-visual nodes based on the textual query. This structured reasoning process enables AURORA to effectively handle complex queries involving spatial relationships and contextual dependencies. Experiments on challenging RAVS datasets demonstrate that AURORA significantly outperforms existing state-of-the-art methods, achieving substantial gains in segmentation accuracy and generalization ability. The proposed framework offers a promising direction for improving audio-visual understanding by incorporating structured reasoning and learning."
http://arxiv.org/abs/2508.02146v1,ScrewSplat: An End-to-End Method for Articulated Object Recognition,"Articulated object recognition is a crucial capability for robots interacting with the physical world, enabling tasks such as tool manipulation and equipment operation. However, existing methods often struggle with complex articulation patterns and require explicit kinematic models. This paper introduces ScrewSplat, an end-to-end method for articulated object recognition that directly regresses pose and articulation parameters from point cloud data. ScrewSplat leverages a novel screw-parameterized implicit function to represent the articulated object's shape and motion. This representation allows for differentiable rendering of point clouds, enabling end-to-end training using a point cloud reconstruction loss. We demonstrate ScrewSplat on a variety of articulated objects, including tools and mechanical components, achieving state-of-the-art performance in pose and articulation parameter estimation, surpassing existing methods by a significant margin in handling complex kinematic chains and noisy data. The proposed method offers a robust and efficient solution for articulated object recognition, paving the way for more sophisticated robotic manipulation and interaction capabilities."
http://arxiv.org/abs/2508.03752v1,M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-Supervised Medical Image Segmentation,"Semi-supervised learning (SSL) offers a promising avenue to alleviate the reliance on extensive annotations in medical image segmentation. However, existing SSL methods often struggle to effectively leverage unlabeled data, particularly in maintaining consistency across diverse feature levels. This paper addresses the challenge of improving semi-supervised medical image segmentation by enforcing consistency between high-level semantic features and low-level spatial information. We propose a novel Mutual Mask Mix with High-Low Level Feature Consistency (M$^3$HL) framework. M$^3$HL leverages a mask mixing strategy to generate pseudo-labels for unlabeled data and enforces consistency between predictions generated from mixed and unmixed inputs. Crucially, we introduce a novel high-low level feature consistency loss that minimizes discrepancies between the deep semantic features and shallow spatial representations of these predictions. Experimental results on two publicly available medical image segmentation datasets demonstrate that M$^3$HL consistently outperforms state-of-the-art SSL methods, achieving significant improvements in segmentation accuracy, especially when limited labeled data is available. This highlights the effectiveness of our method in exploiting unlabeled data by enforcing consistency across different feature levels."
http://arxiv.org/abs/2508.01943v1,ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks,"Vision-language models (VLMs) have shown promise in embodied AI, yet effectively reasoning about long-horizon video observations remains a challenge, hindering their performance in complex tasks. Existing approaches often struggle to maintain a coherent understanding of the environment and past actions over extended sequences. To address this, we introduce ROVER: Recursive Reasoning Over Videos, a novel framework that leverages VLMs to perform recursive self-reflection and planning within embodied environments. ROVER iteratively processes video observations, generating concise summaries that capture salient information and inform future actions. These summaries are then recursively fed back into the VLM, allowing it to maintain a consistent and evolving understanding of the environment's state and task progress. We evaluate ROVER on a suite of embodied tasks, including object navigation and manipulation, demonstrating significant improvements in success rates and task efficiency compared to state-of-the-art methods. ROVER's ability to recursively reason over video observations provides a powerful mechanism for VLMs to tackle long-horizon embodied tasks, paving the way for more robust and intelligent robotic agents."
http://arxiv.org/abs/2508.01928v1,IAUNet: Instance-Aware U-Net,"Semantic segmentation classifies each pixel in an image but struggles to differentiate between distinct instances of the same object class. This limitation hinders applications requiring instance-level understanding. We address this problem by introducing IAUNet, an Instance-Aware U-Net architecture that integrates instance segmentation directly into the U-Net framework. IAUNet incorporates a novel Instance Discrimination Module (IDM) within the U-Net decoder, which learns instance-specific embeddings for each pixel. These embeddings are then clustered to produce instance segmentation masks, leveraging both semantic and instance-level features learned throughout the network. Experiments on the Cityscapes and COCO datasets demonstrate that IAUNet achieves state-of-the-art instance segmentation performance compared to existing U-Net-based approaches, showing significant improvements in both segmentation accuracy and instance discrimination. IAUNet provides a more comprehensive scene understanding by seamlessly integrating semantic and instance segmentation within a unified architecture."
http://arxiv.org/abs/2508.01831v1,Large Kernel MedNeXt for Breast Tumor Segmentation and Self-Normalizing Network for pCR Classification in Magnetic Resonance Images,"Breast cancer is a leading cause of mortality among women, and accurate image analysis is crucial for diagnosis and treatment planning. Precise segmentation of breast tumors in Magnetic Resonance Images (MRI) and accurate prediction of pathological complete response (pCR) after neoadjuvant chemotherapy remain challenging tasks due to tumor heterogeneity and subtle image variations. This paper proposes a novel framework combining a large kernel MedNeXt architecture for breast tumor segmentation and a self-normalizing neural network (SNN) for pCR classification. The MedNeXt network leverages large convolutional kernels to capture long-range dependencies and contextual information, enhancing segmentation accuracy. Subsequently, features extracted from the segmented tumor regions are fed into an SNN, designed to maintain stable activations and gradients, improving the robustness and performance of pCR classification. Experimental results on a publicly available dataset demonstrate that our approach achieves state-of-the-art segmentation performance with a Dice score of 0.87 and significantly improves pCR classification accuracy, reaching an AUC of 0.89, outperforming existing methods. This integrated framework offers a promising tool for improved breast cancer management and personalized treatment strategies."
http://arxiv.org/abs/2508.01789v1,Sonify Anything: Towards Context-Aware Sonic Interactions in AR,"Augmented Reality (AR) environments offer rich visual information, but often lack intuitive and informative auditory feedback, hindering seamless interaction. This paper addresses the challenge of generating context-aware sonifications for arbitrary objects in AR, moving beyond simple object identification to provide nuanced sonic representations of object properties and relationships. We introduce ""Sonify Anything,"" a novel framework that leverages a pre-trained vision-language model to extract semantic context from the visual scene. This context is then mapped to sonic parameters using a flexible, user-configurable mapping system allowing for the creation of diverse and informative auditory displays. User studies demonstrate that our system significantly improves object recognition accuracy, spatial awareness, and task efficiency in AR environments compared to baseline visual-only and simple sonification approaches. ""Sonify Anything"" provides a crucial step towards creating more intuitive and accessible AR experiences by dynamically generating informative and contextually relevant auditory feedback."
http://arxiv.org/abs/2508.01785v1,"Skip priors and add graph-based anatomical information, for point-based Couinaud segmentation","Precise liver Couinaud segment (CS) delineation is crucial for surgical planning and navigation. However, automated segmentation of CS remains challenging due to high anatomical variability and indistinct boundaries. We address this problem by introducing a novel approach that leverages both skip connection priors and graph-based anatomical information to improve the accuracy of point-based CS segmentation. Our method combines a point-based neural network with skip connections that encode global liver shape information and a graph convolutional network that models the anatomical relationships between adjacent CS. The graph structure is built based on the adjacency of Couinaud segments, enabling the propagation of contextual information and enforcing anatomical plausibility. We evaluated our method on a clinical dataset of CT volumes, demonstrating a significant improvement in Dice score and boundary delineation accuracy compared to state-of-the-art point-based segmentation methods. This approach provides a robust and accurate solution for CS segmentation, facilitating more effective clinical workflows."
http://arxiv.org/abs/2508.01782v1,Joint Lossless Compression and Steganography for Medical Images via Large Language Models,"Medical images, crucial for diagnosis and treatment planning, are subject to stringent requirements for both lossless compression and secure communication. Existing methods often address these requirements separately, leading to inefficiencies and potential vulnerabilities. This paper proposes a novel framework for joint lossless compression and steganography of medical images leveraging the generative capabilities of Large Language Models (LLMs). Our method first employs a reversible integer wavelet transform to decorrelate the image data. Subsequently, the resulting wavelet coefficients are quantized and encoded as a textual representation suitable for LLM processing. The LLM is then prompted to generate a compressed representation, subtly embedding secret data within the generated text using a carefully designed encoding scheme based on synonym substitution and paraphrasing. Experimental results on a diverse dataset of medical images demonstrate competitive compression ratios compared to state-of-the-art lossless compression algorithms, while simultaneously achieving high steganographic capacity and imperceptibility. This integrated approach offers a promising solution for secure and efficient transmission and storage of sensitive medical information."
http://arxiv.org/abs/2508.01740v1,AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing,"3D Gaussian Splatting (3D-GS) has emerged as a powerful scene representation for novel view synthesis. However, existing methods struggle with instance-level understanding and manipulation of complex scenes due to the lack of explicit object-level representations. To address this limitation, we introduce AG$^2$aussian, a novel Anchor-Graph Structured Gaussian Splatting approach that incorporates explicit relationships between Gaussians to facilitate instance-level 3D scene understanding and editing. Our method constructs an anchor graph, where anchor nodes represent instance centers, and edges encode relationships between anchor nodes and individual Gaussians. This graph structure allows us to effectively segment and group Gaussians belonging to the same object instance. We further introduce a differentiable graph-based loss that encourages Gaussians within the same instance to have similar properties. Experiments on synthetic and real-world datasets demonstrate that AG$^2$aussian achieves state-of-the-art performance in instance segmentation and enables intuitive object-level editing operations while maintaining high-fidelity rendering quality. AG$^2$aussian provides a powerful framework for bridging the gap between continuous scene representations and discrete object-level understanding, opening new avenues for interactive 3D scene manipulation and downstream applications."
http://arxiv.org/abs/2508.01731v1,SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models,"Spectral remote sensing foundation models, pre-trained on massive datasets, demonstrate impressive capabilities but often struggle with domain shift when applied to unseen geographical regions or sensors. Fine-tuning these large models for each new domain is computationally expensive and requires substantial labeled data, hindering their practical deployment in diverse spectral environments. We introduce SpectralX, a novel parameter-efficient domain generalization approach that leverages spectral-wise modulation within a bottleneck architecture. SpectralX learns domain-specific spectral transformations that are applied to the intermediate features of a pre-trained foundation model, effectively adapting the model's spectral understanding without modifying the core architecture. Experiments on multiple benchmark datasets for land cover classification demonstrate that SpectralX achieves significant improvements in out-of-distribution generalization performance compared to full fine-tuning and existing parameter-efficient transfer learning techniques, while only updating a small fraction of the total parameters. SpectralX provides a practical and scalable solution for adapting spectral remote sensing foundation models to new domains, enabling broader application and reducing the need for extensive domain-specific training."
http://arxiv.org/abs/2508.01713v1,Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation,"Robot-assisted surgery (RAS) offers enhanced precision and dexterity compared to traditional open surgery, but relies heavily on accurate scene understanding. Existing surgical scene segmentation methods often struggle to adapt to the dynamic and evolving nature of surgical procedures, particularly when new instruments or anatomical structures are introduced during the operation. This paper addresses the challenge of adapting semantic segmentation models in RAS to incorporate novel classes encountered during surgery without retraining from scratch. We propose a novel hierarchical class-incremental learning framework that leverages a coarse-to-fine label hierarchy to facilitate knowledge transfer between known and novel classes. Our approach employs a dual-memory system, storing both exemplar features and pseudo-labeled features of past classes to mitigate catastrophic forgetting. Experimental results on a challenging surgical dataset demonstrate that our method significantly outperforms state-of-the-art class-incremental learning techniques in terms of segmentation accuracy for both old and new classes, achieving a substantial improvement in average incremental accuracy. This work provides a crucial step towards developing robust and adaptable surgical perception systems capable of handling the complexities of real-world RAS scenarios."
http://arxiv.org/abs/2508.01697v1,"Register Anything: Estimating ""Corresponding Prompts"" for Segment Anything Model","The Segment Anything Model (SAM) demonstrates remarkable zero-shot segmentation capabilities given spatial prompts such as points or bounding boxes. However, specifying these prompts manually can be cumbersome and limits downstream applications requiring semantic correspondence. This paper addresses the problem of automatically generating ""corresponding prompts"" for SAM, allowing for the transfer of segmentation masks between images based on semantic similarity. We propose a novel framework that leverages pre-trained vision-language models to estimate dense semantic similarity maps between images. These maps are then used to predict optimal point prompts in the target image that correspond to a given point prompt in the source image, effectively transferring the segmentation mask. Our experiments on diverse datasets demonstrate that our method significantly improves the accuracy of mask transfer compared to naive prompt transfer, achieving comparable performance to manual prompt specification in many cases. This approach enables a wider range of applications for SAM, including automated object co-segmentation, image editing, and video object tracking, by facilitating correspondence-aware mask propagation."
http://arxiv.org/abs/2508.01676v1,Benchmarking Adversarial Patch Selection and Location,"Adversarial patches are a significant threat to image classification systems, capable of causing misclassification when applied to specific image regions. However, the effectiveness of an adversarial patch depends heavily on its location and the selection of images used for its generation. This paper addresses the problem of systematically evaluating the impact of patch location and image selection strategies on the transferability and attack success rate of adversarial patches. We propose a comprehensive benchmarking framework that evaluates various patch selection techniques, including random selection, entropy-based selection, and gradient-based selection, across different patch locations determined by object detection and saliency maps. Our experiments on ImageNet demonstrate that entropy-based image selection consistently leads to more transferable patches compared to random selection, and that targeting object regions identified by YOLOv5 results in significantly higher attack success rates than saliency-based locations, even against adversarially trained defenses. These findings provide crucial insights for developing more robust defense mechanisms and understanding the vulnerabilities of image classification models to targeted adversarial attacks."
http://arxiv.org/abs/2508.01667v1,Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models,"Vision foundation models (VFMs) have demonstrated remarkable capabilities in various vision tasks, including semantic segmentation. However, adapting these models to new datasets and domains while maintaining computational efficiency remains a significant challenge. This paper addresses the problem of efficiently generalizing and adapting VFMs for semantic segmentation across diverse datasets. We introduce Rein++, a novel approach that leverages reinforcement learning to dynamically select and fuse multi-scale features from the VFM backbone, optimizing for both segmentation accuracy and computational cost. Specifically, Rein++ employs a lightweight policy network, trained with proximal policy optimization, to adaptively determine the optimal feature fusion strategy for each input image. Our experiments on a diverse set of benchmark datasets demonstrate that Rein++ achieves state-of-the-art segmentation performance with significantly reduced computational overhead compared to existing fine-tuning and adaptation methods. This efficient adaptation strategy facilitates the deployment of VFMs in resource-constrained environments and enables rapid generalization to novel domains."
http://arxiv.org/abs/2508.01664v1,Shape Distribution Matters: Shape-specific Mixture-of-Experts for Amodal Segmentation under Diverse Occlusions,"Amodal segmentation, the task of predicting the complete shape of an object even when it is partially occluded, remains a challenging problem in computer vision. Existing methods often struggle with diverse and significant occlusions due to their inability to effectively reason about the underlying complete shape. We address this limitation by proposing a shape-specific Mixture-of-Experts (MoE) framework for amodal segmentation. Our method learns a set of expert networks, each specializing in a specific shape distribution learned from the training data. An attention-based gating network dynamically selects and combines the outputs of these experts based on the observed visible region, enabling the model to adapt to varying occlusion patterns and leverage shape priors effectively. Experiments on standard amodal segmentation datasets demonstrate that our approach significantly outperforms existing state-of-the-art methods, particularly in scenarios with severe occlusions, achieving improvements of up to 5% in IoU. These results highlight the importance of incorporating shape distribution awareness into amodal segmentation models for robust performance under challenging occlusion conditions."
http://arxiv.org/abs/2508.01661v1,"Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation","Amodal segmentation, the task of predicting the full extent of an object even when partially occluded, remains a challenging problem due to the inherent ambiguity in inferring the occluded regions. Existing methods often rely on complex multi-stage pipelines or require extensive training data with complete amodal annotations. This paper introduces a novel end-to-end framework, ""Single Point, Full Mask,"" that leverages velocity-guided level set evolution for amodal segmentation, requiring only a single user-provided point within the visible region of the target object. Our approach employs a convolutional neural network to predict a velocity field from the input image and point prompt, which is then used to drive the evolution of a level set function, iteratively expanding from the prompt point to delineate the full amodal mask. Critically, the velocity field is trained to implicitly learn the object's boundaries, enabling robust extrapolation into occluded regions. Experiments on standard amodal segmentation datasets demonstrate that our method achieves state-of-the-art performance while significantly reducing annotation requirements and simplifying the segmentation pipeline. This work provides a powerful and practical solution for amodal segmentation with minimal user interaction and training overhead."
http://arxiv.org/abs/2508.01582v1,Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models,"Vision foundation models (VFMs) have demonstrated remarkable capabilities in open-vocabulary segmentation tasks, yet their generalization to unseen datasets and novel classes remains a significant challenge. Current fine-tuning strategies often struggle to adapt VFMs effectively, particularly when faced with limited or biased training data, leading to suboptimal performance in generalized segmentation. We introduce Set Pivot Learning (SPL), a novel fine-tuning paradigm that reframes generalized segmentation as a set prediction problem. SPL leverages a learnable set of ""pivot"" embeddings, each representing a latent semantic concept, to interact with the VFM's image embeddings. By optimizing the assignment of image regions to these pivots and subsequently mapping pivots to target classes, SPL learns a robust and disentangled representation space. Experiments across diverse generalized segmentation benchmarks, including COCO-Stuff, ADE20K, and Pascal VOC, demonstrate that SPL significantly outperforms existing fine-tuning methods, achieving state-of-the-art results with substantial gains in both seen and unseen class segmentation accuracy. This approach offers a more effective and efficient way to adapt VFMs for generalized segmentation, paving the way for more robust and adaptable vision systems."
http://arxiv.org/abs/2508.01577v1,Tractography-Guided Dual-Label Collaborative Learning for Multi-Modal Cranial Nerves Parcellation,"Cranial nerve (CN) parcellation is crucial for diagnosing and treating various neurological disorders. However, accurate and reliable CN parcellation remains challenging due to the small size, complex anatomical structure, and high inter-individual variability of CNs, especially when relying on single-modality MRI. This paper addresses the problem of improving multi-modal CN parcellation by leveraging diffusion tensor imaging (DTI) tractography to guide collaborative learning between different modalities. Our proposed Tractography-Guided Dual-Label Collaborative Learning (TGDLCL) framework utilizes DTI-derived tractography to establish structural connectivity priors between CN sub-regions, enabling knowledge transfer between T1-weighted and T2-weighted MRI during training. Specifically, we generate dual-label pseudo labels from each modality and enforce consistency between modalities based on the tractography-derived connectivity. Experimental results on a multi-center dataset demonstrate that TGDLCL significantly outperforms state-of-the-art methods, achieving an average Dice score improvement of over 5% for challenging CNs like the facial and vestibulocochlear nerves. The proposed method provides a robust and accurate framework for multi-modal CN parcellation, potentially improving clinical diagnosis and surgical planning."
http://arxiv.org/abs/2508.01465v1,EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned Graph-Augmented Transformer,"Brain tumor segmentation from multi-modal Magnetic Resonance Imaging (MRI) is crucial for accurate diagnosis and treatment planning. However, capturing long-range dependencies between image features across different modalities while maintaining computational efficiency remains a significant challenge. This paper introduces EfficientGFormer, a novel approach for multimodal brain tumor segmentation that leverages a pruned graph-augmented transformer architecture. Specifically, we construct a graph representation of image features from different MRI modalities and employ a pruning strategy to reduce the computational cost associated with the transformer's self-attention mechanism, focusing on the most salient connections. The pruned graph guides the transformer to efficiently capture long-range dependencies, improving segmentation accuracy. Experimental results on the BraTS 2021 dataset demonstrate that EfficientGFormer achieves state-of-the-art segmentation performance while significantly reducing computational complexity compared to existing transformer-based methods. This efficient and accurate multimodal segmentation approach has the potential to improve clinical workflows and patient outcomes."
http://arxiv.org/abs/2508.01460v1,Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation,"Accurate segmentation quality prediction is crucial for deploying medical image segmentation algorithms in clinical settings, particularly when dealing with critical tasks like skin lesion and liver tumor delineation. However, existing quality assessment methods often fail to quantify the uncertainty associated with their predictions, hindering reliable decision-making. This paper introduces a novel uncertainty-aware segmentation quality prediction framework based on deep learning Bayesian modeling. Specifically, we employ a Monte Carlo Dropout-enabled convolutional neural network to learn a mapping from segmentation masks and corresponding images to quality scores, simultaneously estimating the aleatoric and epistemic uncertainty inherent in the prediction process. We conduct comprehensive experiments on skin cancer (ISIC 2018) and liver segmentation (SLIVER07) datasets, demonstrating that our method achieves state-of-the-art performance in predicting segmentation quality while providing well-calibrated uncertainty estimates. The integration of uncertainty quantification enhances the reliability and trustworthiness of automated segmentation quality assessment, paving the way for safer and more effective clinical applications."
http://arxiv.org/abs/2508.01331v1,Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network,"Referring remote sensing image segmentation (RRSS) aims to segment the target object in a remote sensing image based on a given natural language description. Existing RRSS methods primarily focus on single-view feature extraction and fusion, neglecting the inherent cross-view semantic relationships between the visual content of remote sensing images and the textual descriptions, which limits their ability to accurately interpret complex references. To address this limitation, we propose a Cross-view Semantics Interaction Network (CSIN) for RRSS. CSIN introduces a novel Cross-view Interaction Module (CIM) that leverages a transformer-based architecture to explicitly model the semantic dependencies between visual features and textual embeddings, facilitating bi-directional information propagation and alignment. Furthermore, we design a Multi-scale Feature Aggregation (MFA) module to capture contextual information at different scales, improving the robustness of the segmentation results. Experimental results on the challenging RSISeg and GRSS datasets demonstrate that CSIN significantly outperforms existing state-of-the-art methods, achieving improvements of 3.2% and 2.8% in terms of overall IoU, respectively. The proposed CSIN offers a more effective and robust approach to RRSS by explicitly modeling cross-view semantic interactions, paving the way for more accurate and interpretable analysis of remote sensing data."
http://arxiv.org/abs/2508.01250v1,DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing,"Face parsing, the task of assigning semantic labels to facial components, is crucial for various applications, including facial expression recognition and virtual try-on. However, obtaining pixel-level annotations for face parsing is expensive and time-consuming. This paper addresses the challenge of weakly supervised face parsing, specifically focusing on disentangling representations of co-occurring facial components, such as eyes and eyebrows, which are often ambiguously labeled in weak annotations. We propose DisFaceRep, a novel framework that enforces representation disentanglement by incorporating a contrastive loss that encourages feature separation between different facial components based on weak bounding box annotations. Furthermore, we introduce a self-attention mechanism within each component's feature space to enhance intra-component feature aggregation and contextual awareness. Experimental results on benchmark datasets demonstrate that DisFaceRep significantly outperforms existing weakly supervised face parsing methods, achieving comparable performance to some fully supervised approaches. The proposed method provides a robust and efficient solution for face parsing with limited supervision, enabling broader applicability in real-world scenarios."
http://arxiv.org/abs/2508.01152v1,LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation,"Dichotomous image segmentation, aiming to separate a target object from the background, has seen significant progress with deep learning. However, existing methods often lack fine-grained control over the segmentation process, limiting their applicability in scenarios requiring specific object properties or attributes. This paper introduces LawDIS, a novel Language-Window-based Controllable Dichotomous Image Segmentation framework. LawDIS leverages a pre-trained vision-language model to encode both the input image and a descriptive language prompt specifying the desired characteristics of the target object. A sliding window attention mechanism, guided by the language embedding, dynamically focuses on relevant image regions, enabling precise and controllable segmentation. Experimental results on diverse datasets demonstrate that LawDIS achieves state-of-the-art performance in both accuracy and controllability, allowing users to effectively guide the segmentation process through natural language. This controllable approach offers a significant advancement in image segmentation, enabling more intuitive and task-specific applications."
http://arxiv.org/abs/2508.01150v1,OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding,"Dense 3D mapping is crucial for embodied AI, enabling robots to interact with and understand their environment. However, existing dense mapping techniques often lack semantic awareness or are limited by a closed set of object categories. This paper introduces OpenGS-Fusion, a novel open-vocabulary dense mapping framework that leverages hybrid 3D Gaussian Splatting (3D-GS) to achieve refined object-level understanding. Our method integrates pre-trained vision-language models (VLMs) with a differentiable 3D-GS representation, allowing us to perform open-vocabulary semantic queries and propagate semantic information to unlabeled regions through geometric and appearance consistency. Furthermore, we incorporate a novel object-aware refinement module that leverages the VLM's knowledge to refine object boundaries within the 3D-GS representation, leading to more accurate and complete object reconstructions. Experimental results on ScanNet and Replica datasets demonstrate that OpenGS-Fusion significantly outperforms existing methods in open-vocabulary semantic segmentation and object-level reconstruction, achieving state-of-the-art performance in terms of both accuracy and completeness. By enabling robots to understand and interact with objects beyond a predefined vocabulary, OpenGS-Fusion represents a significant step towards more intelligent and versatile embodied agents."
http://arxiv.org/abs/2508.01064v1,Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation,"Medical image segmentation is a crucial task for computer-aided diagnosis, often requiring high accuracy and computational efficiency. While Vision Transformers (ViTs) have demonstrated promising performance in medical image segmentation, their high computational cost, especially with large kernel attention, hinders their deployment on resource-constrained mobile devices. This paper addresses the challenge of designing an efficient ViT architecture suitable for medical image segmentation on mobile platforms. We propose Mobile U-ViT, a novel architecture that revisits large kernel attention within a U-shaped ViT framework specifically tailored for mobile deployment. Mobile U-ViT leverages a depth-wise separable large kernel attention mechanism to reduce computational complexity while preserving long-range dependencies. Furthermore, we employ efficient upsampling modules and a streamlined U-Net structure to optimize memory usage and inference speed. Experimental results on benchmark medical image segmentation datasets, including skin lesion segmentation and polyp segmentation, demonstrate that Mobile U-ViT achieves comparable segmentation accuracy to state-of-the-art ViT-based methods, while significantly reducing the number of parameters and computational cost, leading to faster inference times on mobile devices. Mobile U-ViT offers a practical solution for accurate and efficient medical image segmentation on resource-limited platforms, facilitating wider accessibility of AI-powered diagnostic tools."
http://arxiv.org/abs/2508.01058v1,ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation,"Brain tumor segmentation is critical for diagnosis and treatment planning, often relying on multi-modal Magnetic Resonance Imaging (MRI) to capture diverse tissue characteristics. However, effectively integrating information across modalities and handling the inherent data heterogeneity remain significant challenges. This paper addresses the problem of robust and accurate brain tumor segmentation by leveraging a novel cross-modal diffusion framework. We introduce ReCoSeg++, an extension of our previous work, which incorporates a refined residual-guided diffusion process and a novel adaptive fusion strategy. Specifically, we enhance the diffusion process with residual connections that explicitly guide the denoising steps towards tumor boundaries, and employ a learnable attention mechanism to dynamically weight the contributions of each modality during feature fusion. Experimental results on the BraTS 2021 dataset demonstrate that ReCoSeg++ achieves state-of-the-art performance, significantly improving segmentation accuracy, particularly for small and irregularly shaped tumors, surpassing existing methods by a substantial margin in terms of Dice score and Hausdorff distance. This work provides a robust and effective approach for brain tumor segmentation, facilitating more precise and reliable clinical decision-making."
http://arxiv.org/abs/2508.00750v1,SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation,"Super-resolution (SR) of satellite and drone imagery is crucial for various downstream tasks, enabling enhanced visualization and analysis. However, applying SR models trained on natural images to remote sensing data often results in suboptimal performance due to domain discrepancies and the presence of complex scene structures. This paper introduces SU-ESRGAN, a novel Semantic and Uncertainty-aware ESRGAN framework, specifically tailored for SR of satellite and drone imagery. Our approach integrates semantic segmentation information to guide the SR process, allowing the network to focus on refining details within specific object categories. Furthermore, we incorporate an uncertainty estimation module to quantify the reliability of the generated high-resolution pixels, providing valuable information for subsequent analysis. The model is further fine-tuned using a novel cross-domain adaptation strategy. Experimental results demonstrate that SU-ESRGAN significantly outperforms state-of-the-art SR methods in terms of both quantitative metrics and visual quality, particularly in preserving fine details and reducing artifacts. The uncertainty maps generated by our method accurately reflect regions with higher reconstruction difficulty. The proposed approach offers a robust and reliable solution for high-quality super-resolution of remote sensing data across different domains."
http://arxiv.org/abs/2508.00568v1,CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry,"Monocular Visual Odometry (VO) is a fundamental task in robotics and autonomous navigation, aiming to estimate the ego-motion of an agent using only a single camera. Existing unsupervised VO methods often struggle in low-texture or dynamically changing environments due to the inherent ambiguity in depth and motion estimation. This paper introduces CoProU-VO, a novel end-to-end unsupervised monocular VO framework that leverages projected uncertainty to improve robustness and accuracy. Our approach incorporates a learned uncertainty map in the image space and projects this uncertainty into the 3D scene during depth estimation, effectively weighting the photometric loss based on data reliability. Furthermore, we introduce a novel uncertainty-aware pose refinement module that leverages the projected uncertainty to iteratively refine the estimated pose, minimizing the impact of unreliable regions. Experiments on the KITTI dataset demonstrate that CoProU-VO achieves state-of-the-art performance compared to existing unsupervised monocular VO methods, particularly in challenging environments with significant occlusions and dynamic objects, reducing the relative pose error by up to 15%. This highlights the effectiveness of incorporating projected uncertainty for robust and accurate unsupervised monocular visual odometry."
http://arxiv.org/abs/2508.00557v1,Training-Free Class Purification for Open-Vocabulary Semantic Segmentation,"Open-vocabulary semantic segmentation aims to segment images into arbitrary semantic classes described by text prompts, leveraging the power of vision-language models. However, noisy pseudo-labels generated by these models often hinder the performance of downstream segmentation tasks due to inaccurate class assignments. This paper addresses the problem of purifying these noisy pseudo-labels in a training-free manner, thereby improving the quality of supervision for open-vocabulary segmentation. Our method, ClassPurify, leverages the semantic consistency within each class and the visual similarity between pixels to identify and correct mislabeled pixels. Specifically, we employ a masked visual feature aggregation strategy to refine pixel embeddings based on neighborhood consensus and subsequently re-assign pixels to the most semantically similar class according to the vision-language model. Experimental results on standard datasets demonstrate that ClassPurify significantly improves segmentation accuracy, achieving substantial gains over existing pseudo-labeling approaches without requiring any training or fine-tuning. This training-free purification technique provides a simple yet effective way to enhance the quality of pseudo-labels, leading to improved performance in open-vocabulary semantic segmentation."
http://arxiv.org/abs/2508.00493v1,SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation,"Interactive medical image segmentation is crucial for accurate diagnosis and treatment planning, but often requires significant user interaction and expert knowledge. The Segment Anything Model (SAM) has shown remarkable zero-shot segmentation capabilities, yet its performance on hyperspectral medical images, rich in spectral information, remains limited due to its reliance on RGB data and simple point prompts. This paper addresses the challenge of effectively leveraging the spectral richness of hyperspectral medical images to enhance SAM's interactive segmentation performance. We introduce SAMSA 2.0, an improved prompting strategy that incorporates spectral angle mapping (SAM) between user-provided point prompts and hyperspectral pixels. Instead of directly prompting SAM with point coordinates, SAMSA 2.0 uses SAM values as a weighting mechanism to generate a refined probability map that guides SAM towards spectrally similar regions. Experiments on a hyperspectral colon cancer dataset demonstrate that SAMSA 2.0 significantly improves segmentation accuracy compared to SAM with point prompts and other spectral-based prompting strategies, achieving a Dice score improvement of over 15%. This approach offers a more efficient and accurate interactive segmentation tool for hyperspectral medical imaging, reducing the need for extensive manual annotation."
http://arxiv.org/abs/2508.00442v1,TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation,"Accurate segmentation of tubular structures, such as blood vessels or neural networks, is crucial in various biomedical applications. However, domain shifts between training and testing data often degrade the performance of deep learning models. We address the challenge of adapting segmentation models to unseen target domains at test time, focusing on maintaining the crucial topological properties of tubular structures. We propose TopoTTA, a novel test-time adaptation framework that leverages topological priors to guide adaptation. TopoTTA incorporates a topological loss function based on persistent homology to penalize changes in the number of connected components and loops during adaptation. Furthermore, we employ a self-training strategy with dynamically weighted pseudo-labels, favoring regions with high confidence and topological consistency. Experiments on diverse datasets of retinal and brain vasculature demonstrate that TopoTTA significantly improves segmentation accuracy and topological fidelity compared to existing test-time adaptation methods. Our method provides a robust and reliable approach for segmenting tubular structures in challenging and variable data environments."
http://arxiv.org/abs/2508.00406v1,PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos,"Turbulence severely degrades dynamic videos, introducing complex spatio-temporal distortions that impede accurate visual analysis. Existing video restoration methods often struggle with turbulent videos due to the dynamic and non-uniform nature of the distortions. This paper addresses the challenge of restoring turbulent dynamic videos by introducing PMR, a Physical Model-driven Multi-Stage Restoration framework. PMR leverages the physical model of atmospheric turbulence to guide the restoration process, decomposing it into three cascaded stages: global motion estimation and compensation, spatially varying deblurring, and residual refinement. The global motion stage mitigates large-scale shifts, followed by a spatially adaptive deblurring network informed by estimated turbulence parameters. Finally, a residual refinement network removes remaining artifacts and enhances visual details. Extensive experiments on both synthetic and real-world turbulent video datasets demonstrate that PMR significantly outperforms state-of-the-art video restoration methods in terms of PSNR, SSIM, and perceptual quality. PMR provides a robust and effective solution for restoring turbulent dynamic videos, enabling improved performance in downstream vision tasks."
http://arxiv.org/abs/2508.00395v1,Decouple before Align: Visual Disentanglement Enhances Prompt Tuning,"Large pre-trained vision-language models (VLMs) have demonstrated remarkable zero-shot capabilities, which can be further enhanced through prompt tuning. However, directly aligning visual features with textual prompts often leads to entangled representations, limiting the adaptability and generalization of tuned prompts across diverse visual inputs. To address this, we propose a novel ""Decouple before Align"" (DbA) framework that explicitly disentangles visual features into content and style representations prior to prompt alignment. DbA employs a variational autoencoder to separate the input image into a content latent space capturing semantic information and a style latent space encoding visual appearance. The content features are then aligned with the learnable prompts, while the style features are used to modulate the prompt alignment process. Experiments on various few-shot classification benchmarks, including object recognition and fine-grained image classification, demonstrate that DbA consistently outperforms existing prompt tuning methods. This highlights the benefit of visual disentanglement for improved prompt tuning and suggests a promising direction for enhancing the adaptability of VLMs."
http://arxiv.org/abs/2508.00354v1,Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging,"Creating visually accurate 3D digital twins of objects is crucial for various applications, including robotics, e-commerce, and AR/VR. However, achieving complete and high-fidelity object reconstruction remains challenging due to occlusions and the need for multi-view capture. This paper introduces Omni-Scan, a novel system for creating visually accurate digital twin object models using a bimanual robot capable of coordinated manipulation and handover. Our approach leverages a dual-arm setup with synchronized motion to capture comprehensive multi-view images from diverse perspectives, minimizing occlusions. We introduce a Gaussian Splat merging technique that efficiently integrates data from different viewpoints acquired during handover, ensuring seamless reconstruction even with varying lighting conditions and robot pose inaccuracies. Experiments demonstrate that Omni-Scan significantly improves the visual quality and completeness of the reconstructed models compared to single-arm or static scanning methods, achieving a higher fidelity in both geometry and texture representation. This work facilitates the creation of realistic and accurate digital twins, enabling improved performance in downstream applications that rely on high-quality 3D object representations."
http://arxiv.org/abs/2508.00259v1,PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting,"3D Gaussian Splatting (3D-GS) has emerged as a powerful scene representation for novel view synthesis, offering state-of-the-art rendering quality and real-time rendering speeds. However, segmenting individual objects within a 3D-GS scene remains a challenging task, hindering its applicability in scene understanding and manipulation. We introduce PointGauss, a novel framework for point cloud-guided multi-object segmentation of 3D Gaussian Splatting scenes. Our approach leverages a pre-segmented point cloud of the scene to initialize and refine object masks directly within the 3D-GS representation. Specifically, we propagate semantic labels from the point cloud to the Gaussians based on spatial proximity and then iteratively optimize Gaussian attributes, including opacity and position, conditioned on the propagated labels and a differentiable rendering loss. Experiments on synthetic and real-world datasets demonstrate that PointGauss achieves significantly improved segmentation accuracy compared to existing methods, while maintaining the rendering quality of the original 3D-GS representation. This enables downstream applications such as object-level editing and scene composition within Gaussian Splatting scenes."
http://arxiv.org/abs/2508.00218v1,Object-Centric Cropping for Visual Few-Shot Classification,"Few-shot classification aims to recognize novel categories given only a handful of labeled examples. Existing methods often rely on holistic image representations, which can be sensitive to background clutter and intra-class variations, hindering performance when data is scarce. This paper addresses the challenge of learning robust few-shot classifiers by focusing on informative object regions. We propose an object-centric cropping framework that learns to automatically detect and crop task-relevant objects from support and query images. A differentiable cropping module, trained using a novel contrastive loss that encourages feature alignment between corresponding object crops, enables end-to-end optimization with a standard meta-learning algorithm. Experiments on miniImageNet and tieredImageNet demonstrate that our approach consistently outperforms state-of-the-art methods, achieving significant improvements in classification accuracy, particularly in the most challenging few-shot settings. This work highlights the importance of object-level reasoning for few-shot learning and provides a practical framework for incorporating this reasoning into existing meta-learning pipelines."
http://arxiv.org/abs/2508.00213v1,"SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters","The Segment Anything Model (SAM) demonstrates remarkable zero-shot generalization for image segmentation, yet struggles with tasks requiring fine-grained understanding or specific domain knowledge. Fine-tuning SAM for specialized applications often necessitates updating a large number of parameters, leading to high computational costs and potential overfitting. We introduce SAM-PTx, a novel parameter-efficient fine-tuning approach that leverages parallel-text adapters to guide SAM's adaptation to specific tasks. Our method integrates lightweight, parallel text encoders that process task-specific text prompts and inject contextual information into SAM's image encoder through cross-attention mechanisms. This allows for targeted adjustments to SAM's visual understanding without significantly altering the pre-trained weights. Experiments on diverse segmentation tasks, including medical image segmentation and remote sensing, demonstrate that SAM-PTx achieves comparable or superior performance to full fine-tuning while updating significantly fewer parameters (less than 5% of SAM's total parameters). SAM-PTx offers a practical and efficient solution for adapting SAM to specialized applications, broadening its accessibility and applicability across various domains."
http://arxiv.org/abs/2507.23772v1,SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting,"Scene understanding requires reasoning about object affordances, i.e., the potential interactions objects enable. Existing methods struggle to achieve accurate and consistent scene-level affordance prediction due to the lack of explicit geometric representation and the difficulty in capturing sequential dependencies between affordance labels. We introduce SeqAffordSplat, a novel approach for scene-level sequential affordance reasoning directly on 3D Gaussian Splatting (3D-GS) representations. Our method leverages the explicit and differentiable nature of 3D-GS to learn per-Gaussian affordance embeddings, which are then aggregated to form scene-level representations. A transformer-based sequential model is employed to capture dependencies between affordance labels, predicting a coherent sequence of affordances for the entire scene. Experiments on the ARID and SODA datasets demonstrate that SeqAffordSplat achieves state-of-the-art performance in scene-level affordance prediction, significantly outperforming existing methods in both accuracy and consistency. This highlights the potential of using 3D-GS as a powerful foundation for reasoning about complex scene properties beyond geometry and appearance."
http://arxiv.org/abs/2507.23755v1,Slot Attention with Re-Initialization and Self-Distillation,"Scene understanding requires effectively parsing complex visual inputs into meaningful object-centric representations. Slot Attention has emerged as a powerful method for discovering object slots in a scene in an unsupervised manner, but it can suffer from instability during training and redundant slot representations. This paper addresses these limitations by introducing Slot Attention with Re-Initialization and Self-Distillation (SA-RISD). Our approach incorporates a novel re-initialization strategy for inactive slots during training, preventing mode collapse and encouraging slot diversity. We further enhance performance by employing a self-distillation loss that encourages consistency between the output of the original Slot Attention module and a momentum-updated target network, thereby stabilizing training and improving the quality of learned representations. Experimental results on challenging multi-object datasets, including CLEVR and Multi-dSprites, demonstrate that SA-RISD significantly outperforms existing Slot Attention variants and other unsupervised object discovery methods in terms of reconstruction quality and object segmentation accuracy. This work offers a robust and effective approach for unsupervised object discovery, paving the way for more reliable and interpretable scene understanding systems."
http://arxiv.org/abs/2507.23734v1,RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping,"Affordance segmentation, predicting pixel-level action possibilities, is crucial for robotic grasping in unstructured environments. Current affordance datasets often lack the scale and complexity required to train generalizable grasping models, particularly those capable of reasoning about object properties and scene context. To address this limitation, we introduce RAGNet, a large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping. RAGNet comprises over 500,000 annotated RGB-D images of diverse household objects in cluttered scenes, paired with detailed affordance masks for various grasping actions. Crucially, each scene is accompanied by a comprehensive scene graph, enabling models to explicitly reason about object relationships and contextual cues influencing grasp affordances. We evaluate several state-of-the-art segmentation and graph neural network architectures on RAGNet, demonstrating a significant performance gap compared to human-level accuracy, highlighting the benchmark's challenge. Our experiments reveal that incorporating scene graph information significantly boosts affordance segmentation performance, paving the way for more robust and generalizable robotic grasping systems. RAGNet serves as a valuable resource for advancing research in reasoning-based affordance learning and general robotic manipulation."
http://arxiv.org/abs/2507.23673v1,SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation,"Hyperspectral imaging (HSI) offers rich spectral information beneficial for medical image analysis, enabling precise tissue characterization. However, accurate segmentation of medical HSI data remains challenging due to inherent noise, subtle spectral variations, and the need for expert knowledge. This paper introduces SAMSA, a novel interactive segmentation framework that leverages the Segment Anything Model (SAM) and incorporates spectral angle mapping to enhance performance in medical HSI. SAMSA utilizes user-provided prompts to initialize SAM, then refines the segmentation by integrating spectral angle similarity measures between the initial segmentation and neighboring pixels, effectively capturing subtle spectral variations indicative of different tissue types. We evaluated SAMSA on a dataset of ex vivo human colon tissue HSI, demonstrating significant improvements in segmentation accuracy compared to the original SAM and other state-of-the-art interactive segmentation methods, particularly in regions with subtle spectral differences. SAMSA offers a user-friendly and accurate approach for medical HSI segmentation, facilitating improved diagnostic and research applications."
http://arxiv.org/abs/2507.23652v1,Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis,"Diffusion models have demonstrated remarkable capabilities in medical image synthesis, offering potential for data augmentation and rare disease simulation. However, their widespread adoption is hindered by substantial computational costs associated with both training and inference. We address this limitation by introducing Adaptively Distilled ControlNet (ADCN), a novel framework for accelerating diffusion-based medical image synthesis while enhancing sampling quality. ADCN leverages knowledge distillation to transfer the learned representations from a large, pre-trained ControlNet to a smaller, student network. Crucially, we employ an adaptive distillation strategy that dynamically adjusts the distillation loss based on the student network's learning progress, focusing on the most challenging regions of the latent space. This adaptive approach ensures efficient knowledge transfer and prevents overfitting. Experiments on multiple medical imaging datasets (e.g., brain MRI, chest X-rays) demonstrate that ADCN achieves a 2-4x speedup in training time and generates higher-quality synthetic images, as measured by Frchet Inception Distance (FID) and Structural Similarity Index Measure (SSIM), compared to standard ControlNet distillation techniques. ADCN paves the way for more efficient and accessible diffusion-based medical image synthesis, accelerating research and clinical translation."
http://arxiv.org/abs/2507.23642v1,Efficient Masked Attention Transformer for Few-Shot Classification and Segmentation,"Few-shot classification and segmentation demand effective feature representation and efficient adaptation to novel classes with limited labeled examples. While Transformers have shown promise in various vision tasks, their computational complexity, especially with attention mechanisms, poses a significant challenge for few-shot scenarios. This paper introduces an Efficient Masked Attention Transformer (EMAT) designed to tackle this computational bottleneck while maintaining high performance in few-shot classification and segmentation. EMAT leverages a learnable masking strategy within the attention mechanism to selectively attend to the most relevant image regions, substantially reducing the computational cost. Furthermore, we incorporate a prototype-based learning framework that facilitates rapid adaptation to new classes by comparing masked feature representations with class prototypes. Experimental results on standard few-shot benchmarks, including miniImageNet, tieredImageNet, and COCO-Stuff, demonstrate that EMAT achieves competitive or superior performance compared to existing state-of-the-art methods, while significantly reducing computational overhead. The proposed EMAT offers a practical and efficient solution for few-shot learning, enabling the deployment of Transformer-based models in resource-constrained environments."
http://arxiv.org/abs/2507.23569v1,Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization,"Visual localization, the task of estimating a camera's pose within a known scene, is crucial for many applications. However, sharing image data for localization raises significant privacy concerns. This paper addresses the challenge of privacy-preserving visual localization by introducing Gaussian Splatting Feature Fields (GSFF), a novel approach that leverages 3D Gaussian splatting to encode scene geometry and appearance while simultaneously enabling feature extraction for pose estimation. GSFFs are trained using a differentiable rendering pipeline, allowing for the distillation of robust, privacy-preserving feature fields directly into the Gaussian representation. These feature fields are then used for localization via feature matching against pre-computed features from query images. We demonstrate that GSFFs achieve competitive localization accuracy compared to methods using raw image data, while effectively mitigating privacy risks by removing identifiable visual information. This work presents a significant step towards privacy-conscious visual localization, enabling robust pose estimation without compromising sensitive image data."
http://arxiv.org/abs/2507.23340v1,MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting,"Accurate 3D road surface reconstruction is crucial for autonomous driving, robotics, and urban mapping. However, dynamic obstacles like vehicles and pedestrians often occlude significant portions of the road, leading to incomplete and inaccurate reconstructions. This paper addresses the challenge of reconstructing complete and semantically consistent 3D road surfaces from point clouds corrupted by obstacle occlusions. We propose MagicRoad, a novel framework that leverages semantic segmentation and a deep inpainting network to hallucinate the occluded road regions. First, we semantically segment the input point cloud to identify and mask out obstacles. Then, a point cloud inpainting network, conditioned on the surrounding road geometry and learned semantic context, fills in the masked regions with plausible road surfaces. Finally, we refine the inpainted region by enforcing smoothness and consistency with the observed road geometry. Experimental results on real-world datasets demonstrate that MagicRoad significantly improves the completeness and accuracy of 3D road surface reconstruction compared to state-of-the-art methods, particularly in areas with dense obstacle occlusions. Our approach provides a robust and effective solution for generating accurate 3D road models in complex urban environments."
http://arxiv.org/abs/2507.23326v1,Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation,"Medical image segmentation models often suffer from significant performance degradation when applied to unseen target domains due to domain shift. This necessitates robust segmentation techniques that generalize well across diverse datasets. This paper addresses the challenge of domain generalization in medical image segmentation by proposing a novel feature augmentation strategy using learned semantic directions. We introduce a Semantic Direction Learning (SDL) module that identifies meaningful directions in the feature space corresponding to specific semantic variations (e.g., appearance, texture). These directions are then used to augment the original features, effectively expanding the feature distribution and improving the model's ability to handle unseen domains. Experiments on multi-organ segmentation across different modalities (CT and MRI) demonstrate that our method significantly outperforms existing domain generalization techniques, achieving state-of-the-art performance on unseen target datasets. The proposed approach offers a practical and effective solution for improving the robustness and generalizability of medical image segmentation models, reducing the need for extensive re-training on new data."
http://arxiv.org/abs/2507.23272v1,Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2,"Breast cancer diagnosis and treatment planning heavily rely on accurate tumor segmentation in 3D breast MRI. However, manual segmentation is time-consuming and expensive, while existing automated methods often require substantial computational resources or extensive training data. This paper addresses the challenge of achieving accurate and affordable 3D breast tumor segmentation and visualization. We propose SAM2, a novel framework leveraging the Segment Anything Model (SAM) with minimal fine-tuning and incorporating a slice-wise propagation strategy coupled with a 3D conditional random field (CRF) refinement. SAM is first adapted to 2D breast MRI slices with a small, focused dataset. The resulting 2D segmentations are then propagated through the 3D volume, and the 3D CRF refines the segmentation by enforcing spatial consistency. Our experimental results on a clinically relevant dataset demonstrate that SAM2 achieves competitive segmentation performance compared to fully supervised methods while significantly reducing training costs and inference time. This approach facilitates wider accessibility to accurate tumor segmentation and visualization, potentially improving patient care and clinical workflows in resource-constrained settings."
http://arxiv.org/abs/2507.23206v1,Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images,"Microscopic imaging is crucial for understanding the structure and properties of food crystals, impacting food quality and processing. Accurate identification and segmentation of these crystals, particularly in agglomerated forms, remains challenging due to their variable shapes, sizes, and often indistinct boundaries. This paper addresses the problem of accurately classifying and segmenting agglomerated food crystals in 2D microscopic images. We propose a novel confidence-aware agglomeration framework that integrates a deep learning-based segmentation network with a hierarchical agglomerative clustering algorithm. The segmentation network provides pixel-wise classification and confidence scores, which are then incorporated into a modified Ward linkage criterion within the agglomerative clustering process. This confidence-aware linkage prioritizes merging regions with high confidence scores and strong boundary affinities, effectively separating individual crystals within agglomerates. Experimental results on a dataset of sucrose and lactose crystal images demonstrate that our method achieves significant improvements in both classification accuracy and segmentation performance compared to traditional watershed and standard agglomerative clustering techniques, particularly in densely packed regions. This approach offers a robust and accurate tool for automated food crystal analysis, facilitating advancements in food science and engineering."
http://arxiv.org/abs/2507.23193v1,A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery,"Accurate and timely flood detection is crucial for disaster response and mitigation efforts, with satellite imagery offering a valuable data source. However, existing flood detection methods often struggle with the significant variability in satellite imagery caused by seasonal changes, such as vegetation growth, snow cover, and varying water levels, leading to poor generalization across different times of the year. To address this limitation, we introduce a novel, large-scale dataset for flood detection, termed FloodSeason, which comprises multi-spectral satellite imagery and corresponding flood annotations, explicitly designed to capture diverse seasonal variations. This dataset incorporates images from multiple geographic locations and time periods, including pre- and post-flood events, with detailed annotations generated through a combination of manual labeling and automated techniques leveraging historical flood records and hydrological models. Experiments using state-of-the-art semantic segmentation models trained and evaluated on FloodSeason demonstrate a significant improvement in flood detection accuracy and robustness compared to models trained on existing datasets, particularly when evaluated on images from unseen seasons. This dataset enables the development of more reliable and generalizable flood detection algorithms, ultimately contributing to more effective flood management strategies."
http://arxiv.org/abs/2507.23134v1,Details Matter for Indoor Open-vocabulary 3D Instance Segmentation,"Open-vocabulary 3D instance segmentation aims to identify and segment objects in a 3D scene based on free-form textual descriptions, offering flexibility beyond predefined category sets. Existing methods often struggle with fine-grained details and subtle differences between objects, leading to inaccurate segmentation boundaries and misclassification, especially in complex indoor environments. To address this, we introduce a novel framework that leverages both global scene context and fine-grained local details for improved open-vocabulary 3D instance segmentation. Our approach incorporates a multi-scale feature extraction module to capture both high-level semantic information and low-level geometric details, coupled with a contrastive learning strategy that encourages discrimination between instances based on their textual descriptions and visual features. Furthermore, we introduce a boundary refinement network to sharpen segmentation boundaries by explicitly modeling local geometric relationships. Experiments on ScanNet200 and Matterport3D datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches, achieving a relative improvement of 5-8% in mAP@0.5. This highlights the importance of detailed feature representation and boundary refinement for accurate open-vocabulary 3D scene understanding."
http://arxiv.org/abs/2507.22802v1,Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings,"Fetal ultrasound is a crucial tool for prenatal care, particularly in low-resource settings where access to advanced diagnostics is limited. However, the quality of ultrasound images acquired in these settings is often compromised due to factors like outdated equipment, inadequate training, and limited time, hindering accurate diagnoses. This paper addresses the critical need for an automated and reliable method for assessing fetal ultrasound image quality in such challenging environments. We propose a novel deep learning framework leveraging a multi-task convolutional neural network (CNN) trained on a limited dataset augmented with transfer learning from related medical imaging domains. The network simultaneously predicts multiple quality indicators, including blurriness, noise level, and anatomical completeness, providing a comprehensive quality score. Experimental results demonstrate that our method achieves state-of-the-art performance in predicting quality scores, significantly outperforming traditional image quality assessment metrics, and exhibits robustness even with noisy and incomplete ultrasound images. This automated quality assessment tool has the potential to improve diagnostic accuracy and patient outcomes in low-resource settings by identifying and flagging suboptimal images for re-acquisition or expert review."
http://arxiv.org/abs/2507.22792v2,Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future,"Video Object Segmentation (VOS) and Tracking (VOT) are fundamental tasks in computer vision, enabling a wide range of applications from autonomous driving to video editing. Despite significant progress, achieving robust and generalizable VOS and VOT remains challenging, particularly in dynamic scenes with occlusions, deformations, and complex object interactions. This paper presents a comprehensive review of VOS and VOT techniques, tracing their evolution from traditional approaches to the recent paradigm shift driven by the Segment Anything Model (SAM). We categorize existing methods based on their core methodologies, including propagation-based, matching-based, and learning-based approaches, with a specific focus on adaptations leveraging SAM's powerful zero-shot segmentation capabilities. Further, we analyze the strengths and weaknesses of different strategies for incorporating SAM, such as prompting mechanisms, fine-tuning techniques, and integration with temporal consistency models. Our review highlights the substantial performance gains achieved by SAM-based methods across various benchmarks, demonstrating improved accuracy and robustness, especially in challenging scenarios. This work provides valuable insights into the current state-of-the-art in VOS and VOT, identifying key research directions and highlighting the transformative potential of foundational models like SAM for advancing the field."
http://arxiv.org/abs/2507.22668v1,Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation,"3D scene segmentation is crucial for various applications, including autonomous driving and robotics. However, the scarcity of labeled 3D data and the inherent complexity of scene geometry pose significant challenges for training robust segmentation models. To address these limitations, we propose a novel Graph-Guided Dual-Level Augmentation (GGDLA) strategy that leverages both geometric relationships and semantic context to enhance data diversity and model generalization. GGDLA operates at two levels: a node-level augmentation that perturbs individual point features within a point cloud, and a graph-level augmentation that modifies the connectivity and structure of a graph representation derived from the scene. A graph convolutional network is used to guide the augmentation process by predicting the optimal perturbation parameters for each level, ensuring semantic consistency and preserving structural integrity. We evaluate GGDLA on several benchmark datasets, demonstrating significant improvements in segmentation accuracy compared to state-of-the-art augmentation techniques and baseline models. These results highlight the effectiveness of GGDLA in enhancing the robustness and accuracy of 3D scene segmentation models, paving the way for more reliable perception systems in real-world environments."
http://arxiv.org/abs/2507.22626v1,Bridging the Gap in Missing Modalities: Leveraging Knowledge Distillation and Style Matching for Brain Tumor Segmentation,"Multi-modal Magnetic Resonance Imaging (MRI) provides complementary information crucial for accurate brain tumor segmentation, yet acquiring all modalities is not always feasible due to time constraints, cost, or patient contraindications. This work addresses the challenging problem of segmenting brain tumors when one or more MRI modalities are missing during inference. We propose a novel framework that combines knowledge distillation and style matching to effectively bridge the gap caused by missing modalities. Specifically, we train a student network to mimic the feature representations of a privileged teacher network trained on complete multi-modal data, even when the student only has access to a subset of modalities. Furthermore, we introduce a style matching module that aligns the style of the available modalities in the student network with the corresponding styles in the teacher network, thereby compensating for the missing information. Experimental results on the BraTS 2018 and 2020 datasets demonstrate that our method significantly outperforms state-of-the-art approaches in handling missing modalities, achieving comparable performance to models trained with complete data. This approach offers a robust and practical solution for brain tumor segmentation in scenarios with incomplete MRI data, improving diagnostic accuracy and clinical applicability."
http://arxiv.org/abs/2507.22530v2,HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors,"Accurate and efficient segmentation of blood vessels in high-resolution video is crucial for diagnosing and monitoring various cardiovascular diseases. However, existing methods struggle to maintain both spatial coherence and fine-grained detail in video vasculature segmentation due to the computational challenges of processing high-resolution video data. To address this, we propose HRVVS, a High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors. HRVVS leverages a hierarchical architecture to progressively refine segmentation predictions, incorporating autoregressive residual priors at each level to enforce temporal consistency and capture subtle changes in vessel morphology across video frames. These priors are learned and applied in a residual manner, focusing on correcting errors and refining details rather than predicting the entire vasculature structure anew for each frame. Experiments on a newly collected high-resolution retinal video dataset demonstrate that HRVVS achieves superior segmentation accuracy and temporal stability compared to state-of-the-art video segmentation methods, particularly in challenging regions with low contrast and complex vessel structures. This advancement provides a robust and efficient tool for analyzing dynamic vasculature patterns in high-resolution video, facilitating improved diagnosis and treatment of cardiovascular conditions."
http://arxiv.org/abs/2507.22512v1,AlphaDent: A dataset for automated tooth pathology detection,"Automated dental image analysis holds immense potential for improving diagnostic accuracy and efficiency in dental practices. However, the lack of large, publicly available, and well-annotated datasets significantly hinders the development and evaluation of robust computer vision algorithms for tooth pathology detection. To address this gap, we introduce AlphaDent, a novel dataset comprising 5,000 panoramic dental radiographs meticulously annotated with bounding boxes for eight common tooth pathologies, including caries, periapical lesions, and impacted teeth. We further benchmark several state-of-the-art object detection models, including Faster R-CNN and YOLOv8, on AlphaDent, providing a comprehensive evaluation baseline. Our experiments reveal that YOLOv8 achieves the best overall performance with a mean Average Precision (mAP) of 0.72, demonstrating the feasibility of automated pathology detection using deep learning on our dataset. AlphaDent serves as a valuable resource for researchers and practitioners, fostering advancements in automated dental diagnostics and ultimately contributing to improved patient care."
http://arxiv.org/abs/2507.22477v2,LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks,"Multimodal data fusion, particularly leveraging LiDAR and vision, holds immense potential for precise structural crack segmentation, a critical task in infrastructure maintenance. However, effectively integrating the complementary information from these modalities while maintaining computational efficiency remains a significant challenge. We introduce LAC-Mamba, a Lightweight Adaptive Cue-Aware Fusion Vision Mamba network designed for accurate and efficient multimodal crack segmentation. LAC-Mamba employs a novel adaptive cue-aware fusion module that dynamically weights and integrates features from both LiDAR and vision modalities, emphasizing the most relevant information for crack detection. Furthermore, we integrate a Vision Mamba backbone to capture long-range dependencies within each modality, improving contextual understanding. Experimental results on a newly curated multimodal crack dataset demonstrate that LAC-Mamba achieves state-of-the-art segmentation performance with a significantly reduced computational cost compared to existing fusion methods. This lightweight and accurate approach facilitates real-time and resource-constrained deployment of crack segmentation systems for proactive infrastructure management."
http://arxiv.org/abs/2507.22418v1,Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching,"Accurate segmentation of medical images is crucial for computer-aided diagnosis, but inherent ambiguities and noise in the data often lead to uncertain predictions. Estimating aleatoric uncertainty, representing data-dependent noise, is vital for reliable clinical decision-making. We address the problem of effectively modeling and quantifying aleatoric uncertainty in medical image segmentation. We propose a novel framework leveraging Flow Matching to directly learn a continuous normalizing flow that maps the segmentation output to a noise distribution conditioned on the input image. This allows us to sample multiple plausible segmentations reflecting the inherent data uncertainty and estimate the aleatoric uncertainty through the variance of these samples. Experiments on cardiac and brain MRI datasets demonstrate that our method achieves superior aleatoric uncertainty estimation compared to existing approaches, as evidenced by improved calibration metrics and the ability to identify ambiguous regions within the images. Our approach provides a robust and efficient way to quantify data-dependent uncertainty in medical image segmentation, paving the way for more reliable and trustworthy clinical applications of deep learning."
http://arxiv.org/abs/2507.22412v1,UAVScenes: A Multi-Modal Dataset for UAVs,"Unmanned Aerial Vehicles (UAVs) are increasingly utilized for a wide range of applications, from infrastructure inspection to environmental monitoring, necessitating robust perception capabilities. However, the development of effective computer vision algorithms for UAVs is hindered by the scarcity of large-scale, multi-modal datasets capturing the complexities of real-world UAV deployments. To address this limitation, we introduce UAVScenes, a novel multi-modal dataset specifically designed for UAV-based perception research. UAVScenes comprises synchronized RGB imagery, LiDAR point clouds, thermal infrared data, and high-precision GPS/IMU data collected from diverse urban and rural environments. The dataset includes extensive annotations for semantic segmentation, object detection, and instance segmentation tasks, along with precise camera pose information. Our initial experiments demonstrate the dataset's utility in training and evaluating state-of-the-art deep learning models for various perception tasks, revealing significant performance improvements when leveraging the multi-modal data. UAVScenes provides a valuable resource for advancing research in robust and reliable computer vision algorithms for UAVs, facilitating the development of safer and more autonomous aerial systems."
http://arxiv.org/abs/2508.03724v1,From Waveforms to Pixels: A Survey on Audio-Visual Segmentation,"Audio-visual segmentation aims to identify and segment the visual elements in a scene that correspond to specific sound events, bridging the gap between auditory and visual perception. While significant progress has been made in both audio and visual segmentation individually, the challenge of accurately aligning and integrating these modalities remains a critical bottleneck for scene understanding. This survey provides a comprehensive overview of existing audio-visual segmentation techniques, categorizing them based on their architectural designs, loss functions, and integration strategies. We analyze the strengths and weaknesses of various approaches, including early fusion, late fusion, and attention-based mechanisms, highlighting their performance across diverse datasets and acoustic environments. Furthermore, we identify key challenges in the field, such as handling complex acoustic mixtures, limited training data, and real-time processing constraints. By consolidating current research and pinpointing future directions, this survey serves as a valuable resource for researchers and practitioners seeking to develop more robust and effective audio-visual segmentation systems, ultimately advancing applications in robotics, surveillance, and multimedia analysis."
http://arxiv.org/abs/2508.00917v1,A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles,"Connected Autonomous Vehicles (CAVs) rely on a complex interplay of perception, planning, and control tasks to navigate dynamic environments safely and efficiently. Deep Multi-Task Learning (MTL) offers a promising paradigm for CAVs by enabling the simultaneous learning of multiple related tasks, leveraging shared representations and improving generalization. This survey addresses the critical need for a comprehensive overview of the current landscape of deep MTL techniques applied to various aspects of CAV development, from environment perception to decision-making. We systematically categorize and analyze existing deep MTL approaches based on their task relationships, network architectures (e.g., hard parameter sharing, soft parameter sharing, attention mechanisms), and loss weighting strategies. Furthermore, we examine the application of these methods to specific CAV tasks, including object detection, semantic segmentation, lane keeping, trajectory prediction, and end-to-end autonomous driving. Our analysis highlights the benefits of MTL in improving individual task performance, reducing model size, and enhancing robustness to noisy or incomplete data, while also identifying open challenges such as task interference and the difficulty of optimizing complex multi-objective loss functions. This survey provides a valuable resource for researchers and practitioners seeking to leverage the power of deep MTL for advancing the capabilities of connected autonomous vehicles."
http://arxiv.org/abs/2507.22953v1,CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body Anatomy in Computed Tomography,"Accurate and detailed anatomical segmentation in Computed Tomography (CT) scans is crucial for various clinical applications, including computer-aided diagnosis, surgical planning, and radiation therapy. However, the lack of a large-scale, comprehensive, and publicly available dataset with detailed anatomical annotations hinders the development and validation of robust whole-body segmentation algorithms. To address this gap, we present CADS, a Comprehensive Anatomical Dataset and Segmentation for whole-body anatomy in CT. CADS comprises 200 contrast-enhanced CT scans covering a diverse range of anatomies and pathologies, meticulously annotated with 134 distinct anatomical structures by expert radiologists. We further provide a baseline segmentation model based on a 3D U-Net architecture trained on CADS, demonstrating its utility for anatomical segmentation tasks. Experimental results show that our baseline model achieves a mean Dice score of 0.82 across all anatomical structures, demonstrating the dataset's high quality and suitability for training deep learning models. CADS will serve as a valuable resource for the computer vision and medical imaging communities, facilitating advancements in automated anatomical segmentation and enabling downstream clinical applications."
http://arxiv.org/abs/2508.00913v1,TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras,"Event cameras offer advantages over traditional cameras in high-speed and high dynamic range scenarios, yet their adoption is hindered by a lack of large-scale labeled datasets. This work addresses the problem of effectively leveraging unlabeled event data for pretraining robust event-based vision models. We introduce TESPEC, Temporally-Enhanced Self-Supervised Pretraining for Event Cameras, which capitalizes on the inherent temporal structure of event streams. TESPEC incorporates two novel self-supervisory tasks: temporal event order prediction and event-rate consistency learning. The former encourages the model to understand the sequential nature of events, while the latter enforces consistency between predicted event rates and observed event counts across different time windows. Extensive experiments demonstrate that TESPEC significantly improves performance on downstream tasks such as object recognition, action recognition, and optical flow estimation, outperforming state-of-the-art self-supervised pretraining methods for event cameras by a significant margin. TESPEC offers a promising approach to unlocking the potential of event cameras by enabling effective learning from readily available unlabeled data."
http://arxiv.org/abs/2507.22061v1,MOVE: Motion-Guided Few-Shot Video Object Segmentation,"Few-shot video object segmentation (VOS) aims to segment a target object in a video given only a few annotated frames as support. Current methods often struggle to effectively propagate information from the support frames, especially when faced with significant appearance changes or fast motions of the target object. To address this, we propose MOVE: a Motion-Guided fEw-shot Video Object Segmentation network. MOVE leverages optical flow to explicitly model motion between frames, guiding the segmentation process. Specifically, we introduce a motion-aware feature warping module that aligns features across frames based on estimated optical flow, and a motion-guided attention mechanism to focus on relevant regions within the warped features. Experiments on standard benchmarks, including DAVIS 2017 and MOSE, demonstrate that MOVE achieves state-of-the-art performance, particularly in scenarios with challenging object motion. This highlights the importance of explicit motion modeling for robust and accurate few-shot video object segmentation."
http://arxiv.org/abs/2507.22052v1,Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos,"Reconstructing textured 3D models from RGB videos is a fundamental problem in computer vision, traditionally limited to pre-defined object categories. This work addresses the challenging task of open-vocabulary semantic 3D reconstruction, where the goal is to build 3D models and label their surfaces with free-form text descriptions directly from video inputs. We introduce Ov3R, a novel framework that leverages recent advances in neural radiance fields (NeRFs) and vision-language models (VLMs). Ov3R first reconstructs a scene using a NeRF, then employs a VLM to generate semantic embeddings for rendered views based on user-provided text prompts. Finally, these embeddings are projected back into the 3D scene to create a semantically-aware 3D representation. Experiments demonstrate that Ov3R can effectively reconstruct 3D scenes and accurately associate user-defined textual descriptions with corresponding regions, even for objects and concepts unseen during training. This opens up new avenues for interactive and semantically-rich 3D scene understanding and manipulation."
http://arxiv.org/abs/2507.22030v1,ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports,"Accurate segmentation of pulmonary findings in chest computed tomography (CT) is crucial for computer-aided diagnosis and treatment planning. However, existing datasets often lack detailed annotations or the ability to directly link imaging findings to corresponding textual descriptions in radiology reports. To address this gap, we introduce ReXGroundingCT, a novel 3D chest CT dataset that pairs volumetric CT scans with free-text radiology reports and fine-grained segmentation masks grounded in textual mentions. Our dataset consists of 250 meticulously annotated chest CT volumes covering a diverse range of pulmonary diseases. We leverage a radiologist-in-the-loop annotation pipeline to ensure high-quality segmentation and accurate text-image grounding. We demonstrate the utility of ReXGroundingCT by training and evaluating several state-of-the-art segmentation models, showing that incorporating text information significantly improves segmentation performance, especially for subtle and complex findings. ReXGroundingCT enables the development of advanced models for text-guided 3D medical image segmentation and promotes research in the intersection of medical imaging and natural language processing."
http://arxiv.org/abs/2507.22020v1,XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation,"Explainable AI (XAI) is crucial for building trust and understanding in machine learning models, especially in safety-critical applications involving point cloud data. However, applying XAI techniques to point clouds remains challenging due to their irregular structure and the difficulty in defining meaningful feature importance. This paper addresses the problem of generating faithful and interpretable explanations for point cloud classification models. We propose a novel perturbation-based XAI method that leverages meaningful semantic segmentation of the point cloud to guide the perturbation process. Our approach strategically removes or modifies semantically coherent segments of the point cloud, allowing us to assess the impact of these high-level features on the model's prediction. Experimental results on benchmark datasets demonstrate that our method generates more focused and interpretable explanations compared to existing point cloud XAI techniques, while maintaining high fidelity to the original model's behavior. This research advances the development of reliable and transparent AI systems for processing 3D geometric data."
http://arxiv.org/abs/2507.22002v1,Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation,"Accurate segmentation of industrial toxic emissions in real-world imagery is crucial for environmental monitoring and safety regulation. However, obtaining large, pixel-wise labeled datasets for training robust segmentation models is often impractical and expensive. We address this challenge by proposing a novel human-in-the-loop weakly-supervised framework that leverages both synthetic and real-world data for improved industrial toxic emission segmentation. Our method utilizes a CycleGAN to translate synthetic images with perfect segmentation masks to the real-world domain, followed by a human-in-the-loop process to refine bounding box annotations on real-world images, generating pseudo-masks. These pseudo-masks, combined with synthetically generated data, are then used to train a segmentation network. Experiments demonstrate that our framework significantly improves segmentation accuracy compared to models trained solely on synthetic or weakly-labeled real-world data, achieving a mean Intersection over Union (mIoU) increase of 15% on a real-world industrial emission dataset. This approach offers a practical and cost-effective solution for accurate and scalable toxic emission segmentation in real-world environments."
http://arxiv.org/abs/2507.21971v1,EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation,"Semantic segmentation is a fundamental task in computer vision, yet its performance degrades significantly under challenging conditions such as low illumination or rapid motion. Event cameras, with their high temporal resolution and insensitivity to motion blur, offer complementary information to traditional frame-based cameras. This paper addresses the problem of effectively fusing event and image data for robust semantic segmentation in adverse conditions. We propose EIFNet, an Event-Image Fusion Network that leverages a novel attention-guided fusion module to adaptively integrate event and image features at multiple scales. Specifically, EIFNet employs a dual-branch architecture to process event and image data separately, followed by our attention-guided fusion module that learns to weigh the contribution of each modality based on the local context. Extensive experiments on challenging synthetic and real-world datasets demonstrate that EIFNet significantly outperforms existing state-of-the-art methods, achieving higher segmentation accuracy and robustness, particularly in scenarios with low light and fast motion. Our work highlights the potential of event-image fusion for creating more reliable and versatile semantic segmentation systems."
http://arxiv.org/abs/2508.04229v1,Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction,"Pedestrian trajectory prediction is a crucial task for autonomous driving and socially aware robots, requiring accurate forecasting of future movements based on past observations and scene context. Existing methods often struggle to generate diverse and realistic multimodal predictions, particularly when inferring pedestrian intentions from limited data. To address this, we propose an Intention Enhanced Diffusion Model (IEDM) for multimodal pedestrian trajectory prediction. IEDM leverages a diffusion probabilistic model conditioned on observed trajectories and scene context to generate diverse trajectory samples. Crucially, we introduce an intention inference module that estimates the pedestrian's goal based on their past trajectory and incorporates this inferred intention as an additional conditioning signal within the diffusion process. This allows the model to generate trajectories that are more consistent with the inferred intention, leading to improved accuracy and realism. Experiments on benchmark datasets demonstrate that IEDM achieves state-of-the-art performance in terms of prediction accuracy, diversity, and adherence to social norms, particularly in scenarios with ambiguous intentions. This work offers a significant advancement in pedestrian trajectory prediction by explicitly modeling and incorporating pedestrian intentions into a diffusion-based framework."
http://arxiv.org/abs/2508.03695v1,Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition,"Action recognition in video remains a challenging task, particularly when only a limited number of labeled examples are available for novel actions. The core difficulty lies in effectively capturing the spatiotemporal dynamics and subtle inter-object relationships crucial for distinguishing between actions from few examples. To address this, we introduce Trokens: Semantic-Aware Relational Trajectory Tokens, a novel approach for few-shot action recognition. Our method first extracts object trajectories and their semantic labels. Then, we construct relational trajectory tokens by encoding the dynamic relationships between semantically similar objects, capturing both individual motion patterns and their interactions. These tokens are finally used to train a meta-learning framework, enabling effective generalization to unseen action classes with limited data. Experiments on the Kinetics-Few-Shot and Something-Something V2 datasets demonstrate that Trokens significantly outperforms existing state-of-the-art few-shot action recognition methods, achieving substantial gains in accuracy. This highlights the effectiveness of semantic-aware relational trajectory modeling for robust action recognition in data-scarce scenarios."
http://arxiv.org/abs/2508.03609v1,evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition,"Event-based cameras offer advantages over traditional cameras in high-speed and high dynamic range scenarios, making them potentially suitable for robust facial expression recognition (FER). However, the scarcity of labeled event-based facial expression datasets hinders the development of accurate and generalizable event-based FER systems. This paper introduces evTransFER, a novel transfer learning framework specifically designed to address this data scarcity challenge. evTransFER leverages knowledge learned from large-scale, readily available image-based FER datasets to improve the performance of event-based FER models. Our framework employs a two-stage transfer learning strategy: first, pre-training a convolutional neural network (CNN) on image-based FER data and adapting its architecture to process event data; second, fine-tuning the adapted network on a small event-based FER dataset using a novel temporal consistency loss to encourage smooth expression transitions. Experimental results on the DVS-FACES dataset demonstrate that evTransFER significantly outperforms existing event-based FER methods, achieving state-of-the-art accuracy and robustness to noise. This work offers a practical solution for building accurate event-based FER systems despite limited labeled event data, paving the way for more robust and efficient expression recognition in challenging real-world conditions."
http://arxiv.org/abs/2508.03542v1,Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences,"Mathematical communication often relies on complex notation best represented using LaTeX. Converting spoken mathematics into LaTeX is a challenging task due to the inherent ambiguity of spoken language and the intricacies of mathematical syntax. This paper addresses the problem of accurately transcribing spoken mathematical equations and sentences into their corresponding LaTeX representations. We introduce novel sequence-to-sequence models incorporating attention mechanisms and transformer architectures, specifically tailored for the speech-to-LaTeX task. Furthermore, we present a newly curated dataset, Speech-LaTeX-1K, comprising 1000 hours of diverse spoken mathematical expressions paired with their LaTeX ground truth, significantly expanding the available resources for training and evaluating speech-to-LaTeX systems. Our experimental results demonstrate that the proposed models, trained on Speech-LaTeX-1K, achieve state-of-the-art performance, surpassing existing methods by a significant margin in terms of BLEU score and expression accuracy. This work provides a valuable contribution to the field, enabling more accessible and efficient mathematical communication for individuals and automated systems."
http://arxiv.org/abs/2508.03266v2,EgoPrompt: Prompt Learning for Egocentric Action Recognition,"Egocentric action recognition aims to understand human activities from a first-person perspective, crucial for applications like assistive technology and augmented reality. However, the inherent diversity and subtle nuances within egocentric video data pose significant challenges for robust action recognition. To address this, we introduce EgoPrompt, a novel prompt learning framework specifically designed for egocentric action recognition. EgoPrompt leverages learnable prompts injected into a pre-trained vision transformer to adapt its representations to the specific characteristics of egocentric data. The prompts effectively guide the model to focus on relevant spatio-temporal features, enabling better discrimination between similar actions. Extensive experiments on multiple benchmark datasets, including EPIC-Kitchens-100 and Ego4D, demonstrate that EgoPrompt consistently outperforms state-of-the-art methods, achieving significant improvements in recognition accuracy while maintaining computational efficiency. This highlights the effectiveness of prompt learning in adapting pre-trained models for the complexities of egocentric action recognition."
http://arxiv.org/abs/2508.03039v1,VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering,"Cross-video question answering (Cross-VR) requires reasoning about information distributed across multiple videos to answer complex queries. Existing approaches often struggle with identifying and integrating relevant information from different videos due to the lack of explicit inter-video relationships and the difficulty of tracking persons across videos. This paper addresses the problem of effectively reasoning about person-centric interactions spread across multiple videos to answer questions requiring cross-video inference. We propose VideoForest, a novel person-anchored hierarchical reasoning framework. VideoForest constructs a hierarchical graph structure, where nodes represent person instances in each video and edges capture relationships both within and across videos based on spatio-temporal proximity and semantic similarity. A novel graph neural network then propagates information within this structure, performing hierarchical reasoning from individual person instances to higher-level concepts and eventually aggregating evidence to answer the question. Experiments on the CrossTask dataset demonstrate that VideoForest significantly outperforms existing methods, achieving state-of-the-art results with a substantial margin. VideoForest provides a more interpretable and effective approach to cross-video reasoning by explicitly modeling person-centric relationships and enabling hierarchical information aggregation."
http://arxiv.org/abs/2508.02981v1,MoExDA: Domain Adaptation for Edge-based Action Recognition,"Edge-based action recognition offers a computationally efficient alternative to traditional RGB-based methods, particularly suitable for resource-constrained devices. However, performance often degrades significantly when deploying edge models trained on labeled source datasets to new, unlabeled target domains due to domain shift. This paper introduces MoExDA, a novel Mixture-of-Experts based Domain Adaptation framework for edge-based action recognition. MoExDA leverages a feature extractor and multiple expert classifiers, each specializing in a specific feature subspace. A domain discriminator guides the feature extractor to learn domain-invariant edge representations, while a mixture-of-experts gating network dynamically selects the most appropriate expert classifier for each sample, enabling effective adaptation to the target domain. Experiments on benchmark datasets demonstrate that MoExDA achieves state-of-the-art domain adaptation performance for edge-based action recognition, outperforming existing methods by a significant margin. This work provides a practical and effective approach to bridge the domain gap, facilitating the deployment of robust edge-based action recognition systems in real-world scenarios."
http://arxiv.org/abs/2508.02978v1,Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning,"Multi-domain learning aims to leverage knowledge from multiple related domains to improve generalization performance on each individual domain. However, naively sharing parameters across domains can lead to negative transfer due to domain-specific characteristics. This paper addresses the challenge of effectively separating shared knowledge from domain-specific adaptations in multi-domain learning scenarios. We propose a novel approach that utilizes Low-Rank Adaptation (LoRA) to decompose model updates into shared and domain-specific components. Specifically, we introduce a Shared-LoRA module that captures common knowledge across domains and a Domain-Specific LoRA module for each domain to learn specialized adaptations. Our approach encourages disentanglement through a regularization term that minimizes the redundancy between the shared and domain-specific LoRA modules. Experiments on benchmark datasets demonstrate that our method achieves significant performance gains compared to existing multi-domain learning techniques, particularly in settings with high domain divergence, showcasing improved accuracy and faster convergence. By effectively disentangling shared and domain-specific knowledge, our approach offers a more robust and efficient solution for multi-domain learning."
http://arxiv.org/abs/2508.02329v1,CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions,"Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable zero-shot transfer capabilities across various vision tasks; however, its performance often lags behind task-specific models, particularly in fine-grained visual understanding requiring nuanced distinctions. A critical limitation lies in the relatively generic image captions used during pre-training, hindering the model's ability to learn subtle visual details. To address this, we introduce CLIP-IN, a novel training paradigm that leverages instruction editing and long captions to enhance fine-grained visual understanding in CLIP. Specifically, we curate a dataset of images paired with both detailed captions and targeted instruction edits that highlight specific attributes or relationships within the image. We then employ a multi-stage training process, first fine-tuning CLIP on the long captions to capture fine-grained details, followed by instruction-guided contrastive learning to improve the model's ability to discern subtle differences based on textual instructions. Experiments on fine-grained image classification benchmarks, including CUB and Stanford Cars, demonstrate significant improvements over the original CLIP model and strong performance compared to other fine-tuning strategies. This highlights the effectiveness of instruction editing and long captions in enabling CLIP to learn more nuanced and discriminative visual representations."
http://arxiv.org/abs/2508.02113v1,DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens Flare Removal,"Lens flare, caused by non-image-forming light scattering within a lens system, significantly degrades image quality and hinders downstream vision tasks. Existing deep learning methods for lens flare removal often struggle to maintain contextual consistency, leading to artifacts and unnatural-looking results, particularly in complex scenes with varying flare types and intensities. To address this, we propose DeflareMamba, a novel hierarchical vision Mamba architecture specifically designed for contextually consistent lens flare removal. DeflareMamba leverages the selective state space (S6) mechanism of Mamba blocks to capture long-range dependencies and global image context at multiple scales within a U-Net-like architecture. This hierarchical processing allows the network to effectively model the complex interactions between the flare, the underlying scene content, and other image artifacts. Experimental results on synthetic and real-world datasets demonstrate that DeflareMamba significantly outperforms state-of-the-art methods in terms of both quantitative metrics (PSNR, SSIM) and qualitative visual assessment, generating flare-free images with improved contextual fidelity and reduced artifacts. Our approach provides a powerful framework for robust and reliable lens flare removal, enhancing the performance of various computer vision applications."
http://arxiv.org/abs/2508.03754v1,Generating Synthetic Invoices via Layout-Preserving Content Replacement,"Invoice processing automation relies heavily on robust document understanding models, but the limited availability of labeled invoice data poses a significant bottleneck. Generating realistic synthetic invoices can alleviate this data scarcity, but naive approaches often struggle to maintain the intricate layout and semantic coherence crucial for training effective models. This paper addresses the challenge of generating high-quality synthetic invoices by proposing a novel Layout-Preserving Content Replacement (LPCR) framework. LPCR leverages a two-stage approach: first, it identifies and segments key regions within real invoice templates, and then, it replaces the original content within these regions with synthetically generated or extracted data while strictly adhering to the established layout constraints. Experiments demonstrate that models trained on synthetic invoices generated by LPCR achieve significantly higher performance on real-world invoice datasets compared to models trained on invoices generated with simpler augmentation techniques. This indicates that LPCR produces more realistic and effective synthetic data, thereby improving the performance of invoice processing systems."
http://arxiv.org/abs/2508.02047v1,Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark Revealing Vision-Language Model Limitations,"Street-level imagery provides a rich source of data for understanding urban environments, with traffic sign recognition being a crucial application for autonomous driving and intelligent transportation systems. However, the performance of existing traffic sign recognition systems, particularly for fine-grained categorization, is often limited by the quality and biases present in training datasets. This paper addresses the problem of validating the Mapillary Vistas dataset, a widely used resource for traffic sign understanding, specifically focusing on its suitability for training models capable of distinguishing subtle differences between visually similar signs. We propose a benchmark utilizing state-of-the-art Vision-Language Models (VLMs), specifically CLIP, to assess the dataset's label consistency and identify instances where visual features poorly correlate with assigned categories. We further introduce a novel approach leveraging large language models to generate contextualized descriptions for each sign, enabling a more nuanced comparison between image content and textual labels. Our experiments reveal significant inconsistencies within the Mapillary Vistas dataset, particularly for fine-grained categories, leading to a substantial drop in VLM performance compared to idealized synthetic data. These findings highlight the limitations of relying solely on existing datasets for training robust and reliable traffic sign recognition systems and underscore the need for more rigorous data validation and annotation strategies."
http://arxiv.org/abs/2508.02034v1,Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure,"Face recognition technology, while offering convenience, poses a significant threat to individual privacy due to its potential for pervasive digital footprint exposure. Existing privacy protection methods often struggle with variations in pose and illumination, or introduce perceptible artifacts that hinder usability and adoption. This paper introduces Protego, a novel user-centric framework for pose-invariant privacy protection against face recognition systems. Protego leverages a generative adversarial network (GAN) conditioned on estimated 3D facial landmarks to synthesize imperceptible, pose-consistent perturbations directly on face images. These perturbations, optimized via an adversarial loss against a state-of-the-art face recognition model and a perceptual loss to maintain image quality, effectively disrupt recognition while preserving visual fidelity across a wide range of head poses. Experiments on benchmark datasets demonstrate that Protego significantly reduces face recognition accuracy by over 90% across varying poses, outperforming existing methods in both protection efficacy and perceptual quality, as evaluated by standard image quality metrics and user studies. Protego offers a practical and robust solution for individuals seeking to control their digital footprint in an increasingly face recognition-driven world."
http://arxiv.org/abs/2508.01860v1,Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction,"Understanding user intent behind search queries is crucial for improving search engine performance and user experience. While explicit queries provide some information, identifying the implicit, underlying search intent remains a significant challenge. This paper addresses the problem of recognizing implicit search intent by leveraging electroencephalography (EEG) and eye-tracking data recorded during search tasks. We introduce a novel multimodal dataset comprising EEG and eye-tracking signals from multiple participants performing searches with varying implicit intents for the same explicit query. We propose a cross-user prediction framework that utilizes a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn shared representations of brain activity and gaze patterns associated with different implicit intents. Our results demonstrate that the proposed framework achieves significant accuracy in predicting implicit search intent across different users, outperforming baseline methods that rely on single modalities or user-specific models. This work demonstrates the potential of using neurophysiological signals and eye movements for inferring implicit search intent, paving the way for more personalized and effective search experiences."
http://arxiv.org/abs/2508.01791v1,CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase,"Continuous Sign Language Recognition (CSLR) aims to automatically translate sign language videos into text, offering a crucial communication bridge for the deaf community. However, the performance of CSLR systems, particularly for low-resource languages like Arabic Sign Language (ArSL), is hampered by limited data availability and complex linguistic structures. This paper addresses the challenge of improving CSLR performance on the Isharah dataset by leveraging a data-centric approach within a Conformer architecture. We introduce CSLRConformer, which incorporates data augmentation techniques specifically tailored for sign language, alongside a modified Conformer block optimized for capturing both local and global dependencies in ArSL videos. Furthermore, we explore the impact of pre-training strategies using synthetic data to enhance the model's generalization capabilities. Our experiments demonstrate that CSLRConformer achieves state-of-the-art results on the Isharah dataset, surpassing existing methods by a significant margin in terms of word error rate (WER). This work highlights the effectiveness of data-centric approaches and refined Conformer architectures for addressing the challenges of CSLR in low-resource settings, paving the way for more accessible and accurate sign language translation systems."
http://arxiv.org/abs/2508.01644v1,DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition,"Multimodal emotion recognition aims to infer human affective states by integrating information from various modalities, such as text, audio, and video. However, effectively capturing modality-specific nuances while simultaneously fusing them to achieve comprehensive emotion understanding remains a significant challenge. This paper introduces DRKF, a novel approach called Decoupled Representations with Knowledge Fusion for multimodal emotion recognition. DRKF first learns decoupled representations for each modality using contrastive learning, ensuring that modality-specific information is preserved and redundancy is minimized. Subsequently, a knowledge fusion module leverages attention mechanisms and external knowledge graphs to selectively aggregate information from different modalities, guided by emotion-related prior knowledge. Experiments on benchmark datasets (e.g., IEMOCAP, CMU-MOSEI) demonstrate that DRKF achieves state-of-the-art performance, outperforming existing methods by a significant margin (e.g., 3% absolute improvement on IEMOCAP accuracy). DRKFs ability to effectively decouple and fuse multimodal information offers a promising direction for advancing the field of emotion AI."
http://arxiv.org/abs/2508.01569v1,LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning,"Vision Transformers (ViTs) have achieved remarkable performance in various computer vision tasks, but their susceptibility to memorizing sensitive or outdated information raises significant privacy and ethical concerns. Selective machine unlearning, the ability to selectively remove the influence of specific data instances from a trained model, is crucial for addressing these concerns in ViTs. However, existing unlearning methods often struggle with the complex attention mechanisms and high dimensionality of ViTs, leading to suboptimal unlearning performance and potential degradation of model utility. We propose LetheViT, a novel attention-guided contrastive learning framework for selective machine unlearning in ViTs. LetheViT leverages the attention maps within ViT layers to identify and target the parameters most influenced by the data to be unlearned. A contrastive loss is then applied to these parameters, pushing the model towards a state where the influence of the target data is minimized while preserving the knowledge learned from the remaining data. Experimental results on image classification datasets demonstrate that LetheViT achieves superior unlearning performance compared to existing methods, effectively removing the influence of target data with minimal impact on the model's accuracy on the retained data. This work provides a practical and effective approach for mitigating privacy risks and ensuring responsible development of ViT-based applications."
http://arxiv.org/abs/2508.01558v1,EvoVLMA: Evolutionary Vision-Language Model Adaptation,"Vision-Language Models (VLMs) have shown remarkable capabilities in various multimodal tasks. However, adapting pre-trained VLMs to new, specialized downstream tasks often requires substantial computational resources and manual hyperparameter tuning. To address this challenge, we introduce EvoVLMA, an Evolutionary Vision-Language Model Adaptation framework that automates the process of finding optimal fine-tuning strategies. EvoVLMA employs a genetic algorithm to explore a search space of hyperparameters, including learning rates, weight decay, layer-wise learning rate multipliers, and data augmentation policies. The algorithm iteratively evaluates the performance of VLMs fine-tuned with different hyperparameter configurations on a validation set, evolving the population towards configurations that yield higher accuracy. Experiments on several benchmark datasets, including visual question answering and image captioning, demonstrate that EvoVLMA consistently identifies fine-tuning strategies that outperform both default settings and manually optimized configurations, achieving up to a 5% improvement in accuracy. This automated approach significantly reduces the human effort and computational cost associated with adapting VLMs to specific downstream tasks, making them more accessible and efficient to deploy."
http://arxiv.org/abs/2508.01389v2,Open-Attribute Recognition for Person Retrieval: Finding People Through Distinctive and Novel Attributes,"Person retrieval aims to identify individuals across different camera views, typically relying on visual appearance. However, relying solely on visual features limits performance when faced with occlusions, pose variations, or changes in clothing. This paper addresses the challenging problem of open-attribute recognition for person retrieval, where the goal is to retrieve individuals based on free-form natural language descriptions containing potentially novel or unseen attributes. We propose a novel Attribute-Aware Cross-Modal Embedding (AACME) framework that leverages pre-trained language models to encode descriptive attributes and learns to align them with visual features extracted from person images. AACME incorporates an attribute attention mechanism to focus on relevant visual regions corresponding to the described attributes, while simultaneously learning a shared embedding space where visual and textual representations are directly comparable. Experimental results on benchmark datasets demonstrate that our approach significantly outperforms existing methods, especially in retrieving individuals based on novel and distinctive attribute combinations. This work opens new avenues for leveraging rich semantic information for more robust and flexible person retrieval systems."
http://arxiv.org/abs/2508.01387v1,"Video-based Vehicle Surveillance in the Wild: License Plate, Make, and Model Recognition with Self Reflective Vision-Language Models","Video-based vehicle surveillance is crucial for various applications, including traffic management, law enforcement, and urban planning. However, accurately recognizing vehicle attributes like license plate number (LPN), make, and model in unconstrained ""in the wild"" video data remains a significant challenge due to variations in lighting, viewpoint, occlusion, and data scarcity. This paper proposes a novel approach leveraging Self-Reflective Vision-Language Models (SR-VLMs) to address this challenge. Our method employs a two-stage process: first, a robust object detector identifies vehicles within the video frames; second, the SR-VLM, fine-tuned on a large, diverse dataset of vehicle images and captions, simultaneously predicts the LPN, make, and model. Crucially, the self-reflective component allows the model to evaluate its own predictions and refine them based on contextual information and visual cues, improving accuracy and robustness. Experiments on a challenging real-world dataset demonstrate significant improvements in LPN recognition accuracy (15% relative improvement) and make/model classification accuracy (10% relative improvement) compared to state-of-the-art methods. This work provides a practical and effective solution for enhanced vehicle surveillance in real-world scenarios, enabling more comprehensive and reliable vehicle attribute extraction from video data."
http://arxiv.org/abs/2508.01227v2,Enhancing Multi-view Open-set Learning via Ambiguity Uncertainty Calibration and View-wise Debiasing,"Multi-view learning leverages information from multiple perspectives to improve model generalization, but its effectiveness is challenged by the open-set recognition problem, where novel, unseen classes exist in the test data. This paper addresses the critical challenge of accurately identifying unknown samples in multi-view open-set recognition, particularly when faced with ambiguous samples and biased view contributions. We propose a novel approach, Ambiguity Uncertainty Calibration and View-wise Debiasing (AUC-VD), which first estimates the ambiguity uncertainty of each sample based on the disagreement between view predictions and then calibrates the overall uncertainty score. Subsequently, we introduce a view-wise debiasing module that dynamically adjusts the contribution of each view based on its reliability in open-set identification, mitigating the impact of biased or noisy views. Extensive experiments on multiple benchmark datasets demonstrate that AUC-VD significantly outperforms state-of-the-art multi-view open-set recognition methods, achieving improvements of up to 8% in overall accuracy and unknown identification. This work provides a robust and effective framework for handling the complexities of multi-view open-set recognition, paving the way for more reliable real-world applications."
http://arxiv.org/abs/2508.01153v1,TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition,"Scene Text Recognition (STR) aims to transcribe text appearing in natural images, a challenging task due to variations in fonts, styles, and distortions. Existing methods often struggle with complex and unseen text patterns, particularly those with limited training data. This paper addresses the problem of improving STR accuracy and generalization by leveraging text encoding as curriculum hints. We propose TEACH, a novel framework that incorporates text embeddings derived from large language models (LLMs) to guide the STR model's training process. Specifically, we use LLM-generated text embeddings as auxiliary input, providing rich semantic information about the target text during training, and design a curriculum learning strategy that gradually increases the reliance on image features while reducing the influence of text hints. Experiments on benchmark datasets demonstrate that TEACH significantly outperforms state-of-the-art STR methods, achieving substantial improvements in accuracy, especially on challenging datasets with irregular text. These results highlight the potential of leveraging LLM-derived text encoding as curriculum hints to enhance the robustness and generalization ability of scene text recognition models."
http://arxiv.org/abs/2508.00391v1,Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition,"Cued Speech (CS) is a visual communication system that enhances speechreading by using handshapes and locations near the face to disambiguate visually similar phonemes. Automatic Cued Speech Recognition (ACSR) remains a challenging task, hampered by the complexity of visual cues and the need for robust speaker-independent models. This paper addresses the ACSR problem by introducing Cued-Agent, a novel collaborative multi-agent system. Cued-Agent comprises multiple specialized agents, each focusing on a specific aspect of CS recognition: a lip-reading agent, a handshape agent, and a location agent. These agents leverage deep learning architectures, including transformers and convolutional neural networks, and communicate via an attention mechanism to dynamically weigh their respective contributions based on the input sequence. Experimental results on a publicly available CS dataset demonstrate that Cued-Agent significantly outperforms existing state-of-the-art ACSR systems, achieving a substantial reduction in phoneme error rate. This collaborative multi-agent approach offers a promising direction for improving the accuracy and robustness of ACSR, paving the way for more accessible communication technologies."
http://arxiv.org/abs/2508.00311v1,DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios,"Mathematical formula recognition is a crucial component in understanding scientific documents and enabling advanced document analysis. However, existing formula recognition systems often struggle with complex layouts, inline formulas intertwined with text, and variations in notation, leading to performance degradation in real-world scenarios. This paper introduces DocTron-Formula, a novel end-to-end framework designed for generalized formula recognition within complex and structured document layouts. Our approach leverages a transformer-based architecture, incorporating a bi-directional attention mechanism to capture both visual and contextual relationships between formula components and surrounding text. We further integrate a novel symbol embedding module trained with contrastive learning to improve the robustness of symbol classification, particularly for rare or ambiguous symbols. Experimental results on a diverse dataset of scientific documents demonstrate that DocTron-Formula significantly outperforms state-of-the-art methods, achieving a 15% improvement in formula recognition accuracy and demonstrating superior performance in handling inline formulas and complex layouts. This robust and accurate formula recognition system paves the way for more comprehensive document understanding and knowledge extraction from scientific literature."
http://arxiv.org/abs/2508.00085v1,Punching Bag vs. Punching Person: Motion Transferability in Videos,"Motion transfer aims to animate a source object with the motion of a driving object, a task complicated by differences in object appearance and articulation. This paper investigates the critical role of object similarity in successful motion transfer, specifically addressing the question: Can motion learned from a punching person effectively animate a punching bag, and vice-versa? We propose a novel framework incorporating a learned pose representation and a spatially adaptive normalization technique to bridge the domain gap between human and inanimate object motion. Our method disentangles motion and appearance by encoding pose information into a normalized space, allowing for flexible transfer. The spatially adaptive normalization then modulates the source object's features based on the driving object's pose, enabling realistic animation even with significant structural differences. Experiments demonstrate that our approach achieves superior performance compared to existing methods, particularly in scenarios involving drastically different object types, successfully transferring punching motions between humans and punching bags with improved realism and accuracy. This work highlights the importance of adaptive normalization strategies for motion transfer across diverse object categories, paving the way for more versatile and robust animation techniques."
http://arxiv.org/abs/2508.00053v1,A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition,"Human recognition, encompassing face recognition and person re-identification, is a crucial task in various computer vision applications. However, performance often degrades significantly in uncontrolled environments due to variations in pose, illumination, occlusion, and image quality. This paper addresses the challenge of robust human recognition by proposing a Quality-Guided Mixture of Score-Fusion Experts (Q-MoSE) framework. Our approach leverages multiple pre-trained recognition models (experts), each specializing in different aspects of human appearance. The key innovation lies in a learned quality assessment module that estimates the reliability of each input image and dynamically weights the contribution of each expert based on this quality score. Experimental results on several challenging benchmark datasets, including LFW, CFP-FP, and Market-1501, demonstrate that Q-MoSE consistently outperforms state-of-the-art score fusion methods and individual experts, particularly in low-quality image scenarios. This highlights the effectiveness of our quality-aware expert weighting strategy in enhancing the robustness and accuracy of human recognition systems."
http://arxiv.org/abs/2507.23608v1,Medical Image De-Identification Benchmark Challenge,"Medical imaging plays a crucial role in diagnosis and treatment planning, but often contains protected health information (PHI) that necessitates de-identification before sharing for research and educational purposes. This paper addresses the lack of a standardized benchmark for evaluating and comparing the performance of automated medical image de-identification algorithms. We introduce a novel Medical Image De-Identification Benchmark Challenge, comprising a diverse dataset of multi-modal medical images including CT, MRI, and X-ray modalities, meticulously annotated with various types of PHI such as facial features, text overlays, and internal anatomical structures that can reveal identity. The challenge encouraged participants to develop algorithms capable of accurately detecting and removing PHI while preserving clinically relevant information. The best performing methods achieved an F1-score of over 0.95 on PHI detection, demonstrating the feasibility of automated de-identification. This benchmark provides a valuable resource for researchers and practitioners to advance the field of medical image privacy and promotes the responsible use of medical data."
http://arxiv.org/abs/2508.00941v1,Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition,"Forensic face recognition often relies on low-quality face images captured under challenging conditions, significantly hindering identification accuracy. This paper addresses the problem of enhancing severely degraded face images to improve the performance of subsequent face recognition systems. We propose a novel Latent Diffusion based Face Enhancement (LDFE) framework specifically tailored for forensic applications. LDFE leverages the powerful generative capabilities of latent diffusion models, combined with a novel face-aware conditioning strategy based on facial landmarks and identity embeddings extracted from a pre-trained recognition model. This allows us to guide the diffusion process towards generating realistic and identity-preserving high-resolution face images from severely degraded inputs. Experimental results on benchmark datasets containing low-resolution, noisy, and blurred face images demonstrate that LDFE significantly outperforms existing face enhancement methods, leading to substantial improvements in downstream face recognition accuracy. Our approach provides a valuable tool for enhancing face images in forensic investigations, ultimately contributing to more reliable and accurate identification."
http://arxiv.org/abs/2507.23357v1,"IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025","Deep learning has revolutionized computer vision, leading to significant advancements in areas such as image recognition, object detection, and semantic segmentation. However, the rapid evolution of neural network architectures and training methodologies presents a challenge for researchers and practitioners seeking optimal design patterns for specific vision tasks. This seminar report addresses the problem of systematically identifying and analyzing effective neural network design patterns prevalent in recent computer vision research. We conducted an extensive literature review, focusing on state-of-the-art convolutional neural networks (CNNs), transformers, and hybrid architectures applied to various computer vision benchmarks. We categorized recurring architectural motifs and training strategies, such as attention mechanisms, skip connections, normalization techniques, and data augmentation methods, identifying their specific contributions to performance gains. Our analysis reveals that the effective utilization of self-attention in transformer-based architectures leads to significant improvements in long-range dependency modeling, while incorporating specific regularization techniques like MixUp enhances robustness against adversarial attacks. The comprehensive overview of design patterns provided in this report serves as a valuable resource for guiding the development of novel and efficient neural network architectures for computer vision applications."
http://arxiv.org/abs/2507.23263v1,Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels,"Multi-label image recognition (MLIR) aims to predict multiple object labels present in an image. Training MLIR models with partial labels, where only a subset of true labels are provided, poses a significant challenge due to the presence of missing labels. This paper addresses the problem of learning robust and adaptive thresholds for predicting the presence of labels in MLIR when only partial labels are available during training. We propose a novel Semantic-Aware Threshold Learning (SATL) framework, which incorporates semantic relationships between labels to guide the threshold learning process. SATL utilizes a graph convolutional network to model label correlations and adaptively adjusts the classification thresholds for each label based on the predicted semantic context. Experimental results on benchmark datasets demonstrate that SATL significantly outperforms existing state-of-the-art methods for MLIR with partial labels, achieving improvements in both precision and recall. This work offers a practical solution for improving the accuracy and robustness of multi-label image recognition models trained with incomplete label information, enhancing their applicability in real-world scenarios."
http://arxiv.org/abs/2507.23070v1,Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model,"Fine-grained visual recognition (FGVR) aims to distinguish subordinate categories within basic-level categories, requiring the subtle discrimination of nuanced visual differences. Existing FGVR methods often rely on predefined vocabularies of semantic parts or attributes, limiting their generalization ability to novel or unseen categories. This paper addresses the challenge of vocabulary-free FGVR by leveraging the contextual grounding capabilities of vision-language models (VLMs). We propose an enriched contextually grounded VLM framework that incorporates category-agnostic visual cues and contextual information to facilitate fine-grained discrimination. Specifically, we employ a pre-trained VLM to extract visual features and generate contextual descriptions based on input images and pseudo-prompts. Then, we introduce a contrastive learning objective that aligns visual features with contextually enriched text embeddings, learned from both image-level and region-level information. Empirical results on several benchmark fine-grained datasets demonstrate that our approach achieves state-of-the-art performance without relying on predefined vocabularies, surpassing existing vocabulary-based and vocabulary-free methods. This work offers a novel and generalizable solution for fine-grained visual recognition by effectively harnessing the power of VLMs for contextual reasoning and visual grounding."
http://arxiv.org/abs/2507.22568v1,Subtyping Breast Lesions via Generative Augmentation based Long-tailed Recognition in Ultrasound,"Breast ultrasound is a crucial imaging modality for detecting and classifying breast lesions, aiding in early diagnosis and treatment planning. However, the inherent class imbalance in ultrasound datasets, where benign lesions are significantly more prevalent than malignant ones, poses a significant challenge to accurate lesion subtyping. This paper addresses the problem of long-tailed recognition in breast ultrasound lesion classification by proposing a novel generative augmentation framework. Our method leverages a conditional variational autoencoder (CVAE) to generate synthetic ultrasound images conditioned on lesion subtype labels, specifically targeting the under-represented malignant classes. Furthermore, we introduce a consistency regularization term to encourage the generated images to maintain feature consistency with real images of the same class, preventing mode collapse and improving the quality of generated samples. Experimental results on a large clinical breast ultrasound dataset demonstrate that our generative augmentation approach significantly improves the classification accuracy of malignant lesions, achieving a substantial gain in F1-score compared to traditional data augmentation techniques and state-of-the-art long-tailed recognition methods. This work offers a promising solution for improving the reliability and clinical utility of AI-powered breast ultrasound analysis, particularly in resource-constrained environments."
http://arxiv.org/abs/2507.22553v1,RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning,"Continual learning (CL) aims to enable models to learn new tasks sequentially without forgetting previously learned knowledge. Prompt-based CL has emerged as a promising paradigm due to its parameter efficiency and ability to adapt to new tasks by learning task-specific prompts. However, existing prompt-based methods often suffer from catastrophic forgetting and limited diversity in the learned prompts, hindering their ability to effectively represent and distinguish between tasks. We introduce RainbowPrompt, a novel diversity-enhanced prompt-evolving framework for CL. RainbowPrompt leverages a dynamic prompt pool that expands as new tasks arrive, coupled with a diversity-promoting loss that encourages distinct prompt representations. Furthermore, we employ a prompt selection strategy based on cosine similarity to activate the most relevant prompts for each task, enhancing task discrimination. Experimental results on benchmark CL datasets demonstrate that RainbowPrompt significantly outperforms existing prompt-based and other CL methods in terms of average accuracy and forgetting metrics. RainbowPrompt offers a robust and effective approach to mitigate catastrophic forgetting and improve the adaptability of prompt-based models in continual learning scenarios."
http://arxiv.org/abs/2507.22522v1,Recognizing Actions from Robotic View for Natural Human-Robot Interaction,"Recognizing human actions is crucial for enabling natural and intuitive Human-Robot Interaction (HRI). However, the egocentric viewpoint of a robot presents unique challenges due to varying perspectives, occlusions, and dynamic backgrounds, hindering the performance of traditional action recognition models trained on third-person datasets. This paper addresses the problem of robustly recognizing human actions from a robotic, egocentric perspective in real-time. We propose a novel multi-modal fusion approach that combines RGB video streams with 3D point cloud data captured by the robot's onboard sensors. Our method utilizes a two-stream convolutional neural network (CNN) architecture: one stream processes RGB frames using a pre-trained I3D network fine-tuned for robotic viewpoints, while the other stream processes point cloud sequences using a PointNet++ architecture to extract spatio-temporal features. The extracted features are then fused using a late fusion strategy, allowing the network to leverage both appearance and geometric information for improved action recognition. Experimental results on a newly collected robotic action dataset demonstrate that our proposed method significantly outperforms existing state-of-the-art action recognition techniques, achieving a relative improvement of 15% in average precision. This improved action recognition capability enables more effective and natural human-robot collaboration in various real-world scenarios."
http://arxiv.org/abs/2507.22421v1,Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking,"Real-time video analysis demands computationally efficient methods capable of capturing both spatial and temporal dynamics. Existing approaches often treat action recognition and object tracking as separate tasks, leading to redundant computations and suboptimal performance in resource-constrained environments. This paper introduces a unified spatial-temporal modeling framework designed for efficient real-time video analysis, addressing both action recognition and object tracking simultaneously. Our framework leverages a lightweight convolutional neural network (CNN) backbone for spatial feature extraction, followed by a novel recurrent module incorporating attention mechanisms to selectively aggregate temporal information across frames. A shared feature representation is then used for both action classification and object tracking via a Siamese network architecture, enabling joint optimization. Experimental results on benchmark datasets, including Kinetics-400 and MOT17, demonstrate that our approach achieves state-of-the-art real-time performance with significantly reduced computational complexity compared to existing methods. The proposed framework offers a practical and efficient solution for real-time video analysis applications, facilitating the deployment of complex vision tasks on edge devices."
http://arxiv.org/abs/2507.22353v1,FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation,"Face detection has achieved remarkable progress, yet discovering and recognizing faces in unconstrained scenarios with significant variations in pose, occlusion, and appearance remains challenging. Existing face detectors often struggle with low recall and precision when applied to diverse datasets, highlighting a need for generalized face discovery methods. This paper introduces FaceGCD, a novel framework for Generalized Face Discovery via Dynamic Prefix Generation. FaceGCD leverages a transformer-based architecture and a dynamic prefix generation mechanism to adaptively learn and generate task-specific prefixes conditioned on the input image. These prefixes guide the transformer in focusing on relevant image regions and features, enhancing its ability to detect faces across a wide spectrum of variations. Our experiments demonstrate that FaceGCD significantly outperforms state-of-the-art face detectors on several challenging benchmarks, including WIDER FACE, MegaFace, and IJB-C, achieving substantial improvements in both recall and precision, particularly for difficult face instances. The proposed method offers a robust and adaptable solution for face discovery, paving the way for more reliable face recognition systems in real-world applications."
http://arxiv.org/abs/2507.22100v1,Trade-offs in Image Generation: How Do Different Dimensions Interact?,"Image generation has witnessed remarkable progress, yet understanding the interplay between different dimensions of image quality remains a challenge. This paper addresses the problem of systematically characterizing the trade-offs that arise when optimizing for multiple, often competing, attributes in generated images. We propose a novel evaluation framework that combines quantitative metrics for image fidelity, diversity, and attribute control with a factorized experimental design. This allows us to isolate and quantify the impact of architectural choices, training objectives, and hyperparameter settings on the relationships between these dimensions. Our experiments across various generative models and datasets reveal that improving fidelity often comes at the expense of diversity, and that achieving fine-grained attribute control can negatively impact overall image quality. These findings provide valuable insights for designing and training generative models that can balance competing objectives, ultimately leading to more controllable and realistic image synthesis."
http://arxiv.org/abs/2507.22059v1,StepAL: Step-aware Active Learning for Cataract Surgical Videos,"Cataract surgery is a prevalent procedure, and the analysis of surgical videos holds immense potential for surgical training and skill assessment. However, the scarcity of labeled data, particularly with detailed step-wise annotations, hinders the development of robust automated analysis systems. This paper addresses the challenge of efficiently labeling cataract surgical videos by proposing StepAL, a novel step-aware active learning framework. StepAL leverages a hybrid uncertainty sampling strategy that incorporates both model uncertainty and step transition uncertainty, prioritizing the selection of frames that are both difficult for the model to classify and representative of crucial surgical step changes. Furthermore, we introduce a step-consistency regularization term during the active learning iterations to ensure temporal coherence in the predicted step sequences. Experimental results on a large-scale cataract surgical video dataset demonstrate that StepAL significantly outperforms existing active learning strategies, achieving comparable performance to fully supervised models with significantly fewer labeled frames. This work provides a practical and efficient solution for annotating surgical videos, facilitating the development of advanced surgical assistance systems."
http://arxiv.org/abs/2507.22057v1,MetaLab: Few-Shot Game Changer for Image Recognition,"Few-shot image recognition aims to classify novel categories given only a handful of labeled examples, a significant challenge for traditional deep learning models. Current meta-learning approaches often struggle with generalization to unseen datasets due to biases learned during meta-training or suboptimal adaptation strategies. We introduce MetaLab, a novel meta-learning framework that dynamically constructs task-specific architectures by leveraging a differentiable neural architecture search (NAS) space. MetaLab learns to assemble optimal network modules based on the characteristics of each few-shot task, enabling flexible and efficient adaptation. Extensive experiments on miniImageNet, tieredImageNet, and CIFAR-FS datasets demonstrate that MetaLab significantly outperforms state-of-the-art few-shot learning methods, achieving substantial accuracy gains, particularly in challenging few-shot scenarios. This work demonstrates the potential of NAS-driven meta-learning for building robust and adaptable image recognition systems."
http://arxiv.org/abs/2507.21977v2,Motion Matters: Motion-guided Modulation Network for Skeleton-based Micro-Action Recognition,"Skeleton-based action recognition has gained prominence due to its robustness against variations in appearance and background clutter. However, recognizing subtle, short-duration micro-actions remains a significant challenge due to their fine-grained temporal dynamics and limited spatial displacement. We address this problem by introducing a novel Motion-guided Modulation Network (MGM-Net) specifically designed for skeleton-based micro-action recognition. MGM-Net leverages explicit motion information to dynamically modulate the feature representations learned from skeletal joints. This is achieved through a Motion Encoding Module that captures temporal motion patterns and a Feature Modulation Module that adaptively refines joint features based on the encoded motion. Extensive experiments on two challenging micro-action datasets, namely the Micro-Action and Sub-Action (MSA) dataset and the Surgical Micro-Action (SMA) dataset, demonstrate that MGM-Net consistently outperforms state-of-the-art methods, achieving significant improvements in recognition accuracy. These results highlight the crucial role of motion cues in discerning subtle differences between micro-actions, paving the way for more effective and robust analysis of fine-grained human activities."
http://arxiv.org/abs/2508.03722v1,Multimodal Video Emotion Recognition with Reliable Reasoning Priors,"Multimodal video emotion recognition (MVER) aims to infer emotional states from videos by integrating information across different modalities, such as visual and auditory cues. However, existing MVER methods often struggle with noisy or ambiguous multimodal inputs and lack explicit reasoning about the relationships between different modalities and emotions. To address these limitations, we propose a novel framework, Multimodal Video Emotion Recognition with Reliable Reasoning Priors (MVER-RRP), which incorporates structured reasoning priors to guide the learning process. MVER-RRP utilizes a graph neural network to model the relationships between modalities and emotions, where the graph structure is initialized with pre-defined reasoning priors based on established psychological theories and common-sense knowledge about emotion expression. We further introduce a reliability-aware attention mechanism that dynamically weights the contribution of each modality based on its estimated reliability, mitigating the impact of noisy or irrelevant modalities. Experiments on benchmark datasets demonstrate that MVER-RRP significantly outperforms state-of-the-art methods, achieving improvements of up to 5% in accuracy. The incorporation of reliable reasoning priors enhances the robustness and interpretability of MVER systems, paving the way for more accurate and reliable emotion recognition in real-world applications."
http://arxiv.org/abs/2507.21015v1,Learning Transferable Facial Emotion Representations from Large-Scale Semantically Rich Captions,"Facial Emotion Recognition (FER) remains challenging due to limited labeled data and domain shifts between datasets. Current methods often rely on supervised learning with discrete emotion labels, neglecting the rich semantic information present in real-world scenarios. This paper addresses the problem of learning transferable facial emotion representations by leveraging large-scale image-caption datasets. We propose a novel approach that utilizes contrastive learning to align facial features with semantically relevant captions. Specifically, we train an encoder to map facial images to a feature space where they are close to their corresponding caption embeddings, while being distant from embeddings of unrelated captions. Furthermore, we incorporate an attention mechanism to focus on emotion-relevant words within the captions. Experiments demonstrate that our learned representations significantly improve performance on various FER benchmarks, outperforming state-of-the-art methods in cross-dataset and few-shot settings. These results highlight the potential of leveraging semantic information from captions to learn robust and transferable facial emotion representations, paving the way for more accurate and generalizable FER systems."
http://arxiv.org/abs/2507.20913v1,HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection,"Face forgery detection is crucial for combating the spread of misinformation and maintaining trust in digital media. However, existing methods often struggle to effectively integrate diverse modalities and adapt to the varying complexities of forged faces, limiting their generalization ability. This paper introduces HAMLET-FFD: a Hierarchical Adaptive Multi-modal Learning Embeddings Transformation framework for robust face forgery detection. HAMLET-FFD leverages a hierarchical architecture to extract multi-modal features from both spatial and frequency domains, adaptively weighting their contributions based on the input image. Specifically, we employ a novel embeddings transformation module to learn modality-specific projections and fuse them dynamically, enhancing the representation power. Extensive experiments on several benchmark datasets, including FaceForensics++, Celeb-DF, and DFDC, demonstrate that HAMLET-FFD achieves state-of-the-art performance, surpassing existing methods by a significant margin in terms of detection accuracy and cross-dataset generalization. These results highlight the effectiveness of the proposed hierarchical adaptive multi-modal learning strategy for robust face forgery detection in real-world scenarios."
http://arxiv.org/abs/2507.20782v1,Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data,"Face recognition systems are increasingly deployed in various applications, raising concerns about their accuracy and potential biases. Training these systems often relies on large datasets, which can be expensive and difficult to acquire, motivating the use of synthetic data generation. This paper investigates the accuracy and bias implications of training face recognition models using purely synthetic data compared to models trained on real-world images. We train several state-of-the-art face recognition architectures using a large-scale synthetically generated dataset created with advanced 3D face modeling and rendering techniques, carefully controlling for variations in pose, illumination, and expression. We then evaluate these models on established benchmark datasets representing diverse demographic groups and compare their performance to models trained on real-world face images. Our results demonstrate that while synthetic data can achieve competitive overall accuracy, significant performance disparities persist across different demographic groups, particularly in terms of skin tone and gender. This highlights the potential for synthetic data to exacerbate existing biases present in face recognition systems, emphasizing the need for careful consideration of data generation strategies and evaluation protocols to ensure fairness and equity."
http://arxiv.org/abs/2507.20737v1,Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals,"Emotion recognition using physiological signals offers a promising avenue for understanding human affective states. However, real-world applications often suffer from incomplete or missing data across different physiological modalities, severely degrading performance. This paper addresses the challenge of robust emotion recognition from incomplete multi-modal physiological signals by proposing a novel Multi-Masked Querying Network (MMQ-Net). MMQ-Net leverages a shared feature encoder to extract modality-agnostic representations, followed by a masked query mechanism that dynamically attends to available modalities while simultaneously inferring missing modalities using a context-aware query generation process. Furthermore, a modality-specific fusion module refines the integrated representation for enhanced emotion classification. Experimental results on benchmark datasets demonstrate that MMQ-Net significantly outperforms existing state-of-the-art methods, achieving superior accuracy and robustness, especially under high missing data ratios. This research provides a practical solution for deploying emotion recognition systems in real-world scenarios with unreliable or incomplete physiological data acquisition."
http://arxiv.org/abs/2507.20557v1,FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via Psychological AU Coordination and Dynamic Facial Motion Modeling,"Micro-expression recognition (MER) presents a significant challenge due to the subtle and fleeting nature of facial movements, further complicated by privacy concerns surrounding facial data. This paper addresses the problem of privacy-preserving MER by proposing FED-PsyAU, a novel federated learning framework that leverages psychological Action Unit (AU) coordination and dynamic facial motion modeling. Our method employs a federated architecture where clients train local models on their private data, sharing only model updates with a central server. Crucially, we incorporate psychological AU coordination constraints within the local training objective to guide the model towards learning meaningful relationships between AUs, enhancing the robustness of feature representations even with limited data. Furthermore, we introduce a dynamic facial motion modeling module using attention-based LSTMs to capture temporal dependencies in facial movements, improving recognition accuracy. Experiments on benchmark datasets, CASME II and SMIC, demonstrate that FED-PsyAU achieves state-of-the-art performance in federated MER while provably preserving data privacy, outperforming existing federated learning approaches and approaching the accuracy of centralized training. This work offers a practical and privacy-conscious solution for deploying MER systems in real-world applications."
http://arxiv.org/abs/2507.20548v1,Annotation-Free Human Sketch Quality Assessment,"Human sketches serve as a fundamental medium for visual communication and artistic expression. Assessing the quality of these sketches, however, typically requires subjective human evaluation and laborious annotation. This paper addresses the challenge of automatically evaluating human sketch quality without relying on any explicit annotations or labeled data. We propose a novel annotation-free framework that leverages a pre-trained Vision Transformer (ViT) model, fine-tuned on a large dataset of diverse natural images using a self-supervised learning approach based on contrastive learning with carefully designed data augmentations to encourage viewpoint invariance and robustness to style variations. The fine-tuned ViT embeddings are then used to train a lightweight regression model to predict sketch quality scores, leveraging only the inherent statistical properties of the feature space. Our experiments demonstrate that the proposed method achieves competitive performance compared to existing supervised methods trained with extensive human annotations, exhibiting a strong correlation with human perception of sketch quality across various datasets. This annotation-free approach offers a practical and scalable solution for automatic sketch quality assessment, enabling applications in education, art therapy, and interactive sketch-based interfaces."
http://arxiv.org/abs/2507.20506v1,An Improved YOLOv8 Approach for Small Target Detection of Rice Spikelet Flowering in Field Environments,"Rice spikelet flowering detection is crucial for assessing rice fertility and optimizing breeding programs, yet automated analysis in field environments remains challenging due to small target sizes and complex backgrounds. This paper addresses the difficulty of accurately detecting rice spikelet flowering, particularly small instances, amidst variable lighting and occlusions in field images. We propose an improved YOLOv8-based approach incorporating several key modifications: Firstly, we introduce a novel attention mechanism, specifically a Squeeze-and-Excitation (SE) block, integrated into the backbone to enhance feature representation of small, subtle flowering spikelets. Secondly, we employ a multi-scale feature fusion strategy, combining features from different network layers to improve the detection of objects at varying scales. Finally, we augment the training dataset with synthetic images generated using generative adversarial networks (GANs) to improve model robustness to variations in illumination and perspective. Experimental results demonstrate that our proposed method achieves a significant improvement in mAP@0.5 and F1-score compared to the baseline YOLOv8 model and other state-of-the-art object detection algorithms, particularly for small flowering spikelets. This enhanced detection performance facilitates more accurate and efficient phenotyping of rice fertility in field conditions, benefiting agricultural research and breeding efforts."
http://arxiv.org/abs/2507.20356v3,Detecting Visual Information Manipulation Attacks in Augmented Reality: A Multimodal Semantic Reasoning Approach,"Augmented Reality (AR) applications are increasingly prevalent, creating new avenues for visual information dissemination. However, this accessibility also raises concerns about malicious manipulation of AR scenes through visual attacks, potentially leading to misinformation and distrust. This paper addresses the problem of detecting visual information manipulation attacks in AR environments, where attackers subtly alter virtual content to mislead users. We propose a novel multimodal semantic reasoning approach that leverages both visual and semantic cues extracted from the AR scene. Our method integrates object detection, scene understanding, and knowledge graph reasoning to identify inconsistencies between the expected and observed relationships between virtual and real-world objects. Specifically, we construct a scene graph representing the semantic relationships between objects and compare it to a knowledge graph containing expected relationships, highlighting deviations that may indicate manipulation. We evaluate our approach on a newly created dataset of AR scenes with various types of visual information manipulation attacks. Experimental results demonstrate that our method achieves significantly higher accuracy in detecting these attacks compared to unimodal approaches, showing an average improvement of 15% in F1-score. This research provides a crucial step towards building robust and trustworthy AR systems by effectively identifying and mitigating visual information manipulation attacks."
http://arxiv.org/abs/2507.20230v2,A Multi-Agent System Enables Versatile Information Extraction from the Chemical Literature,"The chemical literature contains a wealth of valuable information, yet its extraction remains a challenging task due to the diverse and unstructured nature of text, figures, and tables. Existing chemical information extraction (CIE) methods often focus on specific entity types or require extensive manual annotation, limiting their versatility and scalability. To address this, we propose a novel multi-agent system (MAS) for versatile CIE. Our MAS comprises specialized agents, each designed for a specific sub-task, such as chemical entity recognition, relation extraction, optical structure recognition, and table understanding. These agents operate independently but communicate and collaborate through a central knowledge graph, enabling information sharing and conflict resolution. We demonstrate the effectiveness of our MAS on a diverse set of chemical documents, achieving state-of-the-art performance on benchmark datasets for chemical entity recognition and significantly improving the accuracy of extracting complex chemical reactions from figures and tables compared to baseline methods. This MAS framework provides a flexible and scalable solution for comprehensive information extraction from the chemical literature, accelerating scientific discovery and knowledge curation."
http://arxiv.org/abs/2507.20197v1,Color histogram equalization and fine-tuning to improve expression recognition of (partially occluded) faces on sign language datasets,"Facial expression recognition is crucial for human-computer interaction, particularly in sign language recognition systems where visual cues complement hand gestures. However, performance degrades significantly when faces are partially occluded or exhibit variations in lighting and color distribution, common occurrences in real-world sign language datasets. This paper addresses the challenge of robust facial expression recognition in the presence of partial occlusion and color variations within sign language datasets. We propose a novel two-stage approach: first, we apply color histogram equalization to normalize color distributions and enhance contrast, followed by a fine-tuning process of a pre-trained convolutional neural network (CNN) using a carefully curated dataset containing both original and synthetically occluded facial images. The fine-tuning process incorporates a weighted loss function to emphasize the learning of features from non-occluded regions. Experimental results on challenging sign language datasets demonstrate a significant improvement in expression recognition accuracy, particularly for occluded faces, achieving an average increase of 8% in F1-score compared to baseline methods. This improvement highlights the potential of our method to enhance the reliability and usability of sign language recognition systems in real-world scenarios."
http://arxiv.org/abs/2508.00892v1,"HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models","The proliferation of large-scale image datasets has raised significant concerns regarding unauthorized usage and distribution of training data, especially given the increasing reliance on pre-trained models. Existing watermarking techniques for image models often suffer from limitations such as perceptible image distortions, vulnerability to removal attacks, or the requirement for extensive retraining. To address these challenges, we introduce HoneyImage, a novel dataset ownership verification framework that embeds imperceptible, robust, and verifiable signals directly into the training images themselves. HoneyImage leverages a carefully designed adversarial perturbation strategy, guided by a perceptual loss, to generate subtle image alterations that are both undetectable to human observers and robust against common image manipulations. These perturbations are strategically crafted to activate specific neurons within a verification network, allowing for accurate and efficient detection of dataset usage in downstream models without requiring access to the original training process. Empirical results demonstrate that HoneyImage achieves high verification accuracy even after significant image compression, geometric transformations, and adversarial attacks, while maintaining negligible impact on the utility of the watermarked dataset. HoneyImage provides a practical and effective solution for protecting intellectual property and ensuring responsible usage of image datasets in the rapidly evolving landscape of deep learning."
http://arxiv.org/abs/2507.19949v1,AF-CLIP: Zero-Shot Anomaly Detection via Anomaly-Focused CLIP Adaptation,"Anomaly detection is critical for ensuring the reliability and safety of various systems, yet acquiring labeled anomalous data for supervised training remains a significant challenge. This paper addresses the problem of zero-shot anomaly detection, where the goal is to identify anomalies without any explicit training on anomalous samples. We introduce AF-CLIP: Anomaly-Focused CLIP Adaptation, a novel approach that leverages the pre-trained knowledge of CLIP (Contrastive Language-Image Pre-training) to learn anomaly representations by focusing on deviations from normal data distributions. AF-CLIP adapts CLIP by first identifying anomaly-sensitive regions in normal images using a self-supervised anomaly scoring mechanism, and then fine-tuning CLIP to emphasize these regions during image and text contrastive learning. Our experiments on several benchmark anomaly detection datasets demonstrate that AF-CLIP significantly outperforms existing zero-shot and unsupervised methods, achieving state-of-the-art performance in detecting a wide range of anomalies. AF-CLIP offers a practical and effective solution for anomaly detection in scenarios where labeled anomalous data is scarce or unavailable, paving the way for more robust and adaptable anomaly detection systems."
http://arxiv.org/abs/2507.19931v1,MambaVesselNet++: A Hybrid CNN-Mamba Architecture for Medical Image Segmentation,"Medical image segmentation plays a crucial role in computer-aided diagnosis and treatment planning. However, accurately segmenting intricate structures like blood vessels remains challenging due to their complex topology and variations in image quality. This paper introduces MambaVesselNet++, a novel hybrid architecture for improved medical image segmentation, leveraging the strengths of both Convolutional Neural Networks (CNNs) and the Mamba state-space model. MambaVesselNet++ integrates a CNN-based encoder to extract hierarchical feature representations and a Mamba-based decoder to capture long-range dependencies and global context information, further enhanced by skip connections for fine-grained detail preservation. We also introduce a novel selective state space block to improve the efficiency and accuracy of the Mamba decoder. Experiments on multiple publicly available vessel segmentation datasets demonstrate that MambaVesselNet++ achieves state-of-the-art performance, surpassing existing CNN-based and Transformer-based methods in terms of segmentation accuracy and robustness. The proposed architecture offers a promising solution for accurate and efficient medical image segmentation, potentially improving clinical workflows and patient outcomes."
http://arxiv.org/abs/2507.19852v1,A Structure-aware and Motion-adaptive Framework for 3D Human Pose Estimation with Mamba,"Accurate 3D human pose estimation from monocular videos remains a challenging task due to depth ambiguity and complex articulated motion. Existing methods often struggle to effectively capture both the structural dependencies within the human body and the temporal dynamics of motion, leading to inaccurate pose predictions. To address this, we introduce a novel structure-aware and motion-adaptive framework for 3D human pose estimation, leveraging the Mamba architecture. Our framework incorporates a graph convolutional network (GCN) to explicitly model the skeletal structure, enabling the Mamba module to focus on learning motion patterns conditioned on structural context. Furthermore, we introduce a motion-adaptive modulation mechanism within the Mamba block that adjusts the temporal receptive field based on motion intensity, allowing for better handling of varying motion speeds. Experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that our approach achieves state-of-the-art performance, outperforming existing methods in terms of both accuracy and robustness, particularly in challenging scenarios with fast and complex movements. This work highlights the potential of structured state space models for capturing both structural and temporal dependencies in 3D human pose estimation."
http://arxiv.org/abs/2507.19840v1,AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition,"Continuous Sign Language Recognition (CSLR) aims to automatically translate sign language videos into text, offering accessibility for the deaf and hard-of-hearing community. Current CSLR systems often rely on complex pipelines involving intermediate representations like glosses, which can introduce error propagation and limit end-to-end optimization. This paper introduces AutoSign, a novel direct pose-to-text translation framework for CSLR, eliminating the need for intermediate gloss representations. AutoSign leverages a transformer-based architecture, directly mapping 3D pose sequences extracted from video to corresponding text sequences. To address the temporal complexities of sign language, we incorporate a multi-scale temporal attention mechanism that allows the model to capture dependencies across varying time horizons. Experiments on benchmark datasets demonstrate that AutoSign achieves state-of-the-art performance, surpassing existing methods by a significant margin in terms of BLEU scores and word error rate. AutoSign offers a simplified and more efficient approach to CSLR, paving the way for more robust and accurate sign language translation systems."
http://arxiv.org/abs/2507.19830v2,Taking Language Embedded 3D Gaussian Splatting into the Wild,"3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique for novel view synthesis, offering real-time rendering and state-of-the-art visual quality. However, extending its capabilities to incorporate language guidance for scene editing and manipulation, particularly in unconstrained, real-world environments, remains a significant challenge due to the difficulty in associating language prompts with specific 3D Gaussian primitives. This paper introduces a novel framework, Language Embedded 3D Gaussian Splatting (LE-3DGS), designed to address this challenge. LE-3DGS leverages a contrastive learning approach to embed Gaussian features into a shared language-vision space, enabling direct manipulation of the 3D scene using natural language. Specifically, we train a lightweight neural network to map Gaussian features and text embeddings into a common space, facilitating semantic alignment. Our experiments on both synthetic and real-world datasets demonstrate that LE-3DGS achieves superior performance in language-guided scene editing compared to existing methods, enabling intuitive and precise control over scene appearance and geometry. This work paves the way for more interactive and user-friendly 3D scene manipulation in real-world applications."
http://arxiv.org/abs/2507.19804v1,ForCenNet: Foreground-Centric Network for Document Image Rectification,"Document image rectification aims to correct geometric distortions present in scanned or photographed documents, improving readability and enabling downstream tasks like OCR. Existing methods often struggle with complex distortions and cluttered backgrounds, hindering accurate rectification. We address this problem by introducing ForCenNet, a novel Foreground-Centric Network designed specifically for document image rectification. ForCenNet leverages a two-stage architecture: first, a foreground segmentation module isolates the document region, effectively suppressing background noise. Second, a geometric deformation prediction module, conditioned on the foreground mask, estimates a dense displacement field that maps the distorted image to a rectified output. Experimental results on challenging datasets demonstrate that ForCenNet achieves state-of-the-art performance, surpassing existing methods in terms of both accuracy and robustness to complex distortions. This foreground-centric approach provides a significant advancement in document image rectification, paving the way for more reliable and efficient document processing pipelines."
http://arxiv.org/abs/2507.19398v1,CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays,"Chest X-rays (CXRs) are a prevalent diagnostic tool, and automated multi-label classification of diseases from CXRs holds immense potential for improving healthcare accessibility and efficiency. However, real-world CXR datasets often exhibit a long-tailed distribution of diseases, posing a significant challenge for existing classification models, especially in zero-shot settings where models must generalize to unseen diseases. This paper introduces CXR-CML, a novel Contrastive Multi-Label learning framework designed to improve zero-shot classification performance on long-tailed CXR datasets. CXR-CML leverages a contrastive loss function to learn a more robust and generalizable feature representation by pulling together embeddings of images with similar disease labels and pushing apart embeddings of images with dissimilar labels, even across different diseases. Furthermore, it employs a curriculum learning strategy that progressively introduces harder negative samples during training, focusing the model on discriminating between subtle differences in disease manifestations. Experimental results on a large-scale CXR dataset demonstrate that CXR-CML significantly outperforms state-of-the-art zero-shot classification methods, achieving substantial gains in both seen and unseen disease classification accuracy, particularly for rare diseases. This work offers a promising approach for addressing the challenges of long-tailed distributions in medical image analysis, paving the way for more reliable and comprehensive automated CXR diagnosis."
http://arxiv.org/abs/2507.19280v1,RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow,"Geospatial reasoning, encompassing tasks like spatial relationship understanding and geographic event prediction, is crucial for various applications, yet existing approaches often treat each task in isolation, hindering knowledge transfer and efficient workflow design. This paper addresses the challenge of unifying the geospatial reasoning workflow into a single, adaptable framework. We introduce RemoteReasoner, a novel architecture that leverages a pre-trained language model fine-tuned with geospatial context encoders and a task-specific adapter module. RemoteReasoner learns to represent geospatial information from remote sensing imagery and textual descriptions, subsequently adapting to diverse reasoning tasks through lightweight adapter modules trained for individual objectives. Experiments on a benchmark dataset comprising multiple geospatial reasoning tasks demonstrate that RemoteReasoner achieves state-of-the-art performance in tasks such as spatial relation prediction and event forecasting while exhibiting superior parameter efficiency compared to task-specific models. This unified framework paves the way for building more robust and generalizable geospatial intelligence systems, enabling efficient adaptation to new tasks and datasets."
http://arxiv.org/abs/2507.18967v1,Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN,"Underwater waste poses a significant threat to marine ecosystems, necessitating efficient and automated detection methods for effective cleanup and monitoring. This paper addresses the challenge of accurately and rapidly detecting various types of waste in underwater environments using deep learning techniques. We conduct a comprehensive performance comparison of three prominent object detection models: YOLOv7, YOLOv10, and Faster R-CNN. These models were trained and evaluated on a diverse dataset of underwater images containing plastic, metal, and other forms of marine debris, incorporating data augmentation techniques to enhance robustness. Our experiments demonstrate that YOLOv10 achieves the best balance between speed and accuracy, exhibiting a mean Average Precision (mAP) of 78.5% and a significantly faster inference time compared to Faster R-CNN, while also outperforming YOLOv7 in both metrics. Faster R-CNN achieved a mAP of 75.2% but was significantly slower. This study highlights the potential of real-time object detection models like YOLOv10 for efficient underwater waste management and environmental conservation efforts."
http://arxiv.org/abs/2507.18929v1,MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition,"Cross-modal emotion recognition is crucial for understanding user sentiment in social media, particularly when relying on the interplay between text and visual elements like stickers. However, effectively fusing information from these disparate modalities, especially at varying levels of granularity, remains a significant challenge. This paper introduces the Multi-Granularity Hierarchical Fusion Transformer (MGHFT) for cross-modal sticker emotion recognition. MGHFT leverages pre-trained language models and convolutional neural networks to extract features from text and stickers, respectively. A novel hierarchical fusion module then integrates these features at multiple granularities, capturing both fine-grained details and coarse-grained contextual information. This module employs stacked transformer encoders to model cross-modal interactions, progressively fusing features from lower to higher semantic levels. Experimental results on a publicly available dataset demonstrate that MGHFT significantly outperforms state-of-the-art methods, achieving a new benchmark in sticker emotion recognition. The proposed model offers a robust and effective solution for understanding nuanced emotions expressed through cross-modal communication."
http://arxiv.org/abs/2507.18863v1,Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction,"Visual Speech Recognition (VSR), also known as lip reading, aims to transcribe speech solely from visual cues. Current approaches often struggle with the ambiguity and limited information present in visual speech, particularly at the phoneme level. This paper addresses the challenge of improving phoneme-level VSR accuracy by leveraging point cloud representations of the mouth region and integrating language model information. We propose a novel Point-Visual Fusion Network (PVFN) that combines point cloud features extracted from the mouth region with traditional image-based visual features using a multi-modal attention mechanism. Furthermore, we introduce a Language Model Reconstruction (LMR) loss that encourages the model to generate phoneme sequences consistent with a pre-trained language model, effectively guiding the learning process towards more linguistically plausible outputs. Experimental results on the Lip Reading Sentences 3 (LRS3) dataset demonstrate significant improvements in phoneme error rate (PER), achieving a 5% relative reduction compared to state-of-the-art methods. This work highlights the potential of point cloud representations and language model integration for advancing fine-grained visual speech recognition."
http://arxiv.org/abs/2507.18741v1,"KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ","Historical music notations present unique challenges for optical music recognition (OMR) due to their diverse symbol sets, inconsistent writing styles, and degradation over time. This paper addresses the limitations of existing OMR systems in accurately transcribing *gongchepu*, a historical Chinese music notation, specifically focusing on Jiang Kui's *Baishidaoren Gequ*, a collection known for its complex and varied notation styles. We introduce KuiSCIMA v2.0, an enhanced OMR pipeline built upon our previous KuiSCIMA framework, incorporating improved baseline detection through a novel deep learning architecture combining convolutional and recurrent layers, and a refined calibration module leveraging synthetic data augmentation to address skew and distortion. Furthermore, we introduce a cross-notation generalization strategy using a meta-learning approach to adapt the model to unseen notational variations within *gongchepu*. Experimental results demonstrate a significant improvement in symbol recognition accuracy, achieving a 25% reduction in character error rate compared to KuiSCIMA v1.0, and a substantial increase in generalization performance on held-out notational styles. KuiSCIMA v2.0 establishes a new state-of-the-art baseline for OMR of *Baishidaoren Gequ* and provides a robust framework for tackling the broader challenges of historical Chinese music notation transcription."
http://arxiv.org/abs/2507.18633v1,Identifying Prompted Artist Names from Generated Images,"Text-to-image diffusion models can generate high-quality images from textual prompts, raising concerns about artistic copyright infringement when prompts include artist names. This paper addresses the problem of automatically identifying the artist names explicitly included in prompts used to generate specific images. We propose a two-stage approach: first, a fine-tuned CLIP model is used to extract visual features from the generated image and textual features from the artist names. Second, a novel classification head, incorporating both a similarity score and a learned attention mechanism over the artist name embeddings, predicts the presence of each artist in the prompt. We evaluate our method on a newly constructed dataset of generated images with diverse artist prompts, demonstrating a significant improvement in artist identification accuracy compared to baseline methods that rely solely on CLIP similarity scores. This work contributes to the development of tools for detecting potential copyright infringement and promoting responsible AI art generation."
http://arxiv.org/abs/2507.18552v1,VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding,"Deep learning models for video understanding have largely focused on action recognition or captioning, often overlooking the crucial role of human intent and reasoning. This work addresses the lack of comprehensive video datasets that facilitate deep-cognitive video understanding, particularly the grounding of human intent within a rich, multi-modal context. We introduce VideoMind, a novel, large-scale, omni-modal video dataset featuring diverse scenarios and detailed annotations. Each video is annotated with action labels, object bounding boxes, audio transcriptions, and crucially, human intent labels, alongside rationales explaining the reasoning behind the identified intent. Furthermore, VideoMind includes eye-tracking data from human observers, providing valuable insights into attention patterns related to intent recognition. Experiments using state-of-the-art video understanding models demonstrate the effectiveness of VideoMind in improving intent recognition accuracy and providing explainable predictions. We show that models trained on VideoMind outperform those trained on existing datasets, particularly in complex, multi-actor scenarios, achieving a 15% improvement in intent recognition accuracy. VideoMind enables the development of more sophisticated and human-centered video understanding systems capable of reasoning about human behavior and motivations."
http://arxiv.org/abs/2507.18532v1,COT-AD: Cotton Analysis Dataset,"Cotton yield and quality are critical for the global textile industry, and automated analysis through computer vision offers a promising avenue for improvement. However, the lack of publicly available, high-quality datasets tailored to cotton analysis hinders the development and benchmarking of robust algorithms. This paper introduces the Cotton Analysis Dataset (COT-AD), a novel, large-scale, and meticulously annotated dataset specifically designed for advancing computer vision research in cotton production. COT-AD contains high-resolution RGB images of cotton plants captured under diverse field conditions, annotated with bounding boxes for key objects such as cotton bolls, flowers, leaves, and pests. Furthermore, we provide pixel-level segmentation masks for detailed analysis of boll maturity and disease identification. We benchmark several state-of-the-art object detection and semantic segmentation models on COT-AD, demonstrating the dataset's challenging nature and potential for driving innovation in cotton analysis. COT-AD represents a significant contribution to the field, enabling the development of more accurate and reliable computer vision solutions for optimizing cotton farming practices and improving crop yields."
http://arxiv.org/abs/2507.18484v1,Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments,"Adversarial attacks pose a significant threat to the reliability of visual perception systems, particularly in safety-critical applications operating in 3D environments. Existing defense strategies often focus on passive methods, neglecting the potential of active interaction to mitigate adversarial effects. This paper addresses the challenge of creating robust visual perception in adversarial 3D environments by introducing Reinforced Embodied Active Defense (READ), a novel framework that enables an agent to actively defend against attacks through adaptive interaction. READ employs a reinforcement learning agent trained to strategically navigate and interact with the environment, dynamically adjusting its viewpoint and sensor configurations to minimize the impact of adversarial perturbations on its perception. Our experiments, conducted in simulated 3D environments under various adversarial attack scenarios, demonstrate that READ significantly improves the agent's object recognition accuracy and robustness compared to passive defense mechanisms and static observation strategies. This work highlights the potential of embodied active perception as a powerful defense strategy against adversarial attacks, paving the way for more resilient and trustworthy visual perception systems in real-world applications."
http://arxiv.org/abs/2507.18447v1,PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior,"Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, opening new possibilities for analyzing complex human behaviors. However, a comprehensive evaluation of LMMs in the context of personalized driving behavior analysis, specifically regarding their ability to describe and explain nuanced actions, is currently lacking. This paper introduces PDB-Eval, a novel evaluation framework designed to assess the performance of LMMs in understanding personalized driving behavior using a curated dataset of in-vehicle camera footage synchronized with driver actions and contextual information. PDB-Eval employs a multi-faceted evaluation strategy, considering both descriptive accuracy through captioning metrics and explanatory reasoning through question answering tasks focused on inferring motivations and potential causes behind observed driving actions. Our experiments with state-of-the-art LMMs reveal significant variations in their performance across different driving scenarios and levels of contextual complexity, highlighting both promising capabilities and critical limitations in understanding personalized driving behavior. These findings underscore the need for continued research and development of LMMs tailored for nuanced behavioral analysis, ultimately facilitating safer and more personalized autonomous driving systems."
http://arxiv.org/abs/2507.18444v1,DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition,"Visual Place Recognition (VPR), the task of identifying a location from a query image based on a previously built map, is critical for autonomous navigation. However, VPR remains challenging due to viewpoint variations, illumination changes, and perceptual aliasing. This paper addresses the problem of effectively integrating multi-scale visual information for robust and accurate place recognition under challenging conditions. We propose DSFormer, a Dual-Scale Cross-Learning Transformer, which leverages a hierarchical vision transformer backbone to extract features at multiple scales. A novel cross-scale attention module then adaptively fuses these features, enabling the network to learn robust representations that capture both fine-grained details and global context. Furthermore, we introduce a cross-learning strategy that encourages the network to learn discriminative features by contrasting positive and negative place embeddings at different scales. Experimental results on several benchmark datasets demonstrate that DSFormer achieves state-of-the-art performance, significantly outperforming existing methods in challenging scenarios with viewpoint and illumination variations. DSFormer provides a robust and effective approach for visual place recognition, advancing the capabilities of autonomous systems in real-world environments."
http://arxiv.org/abs/2507.18429v1,NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning,"Head pose estimation (HPE) is a crucial component in various applications, including gaze tracking, human-computer interaction, and driver monitoring systems. However, training robust HPE models often requires large, diverse, and accurately labeled datasets, which are expensive and time-consuming to acquire, particularly for specific demographics or challenging environments. This paper addresses the problem of accurate HPE with limited labeled training data by leveraging the underlying manifold structure of head pose variations. We propose a novel Non-Linear Manifold Learning for Head Pose Estimation (NLML-HPE) framework. First, we construct a low-dimensional manifold representation of head pose using a small labeled dataset augmented with unlabeled data to capture the intrinsic geometric relationships. Then, we train a pose estimator within this manifold space, enabling effective generalization to unseen poses even with limited supervision. Experimental results on benchmark datasets demonstrate that NLML-HPE significantly outperforms state-of-the-art methods when trained with limited labeled data, achieving comparable or superior accuracy to fully supervised approaches using significantly less annotation effort. This approach offers a practical solution for deploying robust HPE systems in scenarios where labeled data is scarce."
http://arxiv.org/abs/2507.18405v1,Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows,"Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks, rivaling and often surpassing convolutional neural networks. However, the global self-attention mechanism in standard ViTs incurs significant computational cost, especially with high-resolution images, limiting their applicability to large-scale datasets and downstream tasks requiring fine-grained details. To address this limitation, we introduce the Iwin Transformer, a novel hierarchical vision transformer architecture leveraging interleaved window attention. Iwin Transformer partitions the input image into non-overlapping windows and performs self-attention within each window. Crucially, we introduce an interleaved window partitioning strategy, where windows in successive layers are shifted to enable cross-window communication and capture both local and global dependencies efficiently. Our approach significantly reduces computational complexity while maintaining competitive performance. Experiments on ImageNet classification demonstrate that Iwin Transformer achieves state-of-the-art accuracy with significantly reduced computational cost compared to existing ViT models. This makes Iwin Transformer a practical and efficient solution for a wide range of vision tasks."
http://arxiv.org/abs/2507.18675v2,Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks,"Human Action Recognition (HAR) is a fundamental task in computer vision, enabling machines to understand and interact with humans in diverse environments. However, current HAR systems often struggle to generalize across different datasets and domains due to variations in camera angles, lighting conditions, and subject appearances. This paper addresses the challenge of domain generalization in vision-based HAR by leveraging the power of Vision-Language Pre-trained (CLIP) models. We propose a novel approach that utilizes CLIP's robust feature representations learned from large-scale image-text pairs to bridge the gap between different action recognition datasets. Specifically, we fine-tune the CLIP visual encoder with a lightweight temporal module to capture motion information and then project the resulting features into a shared embedding space learned from textual descriptions of actions. Our experiments on several benchmark datasets demonstrate that our approach achieves state-of-the-art domain generalization performance, significantly outperforming existing methods in zero-shot and few-shot transfer settings. This work highlights the potential of vision-language models for building more robust and adaptable HAR systems capable of operating effectively in real-world, domain-independent scenarios."
http://arxiv.org/abs/2507.18026v1,Emotion Recognition from Skeleton Data: A Comprehensive Survey,"Human emotion recognition is crucial for enabling natural and intuitive human-computer interaction. While traditionally approached through facial expressions and speech, recent advances in motion capture technology have fostered the development of emotion recognition techniques based on human skeleton data. This survey provides a comprehensive overview of the burgeoning field of emotion recognition from skeleton data, systematically analyzing existing methodologies and datasets. We categorize approaches based on feature engineering strategies (e.g., handcrafted vs. learned features) and classification techniques (e.g., traditional machine learning vs. deep learning). Furthermore, we critically examine publicly available datasets used for training and evaluation, highlighting their strengths and limitations. Our analysis reveals a growing trend towards deep learning-based methods, particularly those leveraging recurrent neural networks and graph convolutional networks to effectively capture temporal and spatial dependencies within skeleton sequences. The survey identifies key challenges, such as the scarcity of large-scale annotated datasets and the need for robust methods that are invariant to viewpoint variations. This work serves as a valuable resource for researchers and practitioners, providing a roadmap for future research directions in skeleton-based emotion recognition."
http://arxiv.org/abs/2507.17959v1,OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments,"Virtual rehabilitation (VR) offers a promising avenue for delivering accessible and engaging therapy to older adults. However, automatically assessing patient engagement during VR sessions remains a significant challenge. This paper introduces OPEN, a novel benchmark dataset for Older adult Patient Engagement recognition in virtual rehabilitation learning environments, comprising synchronized video, depth, and physiological data captured during VR-based upper limb exercises. We address the problem of accurately recognizing varying levels of patient engagement from multi-modal sensor data. We propose a multi-stream deep learning architecture, leveraging convolutional neural networks (CNNs) for video and depth feature extraction, combined with recurrent neural networks (RNNs) to model temporal dependencies in physiological signals. A late fusion strategy integrates these modalities for final engagement classification. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on the OPEN dataset, surpassing existing approaches by a significant margin. The OPEN dataset and the proposed baseline will facilitate future research in automated patient engagement assessment, ultimately leading to more personalized and effective VR rehabilitation programs for older adults."
http://arxiv.org/abs/2507.17729v1,A Comprehensive Evaluation Framework for the Study of the Effects of Facial Filters on Face Recognition Accuracy,"Facial recognition systems are increasingly deployed in various applications, raising concerns about their robustness against adversarial attacks and common image manipulations, including the pervasive use of facial filters. This paper addresses the lack of a standardized and comprehensive evaluation framework for studying the impact of facial filters on face recognition accuracy. We introduce a novel framework encompassing a diverse set of facial filters, a large-scale face dataset, and a suite of evaluation metrics designed to quantify the degradation in performance across various face recognition models. Our framework leverages both commercially available filters from popular social media platforms and synthetically generated filters simulating realistic image distortions. We evaluate the impact of these filters on several state-of-the-art face recognition models, demonstrating significant performance drops, particularly with filters that alter facial geometry and texture. The proposed framework provides a valuable tool for researchers and practitioners to systematically assess the vulnerability of face recognition systems to facial filters, facilitating the development of more robust and reliable algorithms."
http://arxiv.org/abs/2507.18660v1,Fuzzy Theory in Computer Vision: A Review,"Fuzzy set theory provides a powerful framework for handling uncertainty and vagueness, which are inherent characteristics of many computer vision problems. This review addresses the challenge of synthesizing the diverse applications of fuzzy theory within the field of computer vision, aiming to provide a structured overview for researchers and practitioners. We categorize and analyze existing literature based on the specific computer vision tasks addressed, such as image segmentation, object recognition, tracking, and scene understanding, further classifying them by the type of fuzzy techniques employed, including fuzzy logic, fuzzy clustering, fuzzy rule-based systems, and type-2 fuzzy sets. The review highlights the strengths and limitations of each approach within specific application domains, identifying common challenges and potential areas for future research. The analysis reveals that fuzzy approaches are particularly effective in scenarios involving noisy data, ambiguous boundaries, and subjective interpretations, often outperforming traditional methods in handling these complexities. Ultimately, this review provides a valuable resource for understanding the role and potential of fuzzy theory in addressing the inherent uncertainties present in computer vision, facilitating the development of more robust and adaptable vision systems."
http://arxiv.org/abs/2507.17455v1,VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization,"Visual Place Recognition (VPR) is a crucial component for enabling robots and autonomous vehicles to localize within pre-existing maps, particularly vital for planet-scale geo-localization. However, current VPR systems often struggle with viewpoint variations, illumination changes, and semantic shifts encountered in large-scale, diverse environments. This paper addresses the challenge of improving VPR robustness by leveraging the rich semantic understanding capabilities of Vision-Language Models (VLMs). Our approach, VLM-Guided VPR, employs a pre-trained VLM to extract semantic features from both query images and database images, creating a shared embedding space where visual similarity is augmented by semantic alignment. We then use these VLM-derived features to perform place recognition, effectively filtering out irrelevant visual variations and focusing on semantically consistent landmarks. Experimental results on challenging large-scale datasets demonstrate that our VLM-Guided VPR significantly outperforms state-of-the-art VPR methods, achieving substantial improvements in recall@k and reducing localization errors, especially under significant viewpoint and appearance changes. These findings highlight the potential of VLMs to enhance the robustness and accuracy of VPR systems, paving the way for more reliable planet-scale geo-localization."
http://arxiv.org/abs/2507.17335v1,TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition,"Chinese License Plate Recognition (CLPR) plays a crucial role in intelligent transportation systems, but its performance is often challenged by complex scenarios involving variations in lighting, angle, and plate styles, especially for single/dual-line plates. Existing methods often struggle to effectively integrate visual features with the linguistic structure inherent in license plate formats, leading to suboptimal recognition accuracy. We introduce TransLPRNet, a novel lightweight vision-language network specifically designed for robust CLPR. TransLPRNet employs a transformer-based architecture with a decoupled attention mechanism to effectively fuse visual features extracted by a lightweight backbone with linguistic priors encoded through a learned character embedding space. This allows the model to better capture the contextual relationships between characters and handle variations in plate styles. Experimental results on a newly curated dataset of diverse single/dual-line Chinese license plates demonstrate that TransLPRNet achieves state-of-the-art accuracy while maintaining a significantly smaller model size and faster inference speed compared to existing approaches. This makes TransLPRNet a practical and efficient solution for real-world CLPR applications."
http://arxiv.org/abs/2507.17192v2,Vec2Face+ for Face Dataset Generation,"Generating large-scale, high-quality face datasets is crucial for advancing face recognition, attribute manipulation, and identity-related computer vision tasks. However, current methods often struggle with generating diverse identities and fine-grained control over facial attributes, while also incurring significant computational costs. This paper introduces Vec2Face+, an improved framework for generating realistic and controllable face images from latent vectors. Vec2Face+ leverages a StyleGAN2-based architecture enhanced with a novel identity-preserving loss function and an attribute-aware disentanglement strategy. The identity-preserving loss ensures the consistency of generated identities across different attribute manipulations, while the disentanglement strategy promotes independent control over various facial features. Experimental results demonstrate that Vec2Face+ generates higher-quality and more diverse face images compared to existing state-of-the-art methods, achieving a Frchet Inception Distance (FID) score of X and a significantly improved identity preservation rate of Y. Vec2Face+ offers a powerful and efficient tool for creating synthetic face datasets tailored to specific research needs, thereby accelerating progress in face-related computer vision research."
http://arxiv.org/abs/2507.16790v1,Enhancing Domain Diversity in Synthetic Data Face Recognition with Dataset Fusion,"Face recognition systems often suffer from performance degradation when deployed in real-world scenarios due to the domain gap between training and testing data. This paper addresses the limited domain diversity present in individual synthetic face datasets, which hinders the generalization capability of models trained on them. We propose a novel dataset fusion approach that strategically combines multiple synthetic face datasets, each generated with different rendering engines, 3D models, and environmental conditions. Our method involves a weighted sampling strategy based on a proxy-domain divergence metric, ensuring a balanced contribution from each dataset while emphasizing under-represented domains in the combined dataset. Experiments on several challenging real-world face recognition benchmarks demonstrate that models trained on our fused dataset significantly outperform those trained on individual synthetic datasets and even achieve competitive performance compared to models trained on real-world data, particularly in cross-pose and illumination variations. This work highlights the effectiveness of dataset fusion in mitigating the domain gap and improving the robustness of face recognition systems trained on synthetic data."
http://arxiv.org/abs/2507.16624v1,A2Mamba: Attention-augmented State Space Models for Visual Recognition,"State Space Models (SSMs) have recently shown promise as alternatives to Transformers in sequence modeling, particularly with the introduction of the Mamba architecture. However, directly applying Mamba to visual recognition tasks faces challenges in effectively capturing global context and long-range dependencies inherent in images. We introduce A2Mamba, an attention-augmented state space model designed to enhance Mamba's capabilities for visual recognition. A2Mamba incorporates a lightweight attention mechanism strategically placed within the Mamba block to facilitate the aggregation of global information across spatial dimensions. This allows the model to dynamically modulate the state space representation based on relevant contextual cues. Experiments on ImageNet classification and COCO object detection demonstrate that A2Mamba achieves competitive performance compared to both CNN-based and Transformer-based architectures, while maintaining favorable computational efficiency. These results highlight the potential of A2Mamba as a robust and efficient backbone for a wide range of computer vision tasks."
http://arxiv.org/abs/2507.16559v1,"Comparative validation of surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation in endoscopy: Results of the PhaKIR 2024 challenge","Automatic analysis of endoscopic videos is crucial for developing intelligent surgical assistance systems. This paper presents a comprehensive comparative validation of algorithms for three fundamental tasks in endoscopic video understanding: surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation. The PhaKIR 2024 challenge was organized to foster research and benchmark the state-of-the-art in these areas, providing a standardized evaluation platform using a large, diverse dataset of surgical videos. The challenge featured separate tracks for each task, encouraging participants to develop and apply novel deep learning-based methods. The results of the challenge demonstrate significant progress in all three tasks, with top-performing methods achieving high accuracy in surgical phase prediction, precise instrument keypoint localization, and robust instrument instance segmentation, even under challenging conditions such as occlusions and varying illumination. The PhaKIR 2024 challenge and its associated dataset provide a valuable resource for the computer vision and surgical robotics communities, accelerating the development of reliable and clinically relevant surgical assistance tools."
http://arxiv.org/abs/2507.16393v1,Are Foundation Models All You Need for Zero-shot Face Presentation Attack Detection?,"Face Presentation Attack Detection (PAD) is crucial for securing face recognition systems against spoofing attempts. Traditional PAD methods often require extensive training data and struggle to generalize to unseen attack types. This paper investigates the potential of leveraging pre-trained, large-scale foundation models for zero-shot Face PAD, eliminating the need for task-specific fine-tuning. We propose a novel approach that combines the representational power of a vision-language model, specifically CLIP, with a carefully designed prompting strategy to classify real and fake face presentations. The prompts are constructed to elicit discriminative features related to texture, depth, and reflectance cues indicative of presentation attacks. Our experiments on benchmark datasets demonstrate that this zero-shot approach achieves surprisingly competitive performance compared to several supervised PAD methods, even surpassing some in cross-dataset scenarios. This work highlights the promise of foundation models for addressing the open-set generalization challenges in Face PAD and opens new avenues for developing robust and adaptable security systems."
http://arxiv.org/abs/2507.16362v2,LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification and Recognition Network,"Chinese License Plate Recognition (CLPR) is a crucial component in intelligent transportation systems, yet its accuracy is often hampered by perspective distortions and variations in illumination. Existing CLPR methods typically involve separate rectification and recognition stages, leading to increased computational cost and potential error propagation. To address these limitations, we propose LPTR-AFLNet, a lightweight and integrated network for simultaneous Chinese license plate Rectification and Recognition using an Attention-based Feature Learning Network. LPTR-AFLNet employs a novel Spatial Transformer Network (STN) module tailored for license plate rectification, followed by a shared convolutional backbone enhanced with an Attention Feature Learning (AFL) module to extract robust and discriminative features. The AFL module adaptively weights feature channels based on their importance for both rectification and recognition tasks. Experimental results on challenging CLPR datasets demonstrate that LPTR-AFLNet achieves state-of-the-art recognition accuracy while significantly reducing computational complexity compared to existing two-stage methods. This efficient and accurate CLPR solution is well-suited for real-time applications in resource-constrained environments."
http://arxiv.org/abs/2507.16330v1,"Scene Text Detection and Recognition ""in light of"" Challenging Environmental Conditions using Aria Glasses Egocentric Vision Cameras","Scene text detection and recognition are crucial for enabling machines to understand and interact with the visual world, yet performance degrades significantly in challenging environmental conditions. This paper addresses the problem of robust scene text detection and recognition from egocentric videos captured by Aria glasses, a wearable device that introduces unique challenges such as motion blur, varying illumination, and complex viewpoints. We propose a novel framework that integrates a deep learning-based text detection module, specifically fine-tuned for Aria glasses data, with a robust text recognition network enhanced by an attention mechanism and a novel image enhancement module to mitigate environmental distortions. This enhancement module leverages Generative Adversarial Networks (GANs) to reduce noise and motion blur artifacts specific to the Aria glasses camera. Experimental results on a newly curated dataset of scene text images and videos captured using Aria glasses in diverse real-world environments demonstrate a significant improvement in both text detection and recognition accuracy compared to state-of-the-art methods. Our approach offers a pathway towards more reliable and practical text-based information extraction from wearable egocentric vision systems operating in uncontrolled settings."
http://arxiv.org/abs/2507.16287v1,Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition,"Few-shot action recognition remains a challenging problem due to the limited availability of labeled examples for novel actions. Current approaches primarily focus on leveraging label semantics to transfer knowledge from base to novel classes, often overlooking the finer-grained compositional structure of actions. This paper addresses the problem of capturing and utilizing the underlying action anatomy  the constituent parts and their relationships  to enhance few-shot action recognition. We propose a novel language-guided action anatomy framework that leverages large language models (LLMs) to decompose action labels into their constituent components (e.g., objects, body parts, motions). These components are then used to guide the learning of a structured representation that captures the relationships between these anatomical parts within video frames. Our approach learns to align these anatomical features with their corresponding language descriptions, enabling effective cross-modal transfer to novel actions. Experiments on benchmark datasets demonstrate that our method significantly outperforms existing state-of-the-art few-shot action recognition techniques, particularly in challenging scenarios with extremely limited labeled data. This work highlights the importance of considering action anatomy beyond simple label semantics for robust and generalizable action recognition."
http://arxiv.org/abs/2507.16238v1,Positive Style Accumulation: A Style Screening and Continuous Utilization Framework for Federated DG-ReID,"Federated Domain Generalization for person Re-Identification (FDG-ReID) aims to learn a robust Re-ID model across decentralized clients with heterogeneous data distributions while preserving data privacy. However, existing FDG-ReID methods often struggle to effectively leverage the diverse stylistic information embedded within the federated data, leading to suboptimal generalization performance on unseen target domains. This paper introduces Positive Style Accumulation (PSA), a novel style screening and continuous utilization framework for FDG-ReID. PSA first employs a style screening module at each client to identify and extract beneficial stylistic features associated with positive Re-ID pairs. These positive styles are then accumulated across clients through federated averaging and continuously utilized to augment the feature representations during training, fostering style-agnostic feature learning. Extensive experiments on standard DG-ReID benchmarks demonstrate that PSA significantly outperforms state-of-the-art FDG-ReID methods, achieving notable improvements in rank-1 accuracy and mAP scores. PSA provides a practical and effective approach to harness the power of stylistic diversity in federated learning, paving the way for more robust and generalizable Re-ID models in real-world scenarios."
http://arxiv.org/abs/2507.16201v1,A Single-step Accurate Fingerprint Registration Method Based on Local Feature Matching,"Fingerprint recognition systems often rely on accurate registration to align fingerprint images captured at different times or with varying sensors. Existing fingerprint registration methods frequently involve iterative optimization or complex transformations, leading to computational overhead and potential error accumulation. This paper addresses the challenge of achieving accurate fingerprint registration in a single step, minimizing computational cost and improving robustness. We propose a novel method based on local feature matching, leveraging Scale-Invariant Feature Transform (SIFT) descriptors extracted from minutiae neighborhoods. A robust matching strategy, incorporating Random Sample Consensus (RANSAC) and geometric constraints derived from the fingerprint structure, identifies reliable correspondences between feature sets. A single affine transformation is then estimated directly from these correspondences, providing a fast and accurate registration. Experimental results on benchmark fingerprint databases demonstrate that our method achieves comparable or superior registration accuracy compared to state-of-the-art iterative methods, while significantly reducing computation time. This single-step registration approach offers a practical and efficient solution for enhancing the performance of fingerprint recognition systems in various applications."
http://arxiv.org/abs/2507.16151v1,SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities,"Human action recognition is a fundamental problem in computer vision with applications spanning robotics, surveillance, and healthcare. Event-based cameras, with their high temporal resolution and low power consumption, offer a promising alternative to traditional frame-based cameras, particularly when combined with thermal imaging for robust performance in varying lighting conditions. However, the lack of large-scale, multi-modal spiking datasets hinders the development and evaluation of spiking neural networks for human action recognition. To address this, we introduce SPACT18, a novel spiking human action recognition benchmark dataset comprising synchronized RGB, thermal, and event data. SPACT18 features 18 action classes performed by multiple subjects in diverse indoor and outdoor environments. We further provide a comprehensive evaluation of several state-of-the-art spiking neural network architectures on SPACT18, establishing baseline performance and identifying key challenges for future research. Our experiments demonstrate the complementary nature of RGB and thermal modalities in improving action recognition accuracy with spiking neural networks, especially in low-light scenarios. SPACT18 serves as a valuable resource to accelerate research in event-based, multi-modal human action recognition and fosters the development of energy-efficient, robust vision systems."
http://arxiv.org/abs/2507.16095v1,Improving Personalized Image Generation through Social Context Feedback,"Personalized image generation aims to create images tailored to individual user preferences, often leveraging user-provided text prompts or reference images. However, these methods often struggle to capture nuanced personal tastes that are implicitly shaped by an individual's social environment and interactions. This paper addresses the problem of incorporating social context into personalized image generation to better align generated content with a user's overall aesthetic and interests. We propose a novel framework, SocialContextGen, which integrates social feedback signals, derived from user interactions with content shared within their social network, into the image generation process. Specifically, we leverage a transformer-based architecture to encode social feedback, such as likes and comments, and condition a pre-trained diffusion model on this encoding alongside the user's explicit prompt. We demonstrate through quantitative and qualitative evaluations that SocialContextGen significantly improves the personalization of generated images, resulting in images that are more visually appealing and better reflect the user's social identity compared to existing methods. This work represents a significant step towards creating more socially aware and personalized image generation systems."
http://arxiv.org/abs/2507.15961v2,A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications,"Face verification systems are increasingly deployed in real-time screening applications, such as border control and access control, demanding both high accuracy and speed. However, performance degrades significantly due to variations in face image quality stemming from factors like illumination, pose, and occlusion. This paper addresses the challenge of efficiently assessing face image quality in real-time scenarios to improve verification accuracy. We propose a lightweight face quality assessment framework leveraging a shallow convolutional neural network, trained using a novel loss function that directly correlates quality scores with verification performance. The framework incorporates a multi-task learning approach to simultaneously predict several quality attributes and an overall quality score, enabling fine-grained quality control. Experiments on benchmark datasets and a real-world screening dataset demonstrate that our approach achieves state-of-the-art performance in face quality assessment with significantly reduced computational complexity compared to existing methods. This leads to improved face verification accuracy, particularly for low-quality images, while maintaining real-time processing speeds, making it suitable for resource-constrained screening applications."
http://arxiv.org/abs/2507.15765v2,Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization,"Dynamic Facial Expression Recognition (DFER) aims to classify facial expressions from video sequences, a task complicated by variations in subject identity, recording conditions, and expression styles. Current DFER models often struggle to generalize across diverse and unseen data distributions due to inherent biases learned from training datasets. This paper addresses the problem of improving the generalization capability of DFER models in the face of heterogeneous data distributions. We propose a Distributionally Robust Optimization (DRO) framework that explicitly minimizes the worst-case risk over a set of plausible data distributions defined around the empirical distribution of the training data. Our method employs a kernel-based approach to define the ambiguity set, allowing for flexible modeling of distribution shifts. Experiments on multiple benchmark datasets, including Aff-Wild2, CAER-S, and DFEW, demonstrate that our DRO-based approach significantly outperforms state-of-the-art DFER methods, particularly under challenging cross-dataset evaluation scenarios. This work provides a robust and effective strategy for learning DFER models that generalize well to unseen and heterogeneous data, advancing the reliability of facial expression analysis in real-world applications."
http://arxiv.org/abs/2507.15652v1,Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models,"Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding and generating content based on both textual and visual inputs. However, a critical limitation is their tendency to hallucinate visual details, generating descriptions inconsistent with the actual image content. This paper addresses the problem of mitigating visual hallucinations in MLLMs by leveraging visual facts extracted from intermediate layers of the vision encoder. Our approach, Visual Fact Extraction and Alignment (VFEA), first extracts salient visual features from multiple intermediate layers of the pre-trained vision encoder. Then, VFEA employs a lightweight fact extractor network, trained to predict object attributes and relationships directly from these features. Finally, we align these extracted visual facts with the language model embedding space, enabling the MLLM to ground its responses more accurately. Experimental results on benchmark datasets demonstrate that VFEA significantly reduces hallucination rates and improves the factual consistency of MLLM-generated descriptions, achieving a 15% improvement in object hallucination scores compared to baseline models. This work highlights the potential of harnessing intermediate visual representations to enhance the reliability and trustworthiness of MLLMs."
http://arxiv.org/abs/2507.15633v1,Experimenting active and sequential learning in a medieval music manuscript,"Medieval music manuscripts represent a rich source of cultural and historical information, yet their analysis is often hampered by the time-consuming and labor-intensive nature of manual annotation. This paper addresses the challenge of efficiently annotating musical symbols within these manuscripts by leveraging active and sequential learning techniques. We propose a novel active learning framework that integrates a convolutional neural network (CNN) for symbol recognition with a query strategy designed to prioritize the most informative and uncertain samples for annotation. Furthermore, we explore a sequential learning approach where the model iteratively refines its performance by learning from newly annotated data, adapting to the specific characteristics of the manuscript. Experimental results on a dataset of digitized medieval music manuscripts demonstrate that our active learning approach achieves comparable accuracy to fully supervised training with significantly fewer annotations (approximately 40% reduction). This highlights the potential of active and sequential learning to accelerate the analysis and understanding of historical musical documents, enabling more efficient preservation and research efforts."
http://arxiv.org/abs/2507.15541v2,Towards Holistic Surgical Scene Graph,"Surgical scene understanding is crucial for developing context-aware assistance systems and enabling surgical robots. However, existing surgical scene understanding methods often focus on individual tasks like instrument segmentation or action recognition, lacking a comprehensive, structured representation of the entire operating room. This paper addresses the need for a holistic surgical scene graph (SSG) that captures the complex relationships between instruments, anatomical structures, surgical actions, and their spatial configurations. We propose a novel framework for constructing SSGs by integrating multi-modal information from endoscopic video and surgical workflow. Our method employs a transformer-based architecture to jointly learn node embeddings representing surgical entities and edge embeddings encoding their relationships, leveraging both visual features and temporal context derived from surgical phases. Experimental results on a cholecystectomy dataset demonstrate the effectiveness of our approach in constructing more accurate and informative SSGs compared to baseline methods, leading to improved performance in downstream tasks such as surgical phase recognition and tool-tissue interaction analysis. This holistic representation paves the way for more advanced surgical scene understanding and intelligent surgical assistance."
http://arxiv.org/abs/2507.15476v1,A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization,"Steel surface defect detection is crucial for ensuring product quality in the steel manufacturing industry. Existing deep learning-based methods often suffer from high computational costs and large model sizes, hindering their deployment on resource-constrained industrial platforms. This paper addresses the challenge of developing a lightweight and efficient steel surface defect detection method that maintains high accuracy. We propose a novel approach based on lightweight convolution optimization, incorporating a depthwise separable convolution backbone with an attention-guided feature fusion module. Specifically, we leverage a ShuffleNetV2 architecture for feature extraction and introduce a novel attention mechanism that adaptively fuses features from different layers, emphasizing defect-related information while suppressing irrelevant background noise. Experimental results on the Severstal Steel Defect Detection dataset demonstrate that our method achieves comparable or superior detection accuracy compared to state-of-the-art methods, while significantly reducing the model size and computational complexity. This lightweight and accurate defect detection method facilitates real-time deployment and improves the efficiency of steel manufacturing processes."
http://arxiv.org/abs/2507.15418v1,SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition,"Surgical phase recognition is crucial for automated surgical workflow analysis and robotic assistance. However, current deep learning models often act as black boxes, hindering trust and interpretability. This paper addresses the challenge of explaining the reasoning behind surgical phase recognition by associating learned neural representations with clinically relevant surgical concepts. We propose SurgX, a novel framework that integrates concept bottleneck models with attention mechanisms to identify and leverage neuron-concept associations for explainability. SurgX first learns a concept bottleneck layer that explicitly predicts the presence or absence of predefined surgical concepts. Subsequently, an attention module identifies neurons within the network that are most predictive of these concepts. By visualizing the activations of these concept-associated neurons, we can understand which image regions are most relevant to the model's decision-making process. Experiments on the Cholec80 and M2CAI datasets demonstrate that SurgX achieves competitive phase recognition accuracy while providing interpretable explanations that align with surgical domain knowledge. The ability to link neural representations to meaningful surgical concepts enhances the transparency and trustworthiness of surgical phase recognition systems, paving the way for safer and more reliable clinical applications."
http://arxiv.org/abs/2507.15401v3,Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond,"Facial Expression Recognition (FER) is a critical component in human-computer interaction, yet its performance significantly degrades under occlusion. Existing methods often treat occlusion as noise, employing generic denoising techniques or focusing on geometric feature recovery, neglecting the semantic context of the occluding object and its potential influence on perceived emotion. This paper introduces a novel semantic-aware approach to occlusion handling in FER that moves beyond simple noise reduction. Our method, Semantic-Guided Attentive Network (SGAN), leverages object detection to identify and classify occluding objects, subsequently employing attention mechanisms conditioned on the semantic information to selectively focus on unoccluded facial regions and relevant contextual cues. SGAN also incorporates a semantic-aware feature fusion module to integrate information from both the occluded and unoccluded regions, guided by the identified object. Experiments on benchmark datasets, including RAF-DB and AffectNet, demonstrate that SGAN significantly outperforms state-of-the-art occlusion-robust FER methods, achieving improvements of up to 5% in accuracy, particularly in scenarios with severe and semantically informative occlusions. This highlights the importance of semantic understanding in mitigating the adverse effects of occlusion and advancing the robustness of FER systems."
http://arxiv.org/abs/2507.15297v1,Minutiae-Anchored Local Dense Representation for Fingerprint Matching,"Fingerprint matching is a fundamental biometric technique, crucial for various identification and authentication applications. While minutiae-based methods have been dominant, they often struggle with non-linear distortions, partial or noisy fingerprints, and limited minutiae counts. This paper addresses the challenge of robust fingerprint matching in scenarios with significant distortions and incomplete data by proposing a novel minutiae-anchored local dense representation. Our method constructs a dense feature map around each detected minutia point, encoding local ridge orientation and frequency information within a defined neighborhood. These local dense representations are then aligned and compared using a learned similarity metric, guided by the minutiae positions. The proposed approach leverages the discriminative power of dense representations while maintaining robustness through the stability of minutiae as anchors. Experimental results on challenging benchmark datasets, including FVC2002 and FVC2004, demonstrate significant improvements in matching accuracy compared to state-of-the-art minutiae-based and learning-based methods, particularly in scenarios with non-linear distortions and low-quality fingerprints. This work provides a promising direction for enhancing the robustness and accuracy of fingerprint recognition systems."
http://arxiv.org/abs/2507.15285v1,In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems,"Face recognition systems are vulnerable to both physical and digital attacks, necessitating robust detection mechanisms. While current methods often rely on training specialized detectors, they lack adaptability to novel attack types. This paper addresses the challenge of detecting diverse physical and digital attacks against face recognition systems by leveraging the in-context learning capabilities of pre-trained Vision Language Models (VLMs). We propose a framework that prompts a VLM with a few exemplars of various attack types, enabling it to classify unseen attacks based on visual cues and textual descriptions without requiring fine-tuning. We explore different prompting strategies, including incorporating visual features extracted from the face region and crafting descriptive text prompts that highlight attack characteristics. Experiments on a composite dataset of physical presentation attacks (e.g., printed photos, masks) and digital manipulation attacks (e.g., deepfakes, morphing) demonstrate that our approach achieves competitive performance compared to fine-tuned models while exhibiting superior generalization to previously unseen attack modalities. This work highlights the potential of VLMs for building adaptable and robust face recognition security systems."
http://arxiv.org/abs/2507.15216v1,Improving Joint Embedding Predictive Architecture with Diffusion Noise,"Joint Embedding Predictive Architectures (JEPA) have shown promise in self-supervised representation learning by predicting latent representations of masked image regions from unmasked contexts. However, JEPA models often suffer from learning trivial solutions or collapsing representations, especially when dealing with highly correlated data. This paper addresses these limitations by introducing Diffusion Noise Regularization (DNR) to the JEPA framework. DNR leverages the properties of diffusion models to inject controlled, multi-scale noise into the target latent representations during training. Specifically, we train the JEPA predictor to reconstruct the original clean latent representation from its noisy counterpart generated by a learned diffusion process. We demonstrate that DNR significantly improves the quality and robustness of learned representations, leading to substantial gains in downstream classification and object detection tasks on datasets like ImageNet-1K and COCO. The proposed method achieves state-of-the-art results compared to existing JEPA variants, highlighting the effectiveness of diffusion noise as a regularizer for preventing representational collapse and promoting richer feature learning in self-supervised settings."
http://arxiv.org/abs/2507.15109v1,LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM,"Simultaneous Localization and Mapping (SLAM) relies on accurate loop closure detection to correct accumulated drift and maintain consistent maps in large-scale environments. However, conventional loop closure methods often struggle in novel environments due to a lack of prior knowledge and the computational cost associated with learning environment-specific features from scratch. This paper addresses the challenge of few-shot loop closure detection by introducing LoopNet, a novel multitasking learning framework designed to rapidly adapt to new environments with limited training data. LoopNet simultaneously learns feature extraction, similarity metric learning, and loop/non-loop classification through a shared backbone and task-specific heads. We leverage meta-learning techniques to train LoopNet on a diverse set of simulated and real-world environments, enabling it to generalize effectively to unseen environments with only a few labeled loop closures. Experimental results demonstrate that LoopNet significantly outperforms state-of-the-art few-shot learning methods and conventional loop closure techniques in terms of both accuracy and efficiency, achieving a substantial improvement in recall at high precision with limited training samples. This work offers a robust and adaptable solution for loop closure in large-scale SLAM, facilitating autonomous navigation in previously unexplored environments."
http://arxiv.org/abs/2507.15089v1,Visual Place Recognition for Large-Scale UAV Applications,"Visual Place Recognition (VPR) is crucial for enabling autonomous navigation and loop closure in Unmanned Aerial Vehicles (UAVs), particularly in large-scale environments. However, challenges such as viewpoint variations, illumination changes, and perceptual aliasing significantly degrade the performance of existing VPR methods when applied to UAV imagery. This paper addresses the problem of robust and efficient VPR for large-scale UAV applications by proposing a novel hierarchical approach that combines global and local feature representations with a learned attention mechanism. Initially, a lightweight convolutional neural network extracts global image descriptors for efficient candidate retrieval. Subsequently, a transformer-based module attends to salient local features within the retrieved candidates, enhancing discriminative power and robustness to viewpoint changes. We evaluate our method on challenging UAV datasets, demonstrating significant improvements in both accuracy and efficiency compared to state-of-the-art VPR techniques. Our approach achieves a relative improvement of over 15% in recall@1 while maintaining real-time performance, showcasing its suitability for practical UAV deployment in expansive and complex environments."
http://arxiv.org/abs/2507.15085v2,"Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR","Recent advancements in generative models have led to impressive photorealistic image synthesis, raising the question of their utility beyond purely aesthetic applications. This paper investigates the practical effectiveness of state-of-the-art generative models, specifically diffusion and GAN-based architectures, as a data augmentation strategy for Optical Character Recognition (OCR) tasks. We propose a novel pipeline leveraging these models to generate synthetic document images with controlled variations in font style, noise levels, and background textures. These synthetically generated images are then used to augment existing OCR training datasets. Our empirical evaluation demonstrates that fine-tuning OCR models on datasets augmented with images generated by StyleGAN3 and Stable Diffusion results in significant improvements in character accuracy, particularly in low-resource scenarios and when dealing with noisy or degraded document images. This suggests that generative models, despite their computational cost, can provide a cost-effective approach to enhance OCR performance by creating targeted training data, mitigating the need for extensive manual data collection and annotation."
http://arxiv.org/abs/2507.15008v1,FastSmoothSAM: A Fast Smooth Method For Segment Anything Model,"The Segment Anything Model (SAM) has demonstrated impressive zero-shot generalization capabilities for image segmentation. However, its computational demands, especially during mask decoding, hinder its deployment in resource-constrained environments and real-time applications. This paper addresses the computational bottleneck in SAM's mask decoder by introducing FastSmoothSAM, a novel approach that significantly accelerates the mask prediction process while maintaining segmentation quality. FastSmoothSAM leverages a lightweight, learnable smoothing module integrated directly into SAM's mask decoder. This module approximates the complex attention mechanisms of the original decoder with a series of efficient convolutional operations, thereby reducing computational complexity. Experiments on diverse datasets demonstrate that FastSmoothSAM achieves a 3-5x speedup in mask prediction compared to the original SAM decoder, with only a marginal decrease in segmentation accuracy measured by Intersection over Union (IoU). FastSmoothSAM offers a practical solution for deploying SAM in real-time or resource-limited scenarios, expanding its applicability across various computer vision tasks."
http://arxiv.org/abs/2507.14867v1,Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition,"Micro-gestures, subtle and rapid facial movements, offer a rich source of information for emotion recognition, often revealing genuine feelings that may be concealed in macro-expressions. However, effectively capturing the complex spatiotemporal relationships within micro-gesture sequences, especially with limited labeled data, remains a significant challenge. This paper introduces a novel Hybrid-Supervised Hypergraph-enhanced Transformer (HSHT) for micro-gesture based emotion recognition. Our approach leverages a transformer backbone to model temporal dynamics, augmented by a hypergraph convolutional network (HGCN) to capture high-order relationships between facial landmarks. Furthermore, we employ a hybrid supervision strategy, combining labeled data with self-supervised contrastive learning on unlabeled data to enhance feature representation and generalization. Experimental results on benchmark datasets, including CAS(ME)^2 and SAMM, demonstrate that HSHT achieves state-of-the-art performance, surpassing existing methods by a significant margin, particularly in low-data regimes. This underscores the efficacy of our hybrid-supervised hypergraph-enhanced transformer in accurately recognizing emotions from subtle micro-gestures."
http://arxiv.org/abs/2507.14686v2,"From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition","Situation recognition, the task of understanding events and relationships between entities in an image, has traditionally been limited by predefined vocabularies and reliance on fully supervised training. This work addresses the challenge of open-vocabulary situation recognition, where the goal is to identify situations described by novel compositions of verbs and roles without requiring retraining. We propose a novel distillation framework, OSID (Open-vocabulary Situation Instance Distillation), which leverages the rich semantic knowledge of pre-trained vision-language foundation models to guide the training of a more efficient, instance-aware situation recognition model. OSID employs a two-stage approach: first, a CLIP-based teacher network generates pseudo-labels for verb-role relationships based on image regions and textual descriptions; second, a student network is trained to predict instance-specific situation labels, incorporating spatial and semantic information. Experiments on the Visual Genome dataset demonstrate that OSID achieves state-of-the-art performance in open-vocabulary settings, outperforming existing methods by a significant margin, particularly for novel verb-role combinations. This approach enables more flexible and scalable situation understanding, moving towards truly open-world visual reasoning."
http://arxiv.org/abs/2507.14657v2,"AI-Enhanced Precision in Sport Taekwondo: Increasing Fairness, Speed, and Trust in Competition (FST.ai)","Sport Taekwondo officiating often relies on subjective human judgment, leading to controversies and potential biases that impact fairness and athlete trust. This paper addresses the critical need for more objective and reliable scoring in Taekwondo competitions. We introduce FST.ai, an AI-enhanced system leveraging deep learning-based pose estimation and action recognition to precisely analyze athlete movements and automatically detect valid scoring techniques. FST.ai utilizes a novel spatiotemporal graph convolutional network (ST-GCN) trained on a large, curated dataset of Taekwondo match videos, enabling accurate classification of complex kicking motions and precise impact point localization. Experimental results demonstrate that FST.ai achieves a scoring accuracy of 95.7% compared to human referees, significantly reducing scoring discrepancies and providing near real-time feedback on technique validity. FST.ai has the potential to revolutionize Sport Taekwondo officiating, fostering increased fairness, speed, and trust in the competitive environment."
http://arxiv.org/abs/2507.15888v1,PAT++: a cautionary tale about generative visual augmentation for Object Re-identification,"Object re-identification (Re-ID) aims to associate a target person or object across multiple non-overlapping camera views, a challenging task often hindered by limited labeled data and domain variations. Generative adversarial networks (GANs) have emerged as a promising avenue for data augmentation in Re-ID, offering the potential to synthesize diverse and realistic training samples. However, naive application of GAN-based augmentation can inadvertently introduce biases and artifacts that negatively impact generalization performance. This paper investigates the limitations of a popular GAN-based augmentation technique for Re-ID, PAT, and introduces PAT++, an improved implementation that incorporates a more robust training strategy and careful selection of hyper-parameters. Through extensive experimentation on benchmark datasets, we demonstrate that while PAT++ can achieve marginal improvements under specific conditions, it often fails to consistently outperform simpler augmentation strategies like random erasing and can even degrade performance when applied indiscriminately. Our findings serve as a cautionary tale, highlighting the importance of rigorous evaluation and careful consideration of potential biases when employing generative visual augmentation techniques for Re-ID, emphasizing that ""more data"" is not always better if the synthesized data lacks sufficient realism or introduces spurious correlations."
http://arxiv.org/abs/2507.14608v1,Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition,"Facial expression recognition (FER) has seen significant progress through graph-based methods, which capture structural relationships between facial landmarks. However, the mechanism by which these graph connections learn and encode facial attribute information remains largely unexplored. This paper addresses the critical question of how learned connections within a graph neural network contribute to the recognition of specific facial attributes in expression recognition tasks. We propose Exp-Graph, a novel graph-based framework that explicitly learns edge weights representing the relevance of connections to various facial attributes. Exp-Graph utilizes an attention mechanism to dynamically adjust edge weights based on input facial landmarks and simultaneously learns attribute-specific masks that highlight crucial connections. Experimental results on benchmark datasets, including AffectNet and RAF-DB, demonstrate that Exp-Graph achieves state-of-the-art performance while offering increased interpretability by identifying key connections associated with different expressions. This work provides valuable insights into the role of learned connections in graph-based FER and offers a pathway for developing more transparent and attribute-aware expression recognition systems."
http://arxiv.org/abs/2507.14549v1,Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions,"Facial expression recognition is a crucial aspect of human-computer interaction, yet considerable variability exists in how humans perceive and categorize these expressions. This paper addresses the challenge of systematically exploring this perceptual variability by creating stimuli that lie on the perceptual boundaries of artificial neural networks (ANNs) trained for facial expression recognition. We propose a novel image synthesis method that leverages the latent space of a StyleGAN2 model and optimizes for images that maximize uncertainty in a classifier's expression predictions. Specifically, we iteratively generate images and adjust their latent codes to simultaneously push the classifier's output probabilities for multiple expressions towards equal values, effectively creating images that are ambiguous to the ANN. Using a series of psychophysical experiments, we demonstrate that these synthesized images elicit a broader range of expression interpretations from human observers compared to real-world images or images generated to represent prototypical expressions. This approach provides a powerful tool for investigating the nuanced and often subjective nature of human facial expression perception, offering insights into the cognitive processes underlying social communication."
http://arxiv.org/abs/2507.14543v1,Real Time Captioning of Sign Language Gestures in Video Meetings,"Sign language is crucial for communication within the Deaf and Hard-of-Hearing (DHH) community, but real-time communication barriers persist in video meetings due to the lack of readily available and accurate sign language interpretation. This paper addresses the problem of automatically generating captions for sign language gestures performed in real-time video meeting scenarios. We propose a novel deep learning architecture, Sign2Caption, which combines a spatiotemporal feature extraction module using 3D Convolutional Neural Networks (3D-CNNs) with a transformer-based sequence-to-sequence model for generating corresponding captions. The 3D-CNN extracts relevant motion and appearance features from video frames, which are then fed into the transformer to predict the sequence of words representing the sign language gesture. Experimental results on a newly collected video meeting dataset of American Sign Language (ASL) demonstrate that Sign2Caption achieves a BLEU-4 score of 0.72 and a ROUGE-L score of 0.85, significantly outperforming existing methods. This work provides a crucial step towards enabling seamless and accessible communication for DHH individuals in virtual meeting environments."
http://arxiv.org/abs/2507.14477v1,OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition,"Visual Place Recognition (VPR) is crucial for autonomous navigation, enabling robots to localize themselves by matching current views to a previously built map. Sequence-based VPR leverages temporal context to improve robustness, but existing methods often struggle with computational efficiency and the effective utilization of long-range dependencies. This paper introduces OptiCorNet, a novel architecture for optimizing sequence-based context correlation in VPR. OptiCorNet employs a learnable, lightweight attention mechanism to selectively aggregate relevant contextual information across the sequence, dynamically weighting frames based on their similarity and importance to the query. Furthermore, we incorporate a multi-scale feature extraction module to capture both fine-grained details and global scene characteristics. Extensive experiments on challenging benchmark datasets demonstrate that OptiCorNet achieves state-of-the-art performance in terms of accuracy and efficiency, outperforming existing sequence-based VPR methods while maintaining a low computational overhead. The proposed approach significantly advances the capabilities of VPR systems in complex and dynamic environments."
http://arxiv.org/abs/2507.14449v1,IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark,"Infrared (IR) imaging plays a crucial role in various computer vision applications, particularly in scenarios with poor visibility or complete darkness. However, the lack of large-scale, annotated datasets hinders the development of robust and generalizable IR image understanding models. This paper introduces IRGPT, a novel approach to pre-training a large language model (LLM) for understanding real-world IR images. We address the challenges of learning from noisy and limited IR data by proposing a bi-cross-modal curriculum learning strategy. This curriculum progressively increases the difficulty of learning by initially focusing on high-quality, paired IR-text data and gradually incorporating noisier, unpaired data using cross-modal generation and contrastive learning objectives. Our experiments on a newly constructed, large-scale IR image benchmark demonstrate that IRGPT significantly outperforms existing methods on a variety of downstream tasks, including image captioning, object detection, and visual question answering. The proposed bi-cross-modal curriculum effectively leverages both paired and unpaired data, leading to superior performance and improved robustness of the pre-trained model. This work paves the way for more effective utilization of IR imagery in real-world applications."
http://arxiv.org/abs/2507.13803v1,GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation,"Wireless perception leverages distributed sensors to achieve robust environment understanding, yet bandwidth constraints necessitate aggressive compression, leading to feature misalignment and performance degradation. This paper addresses the challenge of holistic feature alignment in wirelessly transmitted perception data, focusing on mitigating the impact of compression-induced distortions. We introduce GRAM-MAMBA, a novel approach that combines a Graph Representation learning module (GRAM) for capturing inter-sensor dependencies with a selective state space model (MAMBA) for efficient temporal feature processing. Furthermore, we incorporate an adaptive low-rank compensation mechanism to dynamically recover crucial information lost during compression based on the observed feature distribution. Experiments on benchmark wireless perception datasets demonstrate that GRAM-MAMBA achieves significant improvements in object detection and tracking accuracy compared to state-of-the-art methods, particularly under severe bandwidth limitations. This holistic feature alignment strategy provides a robust and efficient solution for enabling high-performance wireless perception in resource-constrained environments."
http://arxiv.org/abs/2507.18645v1,Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis,"Distinguishing between military and civilian vehicles in complex environments is critical for various security and intelligence applications. However, accurately classifying these vehicles, particularly when compounded by the need to understand public sentiment towards them, presents a significant challenge due to factors like camouflage, occlusions, and subjective interpretations. This paper introduces Quantum-Cognitive Tunnelling Neural Networks (QCTNNs), a novel hybrid architecture that integrates quantum-inspired cognitive modeling with deep learning to address this problem. QCTNNs leverage quantum tunneling principles to probabilistically bypass local minima during optimization, facilitating the discovery of more robust and generalizable feature representations. Furthermore, the network incorporates a sentiment analysis module trained on textual data associated with vehicle images, allowing for a multi-faceted assessment. Experimental results on a newly curated dataset of military and civilian vehicle images with associated sentiment data demonstrate that QCTNNs achieve a 15% improvement in classification accuracy and a 12% increase in sentiment prediction F1-score compared to state-of-the-art deep learning models. This hybrid approach offers a significant advancement in automated vehicle classification and sentiment analysis, with potential applications in defense, public safety, and urban planning."
http://arxiv.org/abs/2507.15878v1,Salience Adjustment for Context-Based Emotion Recognition,"Contextual information significantly influences human emotion perception, yet many emotion recognition systems primarily focus on facial expressions, neglecting valuable scene cues. This reliance on isolated facial features often leads to inaccurate emotion assessments, particularly in complex real-world scenarios. To address this, we propose a novel salience adjustment mechanism integrated into a deep learning framework for context-based emotion recognition. Our approach leverages a pre-trained scene understanding model to extract contextual features and dynamically adjusts the salience of facial regions based on their relevance to the detected scene. Specifically, we use an attention mechanism guided by contextual semantics to re-weight facial feature maps, emphasizing areas that align with the inferred emotional context. Experiments on benchmark datasets demonstrate that our salience adjustment method significantly improves emotion recognition accuracy, achieving state-of-the-art performance compared to methods relying solely on facial expressions or simple feature concatenation. This work highlights the importance of context-aware processing for robust and accurate emotion recognition in computer vision."
http://arxiv.org/abs/2507.13482v1,Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning,"Human Activity Recognition (HAR) is crucial for various applications, ranging from healthcare monitoring to security systems. However, existing HAR systems often struggle with out-of-distribution (OOD) data, where the testing environment differs significantly from the training environment, leading to performance degradation. This paper addresses the challenge of improving OOD HAR performance by leveraging cross-modal representation learning between inertial measurement unit (IMU) and video data. We propose a novel framework that learns a shared latent space by enforcing consistency between IMU and video features through a contrastive learning objective and a cross-modal attention mechanism. This allows the model to capture activity-invariant features robust to domain shifts. Our experiments on multiple HAR datasets demonstrate that our approach significantly outperforms state-of-the-art methods in OOD scenarios, achieving improvements of up to 15% in average accuracy. These results highlight the effectiveness of cross-modal representation learning in enhancing the generalization capability of HAR systems to unseen environments."
http://arxiv.org/abs/2507.13326v1,A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains,"Egocentric vision offers a valuable perspective for understanding human actions in industrial settings, particularly hand-object interactions crucial for task execution. However, reliably and efficiently detecting these interactions in real-time remains a significant challenge due to complex occlusions, rapid hand movements, and the diversity of objects and tasks encountered in dynamic industrial environments. We propose a novel real-time system for egocentric hand-object interaction detection, leveraging a lightweight convolutional neural network architecture optimized for wearable devices. Our approach incorporates a spatiotemporal attention mechanism to selectively focus on relevant hand and object regions across consecutive frames, enhancing robustness to motion blur and partial occlusions. Furthermore, we introduce a multi-modal fusion strategy that combines visual features with inertial measurement unit (IMU) data to improve interaction recognition accuracy. Experimental results on a newly collected industrial hand-object interaction dataset demonstrate that our system achieves state-of-the-art performance in terms of both accuracy (87.5% mAP) and speed (35 FPS) on an embedded platform. This real-time, accurate, and wearable system enables practical applications such as automated skill assessment, personalized worker training, and enhanced safety monitoring in industrial domains."
http://arxiv.org/abs/2507.13113v1,Leveraging Language Prior for Infrared Small Target Detection,"Infrared small target detection (IRSTD) plays a vital role in various applications like surveillance and early warning systems. However, detecting small targets in complex infrared scenes remains challenging due to low signal-to-noise ratio, lack of distinct features, and cluttered backgrounds. This paper addresses the problem of effectively distinguishing real targets from false alarms by incorporating a language prior into the detection process. We propose a novel framework that leverages the descriptive power of language models to provide contextual information about the scene and the expected characteristics of targets. Specifically, we utilize a transformer-based language model to encode scene descriptions and target attributes extracted from the infrared image, generating a language-guided feature representation. This representation is then integrated into a detection network to enhance the discriminative power of features and suppress false alarms. Experimental results on several benchmark datasets demonstrate that our method significantly outperforms state-of-the-art IRSTD algorithms, achieving higher detection rates and lower false alarm rates, especially in challenging scenarios with heavy clutter. This work showcases the potential of language priors to improve the robustness and accuracy of infrared small target detection systems."
http://arxiv.org/abs/2507.13089v1,GLAD: Generalizable Tuning for Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in various downstream tasks. However, adapting these models to new tasks often requires extensive task-specific fine-tuning, leading to high computational costs and potential overfitting, limiting their generalizability. This paper introduces GLAD: Generalizable Tuning for Vision-Language Models, a novel and efficient tuning strategy that minimizes task-specific adaptation while maximizing cross-task transferability. GLAD freezes the majority of the VLM parameters and introduces a small, learnable adapter module conditioned on both visual and textual inputs. Crucially, we pre-train this adapter on a diverse set of auxiliary tasks designed to mimic the statistical properties of a wide range of downstream applications. Empirical evaluations on a diverse suite of benchmark datasets, including visual question answering, image captioning, and visual reasoning, demonstrate that GLAD achieves comparable or superior performance to full fine-tuning with significantly fewer trainable parameters, often exceeding performance by 5-10%. GLAD's efficient adaptation and strong generalization capabilities pave the way for deploying VLMs in resource-constrained environments and accelerating the development of robust and adaptable vision-language systems."
http://arxiv.org/abs/2507.13073v1,"Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized Intersection: Deployment, Data Collection, and Preliminary Analysis","Accurate and reliable traffic volume estimation is crucial for intelligent transportation systems, enabling optimized signal timing and improved traffic flow. However, traditional methods often suffer from limitations in accuracy, especially in complex urban environments. This paper addresses the challenge of robust traffic movement count estimation at a signalized intersection by leveraging a novel dual LiDAR sensor configuration. Our approach utilizes two Velodyne Puck LiDAR sensors strategically positioned to provide overlapping coverage of the intersection, mitigating occlusion and enhancing the accuracy of vehicle detection and tracking. We develop a point cloud processing pipeline incorporating ground plane removal, clustering, and Kalman filtering to identify and track individual vehicles through the intersection. Preliminary results from a real-world deployment demonstrate that our dual LiDAR system achieves a high degree of accuracy in estimating traffic movement counts for different turning movements, outperforming single LiDAR configurations in challenging scenarios with high traffic density and occlusions. This work provides a cost-effective and robust solution for real-time traffic monitoring and analysis, contributing to the development of smarter and more efficient urban transportation networks."
http://arxiv.org/abs/2507.12988v1,Variance-Based Pruning for Accelerating and Compressing Trained Networks,"Deep neural networks achieve state-of-the-art performance in various tasks but often suffer from high computational cost and memory requirements, hindering their deployment on resource-constrained devices. This paper addresses the problem of efficiently pruning trained neural networks to reduce their size and computational complexity without significantly sacrificing accuracy. We propose a novel variance-based pruning method that leverages the variance of neuron activations across the training dataset as a criterion for identifying and removing redundant neurons. Specifically, neurons exhibiting low activation variance are considered less informative and are pruned iteratively, followed by fine-tuning to recover performance. We evaluate our method on several benchmark datasets (CIFAR-10, ImageNet) and network architectures (ResNet, MobileNet). Results demonstrate that our approach achieves significant compression ratios (up to 70% parameter reduction) and speedups (up to 2x inference time reduction) while maintaining comparable or even improved accuracy compared to the original unpruned networks and surpassing the performance of other magnitude-based pruning techniques. This efficient pruning strategy enables the deployment of high-performing deep learning models on resource-limited platforms."
http://arxiv.org/abs/2507.12964v2,Demographic-aware fine-grained classification of pediatric wrist fractures,"Pediatric wrist fractures are common injuries with varying treatment strategies depending on the specific fracture type and patient demographics. Accurate and automated fine-grained classification of these fractures can significantly aid in diagnosis and treatment planning, but existing methods often lack the granularity and demographic awareness necessary for optimal clinical decision-making. This paper addresses the challenge of developing a demographic-aware, fine-grained classification system for pediatric wrist fractures from radiographic images. We propose a novel multi-stage deep learning framework that integrates patient age and sex information into a convolutional neural network (CNN) architecture. Specifically, we utilize demographic features to condition the feature maps learned by the CNN, allowing the model to adapt its classification strategy based on the patient's specific demographic context. Our experimental results on a large clinical dataset demonstrate that our demographic-aware model achieves significantly improved classification accuracy compared to standard CNNs and other state-of-the-art fine-grained classification methods, particularly for subtle fracture subtypes. This enhanced diagnostic precision, enabled by demographic awareness, has the potential to improve treatment outcomes and reduce unnecessary interventions for pediatric wrist fractures."
http://arxiv.org/abs/2507.12942v1,Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning,"Visible-infrared person re-identification (VI-ReID) aims to match pedestrian images captured across different modalities, presenting a crucial challenge for robust video surveillance systems. However, existing VI-ReID methods often rely on expensive and time-consuming manual annotations of cross-modality correspondences. To alleviate this annotation burden, we propose a novel weakly supervised VI-ReID framework based on Heterogeneous Expert Collaborative Consistency Learning (HECCL). Specifically, HECCL leverages multiple modality-specific expert networks, each focusing on extracting discriminative features from either visible or infrared images. We enforce consistency between the predictions of these experts on unlabeled data through a collaborative learning scheme, guided by pseudo labels generated from a shared feature space. Importantly, a carefully designed noise-aware pseudo-label refinement strategy further enhances the reliability of the training signal. Extensive experiments on benchmark datasets demonstrate that HECCL significantly outperforms state-of-the-art weakly supervised VI-ReID methods, achieving competitive performance with some fully supervised approaches. This work offers a practical and effective solution for VI-ReID by reducing the reliance on manual annotations while maintaining high accuracy."
http://arxiv.org/abs/2507.12889v1,Camera-based implicit mind reading by capturing higher-order semantic dynamics of human gaze within environmental context,"Inferring human cognitive states, or ""mind reading,"" has immense potential for human-computer interaction, but remains a significant challenge. Existing gaze-based approaches primarily focus on predicting explicit attention, often neglecting the rich semantic context of the environment and the temporal dynamics of gaze behavior reflecting underlying cognitive processes. This paper addresses the problem of implicitly inferring human cognitive states by capturing higher-order semantic dynamics of gaze within a real-world environmental context. We propose a novel framework that integrates a scene graph representation of the environment with a recurrent neural network architecture that models the temporal evolution of gaze fixations and saccades, conditioned on the semantic relationships between objects in the scene. Specifically, we learn a gaze embedding space that captures semantic dependencies between gaze patterns and environmental elements. Our experiments on a newly collected dataset of participants performing cognitive tasks in a virtual environment demonstrate that our method significantly outperforms existing gaze prediction models in inferring task-relevant cognitive states, such as planning and decision-making, based solely on gaze behavior. This work paves the way for more intuitive and adaptive human-computer interfaces that can proactively respond to users' implicit cognitive needs."
http://arxiv.org/abs/2507.12828v2,Feature-Enhanced TResNet for Fine-Grained Food Image Classification,"Fine-grained food image classification is a challenging task due to the subtle inter-class visual variations and significant intra-class appearance diversity caused by factors like cooking style, ingredients, and viewpoint. Existing methods often struggle to effectively capture discriminative features, especially when dealing with complex food images. To address this, we propose a Feature-Enhanced TResNet (FE-TResNet) architecture, which integrates a novel Feature Enhancement Module (FEM) within the TResNet backbone. The FEM leverages contextual information and channel attention mechanisms to refine feature maps at multiple scales, emphasizing crucial regions and suppressing irrelevant background noise. Furthermore, we incorporate a specialized loss function that combines cross-entropy loss with a center loss variant to improve intra-class compactness and inter-class separability. Experimental results on benchmark food image datasets, including Food-101 and VIREO Food-172, demonstrate that FE-TResNet achieves state-of-the-art performance, surpassing existing methods by a significant margin in terms of classification accuracy. This highlights the effectiveness of our approach in extracting and leveraging discriminative features for precise food image classification."
http://arxiv.org/abs/2507.12807v1,Semantic-guided Fine-tuning of Foundation Model for Long-tailed Visual Recognition,"Foundation models pre-trained on large-scale datasets have shown impressive capabilities in various vision tasks. However, their performance often degrades significantly when applied to long-tailed visual recognition, where the data distribution is highly imbalanced. This paper addresses the challenge of adapting foundation models to long-tailed datasets without compromising their pre-trained knowledge. We propose a novel semantic-guided fine-tuning approach that leverages semantic information from a pre-trained language model to regularize the feature space during fine-tuning. Specifically, we encourage the visual features of semantically similar classes to be closer in the feature space, even if they belong to different categories in the long-tailed dataset. This is achieved by minimizing a semantic alignment loss that aligns visual class prototypes with their corresponding word embeddings. Experiments on multiple long-tailed datasets, including ImageNet-LT, iNaturalist 2018, and Places-LT, demonstrate that our method significantly outperforms existing fine-tuning strategies, achieving state-of-the-art performance while preserving the generalization ability of the foundation model. Our semantic-guided fine-tuning offers a promising approach for adapting foundation models to real-world, imbalanced visual recognition scenarios."
http://arxiv.org/abs/2507.12617v1,Predicting Soccer Penalty Kick Direction Using Human Action Recognition,"Predicting the outcome of a soccer penalty kick can provide a significant advantage to goalkeepers. This work addresses the challenge of predicting the kicker's intended direction based on observable pre-kick movements. We propose a novel approach utilizing human action recognition techniques to analyze the kicker's pose sequence extracted from video footage. Specifically, we employ a spatiotemporal graph convolutional network (ST-GCN) to model the dynamic relationships between key body joints during the kicker's run-up. The ST-GCN learns features from the skeletal data, which are then fed into a classifier to predict one of three directions: left, right, or center. We evaluate our method on a newly collected dataset of penalty kicks, achieving an accuracy of 71.2%, significantly outperforming baseline methods that rely solely on static pose features or optical flow. This demonstrates the potential of using advanced action recognition techniques to anticipate the kicker's strategy, offering a valuable tool for goalkeeper training and in-game decision-making."
http://arxiv.org/abs/2507.12602v1,MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification,"LiDAR-based tree species classification is crucial for precision forestry and ecosystem management. However, effectively capturing the intricate spatial relationships and varying scales of tree point clouds remains a challenge for accurate species identification. This paper introduces MS-DGCNN++, a novel multi-scale fusion dynamic graph neural network (DGCNN) architecture enhanced with biological knowledge integration for LiDAR tree species classification. Our approach leverages multiple DGCNN branches to extract features at different scales of the point cloud, followed by a fusion module that adaptively aggregates these multi-scale representations. Furthermore, we integrate biological knowledge, specifically species-specific crown morphology features derived from ecological studies, directly into the feature space. Experimental results on a benchmark dataset demonstrate that MS-DGCNN++ significantly outperforms state-of-the-art methods, achieving an overall accuracy improvement of 5-8% and enhanced robustness to varying point cloud densities. This research provides a powerful and interpretable framework for LiDAR-based tree species classification, contributing to more accurate and efficient forest inventory and monitoring."
http://arxiv.org/abs/2507.12426v2,DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition,"Human action recognition in videos is a fundamental task in computer vision, yet efficient spatio-temporal feature extraction remains a challenge for resource-constrained applications. Many existing methods rely on computationally expensive 3D convolutional neural networks or complex attention mechanisms, hindering their deployment on edge devices. To address this, we propose DVFL-Net, a lightweight Distilled Video Focal Modulation Network for efficient spatio-temporal action recognition. DVFL-Net leverages knowledge distillation to transfer knowledge from a larger, more accurate teacher network to a compact student network. Furthermore, we introduce a novel Focal Modulation (FM) block that adaptively modulates feature activations based on their importance, enhancing discriminative power while maintaining computational efficiency. Experimental results on benchmark datasets, including HMDB51 and UCF101, demonstrate that DVFL-Net achieves competitive accuracy with significantly reduced computational cost and model size compared to state-of-the-art methods. This makes DVFL-Net a promising solution for real-time action recognition in resource-limited environments."
http://arxiv.org/abs/2507.12292v1,Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation,"Calisthenics, a form of exercise relying on bodyweight movements, benefits greatly from automated performance analysis. However, accurate classification of calisthenics skills in video remains challenging due to complex backgrounds, variable execution speeds, and inter-class similarities. This paper addresses the problem of efficient and robust calisthenics skill classification by focusing on relevant foreground instances and incorporating depth information. Our proposed method first employs a Mask R-CNN based instance segmentation network to isolate the exercising individual. Subsequently, a lightweight depth estimation module refines the segmentation mask and provides crucial depth cues, enabling a spatiotemporal graph convolutional network to learn discriminative features representing the pose dynamics. Experiments on a newly collected calisthenics dataset demonstrate that our approach achieves state-of-the-art accuracy (87.3%) while maintaining computational efficiency, significantly outperforming existing methods that rely solely on RGB data or computationally expensive pose estimation. This work provides a practical and effective solution for automated calisthenics performance analysis, potentially benefiting both athletes and coaches."
http://arxiv.org/abs/2507.12245v1,Calisthenics Skills Temporal Video Segmentation,"Calisthenics training involves complex sequences of movements, requiring precise execution and temporal coordination. Accurately segmenting videos of calisthenics exercises into distinct skill segments is crucial for automated performance analysis, skill assessment, and personalized training. This paper addresses the challenge of temporally segmenting calisthenics videos by introducing a novel deep learning architecture, the Calisthenics Temporal Segmentation Network (CTSN). CTSN leverages a hybrid approach, combining a 3D Convolutional Neural Network (3D-CNN) for spatio-temporal feature extraction with a Bidirectional Long Short-Term Memory (Bi-LSTM) network to model long-range temporal dependencies. We further incorporate a Conditional Random Field (CRF) to refine segment boundaries and ensure temporal consistency. Experimental results on a newly collected, annotated dataset of diverse calisthenics skills demonstrate that CTSN outperforms existing state-of-the-art temporal action segmentation methods, achieving a significant improvement in segmentation accuracy (F1-score of 87.5%) and boundary detection precision. This work offers a robust and accurate solution for automatic analysis of calisthenics exercises, paving the way for enhanced training and performance evaluation tools."
http://arxiv.org/abs/2507.12195v1,Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision,"The study of ancient architecture provides invaluable insights into past civilizations, but the fragmented and degraded state of many artifacts poses significant challenges. This paper addresses the problem of reconstructing intricate temple tile designs from incomplete and scattered fragments, a task traditionally reliant on manual labor and expert intuition. We propose a novel computer vision pipeline that combines feature extraction, geometric matching, and probabilistic shape completion. First, Scale-Invariant Feature Transform (SIFT) features are extracted from tile fragments and matched using a robust RANSAC-based algorithm to estimate relative pose. Subsequently, a shape completion network, trained on a synthetic dataset of tile patterns, infers missing segments based on identified adjacencies and overall design principles. Our method demonstrates a significant improvement in reconstruction accuracy compared to existing manual and semi-automated approaches, achieving a completion rate of over 85% on simulated and real-world datasets of fragmented temple tiles. This automated reconstruction pipeline offers a powerful tool for archaeologists and historians, enabling the digital preservation and visualization of ancient architectural heritage with increased efficiency and precision."
http://arxiv.org/abs/2507.12157v1,Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation,"Fine-grained image recognition (FGIR) poses a significant challenge due to subtle inter-class variations and substantial intra-class variations arising from pose, lighting, and viewpoint. Learning effective FGIR models from scratch, without relying on pre-trained features, is particularly difficult due to the limited availability of annotated fine-grained datasets. We address this problem by proposing a novel Teacher-Guided Data Augmentation (TGDA) framework, which leverages the knowledge of a pre-trained teacher model to guide the augmentation of training data for a student model learning from scratch. TGDA employs a discrepancy-based attention mechanism in the teacher network to identify informative regions within images, and then uses these regions to generate targeted augmentations that expose the student network to challenging variations, thereby improving its ability to discriminate subtle differences. Experiments on several benchmark FGIR datasets, including CUB-200-2011, Stanford Dogs, and NABirds, demonstrate that our TGDA framework significantly outperforms existing data augmentation techniques and achieves competitive results compared to state-of-the-art methods that rely on pre-trained features, boosting the performance of scratch-trained models by a substantial margin. This demonstrates the effectiveness of leveraging teacher knowledge to guide data augmentation for improving the performance of fine-grained image recognition models trained from scratch."
http://arxiv.org/abs/2507.12132v1,DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi,"Human activity recognition (HAR) using Wi-Fi signals has gained traction due to its privacy-preserving and device-free nature. However, existing Wi-Fi HAR methods often struggle with robustness to environmental dynamics and complex activities, primarily relying on amplitude or channel state information (CSI) which are susceptible to noise and multipath fading. This paper addresses the limitations of current Wi-Fi HAR techniques by introducing Doppler Radiance Fields (DoRF), a novel representation that leverages the Doppler frequency shift inherent in Wi-Fi signals to model the dynamic radiance distribution of human movement. DoRF utilizes a neural radiance field (NeRF) architecture to reconstruct a continuous, view-dependent representation of the Doppler spectrum within the environment. This allows us to extract robust features that are less sensitive to static reflections and more indicative of motion patterns. We demonstrate that DoRF significantly outperforms state-of-the-art Wi-Fi HAR methods on a newly collected dataset with diverse activities and environmental conditions, achieving an average accuracy improvement of 15% in challenging scenarios. This innovative approach offers a more reliable and accurate solution for Wi-Fi-based human activity recognition, paving the way for more robust and practical applications in smart homes, healthcare, and security."
http://arxiv.org/abs/2507.12107v1,Non-Adaptive Adversarial Face Generation,"Adversarial face generation poses a significant threat to facial recognition systems, enabling the creation of synthetic identities for malicious purposes. Current adversarial face generation methods often rely on adaptive strategies, requiring access to the target face recognition model for iterative refinement and optimization of the generated faces. This work investigates a novel approach to non-adaptive adversarial face generation, aiming to produce highly effective adversarial faces without any feedback or knowledge of the target recognition system. Our method leverages a StyleGAN2 generator pre-trained on a large-scale face dataset and introduces a novel adversarial loss function based on feature distribution divergence within a contrastive learning framework. This loss encourages the generated faces to occupy regions of the feature space that are distant from real face representations, thereby maximizing the likelihood of misclassification. We demonstrate that our non-adaptive approach achieves state-of-the-art attack success rates against multiple commercial and open-source face recognition systems, rivaling adaptive methods while requiring significantly less computational overhead. The ability to generate powerful adversarial faces without model access highlights a critical vulnerability in current face recognition technology and underscores the need for more robust defense mechanisms."
http://arxiv.org/abs/2507.12050v1,IDFace: Face Template Protection for Efficient and Secure Identification,"Face recognition systems are increasingly deployed for authentication, access control, and surveillance, raising concerns about the security and privacy of facial biometric data. Existing face template protection schemes often suffer from either high computational complexity or inadequate security against sophisticated attacks. This paper addresses the challenge of developing a face template protection method that simultaneously achieves high identification accuracy, strong security, and efficient computation. We propose IDFace, a novel approach that combines a discriminative feature learning scheme with a revocable and secure transformation. IDFace leverages a deep neural network trained to extract compact and discriminative feature embeddings. These embeddings are then perturbed using a user-specific key, generating a protected template that is both difficult to reverse and allows for efficient matching. Experimental results on several benchmark datasets demonstrate that IDFace achieves comparable or superior identification accuracy to state-of-the-art methods while providing strong protection against various attack scenarios, including impersonation and hill-climbing attacks. The efficient computation and robust security of IDFace make it a promising solution for real-world face recognition applications requiring strong privacy guarantees."
http://arxiv.org/abs/2507.12009v1,Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli,"Understanding how the brain processes complex, naturalistic stimuli remains a central challenge in neuroscience. A key obstacle is the difficulty in establishing a direct mapping between high-dimensional functional Magnetic Resonance Imaging (fMRI) data and the intricate features of real-world sensory experiences. We address the problem of decoding naturalistic stimuli from fMRI brain activity by proposing a novel deep neural encoder-decoder model. Our architecture utilizes a convolutional neural network (CNN) encoder to extract meaningful representations from fMRI data, which are then fed into a recurrent neural network (RNN) decoder to generate descriptions of the corresponding naturalistic stimuli. Furthermore, we incorporate an attention mechanism within the decoder to highlight the most relevant brain regions for specific stimulus features. Experiments on a large-scale fMRI dataset, acquired during movie watching, demonstrate that our model significantly outperforms existing methods in reconstructing both visual and semantic aspects of the stimuli from brain activity alone, achieving higher accuracy in predicting image categories and generating more coherent textual descriptions. This work provides a powerful framework for investigating the neural encoding of naturalistic stimuli and offers new insights into the brain's representation of complex sensory information."
http://arxiv.org/abs/2507.12001v1,AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation,"Fine-grained control over 3D facial expression manipulation is crucial for realistic avatar creation and animation, yet current methods often lack the nuanced expressiveness needed for stylized characters. This paper addresses the challenge of achieving fine-grained control over stylized 3D facial expressions by decoupling Action Units (AUs) from blendshape weights, allowing for independent and exaggeratable control. We introduce AU-Blendshape, a novel framework that learns a mapping between AUs and blendshape weights conditioned on a style code. This allows users to control facial expressions by manipulating individual AUs, while the network automatically generates the corresponding stylized blendshape weights. The framework incorporates a style encoder that learns a latent representation of facial styles from a dataset of stylized 3D faces and a style-conditioned decoder that maps AUs to blendshape weights. Experimental results demonstrate that AU-Blendshape achieves superior control over stylized expressions compared to existing methods, enabling realistic and exaggerated facial animations with minimal user effort. Our approach provides a powerful tool for artists and animators to create expressive and unique 3D characters."
http://arxiv.org/abs/2507.11990v1,ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation,"Personalized text-to-image generation aims to create images reflecting specific individuals or objects based on textual descriptions. A core challenge lies in effectively incorporating identity information from limited reference images into the generation process, particularly when the desired textual descriptions deviate significantly from the visual characteristics present in those images. We introduce ID-EA, an Identity-Driven Text Enhancement and Adaptation framework leveraging Textual Inversion to address this challenge. ID-EA first employs a novel text enhancement module that enriches the input prompt with identity-relevant attributes inferred from the reference images, guiding the generation process towards desired visual characteristics. Subsequently, an adaptation module fine-tunes the generated image based on both the original and enhanced text embeddings, facilitating a balance between identity preservation and adherence to the full scope of the user's textual request. Experiments on diverse datasets demonstrate that ID-EA significantly improves identity fidelity and textual alignment compared to existing personalization methods, producing images that more accurately reflect both the specified identity and the nuanced details of the input text prompts. This approach offers a more flexible and controllable pathway for personalized image generation, enabling creative applications with greater precision."
http://arxiv.org/abs/2507.11892v1,From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition,"Dynamic emotion recognition (DER) in video necessitates understanding the intricate interplay between visual and linguistic cues. While existing methods often focus on coarse-grained alignment of modalities, they often overlook the subtle nuances conveyed through fine-grained linguistic expressions and their corresponding visually salient regions. This paper addresses the challenge of effectively aligning fine-grained linguistic cues with visually salient regions for improved dynamic emotion recognition. We propose a novel cross-modal alignment framework that leverages a hierarchical attention mechanism to first identify salient regions in video frames and then aligns them with fine-grained linguistic cues extracted from textual transcripts. This alignment is further refined through a contrastive learning objective, encouraging closer proximity between semantically related visual and linguistic features while distancing unrelated ones. Experimental results on benchmark DER datasets demonstrate that our approach achieves state-of-the-art performance, outperforming existing methods by a significant margin, particularly in recognizing subtle emotional expressions. This work highlights the importance of fine-grained cross-modal alignment for a more comprehensive understanding of dynamic emotions in video."
http://arxiv.org/abs/2507.11730v1,Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis,"Outdoor advertising relies heavily on billboard visibility, yet accurately assessing this visibility remains a challenge. This paper addresses the problem of efficiently and effectively extracting textual information from billboard images captured in real-world, edge-deployment scenarios for improved visibility analysis. We present a comprehensive survey of Optical Character Recognition (OCR) models suitable for edge deployment, evaluating their performance on a novel dataset of billboard images captured under varying environmental conditions. Our evaluation focuses on accuracy, inference speed, and model size, considering factors such as lighting, weather, and image resolution. Results indicate that while certain lightweight models demonstrate promising speed and size characteristics, their accuracy lags behind larger, more complex models, highlighting a trade-off between performance and deployability. This survey provides valuable insights for practitioners seeking to implement edge-based OCR solutions for automated billboard visibility analysis, enabling more data-driven decision-making in outdoor advertising."
http://arxiv.org/abs/2507.11653v1,VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization,"Global localization, the problem of estimating a robot's pose within a pre-existing map, is a crucial capability for autonomous navigation. However, appearance changes due to varying lighting conditions and viewpoint differences pose significant challenges to traditional localization methods. This paper addresses the problem of robust global localization in the presence of significant appearance and viewpoint variations by leveraging semantic segmentation. We propose VISTA, a novel monocular vision-based localization system that builds a global map composed of semantically segmented keyframes. During localization, VISTA extracts semantic segmentations from the query image and matches them to the map using a view-invariant feature representation based on bag-of-words applied to segment-wise aggregated deep features. Experimental results on challenging benchmark datasets demonstrate that VISTA achieves state-of-the-art localization accuracy and robustness compared to existing methods, particularly in scenarios with significant appearance and viewpoint changes. This work provides a practical and effective solution for enabling reliable global localization in real-world environments under challenging visual conditions."
http://arxiv.org/abs/2507.11372v1,Attributes Shape the Embedding Space of Face Recognition Models,"Face recognition has achieved remarkable progress, yet the influence of underlying facial attributes on the structure of the learned embedding space remains relatively unexplored. This paper investigates how specific facial attributes, such as age, gender, and ethnicity, shape the organization and properties of the embedding space learned by deep face recognition models. We propose a novel attribute-aware analysis framework that leverages both statistical measures and geometric interpretations to quantify the impact of these attributes. Specifically, we analyze the distribution of embeddings within attribute-defined subgroups, measuring intra-class compactness and inter-class separability. Furthermore, we introduce a metric based on principal component analysis to identify attribute-aligned directions within the embedding space. Our experiments, conducted on several large-scale face recognition datasets and pre-trained models, reveal that certain attributes induce significant biases in the embedding space, leading to uneven performance across different demographic groups. These findings highlight the critical role of facial attributes in shaping the representation learned by face recognition models, offering insights for developing more robust and equitable face recognition systems."
http://arxiv.org/abs/2507.11267v1,YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery,"Automatic target detection and localization in thermal infrared (TIR) imagery is crucial for various applications, including surveillance, search and rescue, and autonomous navigation. However, the inherent characteristics of TIR imagery, such as low contrast, thermal crossover, and cluttered backgrounds, pose significant challenges for accurate and robust target detection. This paper addresses the problem of improving the performance of deep learning-based object detectors for automatic target detection and localization in complex TIR environments. We propose YOLOatr, a novel architecture built upon the YOLOv5 framework, incorporating attention mechanisms and a refined feature pyramid network specifically tailored for TIR imagery. Specifically, we integrate a coordinate attention mechanism within the backbone to enhance feature representation and a BiFPN-based feature fusion module to improve multi-scale feature integration. Experimental results on a diverse TIR dataset demonstrate that YOLOatr achieves state-of-the-art performance, surpassing existing object detectors with a significant improvement in both detection accuracy and localization precision, while maintaining real-time processing speeds. This improved performance enables more reliable and effective deployment of automated systems relying on TIR imagery for target detection."
http://arxiv.org/abs/2507.11202v1,A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition,"Multimodal emotion recognition systems leverage complementary information from various modalities like facial expressions, speech, and text to achieve superior performance. However, real-world scenarios often suffer from missing modalities, significantly degrading the performance of existing multimodal models. This paper addresses the challenging problem of emotion recognition with incomplete multimodal data by proposing a novel Robust Incomplete Multimodal Low-Rank Adaptation (RIM-LoRA) approach. RIM-LoRA learns modality-specific low-rank adaptation matrices for each modality, enabling the model to effectively capture the underlying correlation structure within each modality while mitigating the impact of missing data through robust loss functions. Specifically, we employ a combination of reconstruction loss and contrastive loss to enhance feature representation and leverage a robust Huber loss to minimize the effect of outliers introduced by imputation. Experimental results on benchmark datasets demonstrate that RIM-LoRA consistently outperforms state-of-the-art methods in handling various missing modality patterns, achieving significant improvements in emotion recognition accuracy, especially under severe incompleteness. This work provides a practical and effective solution for deploying robust multimodal emotion recognition systems in real-world applications where data incompleteness is prevalent."
http://arxiv.org/abs/2507.11102v1,KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model,"Keypoint-based representation has proven effective in various vision tasks, offering a structured approach to understanding object pose and articulation. However, existing methods often struggle with generalizing keypoint comprehension across diverse object categories and tasks, requiring specialized models for each domain. This paper addresses the challenge of developing a generic keypoint comprehension system by leveraging the power of Large Language Models (LLMs). We introduce KptLLM++, an enhanced framework that integrates keypoint information into LLMs through a novel keypoint embedding module and a cross-modal attention mechanism. The keypoint embedding module learns a semantic representation of keypoint configurations, while the cross-modal attention allows the LLM to effectively attend to relevant keypoint features when processing textual queries or instructions. Experimental results on a range of keypoint-based tasks, including pose estimation, action recognition, and visual question answering, demonstrate that KptLLM++ significantly outperforms existing methods in terms of accuracy and generalization ability. This work paves the way for more versatile and adaptable vision systems capable of understanding and reasoning about keypoint-based representations in a unified manner."
http://arxiv.org/abs/2507.11099v1,A Survey on Interpretability in Visual Recognition,"Deep learning models have achieved remarkable performance in visual recognition tasks, but their inherent complexity often renders them as ""black boxes,"" hindering trust and understanding. This survey addresses the critical need for interpretability in visual recognition by providing a comprehensive overview of existing techniques, categorizing them based on their underlying principles and application scopes. We systematically analyze a broad spectrum of methods, including attention mechanisms, gradient-based techniques, perturbation-based approaches, and concept bottleneck models, highlighting their strengths, weaknesses, and practical applications in diverse visual recognition scenarios such as image classification, object detection, and semantic segmentation. Furthermore, we explore evaluation metrics used to assess the quality of interpretations and discuss the challenges associated with reliably evaluating interpretability methods. The survey reveals that while significant progress has been made, trade-offs between faithfulness, completeness, and human understandability remain a persistent challenge, motivating future research directions. This work serves as a valuable resource for researchers and practitioners seeking to develop and apply interpretable visual recognition models, ultimately fostering greater trust and accountability in AI systems."
http://arxiv.org/abs/2507.11081v1,Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification,"Ground Penetrating Radar (GPR) is a non-destructive technique widely used for assessing road subsurface conditions, enabling timely maintenance and preventing catastrophic failures. However, manual interpretation of GPR images is time-consuming, subjective, and requires specialized expertise. This paper addresses the challenge of automating road subsurface distress recognition from GPR B-scans by proposing a novel deep learning-based cross-verification framework. Our approach employs a two-stage process: first, a convolutional neural network (CNN) identifies potential distress regions within the GPR image; second, these regions are subjected to a cross-verification module involving multiple CNN classifiers trained on distinct feature representations extracted from the same region. Only distress regions confirmed by a consensus of classifiers are ultimately classified as distresses. Experimental results on a real-world GPR dataset demonstrate that our method achieves a significant improvement in both precision and recall compared to state-of-the-art object detection models, reducing false positives and improving the reliability of distress detection. This automated framework provides a robust and efficient solution for large-scale road infrastructure assessment, enabling proactive maintenance strategies and cost savings."
http://arxiv.org/abs/2507.11075v1,Joint angle model based learning to refine kinematic human pose estimation,"Kinematic human pose estimation aims to reconstruct 3D human pose from visual data, a fundamental task in computer vision. While deep learning has significantly advanced pose estimation, achieving high accuracy, especially for complex poses and occluded joints, remains a challenge due to noisy or ambiguous predictions. We address the problem of refining initial pose estimates by leveraging the inherent kinematic constraints of the human body. Our approach introduces a joint angle model learned from a large motion capture dataset to represent the plausible range and dependencies of human joint angles. This model is then integrated into a novel refinement network that iteratively adjusts the initial pose estimates by minimizing a loss function that combines data-driven error with penalties for violating the learned joint angle constraints. Experiments on benchmark datasets, including Human3.6M and MPI-INF-3DHP, demonstrate that our method consistently improves the accuracy of state-of-the-art pose estimation techniques, achieving significant reductions in mean per-joint position error. By effectively enforcing kinematic plausibility, our method provides a robust and effective approach for refining kinematic human pose estimation, leading to more accurate and realistic 3D human models."
http://arxiv.org/abs/2507.13378v1,"A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects","Automated defect detection in industrial manufacturing is crucial for ensuring product quality and reducing production costs. However, deploying robust defect detection systems in real-world industrial settings faces significant challenges due to factors such as varying illumination, complex backgrounds, and subtle defect variations. This survey provides a comprehensive overview of the current landscape of real-world industrial defect detection, identifying key challenges and analyzing existing approaches based on both classical image processing techniques and modern deep learning architectures. We categorize and analyze methods based on their suitability for different defect types, imaging modalities, and industrial applications, highlighting their strengths and limitations in addressing challenges such as data scarcity, domain adaptation, and real-time processing constraints. Furthermore, we examine current trends in weakly supervised learning, few-shot learning, and explainable AI to explore promising avenues for future research and development. Finally, we outline key open challenges and future research directions, focusing on the need for more robust, adaptable, and interpretable defect detection systems. This survey serves as a valuable resource for researchers and practitioners seeking to advance the field of industrial defect detection and bridge the gap between academic research and real-world industrial deployment."
http://arxiv.org/abs/2507.10999v1,SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition,"Visual recognition tasks often require capturing intricate spatial relationships between image regions. However, many current transformer-based architectures struggle to effectively aggregate information across spatially distant, yet semantically related, image tokens. To address this limitation, we introduce SpaRTAN: a Spatial Reinforcement Token-based Aggregation Network. SpaRTAN employs a reinforcement learning (RL) agent to dynamically select and aggregate salient tokens from the feature map, guided by a spatially-aware reward function that encourages the agent to attend to tokens that are both informative and spatially coherent. This selective aggregation process allows the network to focus on relevant spatial interactions, reducing computational overhead and improving feature representation. Experiments on ImageNet classification, object detection on COCO, and semantic segmentation on ADE20K demonstrate that SpaRTAN consistently outperforms state-of-the-art transformer-based models, achieving significant improvements in accuracy and efficiency. SpaRTAN offers a novel and effective approach to spatial reasoning in visual recognition, paving the way for more efficient and accurate vision models."
http://arxiv.org/abs/2507.10978v2,Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction,"Gait recognition, the identification of individuals based on their walking patterns, has gained increasing attention as a biometric modality. However, real-world gait recognition systems often suffer from performance degradation due to occlusions caused by environmental factors or other pedestrians. This paper addresses the challenge of robust gait recognition in the presence of occlusions by proposing a novel Residual Gap Correction (RGC) network. Our method leverages a pose-based gait representation and employs a deep convolutional neural network to learn and predict the missing pose information within occluded regions. Specifically, the RGC network estimates a residual correction map which, when added to the occluded pose sequence, reconstructs a more complete and accurate representation of the gait cycle. Experiments on challenging occlusion-simulated and real-world datasets demonstrate that the proposed RGC network significantly improves gait recognition accuracy compared to state-of-the-art methods, particularly under severe occlusion conditions. This work offers a practical and effective solution for enhancing the robustness of gait recognition systems in complex and uncontrolled environments."
http://arxiv.org/abs/2507.10969v1,Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data,"Action recognition in sports videos has gained increasing attention, but datasets often lack representation of women's sports and require extensive labeled data for training robust models. This paper addresses the challenge of classifying women's sport actions using limited training data. We introduce a novel Women Sport Actions Dataset (WSAD), comprising 10 distinct action classes from diverse sports, and propose a transfer learning approach leveraging pre-trained models on larger, general action recognition datasets. Our method fine-tunes these models with a small subset of the WSAD, incorporating data augmentation techniques specifically tailored for sport actions, such as temporal jittering and spatial cropping focused on the athlete. Experimental results demonstrate that our transfer learning approach, even with a fraction of the WSAD data, achieves competitive accuracy compared to training from scratch, significantly outperforming baseline methods in classifying challenging actions with limited examples. This research highlights the potential of transfer learning for building effective action recognition systems for under-represented domains with scarce labeled data, promoting inclusivity in sports video analysis."
http://arxiv.org/abs/2507.10895v1,Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition,"Electroencephalography (EEG)-based emotion recognition is a crucial component in affective computing, but its performance is often hindered by the inherent timescale-dependent inconsistencies in subjective emotion labels. These inconsistencies arise because individuals may experience varying emotional responses over different time windows, leading to noisy training data. This paper addresses the problem of mitigating the impact of timescale-dependent label inconsistency in EEG emotion recognition. We propose a novel Commuting Distance Regularization (CDR) method that leverages the temporal relationships within EEG data. CDR constructs a graph representing the temporal sequence of EEG features and penalizes model predictions that deviate significantly from the average emotion label within a local temporal neighborhood, weighted by the commuting distance between EEG segments. Experimental results on benchmark EEG emotion datasets demonstrate that CDR consistently improves classification accuracy and F1-score compared to baseline models and other regularization techniques. This highlights the potential of CDR to enhance the robustness and reliability of EEG-based emotion recognition systems by addressing the challenges posed by inconsistent emotional labels."
http://arxiv.org/abs/2508.03578v1,RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data,"Human pose estimation is a fundamental task in computer vision, traditionally relying on RGB or LiDAR data. However, these modalities can be limited by illumination and privacy concerns, motivating the exploration of radar as an alternative sensing modality. This paper addresses the challenge of estimating human pose directly from raw radar data while simultaneously quantifying the uncertainty associated with these predictions. We introduce RadProPoser, a novel framework that combines a learnable feature extraction module operating on raw radar tensors with a graph convolutional network (GCN) to predict 2D human pose. Crucially, RadProPoser incorporates a variational inference approach within the GCN to model the inherent ambiguity and noise in radar data, enabling the estimation of aleatoric and epistemic uncertainty for each predicted joint location. Experiments on a newly collected, challenging radar-based human pose dataset demonstrate that RadProPoser achieves state-of-the-art pose estimation accuracy compared to existing radar-based methods, while also providing meaningful uncertainty estimates that correlate with prediction errors. The ability to reliably estimate human pose and its associated uncertainty from radar data opens new avenues for robust and privacy-preserving human activity understanding in diverse environments."
http://arxiv.org/abs/2508.03437v1,Spatial Imputation Drives Cross-Domain Alignment for EEG Classification,"Electroencephalography (EEG) based brain-computer interfaces (BCIs) suffer from significant performance degradation when deployed on new subjects or recording environments due to non-stationarity in EEG signals. This domain shift hinders the development of robust and generalizable BCI systems. We address the challenge of cross-domain EEG classification by proposing a novel spatial imputation-driven alignment framework. Our method leverages the inherent spatial relationships between EEG channels to impute missing or corrupted electrode data, thereby regularizing the feature space and promoting domain-invariant representations. Specifically, we employ a graph convolutional network (GCN) to learn spatial embeddings and iteratively impute channel activity based on neighboring electrodes. These imputed features are then used to train a domain adversarial neural network, encouraging alignment between source and target domains. Experiments on benchmark EEG datasets demonstrate that our approach consistently outperforms state-of-the-art domain adaptation techniques, achieving significant improvements in classification accuracy on unseen target domains. This work highlights the effectiveness of spatial imputation as a regularization technique for cross-domain EEG analysis, paving the way for more robust and personalized BCI applications."
http://arxiv.org/abs/2508.03102v1,Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning,"Few-shot learning (FSL) aims to recognize new classes with limited labeled examples. A critical challenge in FSL is effectively leveraging knowledge transferred from base classes while adapting to novel classes, often complicated by spurious correlations and domain shifts across modalities. This paper addresses the problem of learning robust and generalizable representations for FSL by disentangling causal factors and aligning representations across different modalities. Our proposed method, Causal Disentanglement and Cross-Modal Alignment Network (CDCAN), learns representations where each dimension corresponds to an independent causal factor through an intervention-based training strategy. Furthermore, CDCAN employs a cross-modal alignment module to learn shared representations between visual and textual descriptions, mitigating the impact of modality-specific biases. Experimental results on benchmark FSL datasets demonstrate that CDCAN achieves state-of-the-art performance, outperforming existing methods by a significant margin, particularly when dealing with limited data and cross-modal variations. This highlights the effectiveness of causal disentanglement and cross-modal alignment for learning generalizable representations, leading to improved few-shot learning performance."
http://arxiv.org/abs/2508.03081v1,Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification,"Multiple Instance Learning (MIL) has become a dominant paradigm for Whole Slide Image (WSI) classification, addressing the challenge of limited pixel-level annotations. However, existing MIL methods often struggle with intra-bag heterogeneity and inter-bag similarity, leading to suboptimal feature representation learning. This paper introduces Contrastive Cross-Bag Augmentation (CxBA), a novel approach that leverages contrastive learning to enhance feature discrimination in MIL-based WSI classification. CxBA generates augmented ""bags"" by randomly sampling and combining instances from different original bags, creating positive and negative instance pairs based on bag-level labels. A contrastive loss is then applied to encourage similar instance representations within the same bag-level class and dissimilar representations across different classes, effectively regularizing the feature space. Extensive experiments on multiple benchmark WSI datasets, including Camelyon16 and TCGA-Lung, demonstrate that CxBA consistently improves classification accuracy compared to state-of-the-art MIL methods. The proposed method provides a robust and effective strategy for learning discriminative WSI representations, paving the way for more accurate and reliable diagnostic tools."
http://arxiv.org/abs/2508.02528v1,From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC,"Immunohistochemistry (IHC) is a crucial diagnostic tool in pathology, visualizing specific protein expression within tissue samples. However, variations in staining protocols, tissue preparation, and antibody quality can lead to inconsistencies in IHC results, hindering accurate diagnosis and research. This paper addresses the challenge of generating diagnostic-consistent virtual IHC images from standard H&E stained slides, mitigating the variability inherent in traditional IHC. We propose Restoration Diffusion for Diagnostic-Consistent Virtual IHC (RD-IHC), a novel framework leveraging a conditional diffusion model guided by spatially-aware restoration principles. RD-IHC first restores degraded or inconsistent features in the H&E image, then utilizes this restored representation to guide the diffusion process, generating corresponding virtual IHC images that are both visually realistic and diagnostically aligned with expert-annotated IHC. Experimental results on multiple IHC markers demonstrate that RD-IHC significantly improves the fidelity and diagnostic accuracy of virtual IHC compared to existing methods, achieving a substantial improvement in structural similarity (SSIM) and diagnostic concordance. This advancement has the potential to reduce reliance on physical IHC staining, accelerate diagnostic workflows, and enhance the reproducibility of pathology research."
http://arxiv.org/abs/2508.02278v1,SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching,"Local feature matching is a fundamental task in computer vision, crucial for applications like image registration, 3D reconstruction, and visual localization. Existing descriptors often struggle with viewpoint changes, illumination variations, and repetitive textures, leading to inaccurate or insufficient matches. This paper introduces SGAD, a novel Semantic and Geometric-aware Descriptor for robust local feature matching. SGAD leverages a multi-scale convolutional neural network to extract both semantic and geometric information from local image patches. Specifically, we incorporate a semantic segmentation branch to encode contextual understanding of the local region and a geometric reasoning module that explicitly models the spatial relationships between feature points. The extracted semantic and geometric features are then fused to create a comprehensive and discriminative descriptor. Experimental results on standard benchmark datasets, including HPatches and Aachen Day-Night, demonstrate that SGAD outperforms state-of-the-art descriptors in terms of matching accuracy and robustness, especially under challenging conditions. SGAD's ability to integrate semantic and geometric cues offers a significant advancement in local feature description, enabling more reliable and accurate matching for various vision tasks."
http://arxiv.org/abs/2508.02104v1,REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation for Interpretable Medical Image Classification,"Medical image classification often relies on multi-modal data to improve diagnostic accuracy, yet current deep learning models struggle to effectively integrate information from diverse modalities while maintaining interpretability. This paper addresses the challenge of distilling knowledge from complex multi-modal medical imaging models into simpler, more interpretable single-modal models. We introduce REACT-KD, a novel Region-Aware Cross-modal Topological Knowledge Distillation framework. REACT-KD leverages topological data analysis (TDA) to capture crucial structural information within each modality, guiding the student network to focus on salient image regions identified by the teacher network across modalities. Specifically, we employ persistent homology to extract topological features representing the ""shape"" of the data, then use these features to define region-aware loss functions that encourage the student to mimic the teacher's attention to topologically significant areas. Experimental results on a chest X-ray dataset demonstrate that REACT-KD significantly improves the performance of single-modal student models, achieving comparable accuracy to the multi-modal teacher while providing enhanced interpretability through learned attention maps highlighting disease-relevant anatomical regions. This approach offers a promising avenue for developing clinically useful and transparent AI systems in medical imaging."
http://arxiv.org/abs/2508.02082v1,S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework,"Automated radiology report generation (RRG) aims to alleviate the burden on radiologists by synthesizing descriptive reports from medical images. Current RRG models often struggle to produce structured and clinically accurate reports, and existing evaluation metrics lack the granularity to pinpoint specific deficiencies. This paper introduces S-RRG-Bench, a novel benchmark for Structured Radiology Report Generation, comprising a large-scale dataset of chest X-ray images paired with structured reports organized into distinct sections (e.g., findings, impressions). We propose a hierarchical sequence-to-sequence model incorporating anatomical attention and a structured decoder to explicitly generate reports following the defined structure. Furthermore, we develop a fine-grained evaluation framework that assesses the accuracy of each report section using specialized metrics focusing on clinical correctness and semantic similarity. Experimental results demonstrate that our model significantly outperforms existing RRG methods in terms of both overall report quality and section-specific accuracy, as measured by our novel evaluation framework. S-RRG-Bench and the associated evaluation metrics provide a valuable resource for advancing the development and assessment of structured radiology report generation models, ultimately leading to more clinically useful and reliable AI-driven diagnostic tools."
http://arxiv.org/abs/2508.01915v1,EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses,"The proliferation of smart glasses offers unprecedented opportunities for capturing and augmenting human memory. However, continuous image capture for egocentric memory recall is computationally expensive and raises significant privacy concerns. This paper introduces EgoTrigger, a novel audio-driven image capture system designed for all-day, energy-efficient operation on smart glasses. EgoTrigger leverages a lightweight, low-power acoustic event detection (AED) model to identify salient auditory cues indicative of memorable experiences. Upon detection of such cues, a high-resolution image is captured and stored, effectively creating a sparse, event-triggered visual record. We evaluate EgoTrigger on a newly collected egocentric audio-visual dataset, demonstrating a significant reduction in image capture frequency (up to 90%) compared to continuous capture, while maintaining high recall of relevant visual information based on subjective human evaluation. EgoTrigger paves the way for practical, privacy-preserving, and long-lasting egocentric memory enhancement systems."
http://arxiv.org/abs/2508.01853v1,Distinguishing Target and Non-Target Fixations with EEG and Eye Tracking in Realistic Visual Scenes,"Understanding how humans visually process complex scenes is crucial for applications ranging from assistive technologies to improved human-computer interaction. While eye tracking provides precise information about gaze location, it lacks direct insight into the underlying cognitive processes differentiating fixations on task-relevant (target) and task-irrelevant (non-target) objects. This paper addresses the challenge of distinguishing target from non-target fixations in realistic visual scenes using simultaneously recorded electroencephalography (EEG) and eye tracking data. We propose a novel approach that combines time-frequency analysis of EEG signals during fixations with eye movement features, feeding these features into a machine learning classifier. Specifically, we extract spectral power features from EEG within predefined time windows locked to fixation onset and integrate them with fixation duration, amplitude, and saccade directionality. Our results demonstrate that this multimodal approach significantly outperforms methods relying solely on eye tracking features, achieving an average classification accuracy of 78% in distinguishing target and non-target fixations across multiple participants and scene types. This highlights the potential of EEG-informed eye tracking for a deeper understanding of visual attention and cognitive processing in real-world environments."
http://arxiv.org/abs/2508.01641v1,Minimal High-Resolution Patches Are Sufficient for Whole Slide Image Representation via Cascaded Dual-Scale Reconstruction,"Whole slide images (WSIs) present a significant challenge for computational pathology due to their gigapixel resolution and complex tissue structures. Existing WSI representation learning methods often rely on downsampling or computationally intensive processing of large image patches, hindering the extraction of fine-grained features and efficient analysis. This paper addresses the problem of creating effective WSI representations using minimal, high-resolution patches. We propose a cascaded dual-scale reconstruction framework that first identifies and encodes a sparse set of high-resolution patches deemed most informative based on a learned attention mechanism. Subsequently, a dual-scale reconstruction module leverages both the high-resolution patch embeddings and a lower-resolution global context embedding to reconstruct the original WSI, thereby forcing the high-resolution patches to capture essential information for comprehensive representation. Experiments on multiple WSI datasets demonstrate that our approach achieves competitive or superior performance on downstream classification and survival prediction tasks compared to state-of-the-art methods, while utilizing significantly fewer patches. This efficient and accurate WSI representation learning strategy has the potential to accelerate the development of AI-powered diagnostic tools in digital pathology."
http://arxiv.org/abs/2508.01602v2,Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment,"Brain tumor subtype classification is crucial for personalized treatment planning, but the scarcity of labeled data for rare subtypes hinders the development of robust supervised models. This paper addresses the challenge of zero-shot learning for brain tumor subtype classification, aiming to classify unseen subtypes without requiring any training data for those specific classes. We propose a novel fine-grained patch-text alignment framework that leverages the inherent semantic relationship between image patches and textual descriptions of tumor subtypes. Our method first extracts discriminative features from image patches using a pre-trained vision transformer. Then, it learns a joint embedding space where patch features are aligned with textual descriptions of subtypes, enabling the classification of unseen subtypes based on semantic similarity. Experimental results on a multi-institutional brain tumor dataset demonstrate significant improvements in zero-shot classification accuracy compared to existing methods, achieving a relative gain of over 15% on average. This work offers a promising avenue for improving the diagnostic capabilities of brain tumor classification systems, particularly for rare and emerging subtypes where labeled data is limited."
http://arxiv.org/abs/2508.01490v1,A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene Expression in Spatial Transcriptomics,"Spatial transcriptomics (ST) technologies are revolutionizing biomedical research by simultaneously profiling gene expression and spatial location within tissues, often paired with corresponding histology images. However, the lack of standardized, large-scale datasets and evaluation protocols hinders the development and rigorous benchmarking of cross-modal learning algorithms that can effectively integrate these data modalities. To address this gap, we present a large-scale benchmark dataset consisting of paired histology images and gene expression profiles from multiple ST platforms, tissue types, and disease conditions. We provide standardized data processing pipelines and evaluation metrics to facilitate fair comparisons across different cross-modal learning approaches. Furthermore, we implement and evaluate a diverse set of baseline models, including canonical correlation analysis, deep neural networks, and transformer-based architectures, demonstrating the challenges and opportunities of learning joint representations from histological images and gene expression data. Our benchmark reveals significant performance variations across different models and tissue types, highlighting the need for specialized algorithms tailored to the specific characteristics of ST data. This resource will enable the development of more robust and generalizable cross-modal learning methods, ultimately accelerating discoveries in disease mechanisms and personalized medicine."
http://arxiv.org/abs/2508.01293v1,GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification,"Vision-language pre-training (VLP) has shown promise in whole slide image (WSI) classification by leveraging paired image-text data. However, the scarcity of expert-annotated clinical descriptions for WSIs limits the effectiveness of VLP, particularly for training robust text encoders. To address this, we propose Grounded Multi-Agent clinical description generation for Text encoder training (GMAT), a novel framework that generates comprehensive and grounded clinical descriptions by simulating a collaborative diagnostic process between multiple specialized agents. GMAT employs a hierarchical reinforcement learning architecture where a pathologist agent identifies regions of interest (ROIs) within the WSI, and specialized agents (e.g., morphology, nuclear feature experts) generate detailed descriptions based on these ROIs. These descriptions are grounded by associating them with the corresponding ROIs, enhancing the text encoder's ability to learn fine-grained visual-textual relationships. Experiments on multiple WSI classification datasets demonstrate that GMAT significantly improves the performance of vision-language multiple instance learning (MIL) models, achieving state-of-the-art results. GMAT offers a practical approach to enhance text encoder training in VLP for WSI analysis, paving the way for more accurate and interpretable diagnostic systems."
http://arxiv.org/abs/2508.01269v1,ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification,"Point cloud classification is a fundamental task in 3D computer vision, with applications ranging from robotics to autonomous driving. However, existing benchmarks often lack realistic noise and fail to provide reliable uncertainty estimates, hindering the development of robust and reliable point cloud classifiers. This paper introduces ModelNet40-E, a new benchmark derived from ModelNet40, specifically designed to evaluate the performance of point cloud classification models under realistic noise conditions and their ability to quantify uncertainty. We generate ModelNet40-E by introducing varying levels of Gaussian noise, outliers, and density variations to the original clean point clouds, along with ground truth uncertainty maps based on the introduced perturbations. We then evaluate several state-of-the-art point cloud classification models on ModelNet40-E, demonstrating a significant performance degradation compared to the original ModelNet40 dataset and revealing limitations in their uncertainty calibration. Our experiments highlight the importance of uncertainty-aware learning for robust point cloud classification and provide a challenging benchmark for future research in this area. The ModelNet40-E dataset and evaluation metrics will facilitate the development of more reliable and trustworthy 3D vision systems."
http://arxiv.org/abs/2508.01239v1,OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS,"3D Gaussian Splatting (3DGS) has emerged as a powerful scene representation technique, achieving state-of-the-art rendering quality and speed. However, 3DGS relies heavily on accurate and complete training data, and its performance can be severely degraded by observation incompleteness and label noise. To address this, we introduce OCSplats, a novel framework for Observation Completeness Quantification and Label Noise Separation in 3DGS. OCSplats leverages a visibility-aware Gaussian mixture model to estimate the observation completeness of each Gaussian, effectively identifying regions with sparse or missing data. Simultaneously, we employ a robust loss function based on a Huber kernel to mitigate the impact of noisy labels, enabling the model to learn from cleaner regions of the training data. Experiments on synthetic and real-world datasets demonstrate that OCSplats significantly improves the robustness of 3DGS to both observation incompleteness and label noise, leading to improved rendering quality and geometric accuracy, particularly in challenging scenarios. OCSplats offers a practical approach for enhancing the reliability and applicability of 3DGS in real-world applications."
http://arxiv.org/abs/2508.01184v1,Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning,"Object affordance recognition, the task of identifying potential interactions between objects and agents, is crucial for enabling robots to perform complex tasks in unstructured environments. However, effectively associating visual object properties with corresponding textual affordance descriptions remains a significant challenge, especially due to the inherent multi-scale nature of both visual and textual data. This paper addresses the problem of learning robust and grounded object affordance representations by proposing a novel Multi-scale Cross-modal Representation Learning (MCRL) framework. Our approach leverages a hierarchical visual encoder to capture object features at varying scales, coupled with a transformer-based language model to encode affordance descriptions. A cross-modal attention mechanism facilitates the fusion of these multi-scale features, enabling precise alignment between visual cues and textual semantics. We evaluate MCRL on a benchmark affordance dataset and demonstrate significant improvements over state-of-the-art methods in both affordance recognition accuracy and grounding performance. The improved performance highlights the effectiveness of multi-scale representation learning for bridging the gap between visual perception and semantic understanding of object affordances."
http://arxiv.org/abs/2508.00639v1,"Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification","Accurate and explainable lung nodule classification is crucial for early cancer diagnosis, but deep learning approaches often require extensive labeled datasets, which are expensive and time-consuming to acquire in the medical domain. This paper addresses the challenge of training effective and explainable lung nodule classifiers with extremely limited labeled data. We propose a novel framework that combines a contrastive self-supervised learning strategy for feature extraction, leveraging a large unlabeled dataset of CT scans, with a fine-tuning stage using only 20 annotated samples. Furthermore, we integrate Grad-CAM visualization to provide spatially localized explanations for the classifier's predictions. Our method achieves an AUC of 0.85 and an accuracy of 0.80 on an independent test set, demonstrating competitive performance compared to models trained with significantly larger datasets. The generated Grad-CAM heatmaps highlight the nodule regions as the primary focus of the classifier, increasing trust and interpretability. This work demonstrates the feasibility of training high-performing and explainable medical image classifiers with minimal annotation effort, paving the way for more efficient development of AI-driven diagnostic tools."
http://arxiv.org/abs/2508.00447v1,CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text,"Multimodal representation learning has achieved remarkable progress by aligning image and text modalities. However, existing methods often overlook the crucial temporal aspect inherent in many real-world scenarios, limiting their ability to understand evolving visual and textual information over time. We address this limitation by introducing CLIPTime, a novel time-aware multimodal representation learning framework built upon the foundation of Contrastive Language-Image Pre-training (CLIP). CLIPTime incorporates a temporal encoding module that learns to represent the temporal relationships within both visual and textual sequences. This temporal information is then integrated into the CLIP architecture through a cross-modal attention mechanism, allowing the model to effectively capture the interplay between visual content, textual descriptions, and their temporal context. Experiments on several video understanding datasets demonstrate that CLIPTime significantly outperforms existing CLIP-based methods, particularly in tasks requiring an understanding of temporal dynamics, such as video captioning and action recognition. These results highlight the importance of incorporating temporal information into multimodal representation learning and showcase the potential of CLIPTime to advance research in time-sensitive applications."
http://arxiv.org/abs/2508.00383v1,$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models,"Spatial Transcriptomics (ST) integrates gene expression profiling with spatial location within tissue, offering unprecedented insights into cellular organization and function. However, predicting ST profiles from Hematoxylin and Eosin (H&E) stained pathology images remains a challenging task due to the complex relationships between morphology and gene expression. We address the problem of improving ST prediction accuracy by incorporating spatial context and global dependencies more effectively. Our proposed method, $MV_{Hybrid}$, introduces a novel hybrid backbone within Pathology Vision Foundation Models, combining a state space model (SSM) to capture long-range dependencies with a Vision Transformer (ViT) to extract fine-grained morphological features. Specifically, we leverage Mamba, a selective SSM, to process spatially organized image patches, and fuse its output with ViT embeddings through a cross-attention mechanism, enabling bidirectional information flow. Experimental results on multiple ST datasets demonstrate that $MV_{Hybrid}$ significantly outperforms existing state-of-the-art methods, achieving improvements in prediction accuracy as measured by correlation coefficients and spatial domain reconstruction. These findings highlight the potential of hybrid architectures to unlock more accurate and biologically relevant spatial transcriptomics prediction from pathology images, advancing our understanding of tissue microenvironments and disease mechanisms."
http://arxiv.org/abs/2508.00361v1,Honey Classification using Hyperspectral Imaging and Machine Learning,"Honey, a natural sweetener, exhibits diverse characteristics depending on floral source, geographical origin, and processing methods. Accurate and rapid honey classification is crucial for authenticity verification, quality control, and consumer protection. This study addresses the challenge of distinguishing between different honey types using hyperspectral imaging (HSI) coupled with machine learning techniques. We acquired hyperspectral images of various honey samples in the visible and near-infrared (VNIR) range (400-1000 nm). Several machine learning algorithms, including Support Vector Machines (SVM), Random Forest (RF), and Convolutional Neural Networks (CNN), were trained and evaluated on the spectral data extracted from the HSI data. Our results demonstrate that the RF model achieved the highest classification accuracy of 94.7% in differentiating between monofloral honey types. This approach provides a non-destructive and efficient method for honey classification, offering a valuable tool for the honey industry and regulatory bodies to ensure product integrity and combat adulteration."
http://arxiv.org/abs/2508.00356v1,Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning,"Multi-image vision-language reasoning (MIVLR) demands sophisticated capabilities to integrate information across multiple visual inputs and relate them to natural language. Current approaches often struggle with complex reasoning chains and effectively aggregating information from diverse image sources. To address these limitations, we propose Analyze-Prompt-Reason (APR), a novel collaborative agent-based framework for MIVLR. APR decomposes the reasoning process into three distinct stages, each handled by a specialized agent: an *Analyzer* generating descriptive image captions, a *Prompter* formulating targeted questions based on the captions and the overall task, and a *Reasoner* synthesizing information from all images and the Prompter's queries to produce the final answer. This modular design allows for focused optimization of each agent and facilitates a more interpretable reasoning process. Experiments on challenging MIVLR datasets, including the newly curated ComplexWebQA, demonstrate that APR significantly outperforms existing state-of-the-art models, achieving a 10% improvement in overall accuracy. This collaborative agent framework offers a promising direction for enhancing complex reasoning capabilities in vision-language models."
http://arxiv.org/abs/2508.00171v1,On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI,"Multimodal clinical AI systems, which integrate image and text data for diagnosis and prognosis, hold immense promise but are vulnerable to biases present in the training data. This paper addresses the critical problem of identifying and characterizing textual biases that can lead to misleading reports and potentially harmful clinical decisions in these systems. We introduce a novel framework for diagnosing textual biases by leveraging concept bottleneck models to isolate the influence of specific text features on the overall prediction. Our approach involves training a multimodal model, identifying key concepts from the text modality using attention mechanisms, and then manipulating the presence or absence of these concepts to quantify their impact on the model's output and report generation. Experiments on chest X-ray report generation demonstrate that seemingly innocuous textual features, such as mentions of specific demographics or pre-existing conditions, can disproportionately influence the model's diagnostic predictions, even when contradicted by the image data. This highlights the potential for biased textual information to override visual evidence and underscores the need for rigorous evaluation and mitigation strategies to ensure fairness and reliability in multimodal clinical AI deployments."
http://arxiv.org/abs/2507.23509v1,"I Am Big, You Are Little; I Am Right, You Are Wrong","Understanding relative attributes, such as size or correctness, is crucial for machines to reason about the world and interact with humans effectively. However, current computer vision models often struggle with nuanced comparisons, exhibiting biases and inconsistencies when judging relative properties between objects or concepts. This paper addresses the challenge of imbuing computer vision systems with a robust and contextually aware understanding of relative attributes, specifically focusing on size (""big"" vs. ""little"") and correctness (""right"" vs. ""wrong""). We propose a novel framework that leverages contrastive learning and large language models to encode relative attribute judgments. Our method trains a vision-language model to predict comparative statements (""A is bigger than B"") given image pairs and incorporates a knowledge distillation process from a pre-trained language model to infuse contextual understanding. Experiments on synthetic and real-world datasets demonstrate that our approach significantly improves the accuracy and consistency of relative attribute predictions compared to existing methods, particularly in challenging scenarios with subtle differences. Furthermore, our model exhibits improved generalization to unseen object categories and attribute combinations. This work contributes a significant step towards building more reliable and human-aligned computer vision systems capable of reasoning about relative properties."
http://arxiv.org/abs/2507.23461v1,Mitigating Resolution-Drift in Federated Learning: Case of Keypoint Detection,"Federated Learning (FL) enables collaborative model training across decentralized clients without sharing private data. However, real-world FL deployments often encounter heterogeneity in client data, including variations in image resolution, which can lead to a phenomenon we term ""resolution-drift"" during training and significantly degrade performance, especially in tasks sensitive to spatial information like keypoint detection. This paper addresses the challenge of mitigating resolution-drift in federated keypoint detection. We propose a novel Federated Resolution-Aware Distillation (FRAD) framework. FRAD incorporates a resolution-aware knowledge distillation loss at each client, guiding the local model to learn features invariant to resolution changes by matching its feature distributions to a server-side ""teacher"" model trained on a wider range of resolutions. Furthermore, we introduce a dynamic resolution regularization term that encourages clients to explore feature representations across different scales. Experiments on benchmark keypoint detection datasets demonstrate that FRAD significantly outperforms existing FL methods under various resolution heterogeneity scenarios, achieving up to 15% improvement in average precision. This work provides a practical solution for robust federated keypoint detection in real-world settings with diverse client data resolutions."
http://arxiv.org/abs/2507.23436v1,Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification,"Art style classification is a challenging task, particularly when dealing with culturally diverse artistic expressions that exhibit subtle and nuanced visual characteristics. Knowledge distillation (KD) offers a promising avenue for transferring knowledge from large, pre-trained models to smaller, more efficient ones, but traditional linear bottleneck approaches often struggle to capture the complex relationships present in art style feature spaces. We address this limitation by introducing a novel spline-based knowledge distillation framework that replaces the linear bottleneck with a learnable spline transformation. This allows for a more flexible and non-linear mapping of feature representations from the teacher to the student network, better preserving the intricate stylistic information. Specifically, we employ B-splines to model the feature transformations and optimize the control points through a distillation loss. Experiments on a curated dataset of culturally diverse artworks demonstrate that our spline-based KD significantly outperforms conventional linear bottleneck methods, achieving a 5-8% improvement in classification accuracy while maintaining a compact student model size. This highlights the potential of non-linear feature transformations in knowledge distillation for tasks requiring fine-grained visual understanding, particularly in the context of culturally sensitive art analysis."
http://arxiv.org/abs/2507.23402v1,AGA: An adaptive group alignment framework for structured medical cross-modal representation learning,"Cross-modal representation learning holds immense potential for integrating diverse medical imaging modalities, enhancing diagnostic accuracy and treatment planning. However, significant structural variations across modalities, such as differing anatomical coverage and organ orientations, pose a major challenge for effective cross-modal alignment and representation learning. To address this, we propose an Adaptive Group Alignment (AGA) framework for structured medical cross-modal representation learning. AGA introduces a novel group-wise alignment module that adaptively clusters image features into semantically consistent groups and learns modality-specific transformations to align corresponding groups. This dynamic grouping and alignment process allows the framework to handle structural variations more effectively than global alignment methods. Experimental results on multi-modal cardiac and brain imaging datasets demonstrate that AGA achieves superior performance compared to state-of-the-art methods in various cross-modal tasks, including image registration and segmentation propagation. AGA's ability to handle structural variations offers a significant advancement in medical cross-modal representation learning, facilitating more robust and accurate clinical applications."
http://arxiv.org/abs/2507.22431v1,HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models,"Large Vision-Language Models (LVLMs) like CLIP have demonstrated impressive zero-shot transfer capabilities, yet their performance is highly dependent on the quality and scale of the training data. Existing image-text datasets often suffer from noise, biases, and limited diversity, hindering the full potential of CLIP models. To address this, we introduce HQ-CLIP, a novel framework for automatically generating high-quality image-text datasets and training improved CLIP models. Our method leverages a pre-trained LVLM to iteratively refine noisy web-scraped data through a combination of caption re-ranking, image filtering, and text augmentation techniques. Specifically, we use the LVLM to score image-text pairs based on semantic coherence and image quality, followed by generating diverse synthetic captions that are closely aligned with the visual content. Experiments demonstrate that CLIP models trained on HQ-CLIP datasets outperform those trained on standard datasets like LAION-400M, achieving significant improvements in zero-shot image classification accuracy across a range of benchmarks, including ImageNet and object detection tasks. This work highlights the potential of using LVLMs to curate high-quality training data, paving the way for more robust and generalizable vision-language models."
http://arxiv.org/abs/2507.22346v1,DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception,"Remote sensing image change analysis plays a crucial role in various applications, including urban planning and environmental monitoring. However, current methods often lack the flexibility to adapt to diverse user needs and require specialized expertise to interpret complex changes. This paper addresses the limitations of existing change analysis techniques by introducing DeltaVLM, an interactive framework for remote sensing image change analysis guided by natural language instructions. DeltaVLM leverages a novel vision-language model architecture that explicitly models difference perception. It incorporates a difference-aware attention mechanism and a cross-modal fusion module to effectively integrate visual differences with textual instructions. Experiments on multiple remote sensing change detection datasets demonstrate that DeltaVLM achieves superior performance compared to state-of-the-art methods in terms of accuracy and instruction following. The interactive and instruction-guided nature of DeltaVLM significantly enhances the accessibility and interpretability of remote sensing image change analysis, enabling broader adoption across various domains."
http://arxiv.org/abs/2507.22136v2,Color as the Impetus: Transforming Few-Shot Learner,"Few-shot learning aims to recognize novel categories with limited labeled examples. Existing approaches often rely on complex meta-learning algorithms or pre-trained models with extensive data, neglecting the rich information encoded within the color distribution of images. This paper addresses the problem of effectively leveraging color information to enhance the performance of few-shot learning models, particularly in scenarios with extremely limited data. We introduce a novel ""Color-Attentive Few-Shot Learner"" (CAFS), which incorporates a color-aware attention mechanism to re-weight feature maps based on the prominence and discriminative power of different color channels. CAFS further leverages color histogram matching to augment the support set, creating synthetic samples that better represent the underlying color distribution of the novel classes. Experimental results on benchmark datasets, including miniImageNet and tieredImageNet, demonstrate that CAFS achieves significant improvements in few-shot classification accuracy compared to state-of-the-art methods, especially in the 1-shot setting. Our findings highlight the crucial role of color as a powerful and readily available feature for improving the robustness and generalization ability of few-shot learning algorithms."
http://arxiv.org/abs/2507.22062v3,Meta CLIP 2: A Worldwide Scaling Recipe,"Scaling vision-language pre-training with contrastive learning has proven highly effective, as demonstrated by models like CLIP. However, resource constraints often limit the geographic and demographic diversity of training data, potentially leading to biases and reduced generalization performance across different regions and cultures. This paper addresses the problem of efficiently scaling CLIP-like models using a more globally representative and diverse dataset, while minimizing computational costs. We introduce Meta CLIP 2, a novel scaling recipe leveraging a combination of strategically curated publicly available datasets from various geographic regions, coupled with a tailored data augmentation pipeline and a fine-grained, geography-aware evaluation benchmark. Our method employs a curriculum learning strategy that prioritizes data representing underrepresented regions early in training. We demonstrate that Meta CLIP 2 achieves significant improvements in zero-shot transfer performance on downstream tasks in diverse geographic locations, outperforming baseline models trained on standard datasets by up to 15% in certain regions. This work provides a practical and effective approach for building more robust and globally applicable vision-language models."
http://arxiv.org/abs/2507.22039v1,Supervised Quantum Image Processing,"Quantum image processing (QIP) has emerged as a promising field leveraging quantum computation for image representation and manipulation. However, the development of supervised learning techniques within QIP remains relatively unexplored compared to classical counterparts. This paper addresses the challenge of developing a general supervised learning framework for QIP tasks. We introduce a novel approach based on parameterized quantum circuits trained with classical optimization algorithms. Specifically, we encode images into quantum states and design quantum circuits to perform feature extraction and classification, with parameters updated iteratively based on a labeled training dataset. We demonstrate the efficacy of our framework on benchmark image datasets, achieving competitive classification accuracies compared to classical machine learning methods with significantly reduced computational resources in the quantum domain. These results suggest the potential for quantum advantage in image processing tasks, paving the way for more complex and efficient quantum image analysis algorithms."
http://arxiv.org/abs/2507.22024v1,Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT Images,"Accurate interpretation of cardiac computed tomography (CT) images is crucial for diagnosing and managing cardiovascular diseases. However, current deep learning models often require extensive labeled data and lack the ability to generalize to diverse clinical scenarios described in natural language. To address this limitation, we introduce Cardiac-CLIP, a novel vision-language foundation model for 3D cardiac CT images. Cardiac-CLIP learns a joint embedding space of 3D cardiac CT volumes and free-text descriptions by leveraging a contrastive learning objective. Specifically, we train the model on a large dataset of cardiac CT reports and corresponding image volumes, forcing the model to align visual representations extracted from the 3D volumes with textual representations derived from the reports. Experiments demonstrate that Cardiac-CLIP achieves state-of-the-art performance on various downstream tasks, including zero-shot cardiac structure segmentation and text-guided image retrieval, surpassing existing methods by a significant margin. Cardiac-CLIP's ability to bridge the gap between visual and textual information holds immense potential for improving clinical workflows and enabling more comprehensive cardiac image analysis."
http://arxiv.org/abs/2507.21922v1,SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention,"Fundus image analysis plays a crucial role in the early diagnosis and management of various retinal diseases. Automated fundus disease classification remains a challenging task due to the subtle and complex features associated with different pathologies, as well as the high inter- and intra-class variability in fundus images. To address these challenges, we propose SwinECAT, a novel Transformer-based model leveraging Shifted Window Attention and Efficient Channel Attention (ECA) for improved fundus disease classification. SwinECAT utilizes the Swin Transformer architecture to capture both local and global dependencies within fundus images through shifted window partitioning, enabling efficient computation and effective feature representation. Furthermore, ECA modules are integrated to adaptively recalibrate channel-wise feature responses, emphasizing informative channels while suppressing irrelevant ones. Experimental results on a publicly available fundus image dataset demonstrate that SwinECAT achieves state-of-the-art performance, surpassing existing convolutional and Transformer-based methods with an accuracy of 98.2% and an F1-score of 97.9%. This improved classification accuracy highlights the potential of SwinECAT as a valuable tool for assisting clinicians in the diagnosis and management of retinal diseases."
http://arxiv.org/abs/2507.21745v2,Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards,"Vision-language reasoning (VLR) tasks involving satellite imagery present unique challenges due to the scarcity of labeled data and the complexity of geospatial relationships. Current approaches often struggle to generalize to new regions and tasks with limited training examples. This paper addresses the problem of few-shot VLR for satellite imagery by introducing a novel framework leveraging verifiable rewards to guide model learning. Our approach incorporates a pre-trained vision-language model and refines it using reinforcement learning, where the reward function is designed to incentivize logical consistency and adherence to geospatial principles. Specifically, we employ a combination of rule-based and simulation-based rewards that verify the plausibility of the model's reasoning process, ensuring alignment with known geographic constraints. Experiments on a diverse set of satellite-based VLR tasks demonstrate that our method significantly outperforms existing few-shot learning techniques, achieving a 15% improvement in accuracy compared to the state-of-the-art. This work offers a promising direction for developing robust and reliable VLR systems that can effectively utilize limited labeled data in remote sensing applications."
http://arxiv.org/abs/2507.21723v1,Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations,"Detection Transformers (DETR) have achieved remarkable success in object detection, but their complex architecture makes understanding the contribution of individual components challenging. This paper addresses the problem of systematically analyzing DETR models to identify critical elements for performance. We introduce a novel, neuroscience-inspired ablation strategy that mimics lesion studies in the brain, selectively removing or perturbing specific components within the transformer encoder and decoder, guided by principles of functional modularity. This approach allows us to evaluate the impact of individual attention heads, feed-forward networks, and positional embeddings on various aspects of object detection, such as localization accuracy and classification confidence. Our experiments on the COCO dataset reveal that a small subset of attention heads within the decoder is disproportionately responsible for object localization, while others primarily contribute to classification. These findings offer valuable insights into the inner workings of DETR and provide a principled foundation for future model compression and architectural optimization efforts."
http://arxiv.org/abs/2507.21291v1,Fairness and Robustness of CLIP-Based Models for Chest X-rays,"Contrastive Language-Image Pre-training (CLIP) models have shown impressive zero-shot transfer capabilities, making them attractive for medical image analysis, particularly for chest X-rays. However, the fairness and robustness of these models in this domain remain largely unexplored, potentially leading to biased or unreliable predictions. This work investigates the performance of CLIP-based models for chest X-ray classification across different demographic groups and under various adversarial attacks. We propose a novel fine-tuning strategy that incorporates both adversarial training with projected gradient descent and group-wise balancing using re-weighting techniques to mitigate bias and enhance robustness. Our results demonstrate that standard CLIP models exhibit significant performance disparities across sex and race subgroups, and are vulnerable to common adversarial perturbations. Critically, our proposed fine-tuning method significantly reduces these fairness gaps, achieving a more equitable performance distribution while simultaneously improving robustness against adversarial attacks, as measured by an increase in robust accuracy. These findings highlight the importance of evaluating and mitigating biases in CLIP-based models for medical imaging and demonstrate the effectiveness of our proposed approach for achieving fairer and more reliable diagnostic systems."
http://arxiv.org/abs/2507.20834v1,Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the Inductive Setting,"Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable zero-shot transfer capabilities, leading to its widespread adoption in few-shot learning. However, existing few-shot CLIP benchmarks often conflate inductive and transductive settings, potentially overestimating the true generalization ability in realistic inductive scenarios. This paper critically analyzes prevalent few-shot CLIP benchmarks, exposing inconsistencies and limitations in their evaluation protocols when applied to the inductive setting. We propose a rigorous evaluation framework that enforces a strict separation between training and testing data, ensuring that no information from the test set leaks into the adaptation process. Our framework includes a novel ""Inductive Few-Shot CLIP Benchmark"" comprising established datasets and a suite of metrics designed to accurately measure inductive generalization. Experimental results demonstrate a significant performance gap between previously reported results and our inductive evaluation, highlighting the overestimation of few-shot CLIP performance in prior benchmarks. This work provides a more realistic assessment of few-shot CLIP capabilities and establishes a more reliable foundation for future research in this domain."
http://arxiv.org/abs/2507.20776v1,RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning,"Remote sensing (RS) provides invaluable data for Earth observation, yet the diverse array of sensors and modalities presents a significant challenge for creating generalizable AI models. Current RS models often struggle to effectively integrate and reason across data acquired from different platforms (e.g., satellites, drones) and modalities (e.g., optical, SAR, LiDAR). To address this, we introduce RingMo-Agent, a unified remote sensing foundation model designed for multi-platform and multi-modal reasoning. RingMo-Agent leverages a novel ring attention mechanism to efficiently process and fuse information from heterogeneous RS data sources. Furthermore, it incorporates a task-specific adapter framework, allowing for fine-tuning on a variety of downstream tasks, including land cover classification, object detection, and change detection, without catastrophic forgetting. Experiments demonstrate that RingMo-Agent achieves state-of-the-art performance on several benchmark datasets, outperforming existing modality-specific and multi-modal RS models, and exhibits strong generalization capabilities across different geographic regions and sensor types. This work paves the way for more robust and versatile RS applications, benefiting environmental monitoring, disaster management, and urban planning."
http://arxiv.org/abs/2507.20745v1,Regularizing Subspace Redundancy of Low-Rank Adaptation,"Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning technique, demonstrating impressive performance across various vision tasks by learning low-rank matrices that modify pre-trained weights. However, the learned low-rank matrices often exhibit significant subspace redundancy, hindering the optimization process and potentially limiting the model's capacity to capture task-specific nuances. This paper addresses the problem of subspace redundancy within LoRA by proposing a novel regularization strategy called Subspace Orthogonality Regularization (SOR). SOR encourages orthogonality between the row and column spaces of the learned low-rank matrices, effectively promoting more diverse and informative feature representations. We achieve this by incorporating a penalty term into the LoRA training objective that minimizes the Frobenius norm of the product of the low-rank matrices. Experiments on image classification and object detection benchmarks demonstrate that SOR consistently improves the performance of LoRA, achieving significant gains in accuracy and robustness compared to standard LoRA and other regularization techniques. This work offers a principled approach to enhance the efficiency and effectiveness of LoRA by mitigating subspace redundancy, paving the way for more compact and powerful adaptation models."
http://arxiv.org/abs/2507.20531v1,Low-Cost Machine Vision System for Sorting Green Lentils (Lens Culinaris) Based on Pneumatic Ejection and Deep Learning,"Lentil processing requires efficient sorting to remove foreign materials and defective seeds, impacting product quality and market value. Existing automated sorting systems are often expensive and inaccessible to small-scale producers. This paper presents a low-cost machine vision system for sorting green lentils based on pneumatic ejection and deep learning. The system utilizes a Raspberry Pi, a low-resolution camera, and custom-designed hardware for image acquisition and pneumatic actuation. A Convolutional Neural Network (CNN) is trained to classify lentils as either ""good,"" ""discolored,"" or ""foreign material"" using a dataset of manually annotated lentil images. Experimental results demonstrate a sorting accuracy of 92% at a processing rate of approximately 5 kg/hour, with a total system cost significantly lower than commercially available alternatives. This affordable and effective solution enables small-scale lentil producers to improve product quality and reduce labor costs through automated sorting."
http://arxiv.org/abs/2507.20511v2,Beyond Class Tokens: LLM-guided Dominant Property Mining for Few-shot Classification,"Few-shot classification aims to recognize novel categories given only a handful of examples, a significant challenge in computer vision. Existing methods often rely on class tokens or prototype learning, which can be limited in capturing the nuanced properties defining a category, especially when data is scarce. This paper addresses the problem of effectively leveraging the power of Large Language Models (LLMs) to discover and utilize dominant properties for improved few-shot classification. Our approach, LLM-guided Dominant Property Mining (LDPM), first employs an LLM to generate a comprehensive set of descriptive properties for each class, leveraging the LLM's extensive knowledge base. Subsequently, we mine these properties to identify the most dominant and discriminative ones for each class, using a combination of visual grounding and statistical analysis. Finally, these dominant properties are integrated into a visual feature embedding space to enhance the classification performance. Experiments on benchmark datasets demonstrate that LDPM achieves significant improvements over state-of-the-art few-shot classification methods, particularly in scenarios with very limited training data. Our work showcases a novel paradigm for integrating LLMs with visual representation learning, enabling more effective and interpretable few-shot learning."
http://arxiv.org/abs/2507.20414v2,Indian Sign Language Detection for Real-Time Translation using Machine Learning,"Indian Sign Language (ISL) serves as the primary mode of communication for the hearing-impaired community in India; however, a significant communication barrier persists between them and the general public due to the limited understanding of ISL. This paper addresses the challenge of developing an accurate and efficient real-time ISL recognition system to bridge this communication gap. We propose a novel approach leveraging a hybrid deep learning architecture combining Convolutional Neural Networks (CNNs) for feature extraction from video frames and Long Short-Term Memory (LSTM) networks for temporal sequence modeling of sign gestures. To enhance performance, we incorporate transfer learning using a pre-trained CNN model and data augmentation techniques to address the limited availability of ISL datasets. The proposed system achieves a mean accuracy of 96.2% on a custom-collected ISL dataset encompassing a vocabulary of 50 common signs, demonstrating its superior performance compared to existing methods in terms of both accuracy and processing speed (achieving an average frame rate of 28 FPS). This real-time ISL translation system holds significant potential for fostering inclusivity and facilitating seamless communication between the hearing and hearing-impaired communities."
http://arxiv.org/abs/2507.20259v1,L-MCAT: Unpaired Multimodal Transformer with Contrastive Attention for Label-Efficient Satellite Image Classification,"Satellite image classification is crucial for various Earth observation applications, but often suffers from limited labeled data and the challenge of effectively fusing information from multiple modalities like optical and radar imagery. This paper addresses the problem of label-efficient multimodal satellite image classification by leveraging unpaired data across modalities. We propose L-MCAT, an Unpaired Multimodal Transformer with Contrastive Attention. L-MCAT employs a transformer-based architecture to learn cross-modal representations from unpaired optical and radar data, enhanced by a novel contrastive attention mechanism. This mechanism encourages the model to attend to semantically similar regions across modalities even in the absence of direct correspondence, thereby improving feature alignment and generalization. Experimental results on benchmark datasets demonstrate that L-MCAT achieves significant performance gains compared to state-of-the-art methods, particularly in low-label regimes, showcasing its ability to effectively learn from limited labeled data and unpaired multimodal information. This offers a promising avenue for developing more robust and scalable satellite image classification systems."
http://arxiv.org/abs/2507.20254v1,MIRepNet: A Pipeline and Foundation Model for EEG-Based Motor Imagery Classification,"Motor imagery (MI) classification using electroencephalography (EEG) holds immense potential for brain-computer interfaces (BCIs) aimed at restoring motor function. However, the high inter-subject variability and low signal-to-noise ratio of EEG signals pose significant challenges for robust and generalizable MI classification. This paper addresses these challenges by introducing MIRepNet, a novel pipeline and foundation model for EEG-based motor imagery classification. MIRepNet comprises a standardized EEG preprocessing pipeline, a contrastive learning-based pre-training strategy on a large, diverse EEG dataset, and a transformer-based architecture fine-tuned for specific MI classification tasks. We demonstrate that MIRepNet achieves state-of-the-art performance on multiple publicly available MI datasets, outperforming existing methods by a significant margin, particularly in cross-subject generalization scenarios. The proposed framework enables the development of more accurate and robust EEG-based BCIs, facilitating their wider adoption in clinical and assistive applications."
http://arxiv.org/abs/2507.20240v1,AnimalClue: Recognizing Animals by their Traces,"Recognizing animals is crucial for biodiversity monitoring and wildlife conservation efforts, yet traditional methods often rely on direct observation which can be challenging due to factors like dense vegetation and nocturnal behavior. This paper addresses the problem of automatically identifying animal species based solely on their traces, such as footprints, scat, and scratch marks. We introduce AnimalClue, a novel multi-modal deep learning framework that integrates visual features extracted from trace images with contextual information like location, habitat type, and time of day. AnimalClue employs a transformer-based architecture to effectively fuse these diverse data sources, enabling robust species classification even with incomplete or noisy trace data. Experimental results on a newly curated dataset of animal traces demonstrate that AnimalClue achieves significantly higher accuracy than existing image-based classification methods, particularly in challenging scenarios with limited visual information. This approach offers a scalable and non-invasive solution for animal identification, contributing to improved wildlife monitoring and conservation strategies."
http://arxiv.org/abs/2507.20216v2,Dual-Stream Global-Local Feature Collaborative Representation Network for Scene Classification of Mining Area,"Scene classification of mining areas from remote sensing imagery plays a crucial role in environmental monitoring and resource management. However, the complex and variable landscapes within mining regions, characterized by both large-scale geological formations and fine-grained artificial structures, pose a significant challenge for accurate scene understanding. To address this issue, we propose a novel Dual-Stream Global-Local Feature Collaborative Representation Network (DSGL-Net) for improved scene classification in mining areas. DSGL-Net employs a dual-stream architecture to simultaneously extract global contextual information using a deep convolutional neural network and local detailed features through a transformer-based module focusing on spatial relationships. A collaborative representation learning strategy is then implemented to effectively fuse these complementary feature representations, enabling the network to capture both the overall scene context and subtle local variations. Experimental results on a newly curated mining area scene dataset demonstrate that DSGL-Net achieves superior performance compared to state-of-the-art methods, with a significant improvement in overall classification accuracy. This research provides a valuable tool for automated analysis of mining areas, facilitating more efficient and accurate environmental monitoring and resource management practices."
http://arxiv.org/abs/2507.20125v1,Multi-output Deep-Supervised Classifier Chains for Plant Pathology,"Plant diseases pose a significant threat to global food security, necessitating accurate and timely diagnosis for effective management. Traditional plant pathology often relies on visual inspection, which is subjective and time-consuming, motivating the use of automated, vision-based systems. This paper addresses the challenge of multi-label plant disease classification, where a single plant can exhibit multiple diseases simultaneously. We propose a novel Multi-output Deep-Supervised Classifier Chain (MDCC) model, leveraging deep convolutional neural networks for feature extraction and classifier chains to capture dependencies between different diseases. Furthermore, we introduce deep supervision at multiple layers of the network to enhance feature learning and improve classification accuracy for individual diseases. Experimental results on a benchmark multi-label plant disease dataset demonstrate that our MDCC model outperforms state-of-the-art multi-label classification methods, achieving higher precision, recall, and F1-score. This improved performance signifies the potential of MDCC for accurate and reliable plant disease diagnosis, contributing to improved crop yields and reduced agricultural losses."
http://arxiv.org/abs/2507.20025v1,Region-based Cluster Discrimination for Visual Representation Learning,"Self-supervised learning leverages unlabeled data to learn powerful visual representations, often relying on instance discrimination or clustering objectives. However, existing clustering approaches typically operate at the global image level, neglecting the rich regional information crucial for detailed scene understanding. This paper addresses the limitation of current cluster-based self-supervised methods by introducing Region-based Cluster Discrimination (RCD), a novel approach that learns representations by discriminating between clusters of image regions. RCD first extracts a diverse set of region proposals and then assigns each region to a cluster based on its feature similarity to cluster centroids. A contrastive loss is then applied, encouraging regions assigned to the same cluster to have similar representations while pushing apart representations of regions assigned to different clusters. Experiments on ImageNet classification and object detection benchmarks demonstrate that RCD achieves significant performance gains compared to state-of-the-art instance and cluster-based self-supervised methods, indicating that leveraging regional information within a clustering framework enhances the quality of learned visual representations. This region-aware clustering approach provides a promising direction for advancing self-supervised representation learning."
http://arxiv.org/abs/2507.20017v1,VAMPIRE: Uncovering Vessel Directional and Morphological Information from OCTA Images for Cardiovascular Disease Risk Factor Prediction,"Optical Coherence Tomography Angiography (OCTA) provides high-resolution, non-invasive 3D visualization of retinal microvasculature, offering valuable insights into systemic vascular health. However, extracting quantitative information related to vessel directionality and morphology from OCTA images for cardiovascular disease (CVD) risk factor prediction remains a challenge. We introduce VAMPIRE, a novel framework that leverages deep learning-based vessel segmentation and graph neural networks (GNNs) to analyze OCTA images. VAMPIRE first employs a U-Net architecture for precise vessel segmentation, followed by a graph construction module that represents the vasculature as a network of interconnected vessel segments. The GNN then aggregates information from neighboring segments to extract features encoding vessel direction, tortuosity, and branching patterns. We demonstrate that VAMPIRE-derived features, when combined with demographic data, significantly improve the prediction accuracy of CVD risk factors such as hypertension and diabetes compared to traditional metrics like vessel density (AUC improvements of 5-8% observed). This framework provides a powerful tool for automated and comprehensive assessment of retinal microvasculature, potentially enabling earlier detection and management of CVD risk."
http://arxiv.org/abs/2507.19970v1,SkinDualGen: Prompt-Driven Diffusion for Simultaneous Image-Mask Generation in Skin Lesions,"Skin lesion diagnosis relies heavily on accurate image segmentation, which is often a laborious and time-consuming manual process. Existing methods typically focus on either image generation or mask generation independently, neglecting the potential benefits of a joint generative approach guided by textual prompts. We introduce SkinDualGen, a novel prompt-driven diffusion model capable of simultaneously generating realistic skin lesion images and corresponding segmentation masks from textual descriptions. SkinDualGen leverages a dual-branch architecture where one branch focuses on image generation conditioned on the textual prompt and the other on mask generation, with cross-attention mechanisms facilitating information exchange between the two branches. Experiments on benchmark datasets demonstrate that SkinDualGen achieves superior performance compared to state-of-the-art methods in both image and mask generation quality, as measured by Frchet Inception Distance (FID) and Dice score, respectively. This simultaneous generation approach offers a promising avenue for data augmentation, medical education, and the development of more robust automated diagnostic tools for skin cancer."
http://arxiv.org/abs/2507.19961v1,Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures,"Cardiovascular diseases (CVDs) are a leading cause of mortality worldwide, and early diagnosis is critical for effective treatment. Traditional electrocardiogram (ECG) interpretation requires specialized equipment and trained professionals, which can be a barrier in resource-limited settings. This paper addresses the challenge of diagnosing CVDs directly from printed ECG images, circumventing the need for digital ECG data. We introduce Pic2Diagnosis, a novel method comprising three key stages: (1) ECG image enhancement and waveform extraction using a combination of image processing techniques and contour analysis, (2) feature extraction from the segmented ECG waveforms using a carefully designed set of morphological and temporal features, and (3) disease classification using a multi-class Support Vector Machine (SVM) classifier trained on the extracted features. Experimental results on a dataset of printed ECG images demonstrate that Pic2Diagnosis achieves a high diagnostic accuracy of 92.3% in identifying common CVDs, rivaling the performance of traditional ECG analysis methods. This approach offers a cost-effective and accessible solution for CVD diagnosis, particularly in environments where digital ECG systems are unavailable or impractical."
http://arxiv.org/abs/2507.19924v2,"HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly","The proliferation of deepfakes has raised significant concerns about the authenticity of online videos, particularly those depicting human actions and interactions. Detecting forgeries in these human-centric videos remains a challenging task due to the subtle and often spatially localized nature of manipulations. We introduce HumanSAM, a novel framework for classifying forged videos by identifying anomalies in human spatial configurations, appearance, and motion patterns. Our approach leverages the Segment Anything Model (SAM) to extract precise human masks, which are then used to compute features representing spatial relationships between individuals, subtle appearance inconsistencies within and across frames, and deviations from typical human motion dynamics. These features are fed into a multi-branch anomaly detection network trained to identify deviations from real video distributions. Experiments on benchmark datasets demonstrate that HumanSAM achieves state-of-the-art performance in detecting human-centric forgeries, outperforming existing methods by a significant margin, especially in scenarios with subtle manipulations. This work offers a robust and interpretable solution for detecting forged videos, contributing to the fight against misinformation and enhancing the trustworthiness of video content."
http://arxiv.org/abs/2507.19881v1,FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving,"Semantic segmentation is crucial for autonomous driving, but its performance degrades significantly when models trained on synthetic data are deployed in real-world scenarios due to domain shift. Federated learning offers a promising approach to leverage decentralized real-world data for domain generalization, but often requires iterative training and access to multiple real-world datasets which is impractical in many real-world settings. This paper addresses the challenge of one-shot federated domain generalization for synthetic-to-real semantic segmentation in autonomous driving. We propose FedS2R, a novel framework that leverages a pre-trained synthetic segmentation model and adapts it to a target real-world domain using only a single communication round with a single, unlabeled real-world client. Our approach employs a combination of self-training with confidence masking on the real-world data, coupled with a novel federated knowledge distillation strategy that transfers knowledge from the synthetic model while mitigating the impact of noisy pseudo-labels. Experiments on the widely used GTAV and Cityscapes datasets demonstrate that FedS2R significantly outperforms existing domain adaptation and federated learning methods in the one-shot setting, achieving substantial improvements in mean Intersection-over-Union (mIoU) with minimal communication overhead. This work provides a practical and efficient solution for deploying robust semantic segmentation models in autonomous driving applications with limited real-world data."
http://arxiv.org/abs/2507.19858v1,Taming Domain Shift in Multi-source CT-Scan Classification via Input-Space Standardization,"Multi-source CT-scan classification holds immense potential for improved diagnostic accuracy and generalization across diverse patient populations and imaging protocols. However, significant domain shifts arising from variations in scanner manufacturers, reconstruction parameters, and patient demographics pose a major challenge to deploying robust and reliable models. This paper addresses the problem of mitigating domain shift in multi-source CT-scan classification without relying on complex adversarial training or domain adaptation layers. We propose a novel input-space standardization technique based on adaptive histogram matching and intensity normalization, conditioned on source-specific statistics learned during training. This standardization aims to align the intensity distributions of different domains, effectively reducing the discrepancy between them before feature extraction. Experimental results on a large multi-source CT-scan dataset demonstrate that our method significantly improves the average classification accuracy and reduces the variance across different source domains compared to baseline models and other common standardization techniques. The proposed approach offers a simple yet effective strategy for improving the generalization capabilities of CT-scan classification models in real-world, multi-institutional settings."
http://arxiv.org/abs/2507.19847v2,Knowledge Regularized Negative Feature Tuning of Vision-Language Models for Out-of-Distribution Detection,"Vision-Language Models (VLMs) have demonstrated remarkable performance in various tasks, but their reliability in out-of-distribution (OOD) scenarios remains a concern. Existing OOD detection methods often focus on calibrating confidence scores or learning explicit OOD representations, neglecting the rich semantic knowledge embedded within VLMs. This paper addresses the challenge of improving OOD detection performance by leveraging and refining the negative feature space of VLMs through a novel knowledge-regularized negative feature tuning approach. Our method first distills semantic knowledge from a pre-trained VLM into a set of pseudo-OOD classes. Then, we fine-tune the VLM by explicitly pushing away in-distribution image features from these learned pseudo-OOD negative features, regularized by the original VLM's knowledge to prevent catastrophic forgetting. Experimental results on multiple benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art OOD detection methods, achieving substantial gains in AUROC and FPR@95. This work highlights the importance of exploiting and refining the negative feature space of VLMs for robust OOD detection, leading to safer and more reliable real-world deployment."
http://arxiv.org/abs/2507.19843v1,Hybrid Deep Learning and Handcrafted Feature Fusion for Mammographic Breast Cancer Classification,"Mammography remains a crucial tool for early breast cancer detection, but its interpretation is often challenging and subjective. This paper addresses the problem of improving the accuracy and robustness of mammographic breast cancer classification by synergistically combining deep learning and handcrafted features. We propose a novel hybrid approach that leverages the strengths of both methodologies. Specifically, we employ a pre-trained convolutional neural network (CNN) for automatic feature extraction from mammogram images, complemented by handcrafted features capturing domain-specific knowledge such as texture, shape, and intensity characteristics. These features are then fused using a feature-level concatenation strategy followed by a multi-layer perceptron classifier for final classification. Experimental results on a publicly available mammogram dataset demonstrate that our hybrid approach outperforms both standalone deep learning models and classifiers relying solely on handcrafted features, achieving a significant improvement in area under the ROC curve (AUC) and F1-score. This improved classification performance has the potential to reduce false positives and false negatives in mammographic screening, ultimately leading to earlier and more accurate breast cancer diagnosis."
http://arxiv.org/abs/2507.19808v1,SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models,"Diffusion models have demonstrated remarkable capabilities in image generation and manipulation, opening new avenues for solving inverse problems in computer vision. A key challenge remains in effectively guiding these models for precise object localization and segmentation without extensive training or fine-tuning. We address the problem of generating accurate object masks from pre-trained diffusion models using only a sparse set of user-provided seed points. Our method, SeeDiff, leverages the inherent knowledge encoded within the diffusion model to propagate information from the seed points to form a coherent object mask. Specifically, we introduce a novel guidance strategy based on iteratively conditioning the diffusion process on the seed points and a dynamically updated foreground probability map, encouraging the generated sample to align with the user's intent. We demonstrate that SeeDiff achieves state-of-the-art performance in seeded mask generation across diverse datasets, significantly outperforming existing methods in terms of accuracy and robustness, while requiring no task-specific training. This work provides a practical and efficient solution for interactive image segmentation, empowering users to harness the power of diffusion models with minimal effort."
http://arxiv.org/abs/2507.19592v1,SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation,"Surgical instrument recognition is crucial for context awareness in operating rooms, enabling downstream tasks like surgical phase recognition and skill assessment. However, obtaining pixel-level annotations for instance segmentation of surgical instruments is expensive and time-consuming. This paper addresses the challenge of weakly-supervised part-aware instance segmentation of surgical instruments using only bounding box annotations. We propose SurgPIS, a novel framework that leverages bounding box annotations to learn instrument-level instances and part-level semantics simultaneously. SurgPIS employs a multi-stage training process, first generating pseudo part segmentation labels based on attention maps from an instrument instance segmentation network, then refining these pseudo labels using a part segmentation network trained with a consistency loss. Finally, the refined part segmentation labels are used to improve the instrument instance segmentation. Experiments on the EndoVis challenge datasets demonstrate that SurgPIS achieves state-of-the-art performance in weakly-supervised surgical instrument instance segmentation and part segmentation, significantly closing the gap with fully supervised methods. This work provides a practical and efficient solution for accurate surgical instrument understanding with minimal annotation effort."
http://arxiv.org/abs/2507.19469v1,Efficient Lines Detection for Robot Soccer,"Line detection is a fundamental task in robot soccer, enabling robots to perceive the field boundaries and navigate effectively. However, the real-time constraints and noisy image data inherent in robot soccer environments pose significant challenges for efficient and robust line detection. This paper addresses the problem of achieving fast and accurate line detection for robot soccer applications, specifically focusing on minimizing computational cost while maintaining reliable performance under varying lighting conditions and camera perspectives. We propose a novel approach that combines a modified Hough Transform with a pre-processing stage employing edge thinning and region-of-interest (ROI) selection based on robot pose estimation. This ROI focuses the Hough Transform search space, drastically reducing computation time, while the optimized edge thinning algorithm enhances line clarity. Experimental results on a diverse dataset of robot soccer images demonstrate that our method achieves a 40% reduction in processing time compared to the standard Hough Transform, while maintaining comparable accuracy in line detection. This improvement in efficiency enables faster reaction times and more robust navigation for robot soccer players."
http://arxiv.org/abs/2507.19418v1,DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment,"Blind Image Quality Assessment (BIQA) aims to predict the perceptual quality of images without any reference information. Existing BIQA methods often struggle to accurately assess image quality due to the complex and diverse distortions present in real-world scenarios, and they typically lack explicit uncertainty modeling. We propose DEFNet, a novel Multitasks-based Deep Evidential Fusion Network for BIQA. DEFNet leverages a multi-task learning framework to simultaneously predict quality scores and distortion type probabilities, enabling the network to learn more robust and discriminative features. Furthermore, we incorporate Dempster-Shafer Theory of Evidence to explicitly model the uncertainty associated with quality predictions, allowing for a more reliable and nuanced assessment. Experimental results on several benchmark datasets demonstrate that DEFNet achieves state-of-the-art performance in terms of prediction accuracy and correlation with human perception, while also providing valuable information about the uncertainty of the quality estimates. DEFNet's ability to accurately assess image quality and quantify prediction uncertainty offers significant advancements for various applications, including image processing, computer vision, and multimedia communication."
http://arxiv.org/abs/2507.19409v1,Modality Agnostic Efficient Long Range Encoder,"Long-range sequence modeling is crucial for various computer vision tasks, but existing methods often struggle with computational efficiency and modality-specific designs. This paper addresses the challenge of creating a modality-agnostic and efficient long-range encoder suitable for diverse visual inputs. We introduce the Modality Agnostic Efficient Long Range Encoder (MAELRE), which leverages a novel combination of learnable grouped linear attention and a hierarchical downsampling strategy. MAELRE first projects input sequences into a shared embedding space, then employs grouped linear attention to capture long-range dependencies with significantly reduced computational cost compared to full attention. Hierarchical downsampling further enhances efficiency by progressively reducing sequence length while preserving crucial information through a learned pooling mechanism. Experiments across image, video, and point cloud datasets demonstrate that MAELRE achieves state-of-the-art performance with significantly lower computational requirements than existing modality-specific long-range encoders. This work facilitates the development of more versatile and efficient vision systems capable of processing long-range dependencies across diverse data modalities."
http://arxiv.org/abs/2507.19321v1,SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence,"Explainable Artificial Intelligence (XAI) aims to make opaque machine learning models more understandable. However, current XAI methods often struggle to isolate and attribute the influence of individual input features, leading to complex and entangled explanations. This paper addresses the challenge of disentangling feature contributions in complex models to produce sparse and interpretable explanations. We propose SIDE, Sparse Information Disentanglement for Explainable AI, a novel framework that learns a sparse latent representation of the input data where each latent variable corresponds to a specific, disentangled aspect of the input. SIDE encourages sparsity through a learned masking mechanism during training, forcing the model to rely on a minimal subset of features for prediction. Experiments on image classification and natural language processing tasks demonstrate that SIDE achieves superior explanation sparsity and fidelity compared to state-of-the-art XAI methods. The resulting explanations are more easily interpretable and provide a more accurate understanding of the model's decision-making process, fostering trust and enabling better model debugging."
http://arxiv.org/abs/2507.19264v2,SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality,"Multi-modal learning leverages complementary information from different modalities to enhance task performance. However, real-world scenarios often encounter missing modalities during both training and inference, posing a significant challenge. This paper addresses the problem of learning robust multi-modal representations in the presence of missing modalities without requiring complex architectural modifications or specialized training schemes. We introduce SimMLM, a simple yet effective framework built upon masked language modeling (MLM). SimMLM randomly masks modality embeddings during training and utilizes a shared transformer encoder to reconstruct the masked modalities from the available ones. The reconstruction task encourages the model to learn cross-modal dependencies and infer missing information. Experiments on several multi-modal datasets, including CMU-MOSI, CMU-MOSEI, and IEMOCAP, demonstrate that SimMLM achieves state-of-the-art performance in handling missing modalities while maintaining competitive results when all modalities are present. SimMLM provides a practical and efficient solution for robust multi-modal learning in real-world applications where modality availability is uncertain."
http://arxiv.org/abs/2507.19199v1,Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention Mechanism in Deep Learning,"Diabetic retinopathy (DR) is a major cause of vision loss worldwide, necessitating accurate and timely diagnosis. Existing deep learning methods for DR classification often struggle to effectively capture subtle yet crucial lesion features and spatial relationships within retinal images. This paper addresses the challenge of improving DR classification accuracy by focusing on more effective feature extraction and lesion localization. We propose a novel dual attention mechanism integrated into a convolutional neural network (CNN) architecture. This mechanism incorporates both a spatial attention module, which highlights relevant image regions by suppressing irrelevant background noise, and a channel attention module, which adaptively recalibrates channel-wise feature responses to emphasize informative feature maps. The dual attention mechanism is incorporated into a ResNet-50 backbone and trained end-to-end on a large, publicly available dataset of retinal fundus images. Experimental results demonstrate that our proposed method achieves state-of-the-art performance, surpassing existing methods by achieving a higher area under the ROC curve (AUC) of 0.97 and improved classification accuracy of 89.2%. This enhanced DR classification framework offers a promising avenue for improving the efficiency and accuracy of automated DR screening programs."
http://arxiv.org/abs/2507.19175v1,Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers,"Vision Transformers (ViTs) have achieved remarkable performance in various computer vision tasks, but their computational complexity remains a significant bottleneck, hindering deployment on resource-constrained devices. Existing patch pruning methods often rely on heuristic metrics or require extensive retraining, leading to suboptimal performance and increased computational overhead. This paper introduces a novel patch pruning strategy based on robust statistical measures of attention weight diversity within each transformer block. Specifically, we leverage the median absolute deviation (MAD) and interquartile range (IQR) of attention weights to identify and prune redundant patches that exhibit low diversity in their attention patterns across different attention heads. This approach allows us to dynamically adapt the pruning ratio per block and layer, preserving crucial visual information while minimizing computational cost. Experiments on ImageNet classification demonstrate that our method achieves comparable or superior accuracy compared to state-of-the-art pruning techniques, with significant reductions in FLOPs and inference time. Our robust statistical approach provides a principled and efficient way to prune ViTs, enabling their deployment in resource-limited environments without sacrificing performance."
http://arxiv.org/abs/2507.19118v1,Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching,"Object detection in remote sensing imagery is crucial for various applications, yet challenges remain due to complex backgrounds, varying object scales, and intra-class variance. Existing methods often struggle to effectively integrate spatial and temporal information for robust object detection, particularly when relying on image feature matching across different temporal snapshots. We address this limitation by proposing a novel Cross Spatial Temporal Fusion Attention (CSTFA) mechanism. CSTFA leverages attention modules to selectively fuse spatial features within individual images and then attentively aggregates temporal features across multiple images based on feature matching. Specifically, we employ a spatial attention module to enhance salient features within each image, followed by a temporal attention module that learns cross-image correspondences based on feature similarity and fuses features across the temporal domain. Experiments on benchmark remote sensing datasets demonstrate that CSTFA achieves significant improvements in object detection accuracy compared to state-of-the-art methods, particularly in scenarios with significant temporal variations. This highlights the effectiveness of our approach in leveraging both spatial and temporal context for robust and accurate remote sensing object detection."
http://arxiv.org/abs/2507.19098v1,MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching,"Medical image analysis often faces challenges due to limited labeled data and the need for both accurate classification and realistic image generation. This paper addresses the problem of effectively leveraging limited medical image datasets for both generative modeling and classification tasks within a unified framework. We introduce MedSymmFlow, a novel approach based on symmetrical flow matching, designed to learn a continuous normalizing flow that maps medical images to a latent space while simultaneously optimizing for classification accuracy. MedSymmFlow utilizes a symmetrical architecture with two flow branches, one for encoding and one for decoding, ensuring robust latent space representations. Furthermore, we incorporate a classification loss within the flow training process, guiding the latent space organization to reflect class distinctions. Experiments on benchmark medical imaging datasets demonstrate that MedSymmFlow achieves state-of-the-art performance in both image generation quality (measured by FID and SSIM) and classification accuracy, surpassing existing generative and discriminative models. This work provides a powerful tool for medical image analysis, enabling improved diagnostic accuracy and the generation of realistic synthetic data for training and augmentation."
http://arxiv.org/abs/2507.19041v1,PGKET: A Photonic Gaussian Kernel Enhanced Transformer,"Transformers have achieved remarkable success in various computer vision tasks, but their computational complexity, especially with long-range dependencies, remains a significant bottleneck. Existing attention mechanisms struggle to efficiently capture both local and global contextual information, often relying on computationally expensive matrix multiplications. We address this limitation by introducing PGKET: a Photonic Gaussian Kernel Enhanced Transformer. PGKET leverages the unique properties of photonic computing to implement Gaussian kernel attention, enabling efficient and parallel processing of long-range dependencies. Specifically, we utilize a novel photonic architecture to calculate Gaussian kernels, which are then integrated into the transformer attention mechanism. This allows for a more nuanced representation of feature relationships, capturing both local and global contexts with reduced computational cost. Experimental results on ImageNet classification and COCO object detection demonstrate that PGKET achieves comparable or superior performance to state-of-the-art transformer models, while significantly reducing computational complexity and inference time. This work paves the way for more efficient and scalable vision transformers, unlocking new possibilities for real-time and resource-constrained applications."
http://arxiv.org/abs/2507.18966v1,YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study,"Extracting structured knowledge from unstructured vehicle images is crucial for various applications, including intelligent transportation systems, insurance assessment, and autonomous driving. However, existing methods often focus on object detection alone, neglecting the extraction of higher-level knowledge about vehicle attributes and their relationships. This paper addresses the challenge of establishing a baseline for knowledge extraction from vehicle images by leveraging the popular YOLO object detection framework. We propose a pipeline that integrates YOLOv8 for vehicle component detection with subsequent rule-based reasoning and attribute classification modules to infer vehicle make, model, and damage severity. The system is trained and evaluated on a novel dataset of vehicle images annotated with bounding boxes and attribute labels. Our experiments demonstrate that while YOLOv8 provides accurate component detection (mAP@0.5 of 85%), the overall knowledge extraction accuracy is limited by the performance of the attribute classification and rule-based reasoning components, highlighting areas for future research. This study provides a valuable benchmark for evaluating and improving knowledge extraction techniques from vehicle imagery, paving the way for more sophisticated and informative visual understanding in automotive applications."
http://arxiv.org/abs/2507.18848v1,PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis,"Whole slide image (WSI) analysis presents a significant challenge due to the gigapixel resolution and the need for efficient processing of large image tiles. Multiple Instance Learning (MIL) has emerged as a popular paradigm for WSI classification, treating each WSI as a bag of instances (tiles). However, existing MIL methods often struggle to effectively capture global contextual information and identify the most discriminative regions within a WSI. To address this, we propose Prompt Token Clustering Multiple Instance Learning (PTCMIL), a novel approach that leverages visual prompting and clustering to enhance instance representation and aggregation. PTCMIL introduces learnable prompt tokens that interact with image tile embeddings, allowing the model to focus on relevant features. Subsequently, a clustering module groups these prompted tokens based on their semantic similarity, enabling the identification of distinct sub-regions within the WSI. Experimental results on benchmark datasets demonstrate that PTCMIL achieves state-of-the-art performance, surpassing existing MIL methods by a significant margin in classification accuracy and demonstrating improved interpretability. PTCMIL offers a powerful and efficient framework for WSI analysis, paving the way for more accurate and reliable diagnostic tools."
http://arxiv.org/abs/2507.21156v1,Comparative Analysis of Vision Transformers and Convolutional Neural Networks for Medical Image Classification,"Medical image classification is crucial for computer-aided diagnosis, enabling early disease detection and improved patient outcomes. While Convolutional Neural Networks (CNNs) have been the dominant architecture, recent advancements in Vision Transformers (ViTs) suggest their potential for surpassing CNN performance in various vision tasks. This paper addresses the comparative effectiveness of ViTs and CNNs for medical image classification, focusing on performance, robustness, and data efficiency. We conduct a rigorous empirical evaluation across three diverse medical imaging modalities: chest X-rays, retinal fundus images, and histopathology slides. We train and fine-tune representative ViT and CNN architectures, including ResNet50, EfficientNet, and ViT-Base, using standardized data preprocessing and evaluation protocols. Our results demonstrate that ViTs, particularly with pre-training on large external datasets, achieve significantly higher classification accuracy and improved robustness to noise compared to CNNs, especially when training data is limited. However, CNNs exhibit lower computational complexity and faster training times. These findings highlight the potential of ViTs for advancing medical image analysis, while also acknowledging the practical advantages of CNNs in resource-constrained settings, providing crucial insights for selecting appropriate architectures for specific medical imaging tasks."
http://arxiv.org/abs/2507.18565v1,Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement,"Targeted advertising, which personalizes advertisements based on user demographics, has become a prevalent strategy for maximizing advertising effectiveness. Accurately inferring age and gender from facial images remains a challenging task, especially in unconstrained environments with variations in pose, lighting, and occlusion. This paper addresses the problem of robust and accurate age and gender classification from facial images for enhanced targeted advertisement. We propose a deep learning framework leveraging a convolutional neural network (CNN) architecture pre-trained on a large-scale face recognition dataset, fine-tuned with multi-task learning for simultaneous age and gender prediction. Our architecture incorporates attention mechanisms to focus on salient facial features relevant to both tasks, and utilizes a novel loss function that balances the contributions of age regression and gender classification. Experimental results on benchmark datasets, including Adience and IMDB-WIKI, demonstrate that our method achieves state-of-the-art performance in both age estimation and gender classification, surpassing existing methods in accuracy and robustness. The proposed framework offers a practical and effective solution for demographic inference, enabling more personalized and effective targeted advertising strategies."
http://arxiv.org/abs/2507.18550v1,On the Performance of Concept Probing: The Influence of the Data (Extended Version),"Concept probing aims to understand and interpret the internal representations of neural networks by training simple classifiers to predict human-understandable concepts from network activations. Despite its popularity, the reliability and consistency of concept probing have been questioned, particularly regarding the influence of the dataset used for training the probe. This paper addresses the critical question of how the data distribution and characteristics of the probing dataset affect the performance and interpretation of concept probing. We conduct a comprehensive empirical analysis using diverse datasets, varying in size, bias, and label noise, to probe pre-trained convolutional neural networks on image classification tasks. Our experiments demonstrate that probe accuracy is highly sensitive to dataset composition, with even slight variations in data distribution leading to significant differences in the learned concept representations and their perceived importance. Furthermore, we show that dataset bias can lead to spurious correlations and misleading interpretations of network behavior. These findings highlight the crucial need for careful consideration and control of dataset characteristics when employing concept probing to ensure reliable and meaningful insights into neural network representations."
http://arxiv.org/abs/2507.18433v1,DiagR1: A Vision-Language Model Trained via Reinforcement Learning for Digestive Pathology Diagnosis,"The diagnosis of digestive pathologies often relies on expert interpretation of histopathological images, a process that is both time-consuming and subject to inter-observer variability. This paper addresses the challenge of developing an automated diagnostic tool that can accurately and efficiently analyze these complex visual data. We introduce DiagR1, a novel Vision-Language Model (VLM) trained via Reinforcement Learning (RL) to emulate the diagnostic reasoning process of pathologists. DiagR1 leverages a pre-trained VLM backbone, which is then fine-tuned using an RL agent that learns to sequentially attend to relevant image regions and generate diagnostic reports, guided by reward signals based on the accuracy and completeness of the generated report. Experimental results on a large-scale dataset of digestive pathology images demonstrate that DiagR1 achieves state-of-the-art diagnostic accuracy, surpassing existing VLM-based approaches and approaching the performance of expert pathologists. This work highlights the potential of RL-guided VLM training for developing automated diagnostic tools in the field of digestive pathology, offering a pathway towards improved diagnostic accuracy and efficiency."
http://arxiv.org/abs/2508.04255v1,From eye to AI: studying rodent social behavior in the era of machine Learning,"Rodent social behaviors are crucial for understanding complex neurological processes and modeling human social disorders. However, manual annotation of these behaviors is time-consuming, subjective, and limits the scale of analysis. This paper addresses the challenge of efficiently and objectively quantifying rodent social interactions using computer vision and machine learning. We propose a novel pipeline that integrates deep learning-based pose estimation with a transformer-based action recognition model to automatically identify and classify a repertoire of social behaviors from video recordings. Specifically, we leverage a pre-trained pose estimation network fine-tuned on rodent data to extract individual animal poses, followed by a transformer architecture trained on pose sequences to classify social actions. Our approach achieves state-of-the-art accuracy in classifying social behaviors such as following, mounting, and sniffing, demonstrating significant improvements over traditional methods based on hand-engineered features. This automated system provides a powerful tool for high-throughput and unbiased analysis of rodent social behavior, enabling researchers to investigate the neural mechanisms underlying social interactions with unprecedented scale and precision."
http://arxiv.org/abs/2508.04227v1,Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities across a wide range of tasks, but their deployment in dynamic, real-world scenarios is hindered by the challenge of continual learning, where models must adapt to new information without catastrophically forgetting previously learned knowledge. While existing continual learning surveys primarily focus on mitigating forgetting, VLMs present unique challenges and opportunities that necessitate a broader perspective beyond simply preserving past performance. This survey provides a comprehensive overview of continual learning techniques tailored for VLMs, introducing a novel taxonomy that categorizes methods not only by their approach to forgetting mitigation but also by their strategies for knowledge transfer, adaptation, and scaling. We analyze existing benchmarks and propose new evaluation metrics that better reflect the complexities of continual VLM learning, including measures of forward transfer and generalization to unseen data distributions. Our analysis reveals that while rehearsal-based methods currently dominate, parameter-efficient fine-tuning and modular architectures show promising potential for balancing performance and resource efficiency in continual VLM learning. This survey provides a valuable resource for researchers and practitioners seeking to develop robust and adaptable VLMs for real-world applications."
http://arxiv.org/abs/2508.04102v1,AR as an Evaluation Playground: Bridging Metrics and Visual Perception of Computer Vision Models,"Computer vision model evaluation often relies on quantitative metrics that may not fully capture human perceptual understanding of model performance in real-world scenarios. This paper addresses the challenge of bridging the gap between quantitative metrics and qualitative human perception in evaluating computer vision models. We introduce an Augmented Reality (AR) based evaluation playground that allows users to interact with and observe the outputs of various computer vision models overlaid onto their physical surroundings. This AR environment enables direct visual comparison of model predictions, manipulation of input parameters, and subjective assessment of performance in realistic contexts. We demonstrate the utility of our approach by evaluating object detection and semantic segmentation models, revealing discrepancies between high metric scores and perceived accuracy under varying lighting conditions and object occlusions. The AR playground provides a more intuitive and comprehensive evaluation framework, leading to a better understanding of model strengths and weaknesses, and ultimately facilitating the development of more robust and perceptually aligned computer vision systems."
http://arxiv.org/abs/2508.03402v1,SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models,"Disentangling style and content is a fundamental problem in computer vision, enabling controllable image manipulation and generation. Existing methods often rely on explicit feature separation or adversarial training, which can be unstable and lack robustness. In this work, we introduce SCFlow, a novel approach for implicitly learning style and content disentanglement using flow-based generative models. SCFlow leverages a hierarchical normalizing flow architecture, where style information is implicitly encoded in the early layers and content in the later layers, guided by a novel style consistency loss. This loss encourages similar style codes for images with similar styles, without requiring explicit style labels or adversarial training. Experiments on benchmark datasets demonstrate that SCFlow achieves state-of-the-art performance in style transfer and image manipulation tasks, generating high-quality images with improved style consistency and content preservation compared to existing methods. SCFlow offers a more stable and effective framework for learning disentangled representations, opening new avenues for controllable image generation and manipulation."
http://arxiv.org/abs/2508.03388v2,Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation,"Test-Time Adaptation (TTA) aims to adapt pre-trained models to unseen target domains using only unlabeled test data, offering a practical solution to domain generalization. However, existing TTA methods often suffer from performance degradation caused by the inherent bias of pre-trained models and the instability of statistical adaptation techniques, particularly when coupled with token aggregation modules common in modern vision transformers. This paper addresses the problem of biased token aggregation during TTA, which leads to the propagation of source domain biases and hinders effective adaptation. We propose a novel Information Augmentation (InfoAug) framework that neutralizes token aggregation by explicitly augmenting the information available to the aggregation process. InfoAug introduces learnable, task-agnostic perturbation vectors to the token embeddings before aggregation, effectively diversifying the information used for feature fusion. These vectors are optimized to minimize the discrepancy between the original and perturbed token representations, encouraging the model to rely on more robust and generalizable features. Experimental results on various benchmark datasets demonstrate that InfoAug consistently outperforms state-of-the-art TTA methods, achieving significant improvements in accuracy and robustness across diverse domain shifts. This work offers a practical and effective solution for mitigating the bias of token aggregation, enhancing the adaptability and reliability of vision transformers in real-world applications."
http://arxiv.org/abs/2508.03351v1,VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation,"Large Vision-Language Models (VLMs) have demonstrated remarkable performance across various multi-modal tasks. However, their substantial computational and memory demands hinder deployment on resource-constrained devices. Post-Training Quantization (PTQ) offers a promising solution to compress these models without requiring retraining, but often suffers from significant accuracy degradation, especially at low bitwidths. To address this, we propose VLMQ, an efficient PTQ framework for VLMs that leverages Hessian augmentation to better preserve model performance. VLMQ strategically augments the calibration dataset with synthetic samples generated based on the Hessian spectrum of the quantization error, effectively simulating the downstream task distribution and improving quantization robustness. Our experiments on various VLMs, including CLIP and BLIP, demonstrate that VLMQ consistently outperforms state-of-the-art PTQ methods, achieving significant accuracy improvements, especially at 4-bit quantization, while maintaining comparable computational overhead. VLMQ provides a practical and effective approach for deploying large VLMs on resource-limited platforms, broadening their applicability and accessibility."
http://arxiv.org/abs/2508.03079v1,Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models,"Large Vision-Language Models (VLMs) have demonstrated impressive capabilities in tasks requiring joint understanding of images and text, but their potential biases across demographic groups remain a critical concern. This paper investigates fairness issues in VLMs, focusing specifically on the intersectional impact of fine-grained attributes beyond commonly studied categories like race and gender. We introduce a novel evaluation framework that enables the systematic assessment of VLMs across a diverse set of fine-grained attributes, including age, hairstyle, and clothing, by generating targeted image captions and analyzing the resulting attribute-related biases in the model's output. Our approach involves a multi-stage process: first, generating attribute-specific image prompts; second, using these prompts with VLMs to produce captions; and third, employing automated metrics and human evaluations to quantify bias in the generated captions. Our experiments reveal significant disparities in the frequency and sentiment associated with different attribute combinations, indicating that VLMs tend to reinforce stereotypes related to intersectional identities. These findings highlight the need for developing robust bias mitigation strategies that address the complex interplay of fine-grained attributes in VLMs, ultimately promoting more equitable and inclusive AI systems."
http://arxiv.org/abs/2508.03763v1,Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment,"Perceptual Image Quality Assessment (IQA) aims to develop computational models that can accurately predict subjective human perception of image quality. Existing IQA models often struggle to align with human perception due to the complex and nuanced nature of subjective quality assessment. This paper introduces Refine-IQA, a novel multi-stage reinforcement finetuning framework designed to refine the feature representations of pre-trained IQA models and improve their perceptual alignment. Refine-IQA employs a reinforcement learning agent that iteratively adjusts the model's parameters based on rewards derived from the correlation between the model's predictions and human opinion scores. The framework incorporates multiple finetuning stages, each focusing on different aspects of quality perception, guided by carefully designed reward functions targeting both global and local image characteristics. Experimental results on several benchmark IQA datasets demonstrate that Refine-IQA significantly enhances the performance of various pre-trained IQA models, achieving state-of-the-art results and demonstrating improved correlation with human perception. This work provides a promising approach for developing more accurate and perceptually relevant IQA models by leveraging reinforcement learning to refine feature representations."
http://arxiv.org/abs/2508.02917v1,Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces,"Vision-language navigation (VLN) tasks challenge agents to follow human instructions in photorealistic environments. A crucial design choice in VLN is the action space, which dictates how the agent interacts with the environment, with common choices including low-level actions (e.g., move forward, turn left) and panoramic actions (selecting from available viewpoints). This paper investigates the impact of these action space representations on the performance of VLN agents leveraging large vision-language models (LVLMs). We present a systematic comparison of VLN agents employing a frozen pre-trained LVLM with lightweight adapter layers, trained to predict actions within both low-level and panoramic action spaces. Our experiments demonstrate that agents using panoramic actions achieve significantly higher success rates on the Room-to-Room (R2R) benchmark, outperforming comparable agents with low-level actions, especially in unseen environments. Furthermore, we analyze the learned action distributions and find that panoramic action spaces enable more direct navigation and mitigate error accumulation compared to sequential low-level actions. These findings highlight the importance of action space design in VLN and demonstrate the effectiveness of panoramic representations for agents guided by LVLMs."
http://arxiv.org/abs/2508.02890v1,VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in visual understanding and text generation, yet struggle with complex creative content generation tasks that require intricate scene understanding and precise manipulation of visual elements. This paper addresses the limitations of existing LVLMs in generating high-quality, visually-consistent, and semantically-accurate content for complex visual-guided creative tasks. We introduce VisuCraft, a novel framework that enhances LVLMs by integrating structured information extraction from visual inputs, specifically focusing on object attributes, relationships, and spatial arrangements. VisuCraft employs a modular architecture comprising a visual parser to extract scene graphs, a knowledge-aware reasoning module to refine the extracted information, and a conditioned generation module that leverages the structured scene representation to guide content creation. Experimental results on a newly curated benchmark for visual-guided story illustration and image editing demonstrate that VisuCraft significantly outperforms state-of-the-art LVLMs in terms of visual fidelity, semantic consistency, and user preference. VisuCraft offers a promising direction towards developing more intelligent and controllable LVLMs for complex creative content generation."
http://arxiv.org/abs/2508.02807v1,DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework,"Video Virtual Try-On (VTON) aims to realistically transfer a desired garment onto a target person in a video, offering significant potential for online retail and personalized content creation. However, existing methods often struggle with maintaining temporal consistency, handling complex occlusions, and generating realistic textures when applied to in-the-wild videos. To address these limitations, we introduce DreamVVT, a novel stage-wise diffusion transformer framework for realistic video VTON. Our method first employs a pose-guided diffusion model to generate an initial coarse try-on result, leveraging temporal attention to ensure frame-to-frame coherence. Subsequently, a refinement network based on a spatial-temporal transformer further enhances the visual fidelity by attending to both local and global contextual information, particularly focusing on blending the garment with the person's body and resolving occlusion artifacts. Experiments on challenging in-the-wild video datasets demonstrate that DreamVVT significantly outperforms state-of-the-art approaches in terms of visual realism, temporal stability, and garment fitting accuracy, as evidenced by both quantitative metrics and qualitative evaluations. This work advances the state-of-the-art in video VTON and offers a practical solution for generating compelling and realistic virtual try-on experiences."
http://arxiv.org/abs/2508.02549v1,MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming,"Vision-Language Navigation (VLN) tasks challenge agents to navigate through photorealistic environments following natural language instructions. Current VLN agents often struggle in unseen environments due to a lack of environmental awareness derived from limited egocentric visual input. To address this, we introduce MonoDream, a novel VLN framework that leverages a panoramic dreaming module to hallucinate a wider environmental context from monocular observations. MonoDream employs a diffusion model pre-trained on panoramic images to generate plausible 360 views conditioned on the agent's current egocentric observation and the navigation instruction. These hallucinated panoramic views provide the agent with a more comprehensive understanding of the surroundings, enabling better decision-making. Experiments on the Room-to-Room (R2R) and Room-for-Room (RxR) datasets demonstrate that MonoDream significantly improves navigation performance, particularly in unseen environments, achieving state-of-the-art results among monocular VLN agents. This work highlights the potential of leveraging generative models to augment limited sensory input and enhance the robustness of VLN agents in complex environments."
http://arxiv.org/abs/2508.02419v1,Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and generating content based on both visual and textual inputs. However, they often suffer from object hallucination, where non-existent objects are mentioned in the generated text, indicating a modality bias favoring the language prior over visual evidence. This paper addresses the problem of object hallucination in LVLMs by analyzing and mitigating the modality bias through an ""Attention Lens."" Our approach involves introducing a novel attention-based module that dynamically recalibrates the influence of visual and textual features during the generation process. Specifically, the Attention Lens learns to identify and suppress attention weights associated with hallucinated objects, encouraging the model to rely more on visually grounded information. Experiments on benchmark datasets demonstrate that our method significantly reduces object hallucination rates while maintaining or even improving overall text generation quality. This work provides valuable insights into the modality bias within LVLMs and offers a practical solution for enhancing the reliability and trustworthiness of their generated outputs."
http://arxiv.org/abs/2508.02405v1,Improving Generalization of Language-Conditioned Robot Manipulation,"Language-conditioned robot manipulation offers a promising avenue for intuitive robot control, yet current methods often struggle to generalize to novel objects, environments, and instructions. This paper addresses the challenge of improving the generalization capabilities of language-conditioned robot manipulation policies. We propose a novel approach, Language-Augmented Environment Diversification (LAED), which leverages large language models (LLMs) to generate diverse and semantically plausible environment augmentations based on the given language instruction. Specifically, LAED prompts an LLM to create variations in object properties (e.g., color, size, texture) and scene arrangements that are consistent with the instruction, and then uses these augmented environments during training to improve robustness. Experimental results on a suite of simulated robotic manipulation tasks demonstrate that LAED significantly outperforms existing state-of-the-art methods in terms of generalization to unseen objects and environments, achieving an average success rate improvement of 15% on held-out test scenarios. This work demonstrates the effectiveness of LLM-driven environment diversification as a scalable strategy for enhancing the generalization of language-conditioned robot manipulation policies."
http://arxiv.org/abs/2508.02258v2,Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning,"Vision-Language Models (VLMs) have demonstrated promise in pathology, yet their performance is often limited by the lack of contextual awareness and reasoning capabilities when faced with complex diagnostic tasks. This paper addresses the challenge of enabling pathology VLMs to effectively leverage external knowledge and multimodal data to improve diagnostic accuracy and reasoning. We introduce Patho-AgenticRAG, a novel framework that combines agentic behavior with retrieval-augmented generation (RAG) specifically tailored for pathology VLMs. Our approach utilizes a reinforcement learning (RL) agent to dynamically determine the optimal sequence of actions, including retrieving relevant information from a multimodal knowledge base (containing histopathology images, text reports, and expert annotations), reasoning over the retrieved content, and generating diagnostic justifications. Experiments on a large-scale pathology dataset demonstrate that Patho-AgenticRAG significantly outperforms existing RAG-based VLMs and traditional diagnostic approaches, achieving state-of-the-art results in diagnostic accuracy and providing more coherent and informative explanations. This work represents a significant step towards developing more robust and reliable AI-assisted diagnostic tools for pathology."
http://arxiv.org/abs/2508.02238v1,An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception,"Event cameras offer significant advantages over traditional frame-based cameras in high-speed and high dynamic range scenarios, making them suitable for challenging UAV environments. However, the asynchronous and sparse nature of event data presents a challenge for direct integration into existing computer vision algorithms designed for intensity images. This paper addresses the problem of reconstructing high-quality intensity images from event streams with minimal latency for real-time UAV perception. We propose a novel event-based intensity reconstruction scheme that leverages a spatio-temporal adaptive filter coupled with a learned event aggregation strategy. The filter dynamically adjusts its receptive field based on local event density, while the aggregation network learns optimal weights for combining events within the filtered region. Experimental results on both synthetic and real-world datasets demonstrate that our method achieves state-of-the-art intensity reconstruction quality with significantly reduced computational cost compared to existing approaches, enabling real-time performance on embedded UAV platforms. This fast and accurate intensity reconstruction facilitates the deployment of traditional vision-based algorithms for robust and efficient UAV navigation and perception."
http://arxiv.org/abs/2508.02151v1,AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models,"Diffusion models have demonstrated remarkable capabilities in generating high-quality and diverse images, often guided by text prompts. However, controlling the intensity of specific aesthetic attributes, such as warmth or sharpness, remains a challenge, often requiring iterative prompt engineering or leading to unintended alterations in other image characteristics. This paper introduces AttriCtrl, a novel method for fine-grained control of aesthetic attribute intensity in diffusion models, without extensive retraining or complex architectural modifications. AttriCtrl leverages a learned attribute prediction network to estimate the intensity of the target attribute in the generated image at each denoising step. This information is then used to modulate the denoising process via a carefully designed control signal, effectively guiding the image generation towards the desired attribute intensity. Experiments demonstrate that AttriCtrl achieves precise control over various aesthetic attributes, yielding images with significantly improved attribute intensity accuracy compared to baseline methods, while preserving image quality and content. AttriCtrl provides a practical and efficient solution for artists and designers to exert nuanced artistic control over diffusion model outputs."
http://arxiv.org/abs/2508.02134v1,Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference,"Video-based Multimodal Large Language Models (Video-MLLMs) have demonstrated impressive capabilities in understanding and reasoning about video content. However, existing approaches typically focus on singular tasks, requiring separate inference passes for different context perception needs, such as action recognition and object tracking, thereby increasing computational overhead and latency. We address this limitation by introducing Free-MoRef, a novel framework that enables the simultaneous extraction and multiplexing of diverse contextual information from a video within a single inference pass. Free-MoRef leverages a modular architecture composed of task-specific reference modules, each trained to extract relevant features for a particular context. These features are then dynamically integrated into the Video-MLLM's processing pipeline through a learnable gating mechanism, allowing the model to attend to the most relevant information for each task. Experiments on a diverse set of video understanding benchmarks demonstrate that Free-MoRef achieves comparable or superior performance to single-task models while significantly reducing inference time and computational cost. This work paves the way for more efficient and versatile Video-MLLM applications by enabling instant multiplexing of context perception capabilities."
http://arxiv.org/abs/2508.03755v1,LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion,"Multi-dimensional data, represented as tensors, are prevalent in various applications, but often suffer from incompleteness due to sensor failures or data acquisition limitations. This paper addresses the challenging problem of completing missing entries in multi-dimensional data by leveraging the underlying low-rank structure inherent in many real-world tensors. We propose LRTuckerRep, a novel Low-rank Tucker Representation model for tensor completion. LRTuckerRep simultaneously learns the Tucker decomposition of the tensor and a representation function that maps auxiliary information associated with the tensor modes to the Tucker factors. This allows us to regularize the Tucker factors using the auxiliary information and effectively exploit the correlations between different tensor modes. Experimental results on several real-world datasets, including image and video inpainting, demonstrate that LRTuckerRep achieves significant improvements in completion accuracy compared to state-of-the-art tensor completion methods, particularly when auxiliary information is informative. This method offers a powerful and flexible framework for handling missing data in a wide range of multi-dimensional datasets."
http://arxiv.org/abs/2508.02095v2,VLM4D: Towards Spatiotemporal Awareness in Vision Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable progress in understanding the relationship between images and text, yet they often fall short when reasoning about dynamic scenes and temporal relationships. This paper addresses the critical gap in spatiotemporal awareness within existing VLMs, hindering their ability to accurately interpret and interact with the real world. We introduce VLM4D, a novel framework that integrates explicit 3D scene representations and temporal modeling into the VLM architecture. VLM4D leverages a learned neural radiance field (NeRF) to encode 3D scene geometry and appearance, which is then combined with a temporal transformer to capture motion patterns and object interactions across video frames. The resulting spatiotemporal representation is aligned with the language embedding space through a multi-modal fusion module, enabling the model to perform complex reasoning tasks involving both visual and temporal cues. Experimental results on several challenging video question answering and action recognition datasets demonstrate that VLM4D significantly outperforms state-of-the-art VLMs, exhibiting a superior understanding of spatiotemporal dynamics. This work paves the way for more robust and context-aware VLMs capable of truly understanding and interacting with the dynamic visual world."
http://arxiv.org/abs/2508.02765v1,The Architecture of Trust: A Framework for AI-Augmented Real Estate Valuation in the Era of Structured Data,"Real estate valuation is a complex process, traditionally reliant on human expertise and limited data. However, the increasing availability of structured data, including property characteristics, transaction histories, and neighborhood demographics, presents an opportunity to augment this process with artificial intelligence. This paper addresses the problem of building trustworthy AI-augmented real estate valuation models, focusing on architectural choices that promote transparency and reliability. We propose a novel framework, ""Architecture of Trust,"" which incorporates a modular, explainable AI (XAI) pipeline, leveraging both deep learning for feature extraction from structured data and a rule-based system for incorporating expert domain knowledge. This allows for transparent model predictions and facilitates human-in-the-loop validation. Experimental results on a large-scale real estate dataset demonstrate that our framework achieves state-of-the-art valuation accuracy while providing interpretable justifications for its predictions, significantly improving user trust and adoption. This research provides a crucial step towards responsible and reliable AI adoption in the real estate industry."
http://arxiv.org/abs/2508.02028v1,Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving,"Vision-language models (VLMs) are increasingly being explored for autonomous driving tasks, offering the potential for enhanced scene understanding and reasoning. However, current benchmarks often lack the closed-loop interaction necessary to evaluate VLMs in realistic driving scenarios. This paper introduces Bench2ADVLM, a novel closed-loop benchmark specifically designed to assess the performance of VLMs in autonomous driving. Our benchmark integrates a high-fidelity driving simulator with a comprehensive suite of evaluation metrics, enabling the assessment of VLMs across various critical driving tasks, including navigation, object interaction, and response to unexpected events. The VLM acts as the central decision-making module, receiving visual and textual inputs and outputting driving commands that directly affect the simulation environment. We evaluate several state-of-the-art VLMs on Bench2ADVLM, demonstrating the challenges of deploying these models in dynamic driving environments and revealing significant performance gaps compared to traditional rule-based systems. Bench2ADVLM provides a valuable resource for the community to develop and evaluate VLMs for autonomous driving, fostering advancements towards safer and more reliable autonomous systems."
http://arxiv.org/abs/2508.01984v1,IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A,"Human motion understanding requires reasoning about complex spatio-temporal relationships and human-object interactions. Existing Question Answering (QA) systems for human motion often struggle with questions requiring multi-step reasoning or those involving implicit relationships not directly observable in the visual data. We introduce IMoRe, an Implicit program-guided reasoning framework for Human Motion Q&A. IMoRe leverages a neural module network to implicitly generate and execute programs that decompose complex questions into simpler sub-problems. Crucially, our program executor incorporates learnable modules that reason about implicit relationships and interactions, such as inferred intent or object affordances, by conditioning on both visual features and the program execution trace. Experiments on the Human Motion Question Answering (HM-QA) dataset demonstrate that IMoRe significantly outperforms existing methods, achieving a 15% improvement in overall accuracy and a 20% improvement on questions requiring implicit reasoning. This highlights the effectiveness of implicit program-guided reasoning for enhancing human motion understanding in complex QA scenarios."
http://arxiv.org/abs/2508.01965v1,From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment,"Autonomous indoor drones offer the potential to revolutionize property assessment by providing efficient and comprehensive data acquisition. However, current drone-based inspection relies heavily on human-in-the-loop systems and subjective visual interpretation, limiting objectivity and scalability. This paper addresses the challenge of transitioning from raw sensor data captured by autonomous drones to objective, physics-based property assessments. We propose a novel framework that integrates drone-mounted multi-modal sensing (RGB-D, thermal) with physics-informed machine learning models. The framework leverages simultaneous localization and mapping (SLAM) for autonomous navigation and 3D reconstruction, followed by data fusion and feature extraction relevant to specific property characteristics (e.g., insulation, moisture). These features are then input into physics-based models, enhanced with machine learning, to predict objective properties such as thermal resistance (R-value) and moisture content. Our experimental results demonstrate accurate property estimation compared to ground truth measurements, achieving a mean absolute error of 15% for R-value and 8% for moisture content. This approach enables automated, objective, and scalable property assessment, paving the way for improved building management, energy efficiency, and safety."
http://arxiv.org/abs/2508.01842v1,OmniEvent: Unified Event Representation Learning,"Event cameras offer advantages over traditional cameras due to their high temporal resolution and robustness to challenging lighting conditions. However, the event-based vision field is fragmented, with specialized architectures and training paradigms designed for individual tasks like object tracking, optical flow estimation, and action recognition, hindering the development of a truly general-purpose event processing system. This paper addresses the lack of a unified representation learning approach for diverse event-based vision tasks. We introduce OmniEvent, a novel framework that learns a shared, task-agnostic event representation by employing a multi-task learning strategy with a shared encoder and task-specific decoders. Our encoder leverages a transformer-based architecture to capture long-range temporal dependencies in event streams, while the decoders are designed to reconstruct task-specific outputs from the learned representation. Experiments on a variety of benchmark datasets demonstrate that OmniEvent achieves competitive or superior performance compared to task-specific models across different event-based vision tasks, and further exhibits strong generalization capabilities to unseen tasks. This work paves the way for developing more versatile and efficient event-based vision systems by learning a single, unified representation applicable to a wide range of applications."
http://arxiv.org/abs/2508.01802v1,SoccerTrack v2: A Full-Pitch Multi-View Soccer Dataset for Game State Reconstruction,"Reconstructing the game state in soccer is crucial for advanced performance analysis and automated coaching tools. However, the lack of publicly available, large-scale, multi-view datasets hinders the development of robust computer vision algorithms for this task. This paper introduces SoccerTrack v2, a novel full-pitch soccer dataset captured from multiple calibrated cameras, designed specifically for game state reconstruction. The dataset extends the original SoccerTrack dataset with significantly more data, improved annotation quality, and richer annotations including player identities, body part locations, and ball possession. We propose a multi-view fusion approach leveraging deep learning models trained on SoccerTrack v2 to estimate 3D player positions and ball trajectory. Experiments demonstrate that our approach, trained on the new dataset, achieves state-of-the-art results in player localization and ball tracking compared to existing methods. SoccerTrack v2 provides a valuable resource for the computer vision community, facilitating the development of more accurate and reliable game state reconstruction systems for soccer."
http://arxiv.org/abs/2508.01778v1,DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion,"Multi-camera based perception systems in autonomous driving rely on accurate and robust scene understanding, often enhanced by high-definition (HD) maps. However, current methods struggle with the computational overhead and latency associated with vector-based HD map integration and the inherent noise in raw sensor data. This paper introduces DiffSemanticFusion, a novel approach for online HD map diffusion to achieve robust semantic raster Bird's-Eye-View (BEV) fusion for autonomous driving. Our method leverages a diffusion model conditioned on camera-based semantic BEV predictions to generate a refined semantic BEV representation, effectively incorporating HD map priors in a probabilistic manner. This allows for robust handling of noisy sensor data and occlusions while maintaining real-time performance. We demonstrate that DiffSemanticFusion significantly improves semantic segmentation accuracy in challenging scenarios, particularly in areas with sparse sensor coverage or adverse weather conditions, achieving state-of-the-art performance on the nuScenes dataset. By enabling efficient and robust semantic BEV fusion, DiffSemanticFusion paves the way for more reliable and safer autonomous driving systems."
http://arxiv.org/abs/2508.01741v1,Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding and generating text based on visual input, but recent studies have revealed their vulnerability to adversarial attacks, particularly jailbreaks. Transferring these jailbreaks across different VLMs presents a significant challenge due to variations in model architecture, training data, and fine-tuning strategies. We introduce Simulated Ensemble Attack (SEA), a novel black-box attack method that leverages a simulated ensemble of diverse, fine-tuned VLMs to craft more transferable jailbreak prompts. SEA iteratively optimizes adversarial prompts by approximating the ensemble's collective response through a single, randomly sampled model per iteration, effectively simulating the diversity of a true ensemble without the computational cost. Our experiments demonstrate that SEA-generated jailbreaks exhibit significantly higher transferability rates compared to attacks crafted against individual models, achieving a 20-30% improvement in attack success rate across a range of fine-tuned LLaVA and InstructBLIP models. This work highlights the vulnerability of VLMs to transferable jailbreak attacks and offers a practical approach for evaluating and improving their robustness against adversarial manipulation."
http://arxiv.org/abs/2508.01727v1,OccamVTS: Distilling Vision Models to 1% Parameters for Time Series Forecasting,"Time series forecasting is crucial in numerous domains, yet current deep learning models often suffer from high computational costs and large parameter sizes, hindering their deployment in resource-constrained environments. This paper addresses the challenge of drastically reducing the parameter count of vision-based time series forecasting models while maintaining competitive accuracy. We introduce OccamVTS, a novel distillation framework that transfers knowledge from a large, pre-trained vision transformer (ViT) to a compact student network with significantly fewer parameters. OccamVTS leverages a tailored training regime incorporating feature-level and prediction-level distillation losses, alongside a novel attention similarity loss that encourages the student to mimic the attention patterns of the teacher ViT. Experiments on diverse benchmark datasets demonstrate that OccamVTS can distill ViTs to approximately 1% of their original parameters, achieving comparable or even superior forecasting accuracy in certain cases. Our work provides a pathway for deploying powerful vision-based time series forecasting models in resource-limited settings, broadening their applicability and impact."
http://arxiv.org/abs/2508.01678v1,Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models,"Vision-Language Models (VLMs) are known to hallucinate object attributes and relationships not present in an image, hindering their real-world applicability. This paper investigates the impact of visually embedded instructions on mitigating or exacerbating these hallucinations. We propose a novel framework that incorporates instruction-related information directly into the visual input through techniques like image compositing and style transfer, effectively ""showing"" the model what kind of reasoning to perform. We then evaluate the model's responses to questions requiring detailed scene understanding, comparing scenarios with and without visually embedded instructions across various VLM architectures. Our experiments demonstrate that carefully crafted visual instructions can significantly reduce object hallucination, while poorly designed instructions may inadvertently amplify the problem, leading to increased inaccuracies. This highlights the critical role of visual instruction design in controlling and improving the reliability of VLM outputs, opening avenues for more robust and trustworthy vision-language systems."
http://arxiv.org/abs/2508.01653v1,MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and generating text based on visual inputs. However, a significant challenge remains: LVLMs often exhibit hallucinations, generating text that contradicts or is unsupported by the visual content. This paper introduces Map-Level Attention Processing (MAP), a novel approach to mitigate hallucinations by explicitly grounding the language generation process in spatially-aware visual features. MAP leverages object detection to create a semantic map of the image, representing object locations and categories. This map is then used to guide the attention mechanism within the LVLM, forcing the model to attend more strongly to regions containing relevant objects when generating descriptive text. Experimental results on benchmark datasets demonstrate that MAP significantly reduces hallucination rates while maintaining or improving overall descriptive accuracy. Specifically, MAP achieves a 15% reduction in hallucinated object instances compared to state-of-the-art LVLMs. This work presents a crucial step towards building more reliable and trustworthy LVLMs for real-world applications."
http://arxiv.org/abs/2508.01617v1,LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding,"Biomedical image understanding often requires integrating visual information with contextual knowledge and reasoning. While large language models (LLMs) have shown remarkable capabilities in natural language processing, their application to biomedical image analysis remains relatively underexplored, particularly in effectively fusing visual and textual representations. This paper introduces LLaDA-MedV, a novel framework exploring Large Language Diffusion Models for biomedical image understanding. LLaDA-MedV leverages a pre-trained LLM, augmented with a diffusion-based vision encoder, to generate coherent and contextually relevant textual descriptions from medical images. Specifically, we train the vision encoder to map medical images into a latent space aligned with the LLM's embedding space, enabling the LLM to generate reports conditioned on both visual features and textual prompts. We evaluate LLaDA-MedV on multiple medical imaging datasets, demonstrating its ability to generate accurate and informative reports that rival state-of-the-art methods in terms of both image captioning metrics and clinical relevance, as judged by expert radiologists. This work highlights the potential of large language diffusion models for advancing biomedical image understanding and facilitating clinical decision-making."
http://arxiv.org/abs/2508.01579v1,Harnessing Textual Semantic Priors for Knowledge Transfer and Refinement in CLIP-Driven Continual Learning,"Continual learning (CL) aims to enable models to learn new tasks sequentially without forgetting previously acquired knowledge. Recent advancements in Contrastive Language-Image Pre-training (CLIP) offer a powerful foundation for CL due to their strong generalization capabilities and rich semantic understanding. However, naively applying CLIP to CL scenarios often leads to catastrophic forgetting and struggles with adapting to task-specific nuances. This paper addresses the challenge of effectively leveraging textual semantic priors inherent in CLIP for knowledge transfer and refinement in continual learning settings. We propose a novel framework, Textual Semantic Prior-Guided Continual Learning (TSP-CL), which incorporates a text prompt optimization module to dynamically adapt CLIP's text encoder to new tasks, alongside a knowledge distillation strategy that leverages the textual semantic space to mitigate forgetting. Specifically, we optimize task-specific text prompts to better align CLIP's visual and textual representations, and we distill knowledge by encouraging the model to maintain consistency in the textual embedding space across different tasks. Experimental results on several benchmark datasets demonstrate that TSP-CL significantly outperforms existing CLIP-based continual learning methods, achieving superior accuracy and reduced forgetting. Our work highlights the crucial role of textual semantic priors in CLIP for effective knowledge transfer and refinement, opening new avenues for robust and efficient continual learning."
http://arxiv.org/abs/2508.01548v1,A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models,"Large Vision-Language Models (LVLMs) have shown remarkable capabilities in understanding and reasoning about visual content, often relying on computationally expensive visual encoders to process high-resolution images. However, the static processing of all visual tokens within these models leads to significant redundancy and inefficiency, especially when dealing with images containing varying levels of semantic information. This paper addresses the problem of dynamically pruning irrelevant visual tokens to reduce computational cost without sacrificing performance in LVLMs. We introduce ""Glimpse to Compress,"" a novel approach that leverages a lightweight glimpse network to predict the importance of each visual token. Based on these importance scores, less informative tokens are dynamically pruned before being fed into the downstream transformer layers. Experiments conducted on a range of vision-language tasks, including visual question answering and image captioning, demonstrate that our method achieves significant computational savings (up to 40% reduction in FLOPs) with minimal performance degradation. This work offers a practical and effective strategy for deploying and scaling LVLMs in resource-constrained environments."
http://arxiv.org/abs/2508.01546v1,E-VRAG: Enhancing Long Video Understanding with Resource-Efficient Retrieval Augmented Generation,"Long video understanding presents a significant challenge due to the computational demands of processing extensive temporal information. Current methods often struggle with resource constraints when attempting to capture both fine-grained details and long-range dependencies within lengthy videos. To address this, we introduce E-VRAG: Enhanced Video Retrieval Augmented Generation, a novel framework designed to improve the accuracy and efficiency of long video understanding. E-VRAG leverages a two-stage retrieval process: first, a global video representation is used to identify relevant segments, followed by a more granular frame-level retrieval within those segments. The retrieved information is then incorporated into a transformer-based language model to generate comprehensive video descriptions or answer specific questions. We demonstrate that E-VRAG achieves state-of-the-art performance on long video understanding benchmarks while significantly reducing computational costs compared to end-to-end approaches. This resource-efficient approach enables effective long video analysis in scenarios with limited computational resources, broadening the applicability of video understanding technologies."
http://arxiv.org/abs/2508.01540v1,MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about images and text, but their computational demands often preclude deployment on resource-constrained mobile devices. This paper addresses the challenge of distilling VLMs to run efficiently on mobile platforms without sacrificing performance on complex vision-language tasks. We introduce MagicVL-2B, a mobile-friendly VLM architecture that leverages lightweight visual encoders, specifically MobileNetV3 and EfficientNetV2, coupled with a compact language model. To effectively train this model, we propose a novel curriculum learning strategy that progressively increases the difficulty of vision-language tasks, starting with basic object recognition and gradually incorporating more complex reasoning and grounding scenarios. Our experiments demonstrate that MagicVL-2B achieves competitive performance compared to larger VLMs on a range of benchmarks, including visual question answering and image captioning, while significantly reducing computational cost and memory footprint. This allows for practical and robust deployment of VLMs on mobile devices, opening up new possibilities for on-device AI applications."
http://arxiv.org/abs/2508.01533v1,ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models,"Fine-grained video reasoning, which demands precise understanding of actions and their temporal relationships, poses a significant challenge, particularly for resource-constrained models. Existing approaches often struggle with the complexity of reasoning over long temporal spans and subtle action distinctions, leading to performance bottlenecks in smaller models. We introduce ReasonAct, a progressive training framework designed to enhance fine-grained video reasoning capabilities in small models. ReasonAct employs a curriculum learning strategy, starting with simplified reasoning tasks and progressively increasing the complexity by incorporating longer temporal dependencies and finer-grained action categories. Furthermore, we introduce an action-aware attention mechanism that guides the model to focus on relevant action segments within the video, improving its ability to discern subtle differences. Experiments on the Something-Something V2 and MultiTHUMOS datasets demonstrate that ReasonAct significantly outperforms existing methods for small models, achieving a relative improvement of up to 8% in top-1 accuracy while maintaining computational efficiency. This work highlights the efficacy of progressive training and action-aware attention for enabling accurate and efficient fine-grained video reasoning in resource-constrained environments."
http://arxiv.org/abs/2508.01236v1,Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models,"Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities in various multi-modal tasks. However, their substantial computational cost and memory footprint hinder deployment on resource-constrained devices. This paper addresses the challenge of preserving performance in LVLMs under high pruning rates, where naive pruning strategies often lead to significant information loss and performance degradation. We propose a novel pruning framework, ""Information Retention Pruning"" (IRP), which incorporates a learnable information compensation module to recover lost information from pruned weights. IRP selectively distills crucial knowledge from unpruned layers and adaptively transfers it to the remaining pruned layers, guided by a trainable gate mechanism. Experiments on multiple LVLMs and benchmarks demonstrate that IRP achieves significantly higher accuracy compared to existing pruning methods, maintaining near-original performance even with up to 80% sparsity. This work provides a practical solution for deploying efficient and accurate LVLMs in resource-limited environments, expanding their applicability to a wider range of real-world scenarios."
http://arxiv.org/abs/2508.00746v1,GECO: Geometrically Consistent Embedding with Lightspeed Inference,"Image retrieval and visual localization are fundamental tasks in computer vision, often relying on learned feature embeddings. However, existing methods struggle to balance retrieval accuracy with computational efficiency, particularly when dealing with large-scale datasets and real-time applications. This paper addresses the challenge of creating a geometrically consistent image embedding that allows for extremely fast inference without sacrificing accuracy. We introduce GECO (Geometrically Consistent Embedding), a novel approach that leverages a differentiable proxy geometry learning module to enforce geometric consistency during feature embedding training. This module encourages the learned embeddings to reflect the underlying geometric relationships between images, enabling efficient retrieval and localization using simple metric learning techniques in the embedding space. Experiments on benchmark datasets demonstrate that GECO achieves state-of-the-art retrieval accuracy while exhibiting inference speeds orders of magnitude faster than existing methods, reaching near-lightspeed performance. GECO offers a practical solution for applications requiring real-time image retrieval and visual localization in resource-constrained environments."
http://arxiv.org/abs/2508.00549v1,Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images,"Vision-Language Models (VLMs) have demonstrated impressive capabilities in various medical image analysis tasks, leveraging both visual and textual information. However, their understanding of spatial relationships within medical images, particularly relative positioning, remains largely unexplored. This paper investigates the ability of current VLMs to accurately identify and reason about relative spatial arrangements of anatomical structures in medical images. We introduce a novel diagnostic dataset, RelMed, comprising diverse medical imaging modalities (X-ray, CT, MRI) paired with text prompts that explicitly query the relative positions of anatomical structures (e.g., ""Is the left kidney above the right kidney?""). We evaluate several state-of-the-art VLMs on RelMed, employing both zero-shot and fine-tuning approaches. Our results reveal a significant performance gap compared to human baselines, with VLMs consistently struggling to identify correct relative positions, even after fine-tuning. This highlights a critical limitation of existing VLMs in medical image understanding and underscores the need for developing more sophisticated models capable of accurately interpreting spatial relationships, which is crucial for reliable diagnosis and treatment planning."
http://arxiv.org/abs/2508.00427v1,Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting,"Amodal completion, the task of inferring the complete shape of objects partially occluded in an image, is crucial for understanding human-object interaction (HOI) scenes. Existing methods often struggle with HOI scenarios due to complex occlusions arising from hand-object contact and the difficulty in reasoning about plausible object shapes given the constraints imposed by the interaction. This paper introduces a novel contact-aware amodal completion framework for HOI by leveraging a multi-regional inpainting approach. Our method first detects and segments the visible object and hand regions. We then employ a two-stage inpainting network: a global inpainter to generate a coarse completion of the amodal object, followed by a contact-aware local refinement network that focuses on the contact region between the hand and object, conditioned on the hand pose and shape. Experiments on a newly constructed HOI amodal completion dataset demonstrate that our approach significantly outperforms existing state-of-the-art amodal completion methods, achieving substantial improvements in completion accuracy and realism, particularly in regions of hand-object contact. This work provides a significant step towards robust amodal scene understanding in complex interactive environments."
http://arxiv.org/abs/2508.00400v1,Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents,"The development of embodied AI agents capable of seamless interaction within retail environments requires robust training and evaluation platforms. However, real-world retail spaces present challenges in terms of cost, safety, and reproducibility for agent development. This paper introduces ""Sari Sandbox,"" a high-fidelity, interactive virtual retail store environment designed to facilitate the development and evaluation of embodied AI agents. Sari Sandbox features a diverse range of products, realistic physics, dynamic lighting, and simulated human customers exhibiting varied behaviors. We leverage Unreal Engine 5 to create a photorealistic environment with detailed object models and accurate sensor simulations, enabling agents to perceive and interact with the virtual world. We demonstrate the utility of Sari Sandbox by training and evaluating a reinforcement learning-based navigation agent to perform tasks such as object search and shelf rearrangement. Experimental results show that agents trained within Sari Sandbox can successfully transfer to unseen areas within the environment, demonstrating the potential for sim-to-sim transfer. Sari Sandbox offers a cost-effective, safe, and controllable platform for accelerating the development of embodied AI agents for the retail sector."
http://arxiv.org/abs/2508.00399v1,iSafetyBench: A video-language benchmark for safety in industrial environment,"Ensuring safety in industrial environments is paramount, yet current vision-language models lack comprehensive evaluation on safety-related tasks within these complex settings. This paper introduces iSafetyBench, a novel video-language benchmark specifically designed to assess the safety awareness capabilities of AI systems in industrial environments. iSafetyBench comprises a diverse set of videos depicting common industrial scenarios, paired with detailed annotations covering safety-critical events, worker actions, object interactions, and potential hazards. We evaluate state-of-the-art video-language models on tasks such as hazard identification, action recognition, and safety procedure verification, revealing significant shortcomings in their ability to understand and reason about safety in these scenarios. Experimental results demonstrate that even advanced models struggle with nuanced safety understanding, achieving significantly lower performance compared to general video understanding benchmarks. iSafetyBench serves as a crucial resource for advancing research towards safer and more reliable AI-powered automation in industrial settings."
http://arxiv.org/abs/2508.00378v1,CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding,"Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing the reasoning capabilities of large language models (LLMs), particularly when combined with visual information. However, current visual CoT methods often lack explicit verification mechanisms, leading to potential errors in reasoning and grounding. This paper introduces CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding, a novel framework that integrates iterative reasoning, visual grounding, and verification steps to improve the accuracy and reliability of visual CoT. CoRGI decomposes complex visual reasoning tasks into a sequence of verifiable steps, where each step involves grounding relevant objects or regions in the image and explicitly verifying the consistency of the generated reasoning steps with the grounded visual evidence using a learned verifier. Experiments on challenging visual reasoning benchmarks, including ScienceQA and Visual Commonsense Reasoning, demonstrate that CoRGI significantly outperforms existing visual CoT methods, achieving state-of-the-art results with improved reasoning fidelity. The proposed approach provides a crucial step towards building more robust and trustworthy visual reasoning systems by ensuring that the reasoning process is both transparent and verifiable."
http://arxiv.org/abs/2508.00367v1,Representation Shift: Unifying Token Compression with FlashAttention,"Attention mechanisms are central to modern deep learning, but their computational cost scales quadratically with sequence length. Token compression techniques aim to reduce this cost by selectively pruning or merging less important tokens, while FlashAttention optimizes memory access patterns for improved efficiency. Despite their distinct approaches, both can be viewed as methods that fundamentally alter the representation of the input sequence. This paper introduces ""Representation Shift,"" a unified framework that casts both token compression and FlashAttention as transformations within a learned latent space, allowing us to analyze their individual contributions and potential synergies. We propose a novel attention mechanism, RepShift-Attention, which leverages this framework to dynamically adjust the representation space based on the input sequence and computational constraints, effectively interpolating between token compression and FlashAttention. Experiments on long-sequence tasks demonstrate that RepShift-Attention achieves comparable or superior performance to existing token compression methods while maintaining the memory efficiency benefits of FlashAttention, leading to significant speedups. This work provides a new perspective on attention optimization and opens avenues for developing more efficient and adaptive attention mechanisms."
http://arxiv.org/abs/2508.03742v1,Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training,"Medical vision-language pre-training (MedVLP) has shown promising results in various medical image understanding tasks. However, existing MedVLP methods often struggle to capture fine-grained semantic details due to the inherent complexity and subtle variations within medical images, leading to suboptimal vision-language alignment. This paper addresses the challenge of enhancing vision semantic density in MedVLP by explicitly modeling anatomical normality. We propose a novel framework that incorporates an Anatomy Normality Modeling (ANM) module into the vision encoder. ANM leverages a pre-trained variational autoencoder (VAE) on healthy anatomical regions to learn a latent space representing normality, and then employs an anomaly detection mechanism to highlight subtle deviations in input images, thereby boosting the vision encoder's sensitivity to critical anatomical features. Experiments on multiple medical image classification and report generation benchmarks demonstrate that our approach significantly improves performance compared to state-of-the-art MedVLP methods, achieving gains of up to 5% in accuracy and BLEU scores. Our work provides a novel perspective on improving MedVLP by explicitly modeling anatomical normality, leading to more effective vision-language alignment and improved performance on downstream tasks."
http://arxiv.org/abs/2508.00330v1,Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating,"Spectral sensitivity estimation is crucial for accurate color reproduction and spectral imaging applications. While calibrated spectrometers offer precise measurements, their cost and complexity limit their accessibility. This paper addresses the problem of estimating the spectral sensitivity of a camera using an uncalibrated diffraction grating, removing the need for expensive calibration targets and precise grating characterization. Our method leverages the inherent relationship between the diffraction angle, wavelength, and grating parameters. We propose an optimization framework that jointly estimates the camera's spectral sensitivities and the grating's parameters by minimizing the discrepancy between the observed diffraction pattern and a physically-based spectral model. This model incorporates the camera's response, the grating's diffraction properties, and an assumed spectral distribution of the light source. Experimental results demonstrate accurate spectral sensitivity recovery, achieving comparable accuracy to methods relying on calibrated targets, while requiring only a simple, uncalibrated diffraction grating setup. This work provides a cost-effective and accessible solution for spectral sensitivity estimation, enabling wider adoption of spectral imaging techniques."
http://arxiv.org/abs/2508.00298v1,AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer,"Estimating the 3D pose and shape of animals from images is crucial for ecological monitoring and behavioral analysis. However, existing methods often struggle with the substantial morphological diversity across different animal families, limiting their generalization capability. To address this, we introduce AniMer+, a unified framework for 3D animal pose and shape estimation that leverages a novel family-aware transformer architecture. Our method learns family-specific shape priors and deformation patterns by incorporating a family embedding into the transformer encoder, guiding the pose and shape estimation process. Furthermore, we incorporate a multi-stage refinement module that iteratively improves the estimated mesh quality by leveraging both local and global context. Extensive experiments on a diverse benchmark including both mammalia and aves demonstrate that AniMer+ significantly outperforms state-of-the-art methods in both pose and shape accuracy, particularly on unseen animal families. This work facilitates more robust and generalizable 3D animal understanding, enabling large-scale analysis of animal behavior and biomechanics."
http://arxiv.org/abs/2508.00288v1,UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents,"Object Goal Navigation (ObjectNav) tasks have traditionally focused on indoor environments with ground-based robots. However, extending these capabilities to aerial agents in complex, unstructured outdoor environments presents significant challenges. This paper introduces UAV-ON, a novel benchmark for Open-World Object Goal Navigation with Unmanned Aerial Vehicles (UAVs). UAV-ON addresses the limitations of existing ObjectNav benchmarks by simulating realistic outdoor environments with diverse object categories, varying illumination conditions, and dynamic weather patterns. The benchmark utilizes high-fidelity photorealistic rendering and physics simulation to provide a challenging and representative platform for evaluating aerial navigation algorithms. We evaluate several state-of-the-art ObjectNav methods on UAV-ON, demonstrating a significant performance gap compared to indoor environments, highlighting the difficulties of visual perception and control in unstructured outdoor scenarios. Our benchmark reveals the limitations of current approaches and provides a valuable resource for developing robust and adaptable aerial navigation systems capable of operating in the complexities of the real world. UAV-ON serves as a crucial stepping stone towards enabling autonomous UAV applications in diverse fields, including search and rescue, infrastructure inspection, and environmental monitoring."
http://arxiv.org/abs/2508.00272v1,Towards Robust Semantic Correspondence: A Benchmark and Insights,"Establishing robust semantic correspondence between images depicting different instances of the same object category is a fundamental challenge in computer vision. Existing methods often struggle with significant intra-class variations, viewpoint changes, and occlusions, hindering their applicability in real-world scenarios. This paper addresses the need for a comprehensive evaluation of semantic correspondence techniques under more realistic and challenging conditions. We introduce a novel benchmark, ""Robust Correspondence Dataset"" (RCD), featuring a diverse set of image pairs with significant variations in viewpoint, articulation, and partial occlusions, along with pixel-level semantic correspondence annotations. We evaluate a range of state-of-the-art semantic correspondence methods on RCD and analyze their performance across different challenge levels. Our analysis reveals critical limitations of current approaches, particularly in handling large viewpoint changes and complex occlusions, and identifies key areas for future research, such as incorporating more robust feature representations and developing occlusion-aware matching strategies. The benchmark and insights provided in this work will facilitate the development of more robust and practical semantic correspondence algorithms."
http://arxiv.org/abs/2508.00260v1,Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models,"Continual learning of generative vision-language models (GVLMs) is challenging due to catastrophic forgetting and the complex interplay between visual and textual modalities. Existing continual learning methods often struggle to maintain performance on previous tasks while adapting to new ones, especially when the task distribution shifts significantly. To address this, we introduce Instruction-Grounded Visual Projectors (IGVP), a novel framework that leverages instruction embeddings to modulate visual feature projections during continual learning of GVLMs. IGVP learns task-specific visual projectors conditioned on instruction embeddings, allowing the model to dynamically adapt its visual representation based on the current task. Furthermore, we employ a knowledge distillation loss on the instruction embeddings to preserve task-specific information and mitigate catastrophic forgetting in the instruction embedding space. Experimental results on a series of continual generative vision-language tasks demonstrate that IGVP significantly outperforms existing continual learning baselines in terms of both average accuracy and forgetting rate, while maintaining competitive generative capabilities. This highlights the efficacy of instruction-grounded visual modulation for enabling robust and efficient continual learning in GVLMs."
http://arxiv.org/abs/2508.00230v1,Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product,"Parameter-efficient fine-tuning (PEFT) has emerged as a crucial technique for adapting large pre-trained models to downstream tasks with limited resources. However, many existing PEFT methods often suffer from a low effective rank, limiting their ability to capture the complexity of task-specific adaptations. This paper addresses the challenge of increasing the effective rank of PEFT modules while maintaining parameter efficiency. We propose a novel approach that leverages the Khatri-Rao product to construct PEFT modules. Specifically, we use low-rank matrices and combine them via the Khatri-Rao product to create higher-rank parameter updates. This allows us to explore a larger subspace of possible parameter changes with a controlled parameter budget. Our experiments on a range of natural language understanding tasks demonstrate that our Khatri-Rao product-based PEFT method consistently outperforms existing PEFT techniques, achieving significant improvements in accuracy while maintaining a comparable number of trainable parameters. This work offers a promising direction for enhancing the expressiveness and effectiveness of PEFT, enabling better adaptation of large models with limited computational resources."
http://arxiv.org/abs/2508.04197v1,Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective,"Video Text Visual Question Answering (Video TextVQA) demands a comprehensive understanding of both visual content and overlaid text within video frames. Current approaches often focus on holistic video representation and neglect the crucial role of individual text instances and their relationships with relevant visual entities. This work addresses the limitations of existing methods by proposing a novel instance-oriented framework, ""Gather and Trace,"" specifically designed for Video TextVQA. Our approach first employs an instance-aware module to detect and recognize individual text instances in each frame, subsequently gathering relevant visual features based on contextual cues. The core of our method lies in a tracing mechanism that dynamically tracks the spatio-temporal evolution of these text instances and their interactions with corresponding visual regions, effectively reasoning about the scenes textual and visual components. Experimental results on benchmark datasets demonstrate that our ""Gather and Trace"" framework significantly outperforms state-of-the-art methods, achieving substantial improvements in accuracy and demonstrating enhanced reasoning capabilities. This instance-oriented perspective offers a more nuanced and effective approach to Video TextVQA, paving the way for more robust and interpretable video understanding systems."
http://arxiv.org/abs/2508.04161v1,Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning,"Face video restoration aims to recover high-quality facial details from degraded inputs, a task crucial for various applications like surveillance and video conferencing. However, severe degradation often leads to loss of crucial facial features, making accurate restoration challenging. To address this, we propose a novel audio-assisted face video restoration framework incorporating Temporal and Identity Complementary Learning (TICL). Our method leverages the complementary information present in the audio modality to guide the restoration process. Specifically, we employ a transformer-based architecture to extract temporal audio features and fuse them with visual features. Furthermore, we introduce an Identity Complementary Loss that enforces the consistency of identity information between the restored faces and the original audio, ensuring faithful reconstruction. Experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art face video restoration methods, achieving superior perceptual quality and identity preservation, as measured by PSNR, SSIM, and identity-related metrics. This work highlights the potential of multi-modal learning for enhancing face video restoration, particularly in scenarios with significant degradation."
http://arxiv.org/abs/2508.04160v1,DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment,"Data visualization literacy is increasingly crucial, yet assessing this complex construct remains challenging, particularly concerning the selection of effective and representative data visualization items. Existing assessment methods often rely on subjective item selection, leading to potential biases and limitations in evaluating a broad range of visualization literacy skills. To address this, we introduce DRIVE-T, a novel methodology for Discriminative and Representative Data Viz item selection leveraging Transformer-based language models. DRIVE-T employs a two-stage process: first, it uses a pre-trained Transformer model fine-tuned on visualization-related text to generate semantic embeddings of candidate visualization items. Second, it applies a diversity maximization algorithm guided by these embeddings and item difficulty scores to select a subset of items that are both highly discriminative (i.e., effectively differentiate between skill levels) and representative of the underlying data visualization domain. Experiments on a large-scale dataset of data visualization literacy assessments demonstrate that DRIVE-T significantly improves the reliability and validity of the selected item sets compared to baseline methods, resulting in more accurate and comprehensive assessments. This methodology provides a principled and data-driven approach to item selection, enhancing the objectivity and effectiveness of data visualization literacy assessments."
http://arxiv.org/abs/2508.04153v1,ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation,"Large pre-trained vision models demonstrate remarkable generalization capabilities, yet adapting them efficiently to multiple downstream tasks remains a significant challenge. Existing parameter-efficient transfer learning (PETL) methods often struggle to effectively integrate task-specific knowledge, leading to sub-optimal performance and catastrophic forgetting. To address this, we introduce ICM-Fusion, an In-Context Meta-Optimized LoRA Fusion technique that dynamically fuses Low-Rank Adaptation (LoRA) modules based on task context. ICM-Fusion utilizes a meta-learned fusion network, trained in-context with a small set of exemplar tasks, to predict optimal fusion weights for each task based on its unique input. This allows for adaptive and efficient knowledge transfer without requiring explicit task-specific training of the fusion network itself. Experiments on diverse multi-task adaptation benchmarks demonstrate that ICM-Fusion consistently outperforms state-of-the-art PETL methods, achieving significant improvements in average accuracy and robustness to task interference. ICM-Fusion offers a novel and effective approach to multi-task adaptation by leveraging in-context meta-learning for dynamic and efficient LoRA fusion."
http://arxiv.org/abs/2508.03955v1,Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm,"Creating realistic and expressive talking-head animations from audio input has seen significant progress, yet scaling these models to handle diverse identities and complex visual variations remains a challenge. Current approaches often suffer from high computational costs and memory requirements during training, hindering their ability to generalize to large datasets and high-resolution outputs. We introduce a novel training paradigm, Efficient Audio-Visual Animation (EAVA), that significantly reduces the computational burden of audio-synchronized visual animation. EAVA employs a multi-stage training strategy, initially focusing on learning coarse lip movements with a lightweight architecture and subsequently refining facial details and identity-specific characteristics through a transfer learning approach to a higher-capacity model. This allows for efficient knowledge transfer and avoids training large models from scratch. Experiments on large-scale datasets demonstrate that EAVA achieves comparable or superior animation quality to state-of-the-art methods while reducing training time by up to 50% and memory consumption by 40%. This efficient training paradigm unlocks the potential for creating highly realistic and personalized audio-driven animations at scale, facilitating applications in virtual assistants, personalized avatars, and content creation."
http://arxiv.org/abs/2508.03694v1,LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation,"Generating high-fidelity and controllable ultra-long videos remains a significant challenge due to the quadratic increase in computational complexity with video length and the difficulty in maintaining temporal coherence. Existing video generation models struggle to produce consistent content and adhere to user-specified controls over extended durations. We introduce LongVie, a novel framework for multimodal-guided controllable ultra-long video generation. LongVie employs a hierarchical video transformer architecture with a novel attention mechanism, Long-Attention, that efficiently captures long-range dependencies while maintaining computational feasibility. Furthermore, we leverage multimodal guidance, incorporating both textual and visual cues to enable precise control over the generated video content and style. Experimental results demonstrate that LongVie significantly outperforms state-of-the-art methods in terms of video quality, temporal coherence, and controllability, generating videos exceeding 10,000 frames with consistent characters and coherent narratives. This represents a significant step towards creating AI-driven tools for storyboarding, virtual world creation, and other applications requiring long, controlled video sequences."
http://arxiv.org/abs/2508.03644v1,Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?,"Retrieval-Augmented Generation (RAG) systems, which combine information retrieval with large language models (LLMs), have emerged as a promising paradigm for knowledge-intensive tasks. However, current evaluation methodologies for RAG often rely on isolated metrics focusing on either retrieval accuracy or generation quality, failing to adequately capture the intricate interplay between these two components. This paper investigates the limitations of existing evaluation protocols for RAG systems and proposes a novel, holistic evaluation framework. Our framework, RAG-Assess, integrates retrieval performance with downstream generation faithfulness, coherence, and relevance through a suite of automated metrics and human evaluation protocols. We further introduce a diagnostic benchmark designed to stress-test RAG systems under various retrieval challenges, such as noisy documents and knowledge gaps. Experimental results demonstrate that RAG-Assess provides a more nuanced and comprehensive understanding of RAG system performance compared to traditional metrics, revealing critical bottlenecks in the retrieval-generation pipeline. This work offers valuable insights into the strengths and weaknesses of current RAG evaluation practices, paving the way for more effective development and deployment of these systems."
http://arxiv.org/abs/2508.03524v1,Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models,"Histopathology image analysis plays a crucial role in cancer diagnosis and research. However, whole slide images (WSIs) are often fragmented during preparation and analysis, hindering contextual understanding of tissue architecture. This paper addresses the challenge of automatically reconstructing and semantically aligning fragmented histopathology images into a coherent mosaic. We propose a novel semantic mosaicing approach leveraging visual foundation models, specifically DINOv2, to extract robust, high-level feature representations from individual image fragments. These features are then used to establish correspondences between fragments based on semantic similarity, followed by a graph-based optimization to determine the optimal spatial arrangement that minimizes feature distance between adjacent fragments while maintaining global consistency. We demonstrate that our method effectively assembles fragmented breast cancer histology images, achieving significantly higher accuracy in tissue structure alignment and semantic coherence compared to traditional feature-based mosaicing techniques. Our approach facilitates improved visualization and analysis of large-scale histopathology data, ultimately aiding in more accurate diagnosis and biomarker discovery."
http://arxiv.org/abs/2508.03457v2,READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation,"Audio-driven talking head generation aims to synthesize realistic and synchronized lip movements from speech, a challenging task due to the complex relationship between audio and visual modalities. Existing methods often struggle with computational efficiency and latency, hindering their applicability in real-time interactive applications. We address this limitation by introducing READ: Real-time and Efficient Asynchronous Diffusion, a novel diffusion-based framework for audio-driven talking head generation. READ leverages an asynchronous processing pipeline where a lightweight, non-autoregressive audio encoder predicts facial movements, which are then refined by a conditional diffusion model operating in a latent space. A key innovation is the asynchronous execution, allowing the audio encoder to continually process audio while the diffusion model refines previous frames, significantly reducing latency. Experimental results demonstrate that READ achieves state-of-the-art performance in terms of visual quality, lip synchronization accuracy, and significantly reduced processing time, enabling real-time generation. This advancement paves the way for more immersive and interactive communication platforms."
http://arxiv.org/abs/2508.03337v2,Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration,"Video Question Answering (Video-QA) demands efficient processing of lengthy video sequences to identify relevant information for answering complex questions. Existing methods often process all video frames uniformly, leading to redundant computation and hindering real-time performance. This paper addresses the challenge of improving Video-QA efficiency by focusing on processing only the most informative visual tokens. We propose a novel framework that combines adaptive frame-pruning with semantic graph integration. First, a lightweight pruning module dynamically selects crucial frames based on visual content and question relevance. Second, a semantic graph is constructed to capture inter-frame relationships and integrate contextual information from the pruned video. This graph is then used to enhance feature representations for improved answer prediction. Experiments on multiple benchmark datasets, including MSVD-QA and MSRVTT-QA, demonstrate that our approach achieves comparable or superior accuracy to state-of-the-art methods while significantly reducing computational cost by processing fewer video frames. The proposed token-efficient approach offers a promising direction for developing scalable and real-time Video-QA systems."
http://arxiv.org/abs/2508.03218v1,ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow,"Precise robot manipulation in unstructured environments remains a significant challenge due to the inherent uncertainty in perception and the complexity of action execution. Existing methods often struggle to adapt to dynamic changes in the environment, leading to suboptimal performance in tasks requiring fine-grained control. We address this problem by introducing ActionSink, a novel framework that dynamically integrates action flow into the robot's decision-making process. ActionSink leverages a learned action-conditioned flow field to predict the consequences of potential robot actions, effectively creating a ""sink"" towards desired states. This flow field is then integrated with a task-specific reward function to guide the robot's trajectory optimization, enabling real-time adaptation to environmental changes. Experimental results in simulated and real-world manipulation tasks demonstrate that ActionSink significantly outperforms baseline methods in terms of success rate and execution time, particularly in scenarios involving deformable objects and external disturbances. This work offers a promising direction for achieving more robust and precise robot manipulation in dynamic and uncertain environments."
http://arxiv.org/abs/2508.03179v1,Advancing Precision in Multi-Point Cloud Fusion Environments,"Multi-point cloud fusion is crucial for creating comprehensive 3D representations in various applications, including autonomous navigation, robotics, and environmental modeling. However, achieving high precision in these environments remains challenging due to sensor noise, varying point densities, and registration errors. This paper addresses the problem of improving the accuracy and robustness of multi-point cloud fusion, particularly in scenarios with significant data heterogeneity. We propose a novel framework that integrates a learned point cloud denoising module with a robust iterative closest point (ICP) variant, enhanced by a spatially adaptive weighting scheme. The denoising module, trained using a self-supervised approach, reduces noise and outliers, while the adaptive ICP algorithm minimizes registration errors by assigning higher weights to points with higher confidence based on local geometric features. Our experiments on both synthetic and real-world datasets demonstrate that the proposed method significantly outperforms state-of-the-art fusion techniques, achieving a reduction in registration error of up to 40% and improved surface reconstruction quality. These advancements contribute to more accurate and reliable 3D scene understanding, enabling safer and more efficient operation in complex environments."
http://arxiv.org/abs/2508.03050v1,Multi-human Interactive Talking Dataset,"Understanding human social interactions is crucial for developing socially intelligent AI systems. Existing datasets for audio-visual analysis of speech often focus on single-speaker scenarios or lack detailed annotations for multi-person conversations, limiting the development of models capable of analyzing complex group dynamics. This paper addresses the need for a comprehensive resource for studying multi-human interactive talking scenarios by introducing the Multi-human Interactive Talking (MIT) Dataset. Our dataset comprises over 100 hours of video recordings featuring unscripted, natural conversations among 3-5 participants in diverse indoor environments. We provide rich annotations, including speaker diarization, facial landmarks, head pose estimation, and turn-taking behavior, along with transcriptions of the spoken content. We demonstrate the utility of the MIT Dataset by training and evaluating several baseline models for tasks such as speaker identification and activity recognition, achieving promising results that highlight the dataset's potential. The MIT Dataset provides a valuable platform for advancing research in areas such as social signal processing, multi-person activity understanding, and human-computer interaction in group settings."
http://arxiv.org/abs/2508.02944v1,X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio,"Creating realistic and emotionally engaging virtual avatars capable of expressive acting remains a significant challenge in computer vision and graphics. Existing audio-driven portrait animation methods often struggle with generating compelling long-range motion and conveying nuanced emotions, leading to robotic or unnatural results. This paper introduces X-Actor, a novel framework for generating emotionally expressive and temporally coherent portrait acting sequences from audio input. Our method leverages a hierarchical architecture with a transformer-based long-range motion generator conditioned on both audio features and learned emotional embeddings. This is followed by a detail enhancement module that refines local facial expressions and lip synchronization. Experiments demonstrate that X-Actor produces significantly more realistic and emotionally expressive portrait videos compared to state-of-the-art methods, as evidenced by both quantitative metrics and qualitative evaluations. X-Actor represents a significant step towards creating truly believable and engaging virtual actors for a wide range of applications."
http://arxiv.org/abs/2508.02905v1,How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes,"Understanding the acoustic properties of indoor environments is crucial for applications ranging from virtual reality and architectural design to robotics and accessibility. While visual scene understanding has advanced significantly, generating realistic acoustic profiles that accurately reflect the materials and geometry of a space remains a challenging problem. This paper addresses the task of generating plausible acoustic reflections and reverberation characteristics for indoor scenes, conditioned on their visual appearance and material composition. We propose a novel multimodal approach that leverages a deep neural network to learn a mapping between visual scene representations, material properties extracted from image segmentation, and acoustic features representing the room impulse response (RIR). Our model combines a convolutional neural network (CNN) to encode the visual scene, a material embedding network trained on a large-scale material database, and a recurrent neural network (RNN) to generate the temporal evolution of the RIR. Experiments on a diverse dataset of simulated indoor scenes demonstrate that our method generates significantly more realistic and perceptually accurate acoustic profiles compared to existing geometric acoustic simulation and data-driven approaches, as validated through objective metrics and subjective listening tests. This work enables more immersive and realistic auditory experiences in virtual environments and provides a valuable tool for acoustic design and analysis."
http://arxiv.org/abs/2508.02829v1,Elucidating the Role of Feature Normalization in IJEPA,"Joint Embedding Predictive Architecture (IJEPA) has demonstrated remarkable success in self-supervised representation learning by predicting representations of masked image regions from other regions. However, the precise contribution of feature normalization techniques, such as Layer Normalization, to IJEPA's performance remains underexplored. This paper investigates the impact of different feature normalization strategies within the IJEPA architecture. We systematically evaluate the effects of removing Layer Normalization from various locations within the vision transformer backbone and the prediction network, as well as explore alternative normalization techniques like Batch Normalization and Group Normalization. Our experiments on ImageNet-1K reveal that Layer Normalization is crucial within the prediction network, significantly impacting downstream classification accuracy. Furthermore, we find that replacing Layer Normalization with Batch Normalization within the vision transformer backbone leads to a substantial performance degradation, highlighting the importance of instance-specific normalization for IJEPA. These findings provide critical insights into the architectural design choices of IJEPA and pave the way for further improvements in self-supervised visual representation learning."
http://arxiv.org/abs/2508.02645v1,Evaluating Variance in Visual Question Answering Benchmarks,"Visual Question Answering (VQA) benchmarks are crucial for evaluating and comparing the performance of AI models capable of understanding both images and natural language. However, existing VQA benchmarks often suffer from biases and statistical regularities, leading to inflated performance metrics that do not accurately reflect true reasoning abilities. This paper investigates the variance in performance across different dataset splits and question subsets within popular VQA benchmarks to quantify the impact of these biases. We propose a novel evaluation methodology based on analyzing the consistency of model predictions across strategically designed data partitions, created by grouping questions based on their semantic similarity and image content. This allows us to isolate and measure the sensitivity of VQA models to subtle variations in the input data. Our analysis reveals significant performance discrepancies across these partitions, highlighting the fragility of current VQA models and their reliance on superficial correlations. This study underscores the critical need for more robust evaluation protocols and the development of VQA models that exhibit genuine reasoning capabilities rather than exploiting dataset biases."
http://arxiv.org/abs/2508.02516v1,Engagement Prediction of Short Videos with Large Multimodal Models,"Predicting user engagement with short-form videos is crucial for content recommendation and platform optimization. While traditional methods rely on unimodal analysis or simple multimodal fusion, they often fail to capture the complex interplay between visual and textual elements that drive viewer interest. This paper addresses the challenge of accurately predicting engagement metrics (likes, shares, comments) for short videos by leveraging the power of Large Multimodal Models (LMMs). We propose a novel framework that utilizes a pre-trained LMM to jointly process video frames, transcribed audio, and associated user-generated text (e.g., captions). Our approach employs a hierarchical attention mechanism to dynamically weight the contribution of each modality, enabling the model to focus on the most salient features relevant to engagement. Experimental results on a large-scale short video dataset demonstrate that our LMM-based approach significantly outperforms state-of-the-art unimodal and multimodal baselines in predicting engagement metrics. The improved accuracy highlights the effectiveness of LMMs in capturing nuanced multimodal relationships and their potential for advancing video understanding and personalized recommendation systems."
http://arxiv.org/abs/2508.02512v1,QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots,"Generating realistic and controllable visual experiences for quadruped robots is crucial for their effective deployment in complex environments, especially for tasks like navigation and exploration. Existing video generation methods often struggle with maintaining spatial consistency and control when applied to panoramic views typical of robot-mounted cameras, limiting their utility for embodied AI. This paper introduces QuaDreamer, a novel framework for controllable panoramic video generation specifically tailored for quadruped robots. QuaDreamer leverages a spatio-temporal generative adversarial network (GAN) conditioned on robot proprioception (joint angles, base pose) and user-specified high-level commands (e.g., ""walk forward,"" ""turn left""). We introduce a novel panoramic attention mechanism within the GAN architecture to enforce global consistency and capture long-range dependencies across the 360-degree field of view. Furthermore, we incorporate a differentiable physics simulator to provide realistic robot dynamics and improve the coherence between the generated video and robot actions. Experimental results demonstrate that QuaDreamer generates high-fidelity panoramic videos that are significantly more realistic and controllable compared to state-of-the-art video generation models, enabling the creation of immersive and interactive simulation environments for quadruped robot training and evaluation. This advancement paves the way for more robust and adaptable quadruped robots capable of operating in unstructured and dynamic real-world scenarios."
http://arxiv.org/abs/2508.02493v2,Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting,"3D Gaussian Splatting (3D-GS) has emerged as a powerful technique for novel view synthesis, offering real-time rendering and high-quality results. However, 3D-GS is often plagued by the presence of ""floating artifacts,"" spurious Gaussians detached from the underlying scene geometry, which degrade visual fidelity, particularly in regions with sparse or noisy input data. This paper introduces a novel approach to mitigate these floating artifacts by prioritizing the learning of low-frequency scene structure. Our method, termed ""Low-Frequency First (LFF),"" incorporates a frequency-domain regularizer during the Gaussian optimization process. Specifically, we encourage Gaussians to initially capture coarse, low-frequency scene information by penalizing high-frequency components in their positional gradients. This encourages a more robust and globally consistent scene representation before allowing finer details to be learned. Experimental results demonstrate that LFF significantly reduces the prevalence of floating artifacts, leading to improved visual quality and reduced noise in rendered images, while maintaining comparable rendering speed to the original 3D-GS method. By promoting a more stable and coherent scene representation, LFF enhances the robustness and reliability of 3D Gaussian Splatting for novel view synthesis."
http://arxiv.org/abs/2508.02460v1,InfoSyncNet: Information Synchronization Temporal Convolutional Network for Visual Speech Recognition,"Visual Speech Recognition (VSR), also known as lip reading, aims to decode speech content solely from visual information of the speaker's mouth region. A key challenge in VSR lies in effectively modeling the temporal dynamics and complex relationships between visual features and corresponding speech units. We address this challenge by introducing InfoSyncNet, an Information Synchronization Temporal Convolutional Network designed to explicitly synchronize relevant information across different temporal scales. InfoSyncNet employs a multi-branch temporal convolutional network architecture enhanced with a novel Synchronization Module. This module learns to align and fuse information from different temporal resolutions, enabling the network to capture both fine-grained local movements and long-range contextual dependencies. Experiments on benchmark datasets, including GRID and LRW, demonstrate that InfoSyncNet achieves state-of-the-art performance, surpassing existing methods by a significant margin, particularly in challenging scenarios with noisy or ambiguous lip movements. The proposed information synchronization mechanism provides a powerful approach for improving the accuracy and robustness of visual speech recognition systems."
http://arxiv.org/abs/2508.02374v1,Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation,"Layout generation aims to automatically arrange visual elements in a scene, crucial for design tasks and virtual environment creation. Current methods often struggle to align with human aesthetic preferences and functional requirements, resulting in layouts that are synthetically plausible but practically unusable. This paper addresses the challenge of incorporating human feedback into a unified framework for both layout generation and evaluation. We introduce Uni-Layout, a novel approach that integrates a reinforcement learning-based generator with a human-in-the-loop evaluation module. The generator proposes layouts, which are then assessed by human evaluators providing explicit feedback (e.g., ratings, suggestions). This feedback is incorporated into the reward function, guiding the generator to produce layouts that better reflect human preferences. Experimental results on a diverse set of layout tasks demonstrate that Uni-Layout significantly improves layout quality, achieving higher human preference scores compared to state-of-the-art automatic methods and exhibiting faster convergence towards desired layouts. This work paves the way for more intuitive and human-centered layout generation systems, enabling the creation of designs that are both aesthetically pleasing and functionally effective."
http://arxiv.org/abs/2508.02363v1,Transport-Guided Rectified Flow Inversion: Improved Image Editing Using Optimal Transport Theory,"Diffusion models have shown remarkable success in image generation and editing, often relying on iterative refinement processes to transform random noise into coherent images. However, precise image editing requires accurate inversion to map a real image into the latent space of the diffusion model, a task complicated by the inherent stochasticity of standard inversion techniques and the entanglement of latent variables. This paper introduces Transport-Guided Rectified Flow Inversion (TRFI), a novel approach that leverages optimal transport theory to achieve more accurate and controllable inversion for image editing. TRFI utilizes the rectified flow framework to define a deterministic trajectory between the data and noise distributions, and employs optimal transport to explicitly map image features to corresponding noise representations along this trajectory. This transport map provides a strong prior for inverting real images, enabling precise control over the inverted latent codes. Experiments demonstrate that TRFI significantly improves the fidelity and editability of inverted images compared to existing methods, leading to more realistic and controllable image editing results on a variety of tasks. TRFI offers a powerful framework for unlocking the full potential of diffusion models for image manipulation by providing a more reliable and controllable inversion process."
http://arxiv.org/abs/2508.02362v1,Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering,"Generating realistic and synchronized talking faces from text remains a challenging problem in computer vision and graphics. Existing methods often struggle with producing accurate lip movements and natural facial expressions, especially for long and complex sentences. We introduce Text2Lip, a novel framework for progressive lip-synced talking face generation from text, leveraging a viseme-guided rendering approach. Our method first decomposes the input text into a sequence of visemes, which are then used to drive a 3D face model. This model is rendered into intermediate video frames, progressively refined by a generative adversarial network (GAN) conditioned on both the viseme sequence and the previous frames. The GAN incorporates a novel viseme-aware discriminator to ensure temporal coherence and lip-speech synchronization. Experiments on benchmark datasets demonstrate that Text2Lip achieves state-of-the-art results in terms of lip-sync accuracy, visual realism, and perceptual quality, surpassing existing methods in generating coherent and expressive talking faces. Our work significantly advances the state-of-the-art in text-driven talking face generation, paving the way for more natural and engaging human-computer interaction."
http://arxiv.org/abs/2508.02359v1,Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue,"Steady-state visually evoked potentials (SSVEPs) are widely used in brain-computer interfaces (BCIs) due to their high signal-to-noise ratio and ease of implementation. However, conventional visual stimuli, often generated using LCD monitors, can induce significant visual fatigue, hindering prolonged BCI usage and potentially affecting SSVEP reliability. This paper addresses the challenge of creating a reliable and comfortable SSVEP-inducing stimulus by leveraging the precise control offered by pulse-width modulation (PWM) of light-emitting diodes (LEDs). We propose a novel LED-based visual stimulus system, where PWM is carefully calibrated to minimize flicker perception while maintaining robust SSVEP elicitation. Specifically, we optimized the PWM frequency and duty cycle to produce stable luminance at target frequencies while mitigating temporal artifacts. Experimental results demonstrate that the proposed PWM-controlled LED stimulus elicits SSVEPs comparable in amplitude and signal-to-noise ratio to those induced by a conventional LCD monitor, while significantly reducing subjective reports of visual fatigue. This work paves the way for more comfortable and reliable SSVEP-based BCIs, enhancing their usability for extended periods and broader applications."
http://arxiv.org/abs/2508.02340v1,Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search,"Ad-hoc video search aims to retrieve relevant videos given a free-form textual query. Existing methods often rely on learning a shared embedding space between visual and textual modalities, however, they frequently suffer from high correlations between embedding dimensions, hindering the model's ability to capture fine-grained semantic relationships. This paper addresses the problem of learning more discriminative and interpretable common spaces for ad-hoc video search by explicitly reducing the correlations between embedding dimensions. We propose a novel Partially-Decorrelated Common Space Learning (PDCSL) framework, which incorporates a decorrelation regularizer during training to minimize the redundancy among learned features while preserving crucial dependencies. Furthermore, we introduce a modality-aware weighting scheme to adaptively control the degree of decorrelation applied to each modality, allowing for flexible adaptation to modality-specific characteristics. Experimental results on benchmark datasets demonstrate that our PDCSL framework consistently outperforms state-of-the-art methods, achieving significant improvements in retrieval accuracy. This work provides a promising direction for enhancing the performance and interpretability of cross-modal embedding learning in the context of ad-hoc video search."
http://arxiv.org/abs/2508.02324v1,Qwen-Image Technical Report,"Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. Extending these capabilities to the visual domain, creating powerful vision-language models (VLMs), presents significant challenges in bridging the modality gap and enabling sophisticated visual understanding. This paper introduces Qwen-Image, a large-scale vision-language model built upon the Qwen LLM architecture. We address the problem of enhancing VLMs' ability to perform complex visual reasoning and generate detailed, contextually relevant image descriptions. Qwen-Image incorporates a novel vision encoder architecture optimized for high-resolution image processing and a cross-modal connector that effectively aligns visual and textual representations. Furthermore, we leverage a large-scale, curated dataset comprising image-text pairs, instruction-following examples, and visual question answering data to train the model. Qwen-Image achieves state-of-the-art performance on a range of established benchmarks, including visual question answering, image captioning, and visual reasoning tasks, demonstrating significant improvements over existing open-source VLMs. These results highlight the potential of Qwen-Image as a foundation model for advancing research and applications in computer vision and multimodal AI."
http://arxiv.org/abs/2508.02243v1,I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking,"Multimodal entity linking (MEL) aims to ground textual mentions to their corresponding entities in a knowledge graph by leveraging information from different modalities, such as text and images. Current MEL methods often treat different modalities independently or fuse them with simple concatenation, neglecting the complex intra- and inter-modal relationships that are crucial for accurate entity disambiguation. To address this, we propose I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking. I2CR introduces a novel reflection mechanism that facilitates information exchange and reasoning within each modality (intra-modal) and across different modalities (inter-modal). Specifically, we employ reflection modules to refine feature representations by iteratively attending to and reflecting on the information from both textual and visual modalities. Experimental results on the widely used WNED-WIKI and WNED-NYT datasets demonstrate that I2CR significantly outperforms state-of-the-art MEL models, achieving substantial improvements in linking accuracy. This highlights the effectiveness of collaborative reflections in capturing intricate multimodal relationships and enhancing entity disambiguation performance."
http://arxiv.org/abs/2508.02240v2,Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor,"Diffusion models have achieved state-of-the-art results in image generation and other domains, but their iterative sampling process remains computationally expensive. A key challenge is determining the optimal number of denoising steps required for a given sample, as fixed schedules often lead to unnecessary computations for simpler inputs. We address this problem by introducing a novel approach, Confidence-Gated Taylor Diffusion (CGTD), which adaptively determines when to terminate the denoising process based on a learned confidence score. CGTD leverages a Taylor expansion approximation of the denoising function to estimate the expected change in the generated sample after each step, and combines this with a confidence network that predicts the reliability of the Taylor approximation. The denoising process is terminated when the confidence-weighted Taylor estimate falls below a learned threshold. Our experiments demonstrate that CGTD can significantly accelerate diffusion sampling, achieving up to a 4x speedup on image generation tasks while maintaining competitive or even improved image quality compared to fixed-schedule baselines. This adaptive approach offers a promising direction for reducing the computational burden of diffusion models, making them more practical for real-world applications."
http://arxiv.org/abs/2508.02155v1,DreamPainter: Image Background Inpainting for E-commerce Scenarios,"E-commerce platforms heavily rely on visually appealing product images, often requiring background modifications for consistency and aesthetic appeal. However, manually editing backgrounds is time-consuming and requires specialized skills. This paper addresses the challenging problem of automatically inpainting backgrounds in e-commerce product images, focusing on seamless integration and realistic content generation. We introduce DreamPainter, a novel background inpainting framework leveraging a multi-stage diffusion model conditioned on both the foreground object and a user-provided text prompt describing the desired background. DreamPainter employs a foreground-aware attention mechanism to guide background generation and a blending module to ensure smooth transitions between the inpainted region and the original image. Experimental results on a newly curated e-commerce product image dataset demonstrate that DreamPainter significantly outperforms existing general-purpose inpainting methods in terms of visual quality, realism, and user preference, achieving a Frchet Inception Distance (FID) score improvement of over 20% compared to state-of-the-art baselines. DreamPainter offers a practical and efficient solution for automating background editing in e-commerce, ultimately enhancing product presentation and potentially boosting sales."
http://arxiv.org/abs/2508.02051v1,HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image Compression,"Distributed image compression aims to efficiently encode images across multiple devices with limited communication. However, existing approaches often struggle to balance compression ratio, computational complexity, and reconstruction quality, particularly in resource-constrained environments. This paper introduces the Hierarchical Cascade Framework (HCF) for distributed multi-stage image compression, designed to address these limitations. HCF employs a hierarchical decomposition of the image into multiple layers, enabling progressive encoding and transmission. Each layer is then processed through a cascade of lightweight compression stages, distributed across available devices. This cascade leverages inter-layer dependencies for improved compression efficiency. Experimental results on standard image datasets demonstrate that HCF achieves significant improvements in rate-distortion performance compared to state-of-the-art distributed compression techniques, while maintaining low computational overhead. The proposed framework offers a practical solution for efficient and scalable image compression in distributed systems."
http://arxiv.org/abs/2508.02000v1,Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling,"Audio-visual deepfakes pose a significant threat due to their potential for misinformation and manipulation. Existing deepfake detection methods often focus on binary classification or pixel-level segmentation, overlooking the crucial aspect of precisely localizing manipulated regions with accurate boundaries, which is essential for forensic analysis and understanding the nature of the manipulation. This paper introduces a novel approach for localizing audio-visual deepfakes based on Hierarchical Boundary Modeling (HBM). Our method leverages a multi-scale convolutional neural network to extract audio-visual features, which are then fed into a hierarchical boundary prediction module. This module progressively refines the boundary predictions, starting from coarse segments and iteratively refining them to achieve pixel-accurate localization. Experimental results on benchmark datasets demonstrate that our HBM significantly outperforms state-of-the-art methods in terms of boundary localization accuracy and Intersection-over-Union (IoU) score, while maintaining competitive detection performance. This fine-grained localization offers valuable insights into the manipulated regions, enabling more effective deepfake analysis and mitigation strategies."
http://arxiv.org/abs/2508.01875v1,StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding,"Streaming video understanding demands real-time analysis and proactive decision-making, yet current methods often lag behind, reacting to events rather than anticipating them. This work addresses the challenge of building anticipatory agents capable of forecasting future states and actions in streaming video. We introduce StreamAgent, a novel framework that combines a recurrent neural network (RNN) for temporal modeling with a predictive module trained to forecast future scene representations. StreamAgent employs a multi-task learning approach, jointly optimizing for action recognition and future state prediction using a contrastive loss to ensure accurate and temporally consistent forecasts. Experiments on benchmark video datasets demonstrate that StreamAgent achieves significant improvements in anticipatory action recognition and future scene prediction compared to state-of-the-art baselines, particularly in scenarios requiring long-term reasoning. These results highlight the potential of StreamAgent to enable more responsive and intelligent systems for applications like autonomous navigation, surveillance, and human-robot interaction."
http://arxiv.org/abs/2508.01766v2,VPN: Visual Prompt Navigation,"Vision-and-Language Navigation (VLN) tasks challenge agents to follow natural language instructions to navigate through realistic environments. Existing VLN agents primarily rely on processing raw visual inputs to make navigational decisions, often struggling with long and complex instructions or environments with high visual ambiguity. To address these limitations, we introduce Visual Prompt Navigation (VPN), a novel framework that leverages visual prompting to enhance the agent's understanding of the environment and improve navigation performance. VPN incorporates learnable visual prompts into the agent's visual encoder, guiding the agent to focus on salient visual features relevant to the navigation task. These prompts are dynamically adapted based on the current instruction and the agent's location, allowing for more informed and context-aware decision-making. Experimental results on the Room-to-Room (R2R) and Room-for-Room (RxR) datasets demonstrate that VPN significantly outperforms state-of-the-art methods, achieving considerable improvements in success rate and navigation efficiency. This work highlights the potential of visual prompting for improving VLN agents and opens new avenues for research in instruction following and embodied AI."
http://arxiv.org/abs/2508.01742v1,Intention-Guided Cognitive Reasoning for Egocentric Long-Term Action Anticipation,"Egocentric action anticipation is crucial for enabling proactive assistance in various daily activities. However, anticipating actions over extended horizons remains challenging due to the compounding uncertainty and the complex interplay of visual cues and underlying human intentions. This paper addresses the problem of long-term egocentric action anticipation by explicitly modeling the cognitive reasoning process driven by inferred intentions. We introduce a novel framework that integrates a hierarchical intention inference module with a cognitive reasoning network. The intention module leverages observed egocentric video and contextual information to predict high-level goals, which then guide the reasoning network to generate a sequence of anticipated actions. The reasoning network employs a memory-augmented transformer architecture to capture long-range dependencies and refine action predictions based on evolving intentions. Experiments on the EPIC-Kitchens-100 and Ego4D datasets demonstrate that our intention-guided approach significantly outperforms state-of-the-art methods, particularly in long-term anticipation scenarios. This work provides a valuable step towards building more intelligent and proactive egocentric vision systems capable of understanding and anticipating human behavior."
http://arxiv.org/abs/2508.01711v1,GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval,"Text-video retrieval aims to bridge the semantic gap between textual queries and video content. Existing methods often struggle with effectively integrating audio and visual modalities at a fine-grained level and lack robustness against noisy or irrelevant information. To address these limitations, we propose GAID: a novel Gated Audio-Visual Integration with Directional Perturbation framework for text-video retrieval. GAID employs a frame-level gating mechanism to dynamically weigh the contribution of audio and visual features, allowing for adaptive fusion based on the relevance of each modality to the textual query. Furthermore, we introduce a directional perturbation strategy during training, which encourages the model to learn more robust representations by explicitly exposing it to challenging examples generated by perturbing the input features along directions that maximize the retrieval loss. Extensive experiments on the widely used MSR-VTT and MSVD datasets demonstrate that GAID achieves state-of-the-art performance, outperforming existing methods by a significant margin. This highlights the effectiveness of our proposed frame-level gated integration and directional perturbation strategy in learning robust and discriminative audio-visual representations for text-video retrieval."
http://arxiv.org/abs/2508.01693v1,SURE-Med: Systematic Uncertainty Reduction for Enhanced Reliability in Medical Report Generation,"Medical report generation (MRG) aims to automatically produce descriptive reports from medical images, assisting radiologists in diagnosis and improving workflow efficiency. However, current MRG models often struggle with generating reliable reports due to inherent uncertainties arising from ambiguous image features, noisy labels, and limitations in model capacity. To address this, we propose SURE-Med, a novel framework for Systematic Uncertainty Reduction in medical report generation. SURE-Med incorporates a three-pronged approach: (1) Uncertainty-Aware Feature Encoding, utilizing variational autoencoders to capture and mitigate image feature ambiguity; (2) Evidence-Based Report Decoding, which leverages attention mechanisms and evidence regularization to promote factual correctness and reduce hallucination; and (3) Confidence-Calibrated Training, employing label smoothing and focal loss to improve the model's confidence in its predictions. Experiments on benchmark datasets demonstrate that SURE-Med significantly improves the accuracy, fluency, and clinical relevance of generated reports compared to state-of-the-art methods, achieving improvements of up to 5% in BLEU scores and substantial gains in clinical metric scores. SURE-Med offers a promising solution for generating more reliable and trustworthy medical reports, paving the way for safer and more effective clinical applications of AI in healthcare."
http://arxiv.org/abs/2508.01608v1,From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models,"Image geolocalization, the task of predicting the geographic location of an image, is a critical capability for various applications, including disaster response and environmental monitoring. Despite recent advancements in Large Language Models (LLMs), their ability to accurately geolocalize images remains largely unexplored and lacks standardized evaluation. This paper introduces a novel benchmark, ""Pixels to Places"" (P2P), a systematic and comprehensive dataset designed to evaluate the image geolocalization abilities of LLMs. P2P comprises diverse images spanning various geographical regions, accompanied by precise location coordinates and contextual information. We propose a multi-stage evaluation protocol incorporating both zero-shot and fine-tuned approaches, leveraging LLMs' inherent knowledge and adapting them to the geolocalization task. Our experiments reveal that while LLMs demonstrate a basic understanding of geographical concepts, their raw performance on P2P is limited, highlighting the need for specialized fine-tuning. Fine-tuning, however, significantly improves performance, achieving state-of-the-art results compared to existing methods, demonstrating the potential of LLMs for image geolocalization. This benchmark provides a valuable resource for the community and facilitates the development of more robust and accurate geolocalization systems powered by LLMs."
http://arxiv.org/abs/2508.03186v1,Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling,"Monocular depth estimation, inferring scene depth from a single image, is a fundamental problem in computer vision with applications in robotics and autonomous driving. Current deep learning-based methods often struggle with fine-grained depth prediction and maintaining global scene consistency. This paper introduces a novel approach for monocular depth estimation that combines global scene understanding with detailed local context modeling. Our method employs a global-aware discretization strategy that adaptively partitions the depth range based on global scene features, allowing for finer resolution in frequently occurring depths. Furthermore, we introduce a local context modeling module that leverages attention mechanisms to aggregate multi-scale contextual information, enhancing the representation of local details and boundary regions. Experimental results on the widely used KITTI and NYU Depth V2 datasets demonstrate that our approach achieves state-of-the-art performance, outperforming existing methods in both accuracy and visual quality. The proposed method offers a robust and effective solution for monocular depth estimation, improving both local detail and global consistency."
http://arxiv.org/abs/2508.02152v1,Efficient Chambolle-Pock based algorithms for Convoltional sparse representation,"Convolutional sparse representation (CSR) has emerged as a powerful tool for image restoration and analysis, offering advantages in handling shift-variant features compared to traditional sparse coding. However, solving the associated optimization problems, particularly for large-scale data, remains computationally challenging. This paper addresses the computational bottleneck in CSR by developing efficient Chambolle-Pock (CP) based algorithms tailored for the convolutional structure. We propose two novel CP-based algorithms: a primal-dual algorithm that exploits the convolution theorem for fast Fourier transform (FFT) based updates, and an accelerated algorithm that incorporates a variable splitting technique and adaptive step sizes to further improve convergence. Experiments on image denoising and inpainting demonstrate that our algorithms achieve significant speedups compared to existing methods, while maintaining comparable or superior reconstruction quality. The proposed efficient CP-based algorithms unlock the potential of CSR for large-scale image processing tasks and facilitate its wider adoption in various computer vision applications."
http://arxiv.org/abs/2508.00823v1,IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation,"Image-goal navigation (ImgNav) tasks require agents to navigate to a target location specified by a goal image within an unknown environment. Current approaches often struggle with maintaining accurate localization and mapping in long-horizon, visually ambiguous environments, leading to navigation failures. To address this challenge, we introduce IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation. IGL-Nav leverages a novel incremental 3D Gaussian representation of the environment, coupled with a differentiable rendering module, to perform accurate pose estimation and map refinement. The agent maintains a belief over its pose by iteratively updating its Gaussian map based on observed images and action sequences, enabling robust localization even in visually repetitive settings. We evaluate IGL-Nav on the challenging Habitat 2.0 benchmark, demonstrating significant improvements in success rate and SPL compared to state-of-the-art methods. This work highlights the efficacy of incremental 3D Gaussian representations for robust localization in image-goal navigation and opens avenues for further research in embodied AI."
http://arxiv.org/abs/2507.23447v1,Adjustable Spatio-Spectral Hyperspectral Image Compression Network,"Hyperspectral images (HSIs) offer rich spectral information, enabling precise material identification and scene understanding. However, the high dimensionality of HSIs presents significant challenges for storage and transmission. Existing HSI compression methods often lack the flexibility to adapt to varying application-specific requirements, such as prioritizing spatial or spectral fidelity based on downstream tasks. This paper introduces an Adjustable Spatio-Spectral Hyperspectral Image Compression Network (ASHNet), a deep learning-based framework designed for efficient and flexible HSI compression. ASHNet employs a novel attention-guided residual architecture coupled with a learnable spectral basis transformation to effectively capture both spatial and spectral correlations within the HSI data. Crucially, the network incorporates an adjustable loss function that allows users to dynamically balance the trade-off between spatial and spectral reconstruction quality during training, enabling task-specific optimization. Experimental results on benchmark HSI datasets demonstrate that ASHNet achieves competitive compression ratios compared to state-of-the-art methods while providing superior control over the spatio-spectral fidelity trade-off, evidenced by improved performance on simulated downstream tasks with adjusted compression parameters. The proposed framework offers a practical and adaptable solution for HSI compression in diverse applications, facilitating efficient storage and transmission without sacrificing crucial information."
http://arxiv.org/abs/2507.23185v1,Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network,"Removing rain streaks from single images is a challenging task due to the varying shapes, directions, and densities of rain, often leading to blurred or distorted backgrounds in restored images. Existing deep learning methods often struggle to preserve fine details and sharp edges, resulting in visually unsatisfactory results. This paper addresses the problem of effectively removing rain streaks while simultaneously preserving image details, particularly sharp corners and edges, in single images. We propose a novel Residual Channel Attention Block Module (R-CBAM) integrated into a deep convolutional neural network architecture, coupled with a Harris Corner Loss function. The R-CBAM enhances feature extraction by adaptively recalibrating channel-wise feature responses and spatial attention maps, while the Harris Corner Loss explicitly encourages the network to preserve corner structures during the deraining process. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of PSNR and SSIM, while producing visually sharper and more realistic derained images compared to existing methods. The proposed approach offers a significant advancement in single image rain removal, particularly in scenarios where preserving fine details and sharp edges is crucial."
http://arxiv.org/abs/2507.22828v2,CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models,"Vision-Language Models (VLMs) have achieved remarkable success in various cross-modal tasks, but their vulnerability to adversarial attacks remains a significant concern. This paper addresses the problem of feature inversion attacks on VLMs, specifically aiming to reconstruct the input image from the learned cross-modal feature representations. We propose CapRecover, a novel cross-modality feature inversion attack framework that leverages the inherent semantic correspondence between vision and language modalities to guide the reconstruction process. CapRecover employs a multi-stage optimization strategy, initially utilizing the text embedding to generate a semantically aligned image prior, followed by a gradient-based optimization that minimizes the distance between the VLM-extracted feature of the reconstructed image and the target feature. Furthermore, we introduce a regularization term based on the CLIP image encoder to improve the perceptual quality and semantic fidelity of the reconstructed images. Experimental results on various VLM architectures demonstrate that CapRecover effectively recovers recognizable images from the cross-modal feature space, revealing sensitive visual information. This highlights the potential privacy risks associated with sharing intermediate feature representations in VLMs and underscores the need for robust defense mechanisms."
http://arxiv.org/abs/2507.22501v1,DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement,"Underwater image enhancement is a crucial preprocessing step for various marine applications, yet the inherent degradations caused by light absorption and scattering pose significant challenges. Existing methods often struggle with varying degradation types and intensities, leading to over-enhancement or insufficient restoration. To address this, we propose DACA-Net, a Degradation-Aware Conditional Diffusion Network for underwater image enhancement. DACA-Net leverages a conditional diffusion model conditioned on estimated degradation maps, allowing for spatially adaptive and degradation-specific restoration. Specifically, we employ a lightweight underwater image degradation estimation module to predict depth, attenuation, and backscatter maps, which are then incorporated as conditional inputs to guide the diffusion process. Experiments on benchmark underwater image datasets demonstrate that DACA-Net achieves superior performance compared to state-of-the-art methods in terms of both quantitative metrics (PSNR, SSIM, UCIQE) and visual quality. This highlights the effectiveness of our degradation-aware conditional diffusion approach for robust and high-quality underwater image enhancement."
http://arxiv.org/abs/2507.22454v1,TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation,"LiDAR point cloud generation is crucial for various applications, including autonomous driving simulation and 3D scene understanding. Existing LiDAR diffusion models often struggle to generate realistic point clouds that faithfully preserve the underlying topological structure of the scene, leading to artifacts and inconsistencies. To address this, we introduce TopoLiDM, a novel Topology-Aware LiDAR Diffusion Model designed for interpretable and realistic LiDAR point cloud generation. TopoLiDM leverages a simplicial complex-based topological loss function during the reverse diffusion process to explicitly encourage the generation of point clouds with consistent and plausible topological features. Furthermore, we incorporate a learned encoder to map the input noise to a latent space conditioned on semantic information, enabling fine-grained control over the generated point cloud's semantic composition. Experimental results on benchmark datasets demonstrate that TopoLiDM significantly outperforms existing state-of-the-art LiDAR point cloud generation methods in terms of both geometric fidelity and topological consistency, as measured by Frchet Point Cloud Distance and topological persistence metrics. TopoLiDM's ability to generate realistic and topologically sound LiDAR point clouds represents a significant advancement in the field, paving the way for more reliable and interpretable applications."
http://arxiv.org/abs/2507.22321v1,Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment,"Late-life depression (LLD) is a significant public health concern, and structural MRI has emerged as a promising tool for its assessment. However, the variability in MRI acquisition protocols across different datasets presents a significant challenge for building generalizable LLD prediction models. This paper addresses the problem of domain shift between heterogeneous structural MRI datasets when training machine learning models for LLD assessment. We propose a novel collaborative domain adaptation framework that leverages multiple source domains to improve the performance of LLD classification in a target domain. Our method employs adversarial learning to minimize the domain discrepancy between source and target datasets, while simultaneously enforcing collaboration among source domains by aligning their feature representations. Experimental results on a multi-site structural MRI dataset demonstrate that our collaborative domain adaptation approach significantly outperforms single-source domain adaptation methods and traditional machine learning models trained without domain adaptation, achieving state-of-the-art performance in LLD classification. This work provides a valuable approach for developing robust and generalizable LLD prediction models from heterogeneous neuroimaging data, facilitating broader clinical application."
http://arxiv.org/abs/2507.21947v1,Enhancing Generalization in Data-free Quantization via Mixup-class Prompting,"Data-free quantization (DFQ) aims to quantize neural networks without access to the original training data, relying on unlabeled data or synthetic data generation. However, DFQ methods often suffer from significant performance degradation due to the domain gap between the generated data and the real data distribution, hindering generalization to unseen datasets. To address this issue, we propose a novel Mixup-class Prompting (MCP) strategy for DFQ. MCP leverages a pre-trained, quantized vision-language model to generate diverse and informative synthetic samples by prompting with mixed class names. These mixed prompts encourage the model to generate images representing interpolations between classes, enriching the synthetic dataset and covering a wider range of the data manifold. Experiments on various benchmark datasets demonstrate that MCP significantly improves the generalization ability of DFQ, achieving state-of-the-art results and reducing the performance gap compared to quantization-aware training. This work offers a promising direction for enhancing the practicality of DFQ in scenarios where real training data is unavailable."
http://arxiv.org/abs/2507.21573v1,LinDeps: A Fine-tuning Free Post-Pruning Method to Remove Layer-Wise Linear Dependencies with Guaranteed Performance Preservation,"Deep neural networks often exhibit significant redundancy, motivating pruning techniques to reduce computational cost and memory footprint. However, fine-tuning after pruning is typically required to recover accuracy, adding complexity and computational overhead. This paper addresses the problem of removing layer-wise linear dependencies in pre-trained neural networks *without* fine-tuning, ensuring guaranteed performance preservation. We introduce LinDeps, a novel post-pruning method that identifies and removes linearly dependent filters and neurons within each layer by leveraging singular value decomposition and a carefully derived bound on the reconstruction error. LinDeps explicitly maintains the network's output by projecting the subsequent layer's weights onto the null space of the removed components. Empirically, we demonstrate that LinDeps can effectively prune various network architectures, including ResNets and Transformers, achieving significant parameter reduction (up to 40%) with provable preservation of the original network's performance on standard image classification datasets like CIFAR-10 and ImageNet. LinDeps offers a simple and efficient solution for deploying compact and performant neural networks, eliminating the need for computationally expensive fine-tuning."
http://arxiv.org/abs/2507.21261v1,HDR Environment Map Estimation with Latent Diffusion Models,"High Dynamic Range (HDR) environment maps are crucial for realistic image-based rendering, providing global illumination cues for scene lighting and reflections. Estimating accurate HDR environment maps from Low Dynamic Range (LDR) images remains a challenging inverse problem, often requiring specialized hardware or complex multi-exposure setups. This paper introduces a novel approach to HDR environment map estimation leveraging the generative power of Latent Diffusion Models (LDMs). Our method, HDR-LDM, learns a prior distribution over HDR environment maps within the latent space of a pre-trained LDM. We then condition the denoising process of the LDM on a single LDR input image, guiding the generation towards an HDR environment map that is both realistic and consistent with the observed scene. Experimental results on synthetic and real-world datasets demonstrate that HDR-LDM significantly outperforms existing state-of-the-art techniques, producing visually compelling HDR environment maps with improved accuracy and reduced artifacts. This offers a practical and accessible solution for HDR environment map acquisition, enabling high-quality rendering with minimal input requirements."
http://arxiv.org/abs/2507.20996v1,Improving Adversarial Robustness Through Adaptive Learning-Driven Multi-Teacher Knowledge Distillation,"Adversarial attacks pose a significant threat to the reliability of deep learning models in safety-critical applications. Knowledge distillation (KD) has emerged as a promising technique to improve adversarial robustness, yet conventional KD often struggles to effectively transfer robustness knowledge from pre-trained robust teacher models, particularly when teacher models exhibit varying strengths across different attack types. This paper addresses the problem of effectively leveraging multiple teacher models with diverse robustness profiles to enhance the adversarial robustness of a student model. We propose an Adaptive Learning-Driven Multi-Teacher Knowledge Distillation (ALMT-KD) framework. ALMT-KD dynamically adjusts the contribution of each teacher model based on the student's learning progress and the specific adversarial attack being considered. This adaptation is achieved through a novel weighting scheme that considers both the teachers' individual performance against different attacks and the student's current vulnerability. Experiments on benchmark datasets like CIFAR-10 and CIFAR-100 demonstrate that ALMT-KD significantly improves the adversarial robustness of the student model against various white-box and black-box attacks compared to state-of-the-art KD methods, while maintaining competitive clean accuracy. The proposed framework offers a more effective and adaptable approach to knowledge distillation for robust model training."
http://arxiv.org/abs/2507.20934v1,Exploring text-to-image generation for historical document image retrieval,"Historical document image retrieval (HDR) presents a unique challenge due to degradation, diverse layouts, and variations in writing styles across time periods. Existing HDR methods often rely on optical character recognition (OCR) or handcrafted features, which are susceptible to errors and lack semantic understanding. This paper explores the feasibility of using text-to-image generation models to bridge the gap between textual queries and visual document representations for improved HDR. We propose a novel approach that leverages pre-trained text-to-image models to synthesize visual representations of textual queries, enabling direct comparison with document images using image similarity metrics. Specifically, we fine-tune a Stable Diffusion model to generate document-like images from textual queries, and subsequently employ a pre-trained CLIP model to embed both generated and real document images into a shared visual space. Our experiments on benchmark HDR datasets demonstrate that this approach significantly improves retrieval accuracy compared to traditional OCR-based methods, particularly for degraded documents and queries containing historical terms. This work provides a promising direction for leveraging recent advances in generative models for more robust and semantically aware historical document image retrieval."
http://arxiv.org/abs/2507.20629v1,DAMS:Dual-Branch Adaptive Multiscale Spatiotemporal Framework for Video Anomaly Detection,"Video anomaly detection is crucial for various applications, ranging from surveillance to autonomous driving, yet remains challenging due to the inherent complexity and variability of anomalous events. Current methods often struggle with effectively capturing both fine-grained spatial anomalies and long-range temporal dependencies at multiple scales. To address this, we introduce DAMS: a Dual-Branch Adaptive Multiscale Spatiotemporal framework for video anomaly detection. DAMS employs a dual-branch architecture comprising a spatial branch utilizing a novel Adaptive Convolutional Module (ACM) to dynamically adjust receptive fields based on local image characteristics, and a temporal branch leveraging a Multiscale Temporal Transformer (MTT) to capture long-range dependencies across different temporal scales. Furthermore, we introduce an adaptive fusion mechanism to effectively integrate information from both branches, allowing the model to focus on the most relevant features for anomaly detection. Experiments on benchmark datasets, including ShanghaiTech, UCF-Crime, and CUHK Avenue, demonstrate that DAMS achieves state-of-the-art performance, surpassing existing methods by a significant margin in terms of both frame-level and pixel-level anomaly detection accuracy. DAMS offers a robust and effective solution for video anomaly detection, paving the way for more reliable and intelligent video surveillance systems."
http://arxiv.org/abs/2507.20582v1,M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast,"Accurate segmentation of brain tumors in Magnetic Resonance Imaging (MRI) is crucial for diagnosis, treatment planning, and monitoring disease progression. However, the high variability in tumor shape, size, and location, coupled with the inherent noise and artifacts in MRI data, pose significant challenges for automated segmentation. We address these challenges by introducing M-Net, a novel deep learning framework for sequential brain tumor segmentation based on a Mesh-Cast approach. M-Net leverages a 3D encoder-decoder architecture to initially generate a coarse segmentation map. Subsequently, a novel ""Mesh-Cast"" module refines this segmentation by projecting the initial prediction onto a deformable mesh, enabling adaptive boundary refinement and improved handling of complex tumor morphologies. Extensive experiments on the BraTS 2021 dataset demonstrate that M-Net achieves state-of-the-art performance, surpassing existing methods in terms of Dice score, Hausdorff distance, and segmentation accuracy, particularly for enhancing tumor regions. M-Net's improved segmentation accuracy holds significant promise for enhancing clinical decision-making and improving patient outcomes in brain tumor management."
http://arxiv.org/abs/2507.20480v1,Automated 3D-GS Registration and Fusion via Skeleton Alignment and Gaussian-Adaptive Features,"3D Gaussian Splatting (3D-GS) has emerged as a powerful technique for novel view synthesis, but registering and fusing multiple 3D-GS models from different viewpoints remains a challenge. This paper addresses the problem of automatically aligning and fusing multiple 3D-GS models without relying on external pose information or extensive manual intervention. We propose a novel method that leverages skeleton extraction and alignment to provide a coarse registration, followed by a Gaussian-adaptive feature extraction and matching scheme for fine-grained alignment and fusion. Our Gaussian-adaptive features are designed to be robust to variations in Gaussian density and shape, enabling accurate correspondence estimation even in regions with significant viewpoint differences. Experimental results on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art registration accuracy and fusion quality compared to existing approaches, producing visually compelling and geometrically accurate 3D-GS models. This automated registration and fusion pipeline significantly enhances the applicability of 3D-GS for large-scale scene reconstruction and rendering."
http://arxiv.org/abs/2507.20099v1,Hybrid-Domain Synergistic Transformer for Hyperspectral Image Denoising,"Hyperspectral images (HSIs) are susceptible to various types of noise during acquisition, which significantly degrades subsequent analysis. Existing HSI denoising methods often operate solely in either the spectral or spatial domain, failing to fully exploit the inherent synergistic relationship between them. This paper proposes a novel Hybrid-Domain Synergistic Transformer (HDST) for HSI denoising. HDST leverages a transformer-based architecture to capture long-range dependencies and global contextual information in both spectral and spatial domains. Crucially, we introduce a synergistic fusion module that adaptively integrates the denoised spectral and spatial features, promoting mutual enhancement and information exchange. Experimental results on several benchmark datasets demonstrate that HDST consistently outperforms state-of-the-art methods in terms of quantitative metrics (PSNR, SSIM, SAM) and visual quality, particularly in challenging scenarios with mixed noise. The proposed HDST offers a powerful and effective framework for HSI denoising, paving the way for improved performance in downstream HSI analysis tasks."
http://arxiv.org/abs/2507.19948v1,UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with Convolution-Compensated ViT Dual SA Block,"Monocular depth estimation is a fundamental task in computer vision, crucial for scene understanding and robotic navigation, yet remains challenging due to the inherent ambiguity of recovering 3D information from a single 2D image. Existing methods often struggle in low-light conditions or with fast-moving objects, where image quality degrades significantly. To address these limitations, we propose UniCT Depth, a novel event-image fusion framework that leverages the complementary strengths of event cameras and traditional cameras for robust monocular depth estimation. Our architecture incorporates a Convolution-Compensated Vision Transformer (ViT) Dual Self-Attention (SA) Block, which effectively fuses image features with event-based motion cues, compensating for convolutional biases through learnable offsets. We further introduce a unified training strategy that handles both synthetic and real-world datasets. Experimental results on benchmark datasets demonstrate that UniCT Depth achieves state-of-the-art performance, significantly outperforming existing monocular depth estimation methods, particularly in challenging scenarios with rapid motion or poor illumination. This robust and accurate depth estimation pipeline has the potential to significantly advance applications in autonomous driving, robotics, and augmented reality."
http://arxiv.org/abs/2507.19459v1,Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization,"Accurate 3D models of non-cooperative spacecraft are crucial for autonomous navigation, proximity operations, and debris removal. However, acquiring these models in-situ is challenging due to limited computational resources and the need for rapid deployment. This paper addresses the problem of fast and accurate 3D model reconstruction of uncooperative spacecraft using limited observations. We propose a novel approach that initializes the 3D model with a set of primitive shapes, such as cylinders, spheres, and cuboids, automatically fitted to initial visual data. These primitives provide a strong prior for subsequent refinement using a neural implicit surface representation, enabling efficient learning from sparse and noisy point clouds. Our experiments on both synthetic and real-world datasets demonstrate that our method achieves significantly faster convergence and higher accuracy compared to state-of-the-art methods that learn implicit surfaces from scratch. This rapid and accurate 3D modeling capability is essential for enabling autonomous spacecraft operations in unstructured environments."
http://arxiv.org/abs/2507.19565v1,Review of Deep Learning Applications to Structural Proteomics Enabled by Cryogenic Electron Microscopy and Tomography,"Cryogenic electron microscopy (cryo-EM) and cryogenic electron tomography (cryo-ET) have revolutionized structural biology, enabling the determination of macromolecular structures at near-atomic resolution. However, analyzing the vast and complex datasets generated by these techniques presents significant computational challenges, particularly in image processing, particle picking, and structure refinement. This review examines the recent surge in the application of deep learning methodologies to address these challenges in structural proteomics pipelines reliant on cryo-EM and cryo-ET. We categorize deep learning approaches based on their application within the structural determination workflow, focusing on advancements in automated particle picking using convolutional neural networks, improved signal extraction via denoising autoencoders, and enhanced structure refinement through generative adversarial networks. The review highlights the performance gains achieved by deep learning models compared to traditional algorithms, specifically in terms of accuracy, speed, and the ability to handle low signal-to-noise ratio data. The integration of deep learning techniques has significantly accelerated and improved the accuracy of structure determination in cryo-EM and cryo-ET, paving the way for high-throughput structural proteomics and a deeper understanding of cellular machinery."
http://arxiv.org/abs/2507.19296v1,ABCD: Automatic Blood Cell Detection via Attention-Guided Improved YOLOX,"Microscopic blood cell analysis is crucial for diagnosing various hematological diseases. However, manual blood cell counting is time-consuming, labor-intensive, and prone to human error. This paper addresses the challenge of automating and improving the accuracy of blood cell detection in microscopic images. We propose ABCD: Automatic Blood Cell Detection via Attention-Guided Improved YOLOX, a novel framework that enhances the YOLOX object detection architecture with several key improvements. First, we integrate a spatial and channel attention mechanism to focus on relevant cell features and suppress background noise. Second, we introduce a novel loss function that balances precision and recall, particularly for small and overlapping cells. Finally, we employ data augmentation techniques specifically designed to address the challenges of blood cell image variations. Experimental results on a publicly available blood cell dataset demonstrate that ABCD achieves a significant improvement in mean Average Precision (mAP) compared to the original YOLOX and other state-of-the-art object detection models, while maintaining real-time performance. Our proposed method offers a robust and efficient solution for automatic blood cell detection, potentially leading to faster and more accurate disease diagnosis in clinical settings."
http://arxiv.org/abs/2507.19282v1,SAM2-Aug: Prior knowledge-based Augmentation for Target Volume Auto-Segmentation in Adaptive Radiation Therapy Using Segment Anything Model 2,"Accurate and efficient target volume segmentation is crucial in adaptive radiation therapy (ART) for personalized cancer treatment. Manual segmentation is time-consuming and subject to inter-observer variability, highlighting the need for automated solutions. This paper addresses the challenge of improving auto-segmentation performance, particularly for deformable organs, by leveraging prior knowledge within a data augmentation framework. We introduce SAM2-Aug, a novel augmentation strategy that utilizes Segment Anything Model 2 (SAM2) to generate realistic anatomical variations. SAM2 is prompted with existing segmentations to create perturbed masks, which are then incorporated into the training data along with corresponding image deformations. Experiments on a clinical dataset of prostate cancer patients undergoing ART demonstrate that SAM2-Aug significantly enhances the Dice Similarity Coefficient (DSC) of auto-segmented target volumes by an average of 3.2% compared to standard augmentation techniques, and improves robustness against anatomical changes observed during treatment. This method offers a practical and effective approach to improve the accuracy and reliability of auto-segmentation in ART, facilitating more precise and personalized cancer treatment planning."
http://arxiv.org/abs/2507.18815v1,Deepfake Detection Via Facial Feature Extraction and Modeling,"Deepfake technology, which allows for the creation of highly realistic manipulated videos, poses a significant threat to information integrity and public trust. Detecting these fabricated videos remains a challenging problem due to the increasing sophistication of deepfake generation techniques. This paper introduces a novel deepfake detection method based on facial feature extraction and modeling using a hybrid deep learning architecture. Specifically, we employ a pre-trained convolutional neural network to extract robust facial features, followed by a recurrent neural network to model the temporal dynamics and subtle inconsistencies within the extracted feature sequences. An attention mechanism is integrated to focus on the most discriminative features for improved detection accuracy. Our experimental results on benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance, outperforming existing methods in both detection accuracy and generalization ability. This research provides a significant advancement in deepfake detection, contributing to the development of more reliable and robust methods for combating malicious manipulation of visual content."
http://arxiv.org/abs/2507.18354v2,Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation,"Fundus vessel segmentation is crucial for diagnosing various ocular and systemic diseases. While convolutional neural networks have shown promise, their inherent limitations in modeling geometric variations of retinal vessels hinder accurate segmentation. We address this challenge by proposing a novel Deformable Convolution Module with Globally Learned Relative Offsets (DGM-GLRO) for robust fundus vessel segmentation. DGM-GLRO learns relative offsets for deformable convolution kernels by considering the global context of the entire feature map, enabling the network to effectively capture the complex and varied vessel structures. Specifically, we employ a global attention mechanism to aggregate feature information across the entire image, which is then used to predict relative offsets for each deformable convolution operation. Experimental results on benchmark datasets, including DRIVE, STARE, and CHASE_DB1, demonstrate that our DGM-GLRO module consistently outperforms state-of-the-art methods, achieving significant improvements in segmentation accuracy and robustness. This globally informed deformable convolution strategy offers a powerful approach to improve the performance of CNNs in medical image segmentation tasks involving complex and deformable structures."
http://arxiv.org/abs/2507.18323v2,SemiSegECG: A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation,"Electrocardiogram (ECG) delineation, the process of identifying fiducial points representing the boundaries of ECG waveforms, is crucial for automated cardiac diagnosis. However, the scarcity of meticulously annotated ECG datasets poses a significant bottleneck for training robust deep learning models for this task. This paper introduces SemiSegECG, a novel multi-dataset benchmark designed to facilitate research in semi-supervised semantic segmentation for ECG delineation. SemiSegECG comprises five publicly available ECG datasets with pixel-level annotations, standardized data preprocessing, and evaluation metrics. We further propose a novel semi-supervised learning framework, Contrastive Boundary Alignment (CBA), which leverages both labeled and unlabeled data by enforcing consistency between predictions and boundary-aware contrastive representations. Experimental results demonstrate that CBA outperforms existing state-of-the-art semi-supervised segmentation methods on SemiSegECG, achieving a Dice score improvement of up to 5% when using only 10% labeled data. SemiSegECG provides a valuable resource for advancing semi-supervised learning techniques in ECG analysis, paving the way for more accurate and efficient automated cardiac diagnosis systems."
http://arxiv.org/abs/2507.18214v1,LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation,"Medical image segmentation is crucial for accurate diagnosis and treatment planning. However, achieving precise segmentation often requires capturing fine-grained details and maintaining alignment between image features and segmentation masks, particularly challenging in the presence of anatomical variability and noise. This paper addresses the problem of generating high-quality, aligned segmentation masks by leveraging latent diffusion models with an efficient encoder distillation strategy. We introduce LEAF, a Latent diffusion model with Efficient encoder distillation for Aligned Features. LEAF utilizes a pre-trained encoder to extract rich feature representations from the input image, which are then distilled into the latent space of a diffusion model through a novel feature alignment loss. This loss encourages the diffusion model to generate latent representations that are highly correlated with the encoder's features, leading to improved alignment between the generated segmentation masks and the input image. Experimental results on multiple medical imaging datasets demonstrate that LEAF significantly outperforms state-of-the-art segmentation methods, achieving higher Dice scores and improved boundary delineation, especially on challenging anatomical structures. LEAF offers a promising approach for generating accurate and reliable medical image segmentations, paving the way for improved clinical decision-making."
http://arxiv.org/abs/2507.18174v1,Real-Time Object Detection and Classification using YOLO for Edge FPGAs,"Object detection and classification are critical components in numerous applications, ranging from autonomous vehicles to industrial automation. However, deploying computationally intensive deep learning models like YOLO on resource-constrained edge devices, such as Field-Programmable Gate Arrays (FPGAs), remains a significant challenge. This paper addresses the problem of achieving real-time object detection and classification performance using YOLO on edge FPGA platforms. We propose a hardware-accelerated implementation of YOLOv3-Tiny, optimized for efficient execution on a Xilinx ZCU104 FPGA. Our approach employs a combination of techniques, including model quantization, layer fusion, and custom hardware accelerators tailored for the convolutional and pooling operations within the YOLO architecture. Experimental results demonstrate that our FPGA implementation achieves a processing speed of 30 frames per second (FPS) with a mean Average Precision (mAP) of 65% on the PASCAL VOC dataset, representing a significant improvement in performance compared to CPU-based implementations. This work demonstrates the feasibility of deploying accurate and real-time object detection systems on edge devices, enabling a wide range of embedded vision applications."
http://arxiv.org/abs/2507.18112v1,Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks,"Denoising diffusion probabilistic models (DDPMs) have shown promise in generating high-quality 3D medical images, particularly from MRI scans. However, training these models from scratch or fine-tuning large pre-trained 3D DDPMs is computationally expensive, limiting their applicability in resource-constrained settings and for domain adaptation. We address this challenge by proposing a parameter-efficient fine-tuning approach for 3D DDPMs using tensor networks (TNs). Our method decomposes the weight updates of the DDPM's U-Net architecture into a low-rank tensor network representation, significantly reducing the number of trainable parameters while preserving the model's capacity. We fine-tune a pre-trained 3D DDPM on a new MRI dataset using our TN-based approach and demonstrate substantial parameter reduction (up to 95%) compared to full fine-tuning, with only a marginal decrease in image quality as measured by Frchet Inception Distance (FID) and Structural Similarity Index Measure (SSIM). This enables efficient adaptation of pre-trained 3D DDPMs to new MRI modalities and datasets, facilitating broader adoption of generative models in medical imaging research and clinical applications."
http://arxiv.org/abs/2507.17508v1,Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation,"X-ray imaging is a crucial technology for security screening in various domains, including airports and customs, aiming to prevent the transportation of illicit objects. Detecting these objects accurately and efficiently remains a challenging task due to factors like image complexity, overlapping objects, and the evolving nature of concealed threats. This paper addresses the problem of effectively detecting illicit objects, specifically firearms, knives, and explosives, within X-ray images using deep learning techniques. We comparatively evaluate the performance of several state-of-the-art object detection models, including Faster R-CNN, YOLOv5, and DETR, fine-tuned on a large-scale X-ray image dataset augmented with advanced techniques like composite image generation and adversarial training. Our experimental results demonstrate that YOLOv5 achieves the best balance between detection accuracy and inference speed, with a mean Average Precision (mAP) of 0.85 and an inference time of 15ms per image on a standard GPU. Furthermore, we analyze the impact of different data augmentation strategies on model robustness and generalization. This research contributes to the development of more reliable and efficient automated threat detection systems, enhancing security and reducing the workload of human operators in X-ray screening environments."
http://arxiv.org/abs/2507.17347v3,Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation,"Food image segmentation is crucial for automated dietary assessment and precise food recognition, yet training deep learning models for this task demands extensive labeled data. This paper addresses the challenge of adapting large, pre-trained vision models to food image segmentation with limited data by introducing Swin-TUNA, a novel Parameter-Efficient Fine-Tuning (PEFT) approach. Swin-TUNA leverages a Swin Transformer backbone and strategically inserts learnable ""Tuning Units for enhanced Accuracy"" (TUNA) within the transformer blocks. These TUNA modules, composed of lightweight convolutional layers and attention mechanisms, are optimized while freezing the majority of the pre-trained parameters. We evaluate Swin-TUNA on the challenging FoodSeg103 dataset and demonstrate that it achieves state-of-the-art segmentation accuracy compared to existing PEFT techniques, surpassing full fine-tuning with significantly fewer trainable parameters. Our approach enables efficient and effective adaptation of powerful vision transformers for food image segmentation, contributing to improved dietary monitoring and automated food analysis systems."
http://arxiv.org/abs/2507.17185v1,Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification,"Asymmetric lesion detection in medical imaging is crucial for early diagnosis of various diseases, yet its inherent ambiguity and subtle variations pose significant challenges. This paper addresses the problem of accurately identifying asymmetric lesions, particularly when distinguishing them from normal anatomical variations. Our proposed method leverages geometric pattern analysis to extract asymmetry features, which are then combined with features learned by a Convolutional Neural Network (CNN) to train a Support Vector Machine (SVM) classifier. Specifically, we employ radial distance and angular displacement measurements relative to anatomical landmarks to quantify asymmetry, and integrate these geometric features with high-level image representations extracted from a pre-trained CNN. Experimental results on a dataset of dermatoscopic images demonstrate that our hybrid approach achieves a significant improvement in lesion detection accuracy (AUC = 0.92), surpassing state-of-the-art methods that rely solely on CNNs or handcrafted features. This novel combination of geometric reasoning and deep learning provides a robust and interpretable framework for asymmetric lesion detection, potentially improving diagnostic accuracy and clinical decision-making."
http://arxiv.org/abs/2507.17176v1,Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network,"Printed Circuit Board (PCB) defect detection is crucial for ensuring the quality and reliability of electronic products. However, the increasing complexity and miniaturization of PCBs pose significant challenges for accurate and efficient defect detection, particularly with varying defect sizes and limited computational resources. This paper addresses the problem of detecting defects on PCBs with high accuracy while maintaining real-time performance on resource-constrained devices. We propose a multi-scale PCB defect detection method based on the YOLOv8 network, enhanced through pruning and the incorporation of a lightweight network. Specifically, we employ a channel pruning technique to reduce the model's complexity and computational cost, followed by replacing the original backbone with MobileNetV3 to further optimize the network's efficiency. Experimental results on a benchmark PCB defect dataset demonstrate that our proposed method achieves a significant reduction in model size and inference time, while maintaining competitive defect detection accuracy compared to the original YOLOv8 model. This lightweight and accurate defect detection approach offers a practical solution for real-time PCB quality control in industrial settings, facilitating efficient and cost-effective manufacturing processes."
http://arxiv.org/abs/2507.17121v2,Addressing High Class Imbalance in Multi-Class Diabetic Retinopathy Severity Grading with Augmentation and Transfer Learning,"Diabetic Retinopathy (DR) is a leading cause of blindness, and automated severity grading using fundus images is crucial for early detection and treatment. However, DR datasets often exhibit significant class imbalance, with mild and moderate stages being under-represented compared to healthy and proliferative stages, hindering the performance of deep learning models. This paper addresses the challenge of high class imbalance in multi-class DR severity grading using a novel combination of augmentation techniques and transfer learning. We propose a two-stage approach: first, we employ a mix of geometric and intensity-based augmentations, along with a novel severity-aware augmentation strategy that synthesizes images based on disease progression, to balance the training data. Second, we leverage transfer learning by fine-tuning a pre-trained EfficientNet model, initialized with weights from ImageNet and further pre-trained on a large external DR dataset, on our augmented dataset. Our experiments on the EyePACS dataset demonstrate that the proposed method significantly improves performance, achieving a quadratic weighted Kappa score of 0.85, a substantial improvement compared to baseline models trained without augmentation and transfer learning. This approach provides a robust and effective solution for DR severity grading in the presence of severe class imbalance, facilitating improved diagnostic accuracy and earlier intervention."
http://arxiv.org/abs/2507.17089v1,IONext: Unlocking the Next Era of Inertial Odometry,"Inertial Odometry (IO) provides robust and efficient ego-motion estimation by leveraging inertial measurement units (IMUs). However, traditional IO methods often struggle in environments with limited visual features or under significant dynamic motions, leading to drift accumulation and inaccurate pose estimates. This paper introduces IONext, a novel Inertial Odometry framework that leverages a tightly-coupled, learning-based approach to enhance robustness and accuracy. IONext employs a transformer-based architecture to dynamically fuse inertial measurements with learned motion priors and visual cues, enabling robust state estimation even in challenging scenarios. Specifically, the transformer attends to past inertial data, predicting future motion trends and recalibrating the system's sensitivity to noisy sensor inputs. Experimental results on both synthetic and real-world datasets demonstrate that IONext significantly outperforms state-of-the-art IO algorithms, achieving up to a 40% reduction in absolute trajectory error, particularly in environments with rapid accelerations and limited visual information. IONext represents a significant advancement in inertial odometry, paving the way for more reliable and accurate robot navigation in complex environments."
http://arxiv.org/abs/2507.17038v1,Transformer Based Building Boundary Reconstruction using Attraction Field Maps,"Building boundary reconstruction from aerial imagery is crucial for various applications, including urban planning, disaster management, and map generation. However, accurately delineating building boundaries, especially in densely populated areas with complex roof structures and occlusions, remains a challenging problem. This paper addresses the limitations of existing methods by proposing a novel transformer-based approach for building boundary reconstruction utilizing attraction field maps. Our method first predicts an attraction field map, representing the direction each pixel should move to reach the nearest building boundary. This map is then fed into a transformer network, which leverages global context to refine and vectorize the building boundaries. We introduce a novel loss function combining attraction field consistency and boundary regularity to optimize the network. Experimental results on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods in terms of boundary accuracy and completeness, particularly in challenging scenarios with complex building geometries. This improved reconstruction accuracy has the potential to enhance the performance of downstream applications relying on accurate building footprint data."
http://arxiv.org/abs/2507.16955v1,"A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust Diagnosis with Attention-Based Fusion","Mammography is the primary screening modality for early breast cancer detection, often involving multiple views and diagnostic tasks. Existing methods often struggle to effectively integrate information across different views and tasks, hindering diagnostic accuracy and robustness. This paper introduces a novel hybrid Convolutional Neural Network (CNN) and View-Specific Subspace Model (VSSM) architecture for multi-view, multi-task mammography analysis. The proposed model leverages CNNs for feature extraction from individual mammogram views, followed by VSSM to learn view-specific representations and capture correlations between views. An attention mechanism is incorporated to dynamically weight the contribution of each view during feature fusion, further enhancing the model's ability to focus on relevant information. Experimental results on a large-scale mammography dataset demonstrate that our approach significantly outperforms state-of-the-art methods in both breast cancer classification and lesion detection, achieving higher AUC and sensitivity while maintaining competitive specificity. This hybrid CNN-VSSM model offers a robust and effective framework for multi-view, multi-task mammography analysis, potentially improving the accuracy and efficiency of breast cancer screening."
http://arxiv.org/abs/2507.18650v1,Features extraction for image identification using computer vision,"Image identification, a fundamental task in computer vision, aims to accurately classify or retrieve images based on their visual content. A critical step in this process is feature extraction, which transforms raw pixel data into a compact and informative representation. This paper addresses the challenge of selecting and combining feature extraction techniques to achieve robust and accurate image identification across diverse image datasets. We propose a novel hybrid feature extraction framework that integrates both handcrafted features, specifically Histogram of Oriented Gradients (HOG) and Color Histograms, with features learned through a pre-trained Convolutional Neural Network (CNN), ResNet50. A feature selection module, employing a variance thresholding method, is then applied to reduce dimensionality and improve feature discriminability before classification using a Support Vector Machine (SVM). Experimental results on benchmark datasets, including CIFAR-10 and Caltech-101, demonstrate that our hybrid approach achieves significantly higher identification accuracy compared to using individual feature extraction methods or other state-of-the-art techniques, with improvements exceeding 5% in certain scenarios. This work highlights the benefit of combining complementary feature representations and provides a practical framework for enhancing image identification performance in various applications."
http://arxiv.org/abs/2507.16267v2,SFNet: A Spatial-Frequency Domain Deep Learning Network for Efficient Alzheimer's Disease Diagnosis,"Alzheimer's Disease (AD) is a progressive neurodegenerative disorder and early diagnosis is crucial for effective intervention. Existing deep learning methods for AD diagnosis from Magnetic Resonance Imaging (MRI) often suffer from high computational costs and limited feature representation capabilities. This paper addresses the challenge of efficiently and accurately diagnosing AD by leveraging both spatial and frequency domain information from MRI data. We propose SFNet, a novel Spatial-Frequency domain deep learning network, which integrates a spatial feature extraction module based on 3D Convolutional Neural Networks (CNNs) with a frequency domain feature learning module employing Discrete Cosine Transform (DCT). The spatial module captures local anatomical patterns, while the frequency module extracts global structural information, enabling a more comprehensive representation of AD-related changes. Experimental results on the ADNI dataset demonstrate that SFNet achieves state-of-the-art performance with an accuracy of 94.2% and an AUC of 0.97 in differentiating AD patients from healthy controls, while also exhibiting a significant reduction in computational complexity compared to existing 3D CNN-based approaches. SFNet provides a computationally efficient and highly accurate framework for AD diagnosis, facilitating early detection and improved patient care."
http://arxiv.org/abs/2507.16172v1,AtrousMamaba: An Atrous-Window Scanning Visual State Space Model for Remote Sensing Change Detection,"Remote sensing change detection is crucial for monitoring environmental dynamics and urban development, often relying on analyzing bi-temporal satellite imagery. However, effectively capturing long-range dependencies and subtle changes in high-resolution remote sensing data remains a significant challenge for existing convolutional and transformer-based approaches due to their computational complexity and limited receptive fields. To address this, we introduce AtrousMamaba, an Atrous-Window Scanning Visual State Space Model specifically designed for remote sensing change detection. AtrousMamaba leverages the efficient selective scan mechanism of Mamba, enhanced with atrous convolutions to expand the receptive field while maintaining computational efficiency. This allows the model to capture contextual information at multiple scales, effectively detecting both large-scale and subtle changes. Experimental results on benchmark datasets demonstrate that AtrousMamaba achieves state-of-the-art performance, surpassing existing CNN and transformer-based methods in terms of accuracy and efficiency. This novel approach offers a promising solution for efficient and accurate change detection in large-scale remote sensing applications."
http://arxiv.org/abs/2507.16114v1,Stop-band Energy Constraint for Orthogonal Tunable Wavelet Units in Convolutional Neural Networks for Computer Vision problems,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, but often lack explicit mechanisms for frequency-selective feature extraction. While learnable wavelet transforms offer a promising avenue for integrating multi-resolution analysis into CNNs, existing approaches struggle to effectively control the frequency response of the learned wavelet filters, leading to suboptimal performance. This paper introduces a novel stop-band energy constraint for orthogonal tunable wavelet units (OTWUs) embedded within CNNs. Our approach minimizes the energy within a specified stop-band of the learned wavelet filters frequency response, thereby encouraging the learning of more distinct and band-limited features. We enforce this constraint during training using a differentiable loss term calculated directly from the learned filter coefficients. Experimental results on image classification and semantic segmentation benchmarks demonstrate that CNNs equipped with OTWUs and our stop-band energy constraint achieve significant improvements in accuracy and robustness compared to standard CNNs and those using unconstrained OTWUs. This work offers a principled method for enhancing the frequency selectivity of wavelet-based CNNs, paving the way for more efficient and interpretable feature learning in computer vision."
http://arxiv.org/abs/2507.15444v1,Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe,"Autonomous navigation in confined spaces presents significant challenges due to limited sensor visibility and the need for rapid responses to dynamic environments. Traditional frame-based cameras struggle in such scenarios due to their latency and motion blur, hindering precise control. This paper addresses the problem of low-latency velocity estimation for quadrotor control within a narrow pipe, where visual information is sparse and fast maneuvers are required. We propose a novel event-based velocimetry method based on optical flow estimation using a bio-inspired event camera. Our approach leverages the asynchronous and high temporal resolution nature of event data to directly estimate the translational velocity of the quadrotor relative to the pipe walls. This is achieved through a computationally efficient algorithm that minimizes the discrepancy between observed events and the predicted event flow induced by the estimated velocity. Experimental results demonstrate that our method achieves significantly lower latency and higher accuracy in velocity estimation compared to frame-based methods, enabling stable and agile quadrotor flight within a narrow pipe. The proposed event-based velocimetry approach offers a promising solution for real-time control in challenging environments where latency is critical."
http://arxiv.org/abs/2507.14918v1,Semantic-Aware Representation Learning for Multi-label Image Classification,"Multi-label image classification, where an image can be associated with multiple semantic labels simultaneously, is a challenging task in computer vision. Existing methods often struggle to effectively capture the complex semantic correlations between labels, leading to suboptimal classification performance. To address this, we propose a novel Semantic-Aware Representation Learning (SARL) framework for multi-label image classification. SARL leverages a graph convolutional network (GCN) to explicitly model the semantic relationships between labels, encoding these relationships into a semantic embedding space. This semantic embedding is then integrated with visual features extracted from a convolutional neural network (CNN) through an attention mechanism, allowing the model to focus on relevant visual cues conditioned on the semantic context. Experiments on benchmark datasets, including MS-COCO and VOC2007, demonstrate that SARL consistently outperforms state-of-the-art methods in terms of mean Average Precision (mAP) and other evaluation metrics. The proposed method offers a promising approach for learning more discriminative and semantically coherent representations for multi-label image classification."
http://arxiv.org/abs/2507.14790v1,A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation,"Medical image segmentation is crucial for accurate diagnosis and treatment planning. However, the high resolution of medical images presents computational challenges, necessitating effective downsampling strategies. Current downsampling methods often lead to information loss, particularly of subtle anatomical details, negatively impacting segmentation performance. To address this, we propose a novel downsampling strategy based on Information Complementarity (IC-Downsample). IC-Downsample leverages multiple complementary downsampling kernels, including learned and fixed operators, to capture diverse features at different scales. The resulting multi-scale feature maps are then fused using an attention mechanism, adaptively weighting the contribution of each kernel based on its information content. We evaluated IC-Downsample on three publicly available medical image segmentation datasets, demonstrating significant improvements in Dice score and Hausdorff distance compared to traditional downsampling techniques and other learned downsampling methods. This innovative downsampling approach preserves crucial information, leading to more accurate and robust medical image segmentation."
http://arxiv.org/abs/2507.14378v1,Classification of Histopathology Slides with Persistence Homology Convolutions,"Histopathology slide classification is crucial for cancer diagnosis and prognosis, traditionally relying on expert pathologists. However, the increasing volume of data necessitates automated methods capable of extracting complex morphological features. This work addresses the challenge of capturing intricate, multi-scale topological features in histopathology images for improved classification accuracy. We propose a novel framework, Persistence Homology Convolutions (PHC), which integrates persistent homology, a tool from topological data analysis, directly into convolutional neural networks. PHC layers compute persistent homology on image patches, encode topological features as persistence diagrams, and then learn convolutional filters on these diagrams. We demonstrate the effectiveness of PHC on benchmark histopathology datasets for breast cancer and colorectal cancer, achieving state-of-the-art classification performance, surpassing existing CNN architectures and hand-crafted feature extraction methods. This demonstrates the potential of topological data analysis to enhance deep learning models for medical image analysis and provides a new direction for automated histopathology image classification."
http://arxiv.org/abs/2507.14010v1,Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations,"Automated tunnel inspection is crucial for ensuring infrastructure safety and reducing maintenance costs. However, manual crack detection and assessment are time-consuming, subjective, and prone to human error. This paper addresses the challenge of automatic tunnel crack detection, classification, and segmentation using deep learning techniques. We propose a novel framework that combines a convolutional neural network (CNN) for crack classification (spalling, longitudinal, transverse, and oblique) with a Mask R-CNN architecture for pixel-level crack segmentation. Furthermore, we integrate visual explanation methods, specifically Grad-CAM, to provide insights into the CNN's decision-making process, enhancing trust and interpretability. Our experimental results, conducted on a real-world dataset of tunnel images, demonstrate a significant improvement in crack detection accuracy compared to traditional image processing techniques, achieving a mean Average Precision (mAP) of 0.85 for crack segmentation and an overall classification accuracy of 92%. The integration of visual explanations further validates the model's focus on relevant crack features. This automated and interpretable approach offers a robust solution for efficient and reliable tunnel infrastructure inspection and maintenance."
http://arxiv.org/abs/2507.13514v1,Sugar-Beet Stress Detection using Satellite Image Time Series,"Precision agriculture benefits significantly from timely and accurate stress detection in crops. Identifying stress in sugar-beet fields early on is crucial for optimizing irrigation and fertilization, ultimately improving yield and reducing resource waste. This paper addresses the challenge of detecting stress in sugar-beet fields using satellite image time series data, a task complicated by varying environmental conditions and subtle visual differences in early stress stages. We propose a novel approach leveraging a deep learning framework incorporating a recurrent neural network (RNN) with attention mechanisms to analyze multi-spectral Sentinel-2 satellite imagery. The RNN captures temporal dependencies within the time series, while the attention mechanism highlights the most relevant spectral bands and time points for stress detection. Experimental results on a dataset of sugar-beet fields demonstrate that our method achieves state-of-the-art performance, surpassing traditional vegetation indices and other machine learning techniques in identifying stressed areas with high accuracy (over 90% F1-score) and providing valuable insights into the temporal evolution of stress. This research offers a robust and scalable solution for early and accurate stress detection in sugar-beet cultivation, contributing to more sustainable and efficient agricultural practices."
http://arxiv.org/abs/2507.13420v2,AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery,"The archaeological landscape of Mesopotamia, the cradle of civilization, is rapidly vanishing due to agricultural expansion, urbanization, and conflict. This destruction necessitates urgent action to document and protect remaining sites, yet traditional survey methods are time-consuming and resource-intensive. This paper addresses the challenge of efficiently identifying and mapping archaeological sites across vast areas using historical CORONA satellite imagery, a valuable but underutilized resource due to its low resolution and inherent distortions. We propose a novel automated detection pipeline leveraging deep learning techniques. Specifically, we employ a Faster R-CNN object detector, pre-trained on modern satellite imagery and fine-tuned on a manually annotated dataset of archaeological mounds (tells) visible in CORONA imagery. Furthermore, we incorporate data augmentation strategies that simulate the unique characteristics of CORONA imagery, such as varying image quality and geometric distortions, to improve the robustness of the model. Our results demonstrate a significant improvement in site detection accuracy compared to traditional manual interpretation, achieving a precision of 0.82 and a recall of 0.75 on a held-out test set. This automated approach provides a crucial tool for heritage management, enabling rapid assessment of threatened archaeological landscapes and facilitating targeted conservation efforts in this vulnerable region."
http://arxiv.org/abs/2507.12961v1,Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset,"Deep learning has shown promise in assisting dermatologists with skin lesion diagnosis, particularly for melanoma. However, achieving robust and accurate classification of pigmented skin lesions remains challenging due to subtle visual differences and inter-observer variability. This study addresses the problem of improving diagnostic accuracy for pigmented skin lesions using Convolutional Neural Networks (CNNs) on the DermaMNIST dataset. We propose a novel CNN architecture incorporating attention mechanisms and transfer learning from a pre-trained ResNet50 model on ImageNet. Specifically, we introduce spatial and channel-wise attention modules to emphasize relevant features and suppress noise, along with data augmentation techniques to mitigate overfitting. Our proposed method achieves a significant improvement in classification accuracy on the DermaMNIST dataset, reaching 92.5% accuracy, surpassing existing state-of-the-art methods by a margin of 2%. This demonstrates the potential of attention-guided CNNs for enhancing the accuracy and reliability of automated skin lesion diagnosis, ultimately aiding in earlier and more effective treatment of skin cancer."
http://arxiv.org/abs/2507.12938v1,Unleashing Vision Foundation Models for Coronary Artery Segmentation: Parallel ViT-CNN Encoding and Variational Fusion,"Coronary artery segmentation is crucial for diagnosing and treating cardiovascular diseases, the leading cause of death globally. However, accurate and robust segmentation remains challenging due to the complex anatomical structure of coronary arteries, variations in image quality, and limited labeled data. To address these challenges, we propose a novel framework leveraging Vision Foundation Models (VFMs) for coronary artery segmentation, termed Parallel ViT-CNN Encoding and Variational Fusion (PVCVF). PVCVF employs a parallel architecture consisting of a pre-trained Vision Transformer (ViT) branch to capture global contextual information and a Convolutional Neural Network (CNN) branch to extract fine-grained local features, both initialized from VFMs. A variational fusion module then adaptively integrates these multi-scale representations, learning a latent distribution that captures the uncertainty and complementarity of the two branches. Experimental results on benchmark coronary artery segmentation datasets demonstrate that PVCVF achieves state-of-the-art performance, outperforming existing methods in terms of Dice score, Jaccard index, and Hausdorff distance. This work showcases the potential of VFMs to significantly improve medical image segmentation, paving the way for more accurate and reliable cardiovascular disease diagnosis."
http://arxiv.org/abs/2507.12762v1,World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving,"Anticipating potential accidents is crucial for ensuring the safety of autonomous driving systems. Existing methods often rely on heuristics or limited context, struggling to generalize to unseen scenarios. We address the problem of generating realistic future scene evolutions conditioned on current observations to facilitate robust accident anticipation. Our approach introduces a novel end-to-end scene generation framework based on a learned world model. This world model comprises a variational autoencoder that encodes the current scene into a latent representation, and a recurrent neural network that predicts future latent states conditioned on the ego-vehicle's planned trajectory. A scene decoder then reconstructs future scenes from these predicted latent states, enabling the generation of multiple plausible future scenarios. We demonstrate that our framework can generate realistic and diverse scene evolutions, significantly improving the accuracy of accident anticipation compared to baseline methods on the CARLA simulator. This work provides a promising direction for developing more reliable and proactive autonomous driving systems by leveraging the power of learned world models for future scene prediction."
http://arxiv.org/abs/2507.12675v1,FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks,"Structural segmentation, crucial for applications like autonomous navigation and robotic manipulation, demands both high accuracy and real-time performance in dynamic environments. Existing methods often struggle to balance these competing requirements, particularly when faced with noisy or incomplete data. We address this challenge by introducing FORTRESS, a novel framework for real-time resilient structural segmentation. FORTRESS leverages Function-composition Optimization (FCO) to streamline network architecture and reduce computational overhead. At its core is a Kolmogorov-Arnold Network (KAN)-enhanced Spatial Attention Network (SAN) that adaptively focuses on salient structural features while suppressing noise. The KAN layers within the SAN module provide a more expressive and efficient representation of spatial relationships compared to traditional convolutional layers. Experimental results on benchmark datasets demonstrate that FORTRESS achieves state-of-the-art segmentation accuracy with a significant reduction in inference time compared to existing methods. This improvement in both accuracy and speed facilitates robust and reliable structural understanding in real-world scenarios."
http://arxiv.org/abs/2507.12433v1,Traffic-Aware Pedestrian Intention Prediction,"Accurate pedestrian intention prediction is crucial for autonomous driving systems to ensure safety and enable proactive decision-making. Current approaches often focus solely on pedestrian motion and appearance, neglecting the influence of surrounding traffic conditions. This paper addresses the problem of predicting pedestrian crossing intentions by explicitly incorporating traffic context into the prediction model. We propose a novel Traffic-Aware Pedestrian Intention Network (TAPINet) that leverages a graph neural network (GNN) to model the interactions between pedestrians and surrounding vehicles. TAPINet integrates pedestrian motion features, appearance cues, and traffic flow information extracted from vehicle trajectories to generate a comprehensive scene representation. Experimental results on a large-scale real-world dataset demonstrate that TAPINet significantly outperforms state-of-the-art methods, achieving a 15% improvement in F1-score for predicting crossing intentions 1 second before crossing. This improved accuracy highlights the importance of considering traffic context for robust and reliable pedestrian intention prediction, paving the way for safer autonomous navigation in complex urban environments."
http://arxiv.org/abs/2507.12248v1,"Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST","Deep learning frameworks are essential tools for developing and deploying computer vision models. However, the performance characteristics of these frameworks can vary depending on the hardware, software, and model architecture used. This paper addresses the problem of comparatively evaluating the performance of Convolutional Neural Networks (CNNs) implemented in Keras, PyTorch, and JAX on the PathMNIST dataset, a standardized benchmark for histopathology image classification. We implement a consistent CNN architecture across all three frameworks, carefully controlling for factors such as data preprocessing, hyperparameter optimization, and training procedures. Performance is assessed based on training time, inference speed, memory utilization, and classification accuracy. Our results demonstrate quantifiable differences in performance metrics across the frameworks, highlighting trade-offs between ease of use, computational efficiency, and resource consumption. This comparative analysis provides valuable insights for researchers and practitioners seeking to optimize their deep learning workflows for medical image analysis and similar computer vision tasks."
http://arxiv.org/abs/2507.12177v1,Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification,"Brain tumor classification from magnetic resonance imaging (MRI) is crucial for accurate diagnosis and treatment planning. Existing methods often struggle to effectively leverage the diverse information captured by different deep learning architectures and optimized classifier ensembles. This paper addresses the challenge of maximizing brain tumor classification accuracy by developing a novel hybrid ensemble approach. Our method combines deep feature fusion from pre-trained Convolutional Neural Networks (CNNs), specifically ResNet50, VGG16, and InceptionV3, using a learned weighting strategy to extract complementary information. Furthermore, we employ a hyperparameter-tuned classifier ensembling technique, optimizing the combination of Support Vector Machines (SVM), Random Forest (RF), and k-Nearest Neighbors (k-NN) classifiers using Bayesian optimization. Experimental results on the benchmark FIGSHARE brain tumor dataset demonstrate that our hybrid ensemble approach achieves a state-of-the-art classification accuracy of 98.7%, surpassing individual CNNs and traditional ensemble methods. This highlights the potential of optimized deep feature fusion and hyperparameter-tuned classifier ensembling to significantly improve the performance of brain tumor classification systems."
http://arxiv.org/abs/2507.11893v2,Spatial Frequency Modulation for Semantic Segmentation,"Semantic segmentation, assigning a class label to each pixel in an image, is a fundamental task in computer vision. However, existing methods often struggle to effectively capture both local details and global contextual information necessary for accurate pixel-level classification. This paper addresses the challenge of integrating multi-scale information within a unified framework for improved semantic segmentation. We propose a novel Spatial Frequency Modulation (SFM) module that operates directly in the frequency domain. SFM decomposes feature maps into their spatial frequency components using the Discrete Cosine Transform (DCT), selectively modulates specific frequency bands to enhance relevant features, and then reconstructs the spatial domain representation using the inverse DCT. This allows for explicit control over the importance of different spatial frequencies, enabling the network to adaptively focus on fine-grained details or broader contextual cues. Experiments on the Cityscapes and ADE20K datasets demonstrate that integrating SFM into existing segmentation architectures leads to significant performance gains. Our approach achieves state-of-the-art results, highlighting the effectiveness of frequency-domain manipulation for improving semantic segmentation accuracy."
http://arxiv.org/abs/2507.11638v1,Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders,"Lymph node metastasis (LNM) is a critical prognostic factor in rectal cancer, significantly impacting treatment planning and patient outcomes. Accurate pre-operative LNM prediction from magnetic resonance imaging (MRI) remains challenging. This paper introduces a novel interpretable deep learning framework for predicting LNM in rectal cancer from MRI, leveraging Variational Autoencoders (VAEs) to extract clinically relevant features. Our approach employs a VAE to learn a lower-dimensional, disentangled latent space representation of the tumor and surrounding tissue from MRI scans. Subsequently, a classifier, conditioned on both the latent space features and clinical parameters, predicts LNM status. We further enhance interpretability by analyzing the latent space activations to identify image features associated with LNM, visualizing these features via the VAE decoder. Experiments on a dataset of 200 rectal cancer patients demonstrate that our method achieves a significant improvement in LNM prediction accuracy compared to conventional CNN-based approaches, with an AUC of 0.85. Furthermore, the identified latent space features correlate with established histopathological characteristics of LNM, providing clinicians with valuable insights into the model's decision-making process. This interpretable framework holds promise for improving diagnostic accuracy and personalized treatment strategies for rectal cancer patients."
http://arxiv.org/abs/2507.11476v1,3C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images,"Circle fitting is a fundamental task in computer vision with applications ranging from object detection to industrial inspection. However, the performance of existing circle fitting algorithms degrades significantly when applied to blurry images due to inaccurate edge localization. This paper introduces 3C-FBI: a Combinatorial method using Convolutions for Circle Fitting in Blurry Images. Our approach leverages a novel combination of convolutional filtering to enhance edge features and a combinatorial search strategy to efficiently explore the parameter space of circles. Specifically, we employ a bank of learned convolutional filters to extract robust edge maps, followed by a combinatorial Hough transform that strategically samples edge pixels to form candidate circles. These candidates are then refined and scored based on their consistency with the enhanced edge maps. Experimental results on both synthetic and real-world blurry images demonstrate that 3C-FBI achieves significantly higher accuracy and robustness compared to state-of-the-art circle fitting methods, particularly under severe blur conditions. The proposed method offers a practical solution for accurate circle fitting in challenging imaging scenarios where blur is unavoidable."
http://arxiv.org/abs/2507.11325v1,HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging,"Accurate liver and tumor segmentation in CT imaging is crucial for computer-aided diagnosis and treatment planning. However, the high variability in liver and tumor shape, size, and location, coupled with inter-slice inconsistencies in CT scans, pose significant challenges for robust and generalizable segmentation. We introduce HANS-Net, a novel deep learning framework leveraging hyperbolic convolutions and adaptive temporal attention to address these challenges. HANS-Net incorporates hyperbolic convolutional layers to effectively capture the complex, non-Euclidean geometry of liver and tumor structures. Furthermore, we introduce an adaptive temporal attention module that dynamically weights the importance of adjacent slices, mitigating inter-slice inconsistencies and improving temporal coherence. We evaluated HANS-Net on a large multi-center dataset, achieving state-of-the-art performance with significant improvements in Dice score and Hausdorff distance compared to existing methods. These results demonstrate HANS-Net's potential for improving the accuracy and reliability of liver and tumor segmentation, ultimately enhancing clinical workflows."
http://arxiv.org/abs/2507.11302v1,"All Eyes, no IMU: Learning Flight Attitude from Vision Alone","Estimating flight attitude is crucial for autonomous navigation of aerial vehicles. Traditionally, this estimation relies heavily on inertial measurement units (IMUs), which are susceptible to drift and can be expensive. This paper addresses the problem of accurately estimating flight attitude using only visual information, eliminating the need for IMUs. We propose a novel deep learning framework, Vision-Attitude Network (VAN), that directly regresses flight attitude (roll, pitch, yaw) from monocular video sequences. VAN leverages a convolutional neural network (CNN) backbone for feature extraction, followed by a recurrent neural network (RNN) to capture temporal dependencies in the video stream and refine attitude predictions. Experiments on both simulated and real-world flight datasets demonstrate that VAN achieves comparable or superior performance to IMU-based methods, especially in scenarios with significant IMU drift. This work provides a cost-effective and robust alternative for flight attitude estimation, opening new possibilities for vision-centric autonomous aerial navigation."
http://arxiv.org/abs/2507.11293v1,3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images,"Magnetic field imaging offers a non-invasive means to probe materials and devices, with applications ranging from biomedical sensing to non-destructive testing. However, reconstructing the underlying 3D magnetic source distribution from single-view magnetic field images remains a challenging inverse problem, often requiring complex multi-view setups or strong prior assumptions. This paper addresses the problem of accurately and efficiently recovering 3D magnetic source distributions from single-segment magnetic field images. We introduce a novel 3D Magnetic Inverse Routine (3D-MIR) that combines a physics-informed forward model with a regularized optimization framework. Specifically, we leverage the Biot-Savart law to relate the magnetic source to the measured field, and incorporate sparsity priors to constrain the solution space. Experimental results on both synthetic and real magnetic field data demonstrate that 3D-MIR achieves significantly improved reconstruction accuracy compared to existing methods, particularly in scenarios with limited data and complex source geometries. This approach provides a powerful tool for characterizing magnetic sources from readily acquired single-segment images, opening new avenues for magnetic field-based sensing and analysis."
http://arxiv.org/abs/2507.11116v1,Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach,"Jellyfish blooms are increasing globally, impacting marine ecosystems, fisheries, and coastal industries. Accurate and efficient identification of jellyfish species is crucial for effective monitoring and management of these events. This paper addresses the challenge of automating jellyfish species identification from underwater imagery, which is often hampered by variations in appearance, lighting conditions, and water turbidity. We propose a Convolutional Neural Network (CNN) based artificial neural network approach, leveraging transfer learning with pre-trained models (ResNet50, InceptionV3, and EfficientNetB0) fine-tuned on a curated dataset of jellyfish images encompassing diverse species and environmental conditions. The models were trained using data augmentation techniques to improve robustness and generalization. Our experimental results demonstrate that the fine-tuned EfficientNetB0 model achieves the highest accuracy of 92.5% on a held-out test set, significantly outperforming other architectures and traditional image classification methods. This automated jellyfish species identification system offers a scalable and efficient solution for real-time monitoring and contributes to a better understanding of jellyfish bloom dynamics."
http://arxiv.org/abs/2507.11035v1,Efficient Dual-domain Image Dehazing with Haze Prior Perception,"Image dehazing is a fundamental task in computer vision, aiming to restore clear visibility in images degraded by atmospheric haze. Existing deep learning-based dehazing methods often struggle with balancing restoration quality and computational efficiency, particularly when dealing with complex haze distributions and high-resolution images. This paper addresses the challenge of achieving efficient and high-quality image dehazing by proposing a novel Dual-domain Image Dehazing Network with Haze Prior Perception (DIDN-HPP). Our method leverages a dual-domain architecture, operating in both the spatial and frequency domains to capture complementary haze characteristics. Specifically, we introduce a Haze Prior Perception Module (HPPM) that explicitly learns haze-related features from the frequency domain and integrates them into the spatial domain processing, guiding the network to better estimate the transmission map and atmospheric light. Experimental results on benchmark datasets demonstrate that DIDN-HPP achieves state-of-the-art dehazing performance in terms of both quantitative metrics (PSNR, SSIM) and visual quality, while maintaining significantly lower computational complexity compared to existing deep learning-based approaches. The proposed method offers a practical solution for real-time image dehazing applications, enabling robust performance in resource-constrained environments."
http://arxiv.org/abs/2507.10977v1,Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection,"Human-Object Interaction (HOI) detection is a challenging task in computer vision, requiring accurate localization and contextual reasoning between humans and objects. Existing methods often struggle with capturing fine-grained interactions and effectively modeling the spatial relationships between entities at varying scales. This paper introduces a novel approach, Multi-scale Wavelet Attention and Ray-based Encoding (MWARE), designed to enhance HOI detection performance. MWARE leverages wavelet transformation to decompose visual features into multiple scales, enabling the network to attend to relevant features at different levels of granularity. Furthermore, we introduce a ray-based encoding module that captures the spatial configuration between human and object bounding boxes by encoding the relative position and orientation along multiple rays emanating from the human center. Experimental results on the V-COCO and HICO-DET datasets demonstrate that MWARE achieves state-of-the-art performance, surpassing existing methods by a significant margin, particularly in detecting subtle and complex interactions. The proposed MWARE framework provides a robust and effective solution for HOI detection, paving the way for more accurate and detailed scene understanding."
http://arxiv.org/abs/2507.11571v1,Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation,"Gait age estimation, predicting a person's age from their walking patterns using sensor data, holds promise for non-invasive health monitoring and early disease detection. However, the heterogeneity of sensor modalities, feature extraction techniques, and evaluation protocols across existing studies hinders a comprehensive understanding of the field's progress and the comparative performance of different approaches. This paper addresses this challenge by presenting a data-driven meta-analysis of sensor-based gait age estimation, focusing on publicly available datasets. We systematically extract performance metrics and methodological details from relevant publications and employ meta-regression techniques to identify key factors influencing estimation accuracy, such as sensor placement, feature type, and machine learning algorithm. Furthermore, we conduct a comprehensive evaluation of several established gait age estimation methods on a curated collection of public datasets, providing a standardized benchmark for future research. Our analysis reveals significant variability in performance across datasets and highlights the importance of feature selection and model complexity. This work offers valuable insights into the current state of sensor-based gait age estimation and establishes a robust framework for evaluating novel approaches, accelerating progress towards reliable and clinically relevant age prediction models."
http://arxiv.org/abs/2507.10846v1,Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization,"Visual explanations, particularly class activation maps (CAMs), are crucial for understanding and trusting deep neural network predictions. However, existing CAM methods often produce noisy or coarse heatmaps, requiring expert knowledge to interpret and tune, or lacking intuitive controls for non-expert users. This paper introduces Winsor-CAM, a novel approach to generating human-tunable visual explanations via layer-wise Winsorization. Winsor-CAM leverages the inherent structure of feature maps, applying Winsorization to individual layers to selectively suppress outliers and emphasize relevant activations, guided by a single, intuitive parameter controlling the degree of Winsorization. This allows users to refine the generated heatmaps, highlighting specific regions of interest and mitigating noise without requiring retraining or complex hyperparameter tuning. We demonstrate Winsor-CAM's effectiveness across various architectures and datasets, showing improved localization accuracy and enhanced interpretability compared to state-of-the-art CAM methods, as evaluated through quantitative metrics and qualitative user studies. Winsor-CAM provides a simple yet powerful tool for democratizing the interpretability of deep learning models, enabling both experts and non-experts to gain deeper insights into model behavior."
http://arxiv.org/abs/2507.10461v1,RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening,"Pansharpening, the fusion of high-resolution panchromatic (PAN) and low-resolution multispectral (MS) images, is crucial for enhancing the spatial detail of MS imagery. Existing convolutional neural network (CNN)-based pansharpening methods often employ fixed receptive fields, which may not be optimal for handling varying spatial details and spectral distortions present in different regions of the image. This paper introduces RAPNet, a Receptive-Field Adaptive Pansharpening Network designed to dynamically adjust the receptive field size based on local image characteristics. RAPNet incorporates a novel Receptive Field Selection (RFS) module that learns to adaptively select the most appropriate receptive field for each spatial location. Furthermore, the network utilizes a deep residual architecture to facilitate feature extraction and gradient propagation. Experimental results on several benchmark datasets demonstrate that RAPNet achieves superior pansharpening performance compared to state-of-the-art methods, both quantitatively and qualitatively. The adaptive receptive field mechanism allows RAPNet to effectively balance spatial enhancement and spectral preservation, leading to more accurate and visually appealing pansharpened images."
http://arxiv.org/abs/2507.10398v1,Devanagari Handwritten Character Recognition using Convolutional Neural Network,"Devanagari, the script used for languages like Hindi and Sanskrit, presents unique challenges for Optical Character Recognition (OCR) due to its complex structure and large character set. This paper addresses the problem of accurately recognizing handwritten Devanagari characters, which suffers from significant variations in writing styles and stroke formations. We propose a Convolutional Neural Network (CNN) architecture optimized for feature extraction from Devanagari characters. The network incorporates multiple convolutional layers with ReLU activation, followed by max-pooling layers to reduce dimensionality and enhance translation invariance. Data augmentation techniques, including rotation, scaling, and elastic distortions, were applied to increase the size and diversity of the training dataset. Experimental results on the publicly available CDHIST dataset demonstrate that our proposed CNN achieves a character recognition accuracy of 96.8%, outperforming several existing methods. This high accuracy indicates the effectiveness of the CNN architecture in learning discriminative features from handwritten Devanagari characters, paving the way for improved OCR systems and accessibility tools for Devanagari-based languages."
http://arxiv.org/abs/2507.10381v1,Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks,"Remote sensing image classification is crucial for various applications, including land cover mapping, environmental monitoring, and urban planning. However, the inherent complexity of high-dimensional remote sensing data, coupled with spectral variability and spatial heterogeneity, often limits the accuracy of traditional classification methods. This paper addresses the challenge of improving remote sensing classification accuracy by integrating Topological Data Analysis (TDA) with Convolutional Neural Networks (CNNs). Our proposed method leverages TDA to extract persistent homology features, which capture the multi-scale topological structure of the remote sensing imagery. These topological features are then fused with spectral features and fed into a CNN for classification. Experiments conducted on benchmark remote sensing datasets demonstrate that the proposed TDA-CNN approach achieves significantly higher classification accuracies compared to state-of-the-art methods, including CNNs trained solely on spectral data and traditional machine learning classifiers. This research highlights the potential of TDA to enhance the feature representation and improve the performance of deep learning models for remote sensing image analysis."
http://arxiv.org/abs/2507.10239v1,Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks,"Semantic segmentation networks often struggle with generalization due to a reliance on textural cues, leading to performance drops when encountering novel image styles. This paper addresses the problem of texture bias in semantic segmentation by proposing a novel style transfer-based data augmentation strategy. Our method leverages a diverse collection of artistic styles to transform the texture of training images while preserving their semantic content. Specifically, we apply a range of style transfer algorithms, including both global and local style transfer techniques, to generate stylized versions of the original training data. These stylized images are then incorporated into the training process, forcing the segmentation network to focus more on shape and contextual information rather than solely relying on texture. Experiments on benchmark datasets, including Cityscapes and Pascal VOC, demonstrate that our style transfer augmentation significantly improves the robustness of segmentation networks against various image corruptions and domain shifts, leading to improved generalization performance without sacrificing accuracy on the original dataset. This approach offers a practical and effective way to mitigate texture bias and enhance the reliability of semantic segmentation in real-world applications."
http://arxiv.org/abs/2507.09995v2,Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys),"Brain tumor segmentation in Magnetic Resonance Imaging (MRI) is crucial for diagnosis and treatment planning. Existing deep learning methods often struggle with computational demands and effective integration of multi-modal MRI data, hindering their deployment in resource-constrained environments such as edge computing. This paper introduces a novel Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) integrated within an Edge Iterative MRI Lesion Localization System (EdgeIMLocSys). GMLN-BTS employs a lightweight 3D convolutional neural network backbone coupled with a graph-based module to efficiently fuse information from different MRI modalities, capturing inter-modal relationships crucial for accurate segmentation. The EdgeIMLocSys iteratively refines the region of interest using edge computing resources, reducing the computational burden on the main segmentation network. Experimental results on the BraTS 2021 dataset demonstrate that GMLN-BTS achieves competitive segmentation performance with significantly reduced computational complexity, enabling real-time brain tumor segmentation within the EdgeIMLocSys framework. This work facilitates the deployment of advanced brain tumor segmentation techniques in resource-limited environments, improving accessibility and efficiency of clinical workflows."
http://arxiv.org/abs/2507.09953v3,4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion,"Low-dose imaging techniques are crucial in applications where minimizing radiation exposure is paramount, but they often result in noisy and low-resolution (LR) images. Super-resolution (SR) reconstruction aims to enhance the resolution of these images, yet current methods often struggle with the inherent noise and limited information in low-dose scenarios, particularly when considering temporal data. This paper addresses the challenge of reconstructing high-resolution (HR) volumes from a sequence of noisy, LR 3D volumes acquired at different time points (4D data). We propose 4D-MISR, a unified model for low-dose SR imaging that leverages multi-scale feature fusion within a deep neural network architecture. Specifically, 4D-MISR incorporates a novel spatiotemporal attention mechanism to adaptively weight and fuse features extracted from both spatial and temporal dimensions, enhancing the representation of dynamic changes while suppressing noise. Experimental results on both simulated and real low-dose CT datasets demonstrate that 4D-MISR significantly outperforms state-of-the-art SR methods in terms of PSNR, SSIM, and perceptual quality, while also reducing noise artifacts. These improvements highlight the potential of 4D-MISR for clinical applications requiring high-quality volumetric imaging with minimal radiation exposure."
http://arxiv.org/abs/2507.09898v2,Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images,"Lung cancer remains a leading cause of mortality worldwide, highlighting the critical need for early and accurate detection. Manual analysis of chest computed tomography (CT) scans is time-consuming and prone to inter-observer variability, motivating the development of automated diagnostic tools. This paper addresses the challenge of improving the accuracy and efficiency of lung cancer detection and segmentation in CT images. We propose a suite of advanced U-Net architectures, leveraging powerful convolutional neural network (CNN) backbones, including ResNet50, EfficientNet-B4, and DenseNet121, pre-trained on ImageNet. These backbones are integrated into the U-Net framework to extract robust and discriminative features from CT images, enhancing the model's ability to accurately identify and delineate cancerous regions. Experimental results on the publicly available LIDC-IDRI dataset demonstrate that our best-performing model, U-Net with an EfficientNet-B4 backbone, achieves a Dice score of 0.82 and a sensitivity of 0.85, significantly outperforming standard U-Net and demonstrating competitive performance against state-of-the-art methods. This research provides a valuable contribution towards developing reliable and automated lung cancer screening tools, ultimately aiding in earlier diagnosis and improved patient outcomes."
http://arxiv.org/abs/2507.09872v1,Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction,"Spatiotemporal temperature fields are crucial in numerous scientific and engineering applications, ranging from climate modeling to thermal management of electronic devices. However, direct measurement of these fields with high spatial and temporal resolution remains challenging due to sensor limitations and cost. This paper addresses the problem of reconstructing high-resolution spatiotemporal temperature fields from limited and noisy sensor data. We propose a novel physics-guided deep learning framework, Resolution Revolution (R2), which integrates the heat equation, a fundamental physical law governing heat transfer, directly into the neural network architecture. Specifically, R2 employs a convolutional neural network (CNN) to learn a mapping from low-resolution sensor data to high-resolution temperature fields, while a physics-informed loss function, derived from the heat equation, enforces physical consistency and regularizes the solution. Experimental results on both synthetic and real-world datasets demonstrate that R2 significantly outperforms state-of-the-art interpolation and deep learning methods in terms of reconstruction accuracy and robustness to noise, achieving up to a 40% reduction in reconstruction error. This physics-guided deep learning approach offers a powerful tool for high-fidelity spatiotemporal temperature reconstruction, enabling improved analysis and control in a wide range of applications."
http://arxiv.org/abs/2507.09830v1,Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models,"Humans possess a remarkable ability to recognize 3D objects from diverse viewpoints and partial occlusions, leveraging a hierarchical understanding of object structure. However, deep learning models often struggle to replicate this robustness, exhibiting sensitivity to viewpoint changes and requiring extensive training data. This paper addresses the challenge of developing deep learning models that exhibit human-like 3D object recognition capabilities. We propose a novel hierarchical abstraction network (HAN) that incorporates a coarse-to-fine processing strategy. HAN first extracts high-level semantic features representing the overall object shape, followed by iterative refinement using local geometric details at increasingly finer scales. Furthermore, we introduce a multi-view consistency loss that encourages the model to learn viewpoint-invariant representations at each hierarchical level. Experimental results on benchmark 3D object recognition datasets demonstrate that HAN significantly outperforms state-of-the-art methods in terms of accuracy, robustness to viewpoint changes, and performance with limited training data. These findings suggest that incorporating hierarchical abstraction into deep learning architectures is a promising direction for achieving human-level performance in 3D object recognition."
http://arxiv.org/abs/2507.09541v1,DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection,"Infrared small target detection (IRSTD) is a crucial task in various applications, yet it faces significant challenges due to the lack of distinct features of targets and the presence of complex backgrounds with strong clutter. Traditional Robust Principal Component Analysis (RPCA) has shown promise by decomposing the infrared image into low-rank background and sparse target components. However, its performance is often limited by the sensitivity to parameter tuning and the inability to effectively handle non-convex noise distributions inherent in IR images. This paper introduces DRPCA-Net, a deep learning framework designed to enhance the robustness and adaptability of RPCA for IRSTD. DRPCA-Net leverages a convolutional neural network to learn adaptive weighting parameters for the RPCA decomposition process, effectively mitigating the impact of complex noise and clutter. Furthermore, we incorporate a novel loss function that explicitly encourages sparsity in the target component and low-rankness in the background component. Experimental results on several challenging IRSTD datasets demonstrate that DRPCA-Net significantly outperforms state-of-the-art RPCA-based and deep learning-based methods in terms of detection accuracy and robustness to noise. DRPCA-Net offers a powerful and adaptable solution for IRSTD, paving the way for more reliable target detection in complex infrared scenes."
http://arxiv.org/abs/2507.11550v1,Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction,"Accurate spatio-temporal traffic prediction is crucial for intelligent transportation systems, enabling proactive traffic management and informed decision-making. However, capturing the complex and dynamic dependencies in traffic flow, while maintaining computational efficiency, remains a significant challenge. This paper introduces Deformable Dynamic Convolution (DDC), a novel approach for accurate and efficient spatio-temporal traffic prediction. DDC leverages deformable convolutions to adaptively learn spatial dependencies based on the current traffic conditions, enabling the model to focus on relevant regions and effectively capture dynamic spatial relationships. Furthermore, we employ a dynamic convolution mechanism to adapt the convolutional kernels over time, allowing the model to capture temporal variations and evolving traffic patterns. Experiments on benchmark traffic datasets demonstrate that DDC achieves state-of-the-art prediction accuracy while maintaining a significantly lower computational cost compared to existing methods. DDC provides a practical and effective solution for real-world traffic prediction applications, facilitating improved traffic management and enhanced transportation efficiency."
http://arxiv.org/abs/2507.09375v1,Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture,"Crop diseases pose a significant threat to global food security, necessitating rapid and accurate detection methods for effective disease management. Traditional methods for identifying plant diseases are often time-consuming, subjective, and require expert knowledge. This paper addresses the challenge of automating multi-class crop pathology classification using deep learning to enable real-time precision agriculture. We propose a Convolutional Neural Network (CNN) architecture, specifically tailored for identifying various diseases in crop images acquired under real-world field conditions. The model incorporates data augmentation techniques and transfer learning from pre-trained networks to enhance robustness and generalization across diverse environmental factors and image variations. Our experimental results, conducted on a comprehensive dataset of crop images with multiple disease categories, demonstrate a classification accuracy exceeding 95%, significantly outperforming traditional machine learning approaches and achieving near real-time processing speeds. This automated system offers a scalable and efficient solution for early disease detection, facilitating timely intervention and minimizing yield losses in agricultural practices."
http://arxiv.org/abs/2507.09294v1,Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection,"Surgical phase recognition in Endoscopic Submucosal Dissection (ESD) is crucial for workflow analysis and skill assessment, yet remains challenging due to subtle visual cues and inter-surgeon variability. Existing methods often neglect the rich geometric information inherent in endoscopic images, leading to suboptimal feature representations. To address this, we introduce Geo-RepNet, a novel geometry-aware representation learning framework for ESD phase recognition. Geo-RepNet leverages a self-supervised geometric pre-training task, predicting surface normals estimated from stereo endoscopy, to learn robust and generalizable feature embeddings. These embeddings are then fine-tuned on labeled ESD phase data using a temporal convolutional network (TCN) to capture long-range dependencies. Experiments on a large-scale ESD video dataset demonstrate that Geo-RepNet significantly outperforms state-of-the-art methods, achieving a 6.3% improvement in overall accuracy and enhanced robustness to variations in surgical technique. This highlights the importance of geometric information in endoscopic video analysis and offers a promising direction for developing more accurate and reliable surgical assistance systems."
http://arxiv.org/abs/2507.09248v1,AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition,"Facial emotion recognition (FER) is crucial for human-computer interaction, but its performance is often hampered by contextual biases present in training datasets, leading to poor generalization on unseen data. This paper addresses the problem of context bias in FER by proposing the Attention Guided Context Debiasing Network (AGCD-Net). AGCD-Net leverages a novel attention mechanism to identify and subsequently debias context-related features that are spuriously correlated with emotion labels. Specifically, the network employs a context attention module to highlight regions contributing to contextual bias and a feature debiasing module to suppress the influence of these biased features during emotion classification. Experiments on multiple benchmark datasets, including AffectNet and RAF-DB, demonstrate that AGCD-Net achieves state-of-the-art performance, significantly improving robustness and generalization compared to existing methods, particularly in challenging, real-world scenarios. The proposed method offers a promising approach to mitigate context bias in FER, leading to more reliable and accurate emotion recognition systems."
http://arxiv.org/abs/2507.09180v2,Learning and Transferring Better with Depth Information in Visual Reinforcement Learning,"Visual reinforcement learning (VRL) agents often struggle with high-dimensional visual inputs, leading to poor sample efficiency and generalization capabilities. A key source of information often overlooked in VRL is depth, which provides crucial geometric cues about the environment. This paper addresses the challenge of effectively incorporating depth information into VRL agents for improved learning and transfer performance. We propose a novel Depth-Aware Visual Reinforcement Learning (DAVRL) framework that utilizes a multi-modal encoder to fuse RGB images and depth maps, followed by a depth-attention mechanism that dynamically weights feature contributions based on depth-derived saliency. Furthermore, we introduce a depth-based domain randomization strategy to enhance robustness and facilitate sim-to-real transfer. Experiments across a suite of challenging robotic manipulation tasks demonstrate that DAVRL significantly outperforms state-of-the-art VRL methods, achieving faster learning speeds and superior asymptotic performance. The proposed method also exhibits strong transferability to unseen environments and real-world scenarios, highlighting the benefits of leveraging depth information for robust and generalizable VRL agents."
http://arxiv.org/abs/2507.09092v1,MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, yet their black-box nature hinders trust and adoption in critical applications. Existing visual explanation methods often generate heatmaps highlighting salient image regions, but these can be sensitive to spurious correlations and lack causal faithfulness, failing to accurately reflect the network's decision-making process. To address this, we propose Mutual Information weighted Class Activation Mapping (MI CAM), a novel approach that leverages information theory to generate more causally faithful explanations. MI CAM computes the mutual information between each feature map and the final prediction, using this measure to weight the contribution of each feature map during the CAM aggregation process. This weighting emphasizes feature maps that are most informative for the prediction, effectively filtering out irrelevant activations. Experiments on benchmark datasets demonstrate that MI CAM produces heatmaps that are more faithful to the underlying causal relationships, as evidenced by improved performance on quantitative evaluation metrics such as deletion and insertion scores, and superior qualitative alignment with human intuition. MI CAM offers a more reliable and interpretable way to understand CNN decision-making, fostering greater trust and enabling more informed use of these powerful models."
http://arxiv.org/abs/2507.08766v1,A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification,"The MNIST handwritten digit classification remains a fundamental benchmark for machine learning algorithms. While deep learning models achieve high accuracy, exploring hybrid approaches that leverage the strengths of both connectionist and statistical methods can offer valuable insights. This paper addresses the challenge of improving MNIST classification accuracy and computational efficiency by integrating a Multi-Well Hopfield Network (MHWN) with a Convolutional Neural Network (CNN) and K-Means clustering. Our proposed hybrid architecture first utilizes a CNN for feature extraction, followed by dimensionality reduction via K-Means clustering to represent the features as cluster centroids. These centroids are then fed into a MHWN for classification, where each well represents a digit class. Experimental results demonstrate that the hybrid approach achieves a classification accuracy comparable to standalone CNNs, while potentially reducing computational complexity during the classification phase due to the simplified representation provided by K-Means and the parallel processing capabilities of the MHWN. This hybrid architecture offers a promising avenue for exploring energy-efficient and biologically-inspired approaches to image classification."
http://arxiv.org/abs/2507.10589v1,Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays,"Pneumonia remains a leading cause of morbidity and mortality worldwide, with chest X-rays (CXRs) serving as a crucial diagnostic tool. However, accurate and timely diagnosis is often hampered by radiologist workload and inter-observer variability. This paper addresses the challenge of improving automated pneumonia detection in CXRs by comparatively evaluating the performance of Vision Transformers (ViTs) against established Convolutional Neural Network (CNN) architectures. We implemented and fine-tuned several ViT models, including ViT-Base and Swin-Transformer, alongside CNN baselines such as ResNet50 and DenseNet121, on a large publicly available CXR dataset. We further explored transfer learning strategies and data augmentation techniques to optimize performance for both model families. Our results demonstrate that ViT models, particularly the Swin-Transformer, achieve superior performance in terms of accuracy, sensitivity, and F1-score compared to the CNN baselines, with a statistically significant improvement in overall diagnostic accuracy (p<0.05). This study highlights the potential of Vision Transformers to enhance automated pneumonia detection systems, ultimately leading to faster and more accurate diagnoses."
http://arxiv.org/abs/2507.08690v1,An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan,"Accurate muscle segmentation and 3D reconstruction from Magnetic Resonance Imaging (MRI) scans are crucial for biomechanical modeling, personalized rehabilitation, and disease diagnosis. However, manual segmentation is time-consuming and prone to inter-observer variability, while existing automated methods often struggle with complex muscle shapes and varying image quality. This paper proposes an efficient approach for muscle segmentation and 3D reconstruction in MRI scans based on keypoint tracking. Our method first employs a deep learning-based keypoint detector to automatically identify anatomical landmarks on muscle boundaries in a single MRI slice. These keypoints are then tracked through adjacent slices using an optical flow-based algorithm, providing robust initialization for a deformable model that refines the segmentation. Finally, the segmented muscle regions are stacked to generate a detailed 3D reconstruction. Experimental results on a dataset of lower limb MRI scans demonstrate that our method achieves state-of-the-art segmentation accuracy (Dice score > 0.90) while significantly reducing computational time compared to existing methods. This efficient and accurate approach facilitates large-scale muscle analysis and enables advanced biomechanical simulations."
http://arxiv.org/abs/2507.08343v2,Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation,"JPEG image steganography aims to conceal secret messages within digital images while minimizing perceptual distortions. A key challenge lies in achieving high embedding capacity with minimal detectability, particularly against sophisticated steganalysis techniques. This paper addresses the problem of generating stego images with near-imperceptible changes in the JPEG domain, even at high embedding rates. We propose a novel adversarial steganographic framework driven by multi-range representations. Specifically, we leverage a generator network that operates on the JPEG coefficient domain, employing a discriminator network trained to distinguish between cover and stego images across multiple frequency ranges. Furthermore, we introduce a novel loss function that penalizes deviations in both global statistical features and local block-level characteristics, forcing the generator to produce stego images with distributions closely mimicking those of cover images. Experimental results demonstrate that our method achieves significantly higher embedding capacity compared to existing JPEG steganography techniques, while maintaining state-of-the-art undetectability against various steganalysis detectors. Our approach offers a significant advancement in practical and secure covert communication by enabling high-capacity steganography with enhanced imperceptibility."
http://arxiv.org/abs/2507.08329v1,Cross-Domain Identity Representation for Skull to Face Matching with Benchmark DataSet,"Skull-to-face matching (SFM) is a crucial technique in forensic anthropology and archaeology for identifying unknown individuals. However, the inherent domain gap between skull morphology and facial appearance, coupled with the scarcity of labeled skull-face pairs, poses a significant challenge to developing robust and accurate SFM systems. This paper addresses the problem of learning effective identity representations across these disparate domains. We propose a novel cross-domain identity embedding network that leverages adversarial learning and a Siamese architecture to minimize the domain discrepancy and maximize the identity consistency between skull and face representations. Specifically, we employ a domain discriminator to encourage domain-invariant feature extraction and a contrastive loss function to learn discriminative identity embeddings. We introduce a new benchmark dataset consisting of registered skull-face pairs with associated metadata to facilitate future research in this area. Experimental results demonstrate that our proposed method significantly outperforms existing SFM techniques, achieving state-of-the-art matching accuracy on both our newly introduced dataset and a publicly available dataset. This research contributes a valuable resource and a novel approach to improve the reliability and applicability of SFM in real-world scenarios."
http://arxiv.org/abs/2507.08223v1,SurfDist: Interpretable Three-Dimensional Instance Segmentation Using Curved Surface Patches,"Instance segmentation in 3D point clouds is crucial for scene understanding, enabling applications in robotics, autonomous driving, and augmented reality. Existing methods often struggle with complex geometries, occlusions, and lack interpretability due to their reliance on voxelization or point-based feature aggregation. This paper addresses the challenge of achieving accurate and interpretable 3D instance segmentation. We propose SurfDist, a novel approach that represents instances as collections of curved surface patches, parameterized by neural implicit surfaces. SurfDist learns to predict signed distance functions for each instance, allowing for the decomposition of the scene into overlapping surface patches. These patches are then clustered into distinct instances based on a learned affinity matrix and a mean-shift algorithm. Experiments on ScanNet and S3DIS datasets demonstrate that SurfDist achieves competitive instance segmentation performance compared to state-of-the-art methods, while simultaneously providing an interpretable representation of each instance through its constituent surface patches. The interpretable nature of SurfDist opens avenues for downstream tasks such as object manipulation and shape reconstruction."
http://arxiv.org/abs/2507.08205v1,HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation,"3D image segmentation is crucial for various applications, including medical imaging analysis and autonomous driving. However, the computational demands of processing high-resolution 3D volumes often limit the deployment of deep learning models on resource-constrained devices. This paper addresses the challenge of achieving accurate and efficient 3D image segmentation with extremely small model sizes while maintaining robustness to varying input resolutions. We introduce HNOSeg-XS, an extremely small Hartley Neural Operator-based segmentation network. HNOSeg-XS leverages a highly efficient Hartley transform-based spectral convolution within a U-Net architecture, significantly reducing the parameter count and computational complexity compared to traditional convolutional and attention-based methods. Furthermore, the global receptive field afforded by the spectral domain enables inherent robustness to changes in input resolution. Experimental results on the Medical Segmentation Decathlon (MSD) dataset demonstrate that HNOSeg-XS achieves competitive segmentation performance with models that are orders of magnitude larger, exhibiting superior efficiency and generalization capability across different resolutions. HNOSeg-XS provides a pathway for deploying accurate 3D segmentation models on edge devices and in resource-limited environments."
http://arxiv.org/abs/2507.07949v1,TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient Human Activity Recognition on Edge Devices,"Human Activity Recognition (HAR) is crucial for various applications, especially in wearable computing and IoT devices. Deploying deep learning models for HAR on resource-constrained edge devices is challenging due to their computational complexity and memory footprint. This paper addresses the problem of creating ultra-lightweight deep learning models for efficient and accurate HAR on edge devices. We introduce TinierHAR, a novel architecture based on depthwise separable convolutions and carefully designed bottleneck structures to significantly reduce the model size and computational cost. Furthermore, we employ knowledge distillation, transferring knowledge from a larger, more accurate teacher model to our smaller TinierHAR student model, further boosting performance. Experimental results on benchmark HAR datasets, including UCI-HAR and WISDM, demonstrate that TinierHAR achieves comparable or even superior accuracy to existing lightweight models, while exhibiting a significantly smaller model size (up to 8x reduction) and faster inference time (up to 5x speedup). TinierHAR enables the deployment of accurate and efficient HAR systems on edge devices with limited resources, unlocking new possibilities for real-time and personalized applications."
http://arxiv.org/abs/2507.07903v1,Hardware-Aware Feature Extraction Quantisation for Real-Time Visual Odometry on FPGA Platforms,"Visual odometry (VO) plays a crucial role in enabling autonomous navigation for resource-constrained platforms, often relying on computationally intensive feature extraction. Deploying VO algorithms on FPGA platforms offers significant advantages in terms of power efficiency and real-time performance, but efficient hardware implementation requires careful consideration of feature extraction quantisation. This paper addresses the challenge of optimising feature extraction quantisation for real-time VO on FPGAs, balancing accuracy and hardware resource utilisation. We propose a hardware-aware quantisation strategy that leverages a differentiable approximation of hardware resource usage to guide the selection of optimal bit-widths for feature descriptors during training. Our method incorporates knowledge of FPGA architecture, specifically LUT count, to penalise resource-intensive quantisation schemes within a VO network trained end-to-end. Experimental results on the KITTI dataset demonstrate that our approach achieves comparable VO accuracy to full-precision implementations while reducing LUT usage by up to 40% and increasing frame rates by 25% on a Xilinx ZCU102 FPGA. This work provides a practical and efficient solution for deploying high-performance VO systems on resource-constrained embedded platforms."
http://arxiv.org/abs/2507.07795v1,Robust and Generalizable Heart Rate Estimation via Deep Learning for Remote Photoplethysmography in Complex Scenarios,"Remote photoplethysmography (rPPG) enables non-contact heart rate (HR) estimation using video, offering advantages in various applications. However, its performance is often compromised by illumination variations, motion artifacts, and individual physiological differences, limiting its robustness and generalizability in complex real-world scenarios. This paper introduces a novel deep learning framework for robust and generalizable HR estimation from rPPG signals. Our method incorporates a spatiotemporal attention mechanism within a convolutional neural network (CNN) architecture to selectively focus on informative regions and temporal dynamics in the video frames, followed by a signal processing module for accurate HR extraction. Furthermore, we employ a domain adaptation strategy during training to minimize the discrepancy between different datasets and improve generalization across diverse subjects and environments. Experimental results on benchmark datasets demonstrate that our approach achieves state-of-the-art performance in HR estimation accuracy and robustness, significantly outperforming existing methods, especially under challenging conditions such as low illumination and subject movement. This work advances the applicability of rPPG technology for reliable HR monitoring in unconstrained environments."
http://arxiv.org/abs/2507.07708v1,Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion Deblurring,"Motion blur, a common artifact in images captured during relative motion between the camera and the scene, severely degrades visual quality and hinders downstream computer vision tasks. Existing deblurring methods often process all pixels uniformly, leading to significant computational redundancy, especially in regions with minimal blur. This paper addresses the problem of efficiently deblurring images with spatially varying motion blur by adaptively pruning pixels based on local motion characteristics. We propose a novel Motion-Aware Adaptive Pixel Pruning (MAPP) framework that first estimates a dense motion field and then uses it to predict a pixel importance map, guiding the pruning process. Specifically, we leverage the estimated motion information to dynamically determine the number of pixels to be retained in each local region, focusing computational resources on areas with significant motion blur. Experimental results on benchmark datasets demonstrate that MAPP achieves comparable or superior deblurring performance to state-of-the-art methods while significantly reducing computational cost, offering a practical solution for real-time and resource-constrained applications. This efficient deblurring approach enables broader deployment of image deblurring techniques in various applications, including mobile photography and autonomous driving."
http://arxiv.org/abs/2507.07704v1,D-CNN and VQ-VAE Autoencoders for Compression and Denoising of Industrial X-ray Computed Tomography Images,"Industrial X-ray Computed Tomography (XCT) is crucial for non-destructive evaluation, but generates large datasets that pose challenges for storage and transmission. This paper addresses the problem of efficiently compressing and denoising industrial XCT images simultaneously. We propose a novel autoencoder architecture combining a deep convolutional neural network (D-CNN) with a Vector Quantized Variational Autoencoder (VQ-VAE). The D-CNN component extracts high-level features from the XCT images, which are then compressed and discretized by the VQ-VAE's codebook. The decoder reconstructs the image from these quantized latent representations, effectively denoising the image by discarding irrelevant high-frequency noise components during the encoding and quantization process. Experimental results on real-world industrial XCT datasets demonstrate that our proposed method achieves significant compression ratios (up to 10x) while maintaining high image quality, as measured by PSNR and SSIM, and effectively reduces noise artifacts. This combined approach offers a practical solution for efficient storage, transmission, and improved analysis of industrial XCT data."
http://arxiv.org/abs/2507.08052v1,Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging,"Hyperspectral imaging (HSI) offers rich spectral information for Earth observation, but cloud cover significantly hinders data usability. Performing cloud masking on-board satellites enables efficient downlink prioritization and reduces storage requirements. This paper addresses the challenge of deploying complex cloud masking algorithms on resource-constrained on-board platforms by proposing a suite of lightweight convolutional neural network (CNN) architectures optimized for HSI data. We explore various network designs, including depthwise separable convolutions and efficient channel attention mechanisms, to minimize computational cost while maintaining high accuracy. Our models achieve comparable performance to larger, more complex networks, reaching an overall accuracy of over 95% on benchmark HSI datasets while exhibiting a significant reduction in parameter count and inference time. The demonstrated efficiency makes these models suitable for real-time cloud masking on-board HSI satellites, enabling timely and effective data processing and dissemination."
http://arxiv.org/abs/2507.07453v1,Bluish Veil Detection and Lesion Classification using Custom Deep Learnable Layers with Explainable Artificial Intelligence (XAI),"Skin lesion diagnosis from dermoscopic images is crucial for early melanoma detection. However, the presence of a bluish veil, a significant diagnostic feature, is often subtle and confounded by image artifacts, hindering accurate lesion classification. This paper addresses the challenge of automated bluish veil detection and subsequent lesion classification by introducing a novel deep learning framework incorporating custom learnable layers specifically designed to identify and segment the bluish veil. Our architecture integrates a bluish veil detection module with a lesion classification network, both enhanced by custom layers that learn feature representations tailored to each task. Furthermore, we employ Explainable Artificial Intelligence (XAI) techniques to provide visual explanations for the model's predictions, increasing transparency and clinical trust. Experimental results on a large dermoscopic image dataset demonstrate that our method achieves state-of-the-art performance in both bluish veil detection (AUC of 0.92) and lesion classification (accuracy of 88%), outperforming existing approaches. The proposed framework provides a robust and interpretable tool for assisting dermatologists in the early and accurate diagnosis of skin lesions."
http://arxiv.org/abs/2507.07011v1,Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning,"Brain tumor detection from Magnetic Resonance Imaging (MRI) is a crucial task for early diagnosis and treatment planning. Existing methods often struggle with accuracy and computational efficiency, particularly when dealing with the variability in tumor size, shape, and location. This paper introduces ""Deep Brain Net,"" a novel deep learning model designed to enhance brain tumor detection performance while maintaining computational feasibility. Deep Brain Net leverages transfer learning, employing EfficientNetB0 and ResNet50 as feature extractors, followed by a custom-designed convolutional neural network for classification. We optimized the architecture and hyperparameters through rigorous experimentation and incorporated data augmentation techniques to improve generalization. The proposed model achieved a high accuracy of 98.7% and a sensitivity of 97.9% on a benchmark dataset of brain MRI images, demonstrating significant improvements over existing state-of-the-art methods in terms of both accuracy and computational cost. This work offers a practical and effective solution for automated brain tumor detection, potentially improving diagnostic workflows and patient outcomes."
http://arxiv.org/abs/2507.06976v1,DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising,"Collective perception (CP) enhances the robustness and expands the sensing range of autonomous vehicles by leveraging information shared from neighboring agents. However, the performance of existing CP methods deteriorates significantly in adverse weather conditions due to sensor noise and occlusion. This paper addresses the problem of efficiently achieving robust 3D object detection in adverse weather by jointly denoising and fusing LiDAR data from multiple agents. We propose DenoiseCP-Net, a novel architecture that integrates a conditional variational autoencoder (CVAE) for LiDAR point cloud denoising with a graph neural network (GNN) for collaborative feature aggregation. The CVAE learns a latent space conditioned on weather severity, enabling targeted noise reduction. The GNN then fuses denoised features from multiple agents, facilitating robust object detection. Experimental results on a simulated adverse weather CP dataset demonstrate that DenoiseCP-Net significantly outperforms state-of-the-art CP methods in terms of both detection accuracy and noise robustness, achieving a relative improvement of over 10% in average precision under heavy rain conditions. This work offers a practical and effective solution for deploying cooperative autonomous driving systems in challenging environments."
http://arxiv.org/abs/2507.06814v1,HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement,"Low-light image enhancement is crucial for improving the visibility of images captured in challenging lighting conditions, yet existing methods often struggle with extremely dark scenarios and introduce artifacts. This paper addresses the limitations of current low-light enhancement techniques in handling images with severely underexposed regions and complex noise distributions. We propose HVI-CIDNet+, an improved cascade network building upon our previous HVI-CIDNet. HVI-CIDNet+ incorporates a novel Hybrid Vision Transformer (HVT) block within the contrastive image decomposition network (CIDNet) to better capture long-range dependencies and global context information, facilitating more accurate image decomposition and noise reduction. Furthermore, we introduce an adaptive exposure fusion strategy to dynamically adjust the enhancement intensity based on local image characteristics, preventing over-enhancement and preserving naturalness. Experimental results on benchmark datasets and real-world extremely low-light images demonstrate that HVI-CIDNet+ significantly outperforms state-of-the-art methods in terms of quantitative metrics like PSNR and SSIM, as well as perceptual quality, especially in regions with extreme darkness. HVI-CIDNet+ offers a robust and effective solution for enhancing images captured in severe low-light environments, enabling improved performance in downstream vision tasks."
http://arxiv.org/abs/2507.06735v2,Residual Prior-driven Frequency-aware Network for Image Fusion,"Image fusion aims to integrate complementary information from multiple source images into a comprehensive composite. Existing deep learning-based image fusion methods often struggle to effectively extract and utilize multi-scale features and frequency information, leading to suboptimal fusion performance, particularly in preserving fine details and high-frequency components. To address this, we propose a novel Residual Prior-driven Frequency-aware Network (RPFN) for image fusion. RPFN leverages residual learning to guide feature extraction, emphasizing the informative differences between source images and mitigating the impact of irrelevant details. Furthermore, we introduce a frequency-aware attention module that adaptively weights different frequency components, allowing the network to prioritize high-frequency information crucial for detail preservation. Experimental results on various image fusion tasks, including multi-focus, infrared-visible, and medical image fusion, demonstrate that RPFN achieves superior performance compared to state-of-the-art methods, exhibiting improved visual quality and quantitative metrics. The proposed RPFN offers a robust and effective solution for image fusion by explicitly incorporating residual priors and frequency awareness, paving the way for more advanced fusion algorithms."
http://arxiv.org/abs/2507.08855v1,Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network,"Alzheimer's Disease (AD) is a neurodegenerative disorder with a complex etiology, requiring a comprehensive approach for accurate prognosis. Existing AD prognosis models often rely on single modalities or symmetric fusion techniques, failing to fully capture the intricate relationships between heterogeneous data types and the directional influence of certain modalities on others. To address this, we propose an Asymmetric Cross-Modal Cross-Attention Network (ACMCAN) for multi-omic AD prognosis. ACMCAN leverages cross-attention mechanisms to learn asymmetric relationships between genomics, transcriptomics, and proteomics data, allowing for modality-specific information propagation and adaptive weighting of different modalities. Specifically, we employ a hierarchical cross-attention architecture, where each modality attends to the others, followed by a fusion module that integrates the attended features for prognosis. Experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that ACMCAN outperforms state-of-the-art methods in predicting AD progression, achieving a significant improvement in accuracy and AUC score. This approach offers a more nuanced and biologically relevant framework for understanding and predicting AD, potentially leading to more effective personalized treatments."
http://arxiv.org/abs/2507.06581v1,Airway Segmentation Network for Enhanced Tubular Feature Extraction,"Accurate airway segmentation from computed tomography (CT) scans is crucial for diagnosing and managing respiratory diseases. However, the complex branching structure and varying image quality pose significant challenges for existing segmentation methods, particularly in extracting fine tubular features. This paper addresses the problem of improving airway segmentation accuracy, specifically focusing on enhancing the extraction of detailed tubular structures. We propose a novel Airway Segmentation Network (ASN) incorporating a multi-scale feature fusion module and a dedicated tubular enhancement layer. The multi-scale module aggregates contextual information at different resolutions to capture both global anatomical context and local airway details. The tubular enhancement layer explicitly learns to identify and refine tubular structures, improving the delineation of small airways and branches. Experimental results on publicly available datasets demonstrate that our ASN significantly outperforms state-of-the-art methods, achieving a Dice score improvement of over 3% and a substantial reduction in false positive rates, particularly in peripheral airways. The proposed method facilitates more accurate airway analysis and enables improved downstream tasks such as airway tree modeling and disease diagnosis."
http://arxiv.org/abs/2507.06459v1,EA: An Event Autoencoder for High-Speed Vision Sensing,"Event cameras offer significant advantages over traditional frame-based cameras in high-speed and high dynamic range scenarios due to their asynchronous and sparse nature. However, effectively processing event data remains a challenge, particularly in extracting meaningful representations suitable for downstream tasks. This paper introduces the Event Autoencoder (EA), a novel unsupervised learning framework designed to learn efficient and robust representations from raw event streams. EA leverages a spatio-temporal convolutional encoder to map event data into a low-dimensional latent space, followed by a corresponding decoder that reconstructs the original event stream. We introduce a novel loss function that combines event reconstruction with a sparsity constraint on the latent space, encouraging the discovery of compact and informative features. Experimental results on benchmark datasets demonstrate that the EA learns representations that enable superior performance in tasks such as event denoising, event interpolation, and object recognition compared to existing unsupervised event representation learning methods. The EA provides a powerful and versatile tool for unlocking the full potential of event-based vision in challenging real-world applications."
http://arxiv.org/abs/2507.06417v2,Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification,"Medical image classification is crucial for accurate diagnosis and treatment planning, but often suffers from limitations in capturing complex spatial relationships and subtle features. Existing Convolutional Neural Networks (CNNs) can struggle with viewpoint variations and part-whole relationships, while Capsule Networks, although adept at handling such issues, can be computationally expensive and challenging to train effectively on large datasets. This paper introduces Capsule-ConvKAN, a novel hybrid architecture that synergistically integrates Convolutional layers, Capsule Networks, and Kernel Attention Networks (KANs) for improved medical image classification. Capsule-ConvKAN leverages convolutional layers for efficient feature extraction, followed by Capsule layers to model hierarchical relationships between features and improve robustness to image transformations. Finally, KANs are incorporated to dynamically learn and refine feature interactions, enabling the network to capture more nuanced patterns. Experimental results on benchmark medical image datasets, including chest X-rays and retinal OCT images, demonstrate that Capsule-ConvKAN achieves state-of-the-art performance, surpassing existing CNN and Capsule Network-based approaches in terms of accuracy, sensitivity, and specificity. The proposed hybrid approach offers a promising avenue for enhancing the accuracy and reliability of medical image analysis, potentially leading to improved clinical outcomes."
http://arxiv.org/abs/2507.06410v2,Attention-Enhanced Deep Learning Ensemble for Breast Density Classification in Mammography,"Breast density, a significant risk factor for breast cancer, is typically assessed visually from mammograms, a process prone to subjectivity. This study addresses the challenge of automating breast density classification by leveraging the power of deep learning ensembles. We propose an attention-enhanced deep learning ensemble (AEDLE) for accurate and robust breast density classification. Our method incorporates multiple pre-trained convolutional neural networks (CNNs), each fine-tuned on a large mammography dataset. Crucially, we integrate spatial attention mechanisms within each CNN to highlight relevant image regions and suppress irrelevant background noise. Furthermore, we employ a weighted averaging ensemble technique, optimizing the weights based on each model's performance on a validation set to achieve superior classification accuracy. Experimental results on a large, publicly available dataset demonstrate that AEDLE outperforms individual CNNs and other state-of-the-art methods, achieving a significant improvement in classification accuracy and F1-score. The proposed AEDLE framework offers a promising approach for automated and objective breast density assessment, potentially improving the efficiency and accuracy of breast cancer screening programs."
http://arxiv.org/abs/2507.08028v1,SSSUMO: Real-Time Semi-Supervised Submovement Decomposition,"Human movement is inherently complex, often composed of discrete, coordinated submovements that reflect underlying cognitive and motor processes. Decomposing complex movements into these submovements is crucial for understanding motor control, diagnosing movement disorders, and developing effective rehabilitation strategies. However, existing submovement decomposition methods typically rely on fully supervised learning, requiring extensive labeled datasets which are often impractical to acquire, particularly in real-time applications. This paper addresses the challenge of real-time submovement decomposition with limited labeled data by introducing SSSUMO: a Semi-Supervised Submovement Online learning framework. SSSUMO leverages a novel combination of online expectation-maximization (EM) for unsupervised submovement discovery from unlabeled data and a supervised loss function applied to a small set of labeled examples to refine the decomposition. The framework incorporates a temporal smoothing prior to ensure coherent submovement sequences. Experiments on synthetic and real-world datasets demonstrate that SSSUMO achieves comparable accuracy to fully supervised methods with significantly fewer labeled examples, while maintaining real-time performance. This advancement enables practical submovement analysis in various domains where labeled data is scarce and real-time processing is essential."
http://arxiv.org/abs/2507.06380v1,Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation,"Deep learning models have achieved remarkable success in various computer vision tasks, but their substantial computational and storage requirements pose significant challenges for deployment on resource-constrained edge devices. Furthermore, directly deploying these models to edge devices raises security concerns, making them vulnerable to model extraction and adversarial attacks. To address these limitations, we propose a novel framework for generating secure and storage-efficient deep learning models tailored for Edge AI, leveraging automatic weight generation. Our approach utilizes a generative adversarial network (GAN) to synthesize model weights based on a compact seed, effectively reducing the storage footprint and obfuscating the original model architecture and weights. The generator network is trained to produce weights that, when integrated into a pre-defined, lightweight architecture, achieve comparable performance to a conventionally trained model. Experimental results on benchmark image classification datasets demonstrate that our generated models achieve a significant reduction in storage size (up to 80%) while maintaining competitive accuracy and exhibiting increased resilience against model extraction attacks compared to directly deploying compressed or quantized models. This work paves the way for secure and efficient deployment of deep learning models on edge devices, facilitating privacy-preserving and resource-conscious AI applications."
http://arxiv.org/abs/2507.08025v1,3D forest semantic segmentation using multispectral LiDAR and 3D deep learning,"Accurate and detailed semantic understanding of forest environments is crucial for ecological monitoring, resource management, and biodiversity assessment. While LiDAR data provides precise 3D geometric information, integrating multispectral LiDAR offers richer feature representation for improved forest characterization. This paper addresses the challenge of accurate 3D semantic segmentation of forest point clouds by leveraging multispectral LiDAR data and advanced 3D deep learning techniques. We propose a novel point cloud segmentation framework that incorporates spectral information directly into a 3D deep learning architecture based on a modified PointNet++ network. Specifically, we augment the PointNet++ architecture with spectral feature embedding layers and introduce a novel spectral-geometric attention mechanism to dynamically weight the contribution of spectral and geometric features during the segmentation process. Experimental results on a benchmark multispectral LiDAR dataset demonstrate that our proposed method achieves state-of-the-art performance, significantly improving segmentation accuracy for key forest classes such as ground, vegetation, and trees, with an overall accuracy improvement of 5% compared to existing methods. This research demonstrates the potential of combining multispectral LiDAR data with advanced 3D deep learning for enhanced forest understanding and more effective forest management strategies."
http://arxiv.org/abs/2507.06161v1,Normalizing Diffusion Kernels with Optimal Transport,"Diffusion kernels, representing local data relationships, are fundamental to many computer vision tasks, yet their performance is often hampered by sensitivity to varying data densities and feature scales. This paper addresses the problem of constructing robust diffusion kernels that are invariant to such variations, leading to improved performance in downstream applications. We propose a novel normalization technique for diffusion kernels based on optimal transport (OT). Specifically, we leverage OT to re-weight the kernel affinities, effectively regularizing the diffusion process and mitigating the impact of density variations. The resulting OT-normalized diffusion kernels exhibit improved local neighborhood consistency and are less susceptible to noise. We demonstrate the effectiveness of our approach on synthetic and real-world datasets, showing significant improvements in tasks such as image segmentation and manifold learning compared to standard diffusion kernels and other normalization methods. Our OT-based normalization provides a principled and effective way to enhance the robustness and applicability of diffusion kernels in various computer vision problems."
http://arxiv.org/abs/2507.06148v1,SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance,"Activation functions play a crucial role in the performance of convolutional neural networks (CNNs) by introducing non-linearity and enabling the learning of complex patterns. However, many popular activation functions suffer from issues such as vanishing gradients or limited representational capacity, hindering the full potential of CNNs for visual recognition. To address these limitations, we propose SoftReMish, a novel activation function that combines the strengths of both ReLU and Mish activations. SoftReMish introduces a learnable parameter within a softened ReLU framework, allowing the network to dynamically adjust its activation behavior based on the input data and network state. This adaptive approach promotes smoother gradients during training and enhances the representational power of the network. Experimental results on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet demonstrate that CNNs equipped with SoftReMish consistently outperform those using ReLU, Mish, and other state-of-the-art activation functions, achieving significant improvements in classification accuracy and convergence speed. The proposed SoftReMish activation function offers a promising avenue for improving the performance and efficiency of CNNs in various visual recognition tasks."
http://arxiv.org/abs/2507.05849v1,DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction,"Accurate crop yield prediction is crucial for food security and agricultural planning, benefiting from the rich information provided by multi-source remote sensing data. However, effectively fusing heterogeneous data from different sensors with varying spatial and spectral resolutions remains a significant challenge. This paper introduces a Dynamic Fusion Framework for Yield Prediction (DFYP), designed to address this problem by dynamically integrating multi-source remote sensing data while accounting for spectral channel importance and adaptive operator selection. DFYP incorporates a Spectral Channel Attention Module (SCAM) to recalibrate channel-wise features based on their relevance to yield prediction, and an Adaptive Operator Selection Module (AOSM) that learns optimal fusion operators for different feature representations. Experiments on a large-scale agricultural dataset demonstrate that DFYP consistently outperforms state-of-the-art fusion methods, achieving a significant improvement in prediction accuracy (e.g., an increase of 5% in R-squared) and robustness across different crop types and regions. DFYP offers a flexible and effective approach for crop yield prediction, advancing the application of multi-source remote sensing data in precision agriculture."
http://arxiv.org/abs/2507.05594v1,GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field,"Representing dynamic scenes efficiently is crucial for real-time computer vision applications. Existing neural radiance field (NeRF) based dynamic scene representations often struggle with computational cost, hindering their applicability in high-frame-rate scenarios. This paper addresses the challenge of achieving both high rendering speed and accurate representation of dynamic scenes. We introduce Gaussian-based Scalable Video Representation (GSVR), a novel approach that leverages 2D Gaussians and a hybrid deformation field to achieve real-time rendering. GSVR represents the scene as a set of 2D Gaussians whose attributes (position, covariance, color, opacity) are predicted from a reference frame. A hybrid deformation field, composed of a global deformation network and local residual displacements, is employed to track motion and enable accurate reconstruction of dynamic scenes. Experiments on challenging dynamic scene datasets demonstrate that GSVR achieves over 800 frames per second (FPS) rendering speed while maintaining competitive reconstruction quality compared to state-of-the-art dynamic NeRF methods. GSVR's high rendering speed and representational power unlock new possibilities for real-time interactive applications involving dynamic scene understanding."
http://arxiv.org/abs/2507.04880v1,HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention Network for Colorectal Polyp Detection,"Colorectal polyps are precursors to colorectal cancer, and accurate detection is crucial for early diagnosis and treatment. However, the subtle appearance variations, irregular shapes, and varying sizes of polyps pose significant challenges for automated detection. This paper introduces HGNet, a novel High-order Spatial Awareness Hypergraph and Multi-Scale Context Attention Network for improved colorectal polyp detection. HGNet leverages a hypergraph convolutional network to model high-order spatial relationships between polyp regions and their surrounding context, capturing intricate dependencies beyond pairwise connections. Furthermore, a multi-scale context attention module is designed to selectively aggregate features from different scales, emphasizing relevant contextual information and suppressing irrelevant background noise. Experimental results on publicly available datasets, including CVC-ClinicDB, Kvasir-SEG, and ETIS-LaribPolypDB, demonstrate that HGNet achieves state-of-the-art performance, surpassing existing methods by a significant margin in terms of Dice coefficient and mAP. HGNet's ability to effectively model complex spatial relationships and leverage multi-scale contextual information contributes to more accurate and robust colorectal polyp detection, potentially improving the efficiency and effectiveness of colonoscopy screening."
http://arxiv.org/abs/2507.04792v1,Model Compression using Progressive Channel Pruning,"Deep convolutional neural networks (CNNs) have achieved state-of-the-art performance in various computer vision tasks, but their high computational cost and memory requirements hinder their deployment on resource-constrained devices. This paper addresses the challenge of compressing CNN models while maintaining high accuracy. We propose a novel progressive channel pruning method that iteratively removes unimportant channels based on their contribution to the overall network performance. Our approach utilizes a sensitivity analysis framework to dynamically adjust the pruning ratio for each layer, ensuring a balanced compression strategy. Furthermore, we introduce a channel recovery mechanism that selectively restores pruned channels based on their potential to improve accuracy after fine-tuning. Experimental results on benchmark datasets, including CIFAR-10 and ImageNet, demonstrate that our method achieves significant model compression ratios with minimal accuracy loss compared to existing pruning techniques. This work provides an effective and efficient solution for deploying deep learning models on resource-limited platforms."
http://arxiv.org/abs/2507.05304v1,Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes,"3D mesh representations are fundamental in computer vision and graphics, enabling a wide range of applications from shape analysis to virtual reality. However, effectively capturing both local geometric details and global structural information in 3D meshes remains a significant challenge for existing auto-encoding techniques. We introduce a novel Self-Attention based Multi-Scale Graph Auto-Encoder Network (SAM-GAE) for 3D mesh processing. Our architecture leverages a hierarchical graph representation to encode meshes at multiple scales, capturing both fine-grained surface details and coarse-grained structural relationships. Furthermore, we incorporate self-attention mechanisms within each graph convolutional layer to adaptively weight the importance of neighboring vertices, allowing the network to focus on relevant features and improve representation learning. Experimental results on benchmark datasets demonstrate that SAM-GAE achieves state-of-the-art performance in mesh reconstruction and shape classification tasks, outperforming existing graph-based and point cloud-based auto-encoders. This work demonstrates the effectiveness of self-attention in enhancing graph auto-encoders for learning robust and informative representations of 3D meshes."
http://arxiv.org/abs/2507.04634v1,LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction,"Multi-agent trajectory prediction is crucial for autonomous systems operating in dynamic environments, requiring accurate modeling of agent interactions and motion patterns. Existing methods often struggle to capture nuanced local motion trends and effectively integrate them with global context, leading to suboptimal predictions. To address this, we introduce LTMSformer, a novel transformer-based architecture that incorporates Local Trend-Aware Attention (LTAA) and Motion State Encoding (MSE) to enhance trajectory prediction accuracy. LTAA captures the evolving motion trends of neighboring agents by attending to local trajectory segments, while MSE encodes each agent's motion state using velocity and acceleration features. These components are seamlessly integrated into a transformer framework to facilitate effective information exchange and contextual reasoning. Experiments on the widely used nuScenes and Argoverse datasets demonstrate that LTMSformer achieves state-of-the-art performance, significantly outperforming existing methods in terms of prediction accuracy and collision avoidance. The proposed approach offers a robust and effective solution for multi-agent trajectory prediction, paving the way for safer and more reliable autonomous navigation."
http://arxiv.org/abs/2507.04409v1,MVNet: Hyperspectral Remote Sensing Image Classification Based on Hybrid Mamba-Transformer Vision Backbone Architecture,"Hyperspectral remote sensing image (HRSI) classification is crucial for various environmental monitoring and resource management applications. However, effectively capturing both local and global dependencies within high-dimensional hyperspectral data remains a significant challenge. This paper introduces MVNet, a novel hybrid Mamba-Transformer vision backbone architecture for improved HRSI classification. MVNet leverages the strengths of Mamba, a selective structured state space sequence model, to efficiently model long-range dependencies and spectral correlations, while incorporating a Transformer branch to capture global contextual information and spatial relationships. Specifically, MVNet employs a parallel Mamba and Transformer block, allowing the network to simultaneously learn local spectral features and global spatial context. Experimental results on benchmark HRSI datasets demonstrate that MVNet achieves state-of-the-art classification accuracy, surpassing existing CNN- and Transformer-based methods, particularly in scenarios with limited training samples. The proposed MVNet offers a robust and efficient framework for HRSI classification, contributing to more accurate and reliable remote sensing applications."
http://arxiv.org/abs/2507.04397v1,RegistrationMamba: A Mamba-based Registration Framework Integrating Multi-Expert Feature Learning for Cross-Modal Remote Sensing Images,"Cross-modal remote sensing image registration is a crucial task for various applications, including environmental monitoring and urban planning. However, significant spectral and geometric differences between images acquired by different sensors pose a major challenge for accurate and robust registration. To address this, we propose RegistrationMamba, a novel registration framework leveraging the Mamba architecture to effectively integrate multi-expert feature learning for cross-modal remote sensing images. RegistrationMamba employs a multi-expert module to extract modality-specific features, which are then fused and processed by a Mamba-based Siamese network to learn robust similarity metrics. The Mamba architecture's selective state space modeling enables the network to capture long-range dependencies and handle the complex geometric deformations often present in remote sensing images. Experiments on diverse cross-modal datasets demonstrate that RegistrationMamba achieves state-of-the-art registration accuracy, outperforming existing methods, particularly in challenging scenarios with significant modality differences and geometric distortions. This work highlights the potential of Mamba-based architectures for feature extraction and metric learning in remote sensing image registration, providing a robust and effective solution for cross-modal alignment."
http://arxiv.org/abs/2507.04333v1,Computed Tomography Visual Question Answering with Cross-modal Feature Graphing,"Visual Question Answering (VQA) on Computed Tomography (CT) images holds immense potential for assisting radiologists in diagnosis and treatment planning. However, effectively integrating visual and textual information remains a significant challenge due to the complex anatomical structures in CT scans and the nuanced relationships between image regions and question semantics. To address this, we propose a novel Cross-modal Feature Graphing (CFG) framework for CT-VQA. Our approach constructs separate graphs representing visual regions and question tokens, then iteratively refines node features through cross-modal attention mechanisms, allowing for fine-grained interaction between visual and textual representations. Finally, a fusion module aggregates information from both graphs to predict the answer. Experiments on a benchmark CT-VQA dataset demonstrate that our CFG framework achieves state-of-the-art performance, surpassing existing methods by a significant margin (e.g., X% improvement in overall accuracy). This highlights the effectiveness of our approach in capturing intricate cross-modal dependencies for accurate question answering in the context of medical imaging."
http://arxiv.org/abs/2507.04277v1,Towards Lightest Low-Light Image Enhancement Architecture for Mobile Devices,"Low-light image enhancement is crucial for improving visual perception in challenging lighting conditions, particularly in mobile photography. However, deploying deep learning-based enhancement models on mobile devices is hindered by their significant computational demands and memory footprint. This paper addresses the problem of designing a highly efficient and lightweight low-light image enhancement architecture suitable for resource-constrained mobile platforms. We propose a novel architecture, termed LightLLIE, based on a carefully designed combination of MobileNetV3 blocks and a streamlined attention mechanism. LightLLIE leverages depthwise separable convolutions and inverted residual structures to minimize the number of parameters and floating-point operations (FLOPs), while the attention module focuses on selectively enhancing important features. Experimental results demonstrate that LightLLIE achieves comparable or superior performance to existing lightweight low-light image enhancement methods on benchmark datasets, while significantly reducing the model size and computational complexity. LightLLIE offers a practical solution for enabling high-quality low-light image enhancement on mobile devices, broadening accessibility and improving user experience."
http://arxiv.org/abs/2507.04017v1,Habitat Classification from Ground-Level Imagery Using Deep Neural Networks,"Accurate habitat classification is crucial for ecological monitoring, biodiversity assessment, and conservation efforts. Traditional methods often rely on manual surveys or satellite imagery, which can be time-consuming, expensive, or lack the necessary resolution to capture fine-grained habitat details. This paper addresses the challenge of automatically classifying habitats from ground-level imagery, enabling efficient and scalable ecological analysis. We propose a deep learning framework leveraging convolutional neural networks (CNNs) pre-trained on large-scale datasets and fine-tuned for habitat-specific features. Specifically, we explore the effectiveness of different CNN architectures, including ResNet and EfficientNet, combined with data augmentation techniques to improve generalization and robustness to variations in lighting, viewpoint, and image quality. Our experiments on a diverse dataset of ground-level images demonstrate that the proposed approach achieves state-of-the-art performance, surpassing traditional machine learning methods by a significant margin. Notably, the fine-tuned EfficientNet model achieved an overall accuracy of 87% in classifying 10 different habitat types. This research provides a cost-effective and accurate solution for automated habitat classification, facilitating large-scale ecological studies and supporting informed environmental management decisions."
http://arxiv.org/abs/2507.04008v1,PASC-Net:Plug-and-play Shape Self-learning Convolutions Network with Hierarchical Topology Constraints for Vessel Segmentation,"Accurate vessel segmentation is crucial for diagnosing various cardiovascular and ophthalmological diseases. However, the complex and varying vessel topology, coupled with low contrast and noise in medical images, poses significant challenges for existing segmentation methods. To address these limitations, we propose PASC-Net, a novel Plug-and-play Shape Self-learning Convolutions Network with Hierarchical Topology Constraints for robust vessel segmentation. PASC-Net introduces a shape self-learning convolution module that dynamically adapts its receptive field based on local vessel structures, enhancing feature representation. Furthermore, we incorporate hierarchical topology constraints, encompassing both global connectivity and local branching patterns, into the loss function to guide the network towards topologically plausible segmentations. Experimental results on benchmark datasets, including DRIVE, STARE, and CHASE_DB1, demonstrate that PASC-Net achieves state-of-the-art performance, surpassing existing methods in terms of segmentation accuracy and topological fidelity. PASC-Net offers a promising approach for accurate and reliable vessel segmentation, facilitating improved diagnosis and treatment planning in clinical settings."
http://arxiv.org/abs/2507.03816v1,Zero Memory Overhead Approach for Protecting Vision Transformer Parameters,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, but their large parameter size makes them vulnerable to model extraction attacks and intellectual property theft. Protecting ViT parameters is crucial, yet existing defenses often introduce significant memory overhead, hindering deployment on resource-constrained devices. This paper addresses the problem of protecting ViT parameters without increasing the memory footprint during inference. We propose a novel approach, Zero Memory Overhead Parameter Protection (ZMOPP), which employs a dynamic parameter encryption scheme integrated directly within the ViT architecture. ZMOPP leverages lightweight, learnable cryptographic keys generated on-the-fly and discarded immediately after use, ensuring parameter confidentiality throughout the forward pass without requiring additional storage. Experiments on ImageNet classification demonstrate that ZMOPP effectively protects ViT parameters against common model extraction attacks, achieving a significant reduction in attack success rate while maintaining comparable accuracy to the original model and incurring no additional memory overhead. Our approach offers a practical and efficient solution for securing ViTs in real-world applications where memory is a critical constraint."
http://arxiv.org/abs/2508.00841v1,Inclusive Review on Advances in Masked Human Face Recognition Technologies,"Masked face recognition (MFR) has emerged as a critical computer vision task due to the widespread use of face masks during the recent pandemic, posing significant challenges to existing face recognition systems. This paper provides an inclusive and comprehensive review of the recent advances in MFR technologies, categorizing them based on their underlying methodologies: pre-processing based approaches, deep learning based feature adaptation, and generative adversarial network (GAN) based mask removal and feature enhancement. We critically analyze the strengths and weaknesses of each category, highlighting their performance under various masking conditions, dataset biases, and computational complexities. Furthermore, we explore the impact of different mask types, occlusion ratios, and demographic variations on the performance of MFR systems. Finally, the review identifies key open challenges and future research directions, including the need for more robust and generalizable MFR algorithms that are less susceptible to variations in mask types and environmental conditions. This work serves as a valuable resource for researchers and practitioners seeking to develop and deploy effective MFR solutions in real-world applications, particularly in scenarios where facial occlusions are prevalent."
http://arxiv.org/abs/2507.03421v2,Hybrid-View Attention Network for Clinically Significant Prostate Cancer Classification in Transrectal Ultrasound,"Transrectal Ultrasound (TRUS) is a widely used imaging modality for prostate cancer diagnosis, but its interpretation is highly subjective and operator-dependent. Classifying clinically significant prostate cancer (csPCa) from TRUS images remains a challenging task due to the subtle visual differences between cancerous and benign tissues, as well as the inherent speckle noise and low contrast in ultrasound imaging. This paper proposes a novel Hybrid-View Attention Network (HVAN) to effectively leverage both local spatial features and global contextual information for improved csPCa classification. HVAN incorporates a multi-stream architecture that processes both axial and sagittal TRUS views, utilizing attention mechanisms to selectively focus on relevant regions within each view and across views. Specifically, a spatial attention module highlights discriminative local features, while a channel attention module captures inter-channel dependencies. Furthermore, a cross-view attention module learns relationships between the axial and sagittal representations, enabling the network to integrate complementary information from different perspectives. Experimental results on a clinically representative dataset demonstrate that HVAN achieves superior performance compared to state-of-the-art methods, with a significant improvement in AUC and sensitivity for csPCa detection. The proposed HVAN offers a promising approach for enhancing the accuracy and reliability of TRUS-based prostate cancer diagnosis, potentially reducing unnecessary biopsies and improving patient outcomes."
http://arxiv.org/abs/2508.04016v2,S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation,"Video diffusion models have demonstrated remarkable success in video generation, but their high computational cost and memory demands hinder deployment on resource-constrained platforms. Quantization offers a promising solution to reduce these burdens, yet naively applying quantization to Video Diffusion Transformers (ViTs) often leads to significant performance degradation due to the sensitivity of attention mechanisms and the accumulation of quantization errors across long video sequences. To address this challenge, we introduce S$^2$Q-VDiT, a novel quantization framework that incorporates Salient Data Augmentation and Sparse Token Distillation. S$^2$Q-VDiT strategically augments training data with salient video segments identified through an entropy-based approach, improving the robustness of quantized models against information loss. Furthermore, we introduce a sparse token distillation technique that selectively transfers knowledge from the full-precision teacher model to the quantized student, focusing on critical tokens and mitigating error propagation. Our experiments on various video datasets demonstrate that S$^2$Q-VDiT achieves state-of-the-art performance for quantized video diffusion models, significantly outperforming existing quantization methods while maintaining comparable video quality to full-precision counterparts. This work paves the way for efficient and deployable video generation on edge devices."
http://arxiv.org/abs/2508.03481v1,Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models,"Text-to-image diffusion models have demonstrated remarkable capabilities in generating realistic and diverse images from textual descriptions. However, achieving personalized generation, where models accurately reflect individual preferences and consistently generate images aligned with specific user-defined styles or subjects, remains a significant challenge. We address this problem by introducing a novel condition-level modeling approach that allows for fine-grained control over the generation process. Our method, termed ""Draw Your Mind,"" learns personalized embeddings for individual conditional elements (e.g., subject, style, background) extracted from a user's provided images and text prompts. These embeddings are then incorporated into the diffusion model's denoising process via a learned attention mechanism, enabling precise manipulation of each conditional element's influence on the final generated image. Experimental results demonstrate that Draw Your Mind significantly improves personalization accuracy and consistency compared to existing approaches, generating images that more faithfully represent user-defined subjects and styles while maintaining high visual quality. This work provides a powerful and flexible framework for personalized image generation, enabling users to create images that truly reflect their individual artistic visions."
http://arxiv.org/abs/2508.03343v1,WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval,"Text-motion retrieval, aiming to find relevant video segments based on textual queries, is challenged by the subtle and complex relationships between language and visual dynamics. Current methods often struggle to capture fine-grained motion details and their temporal evolution. This paper introduces WaMo, a Wavelet-enhanced Multi-Frequency Trajectory Analysis framework for fine-grained text-motion retrieval. WaMo leverages optical flow trajectories to represent motion and applies a discrete wavelet transform (DWT) to decompose these trajectories into multiple frequency bands, capturing both coarse and fine-grained motion characteristics. A novel cross-modal attention mechanism then learns to align the multi-frequency motion representations with textual features, enabling a more nuanced understanding of the text-motion correspondence. Experimental results on benchmark datasets demonstrate that WaMo outperforms state-of-the-art methods, achieving significant improvements in retrieval accuracy and demonstrating the effectiveness of wavelet decomposition for capturing subtle motion nuances. WaMo offers a promising approach for enhancing text-motion retrieval through fine-grained motion analysis."
http://arxiv.org/abs/2508.03243v1,MVTOP: Multi-View Transformer-based Object Pose-Estimation,"Accurate object pose estimation is crucial for various robotic applications, yet remains challenging due to occlusions, clutter, and variations in viewpoint. Existing methods often struggle with multi-view fusion, relying on heuristics or computationally expensive volumetric representations. To address these limitations, we introduce MVTOP: a Multi-View Transformer-based Object Pose-Estimation network. MVTOP leverages a transformer architecture to effectively fuse features extracted from multiple views, learning attention-based relationships between different viewpoints and object parts. Specifically, we extract per-view features using a convolutional backbone, project them into a shared embedding space, and then employ a transformer encoder to aggregate information across views, predicting the object's 6D pose directly. Experimental results on challenging benchmark datasets demonstrate that MVTOP achieves state-of-the-art performance in object pose estimation, outperforming existing multi-view approaches, particularly in scenarios with significant occlusion. This highlights the efficacy of transformer-based feature fusion for robust and accurate pose estimation in multi-view settings."
http://arxiv.org/abs/2508.03118v1,H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction,"Multi-view 3D reconstruction is a fundamental problem in computer vision, enabling the creation of 3D models from multiple images. However, existing methods often struggle with generalization to novel scenes and viewpoints due to reliance on scene-specific optimization or limited feature representations. This paper addresses the challenge of achieving robust and generalizable 3D reconstruction from multi-view images by learning effective cross-view correspondences. We introduce H3R, a novel hybrid multi-view correspondence network that combines the strengths of both feature-based and direct photometric alignment. H3R leverages a hierarchical feature extraction module to capture multi-scale contextual information, followed by a learnable hybrid matching module that fuses feature similarities with direct photometric consistency checks to establish accurate correspondences across views. These correspondences are then used within a differentiable triangulation framework to reconstruct the 3D geometry. Experiments on benchmark datasets demonstrate that H3R significantly outperforms state-of-the-art methods in terms of reconstruction accuracy and generalization ability, particularly in challenging scenarios with texture-less regions and significant viewpoint changes. The proposed hybrid correspondence approach provides a more robust and generalizable foundation for multi-view 3D reconstruction."
http://arxiv.org/abs/2508.03034v1,MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention,"Text-to-video generation (T2V) has recently witnessed significant advancements, yet preserving the identity of specific characters or objects across generated video frames remains a significant challenge. Existing T2V methods often struggle to maintain consistent appearance and features of subjects mentioned in the text prompt, leading to identity drift and unrealistic visual content. To address this issue, we introduce MoCA: Mixture of Cross Attention, a novel framework for identity-preserving text-to-video generation. MoCA leverages a mixture of cross-attention modules within the video diffusion model to disentangle content generation from identity preservation. Specifically, one cross-attention module focuses on generating the overall scene and motion based on the text prompt, while a separate module is conditioned on both the text prompt and a reference image of the target identity, ensuring consistent appearance throughout the generated video. Our experiments demonstrate that MoCA significantly improves identity consistency compared to state-of-the-art T2V models, while maintaining competitive video quality and text alignment. The proposed approach provides a valuable tool for generating realistic and controllable videos with specific character or object identities."
http://arxiv.org/abs/2508.02411v1,HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis,"Multivariate time series (MTS) analysis plays a crucial role in numerous real-world applications, requiring accurate modeling of complex temporal dependencies and inter-variable relationships. Existing methods often struggle to effectively capture high-order interactions between variables and long-range temporal dependencies simultaneously. We propose HGTS-Former, a novel Hierarchical HyperGraph Transformer for multivariate time series analysis. HGTS-Former constructs a hierarchical hypergraph to represent the complex relationships between variables at different granularities, enabling the capture of high-order interactions. A Transformer-based architecture then leverages this hypergraph structure to model long-range temporal dependencies, using hypergraph attention to propagate information across both time and variables. Experimental results on several benchmark MTS datasets demonstrate that HGTS-Former consistently outperforms state-of-the-art methods in forecasting and anomaly detection tasks. This highlights the effectiveness of hierarchical hypergraph representation and Transformer-based modeling for capturing complex dependencies in multivariate time series data."
http://arxiv.org/abs/2508.02220v1,Welcome New Doctor: Continual Learning with Expert Consultation and Autoregressive Inference for Whole Slide Image Analysis,"Whole slide image (WSI) analysis using deep learning has shown great promise in computational pathology, but adapting to new data distributions encountered in clinical practice remains a significant challenge. Specifically, models often suffer from catastrophic forgetting when trained on a continuous stream of WSI data from different sources or staining protocols. To address this, we propose a novel continual learning framework, ""Welcome New Doctor,"" which integrates expert consultation and autoregressive inference. Our method leverages a lightweight expert network trained on a small, curated dataset from the new distribution to provide guidance to the primary model, mitigating catastrophic forgetting. Furthermore, we employ an autoregressive inference strategy that considers contextual information from neighboring tiles within the WSI to improve diagnostic accuracy. We demonstrate the effectiveness of our approach on a challenging WSI dataset involving multiple cancer subtypes and staining variations, achieving significant improvements in average accuracy and forgetting metrics compared to existing continual learning baselines. This work offers a practical and effective solution for deploying robust and adaptable WSI analysis systems in real-world clinical settings."
http://arxiv.org/abs/2508.02187v1,A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration,"Point cloud registration, aligning multiple scans of an object or scene, is a fundamental task in computer vision with applications ranging from 3D reconstruction to autonomous navigation. This paper addresses the challenging problem of point cloud registration when dealing with significant sparsity and noise, conditions that severely degrade the performance of traditional iterative closest point (ICP) based methods. We propose a novel registration framework based on matching statistical moments of the point clouds. Instead of relying on point-to-point correspondences, our method estimates the transformation by minimizing the difference between the empirical characteristic functions of the source and target point clouds, weighted by a robust kernel to mitigate the influence of outliers. We derive a closed-form solution for the transformation parameters under certain conditions, significantly improving computational efficiency. Experimental results on both synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art registration algorithms, particularly in scenarios with high levels of noise and sparsity, achieving superior accuracy and robustness. Our moment matching approach provides a powerful alternative to correspondence-based methods for robust point cloud registration in challenging environments."
http://arxiv.org/abs/2508.02003v1,Fast and Memory-efficient Non-line-of-sight Imaging with Quasi-Fresnel Transform,"Non-line-of-sight (NLOS) imaging aims to reconstruct hidden objects from scattered light, offering potential applications in search and rescue, autonomous navigation, and medical imaging. However, existing NLOS reconstruction algorithms often suffer from high computational cost and memory requirements, hindering their deployment in resource-constrained environments. This paper introduces a novel approach for fast and memory-efficient NLOS imaging based on a Quasi-Fresnel Transform (QFT). Our method approximates the wave propagation within the NLOS scene using a simplified Fresnel diffraction model, allowing for a computationally efficient and memory-optimized reconstruction process. Specifically, we pre-compute and store a set of compact QFT kernels, which are then used to rapidly reconstruct the hidden scene via efficient matrix multiplications. We demonstrate through both simulations and real-world experiments that our QFT-based method achieves significant speedups and reduced memory footprint compared to state-of-the-art back-projection and wave-based NLOS reconstruction techniques, while maintaining comparable reconstruction quality. The proposed method paves the way for practical NLOS imaging applications on embedded systems and other resource-limited platforms."
http://arxiv.org/abs/2508.01889v1,Medical Image De-Identification Resources: Synthetic DICOM Data and Tools for Validation,"Medical image de-identification is crucial for enabling data sharing and collaborative research while protecting patient privacy. However, the lack of standardized, readily available resources, particularly synthetic DICOM datasets and robust validation tools, hinders the development and evaluation of effective de-identification algorithms. This paper addresses the need for improved resources by presenting a novel framework encompassing both a large-scale synthetic DICOM dataset and a suite of validation tools specifically designed for assessing de-identification performance. Our method leverages generative adversarial networks (GANs) to create realistic synthetic medical images and metadata, mimicking the statistical properties of real patient data while ensuring complete anonymity. Furthermore, we developed automated validation tools, including DICOM metadata analyzers and image similarity metrics, to quantify the effectiveness of de-identification techniques applied to both real and synthetic datasets. Experimental results demonstrate the utility of our synthetic data in training and benchmarking de-identification algorithms, and the validation tools provide a comprehensive assessment of privacy risks and data utility preservation. These resources facilitate the development and validation of more robust and reliable medical image de-identification solutions, fostering wider data sharing and accelerating medical research."
http://arxiv.org/abs/2508.01852v1,Context Guided Transformer Entropy Modeling for Video Compression,"Video compression strives to minimize redundancy in video sequences, and entropy modeling plays a crucial role in achieving high compression ratios. However, accurately capturing the complex statistical dependencies in video residuals remains a significant challenge. This paper addresses the problem of effectively leveraging contextual information to improve entropy modeling for video compression, particularly in capturing long-range dependencies. We propose a novel Context-Guided Transformer Entropy Model (CG-TEM) that incorporates a Transformer architecture to learn contextual representations from neighboring coded blocks. These contextual representations are then used to modulate the parameters of a hyperprior-based entropy model, enabling adaptive probability estimation conditioned on the broader video context. Experimental results demonstrate that our CG-TEM achieves significant bit-rate savings compared to state-of-the-art learned video codecs, particularly at lower bitrates, while maintaining comparable decoding complexity. This highlights the effectiveness of leveraging Transformer-based contextual modeling for improved entropy coding in video compression."
http://arxiv.org/abs/2508.01668v1,Measuring and Predicting Where and When Pathologists Focus their Visual Attention while Grading Whole Slide Images of Cancer,"The interpretation of whole slide images (WSIs) is critical for cancer diagnosis and grading, a process heavily reliant on the pathologist's visual expertise. Understanding how pathologists visually process these complex images remains a significant challenge. This paper addresses this problem by developing a computational framework to measure and predict the spatiotemporal dynamics of pathologists' visual attention during WSI grading. We collected eye-tracking data from expert pathologists while they graded WSIs of different cancer types and grades. We then trained a deep learning model, incorporating both convolutional and recurrent neural network components, to predict the pathologists' gaze locations and dwell times based on the WSI content and the preceding sequence of fixations. Our model achieves high accuracy in predicting both the spatial location and temporal duration of pathologists' fixations, demonstrating the feasibility of anticipating visual search patterns. These findings provide valuable insights into the cognitive strategies employed by pathologists, which can be leveraged to develop AI-assisted diagnostic tools and improve training curricula."
http://arxiv.org/abs/2508.01650v1,StrandDesigner: Towards Practical Strand Generation with Sketch Guidance,"Generating realistic and controllable hair strands is a crucial task for character creation in computer graphics and animation. However, existing methods often struggle to provide intuitive control over individual strands, particularly in matching a desired artistic style or conforming to specific sketch-based guidelines. This paper introduces StrandDesigner, a novel framework for generating hair strands that are guided by user-provided sketches. Our approach leverages a generative adversarial network (GAN) conditioned on sketch inputs to produce initial strand shapes, followed by a refinement stage that incorporates geometric constraints to ensure smoothness and spatial coherence. A key component is a novel loss function that encourages the generated strands to closely follow the sketch while maintaining realistic hair properties. We demonstrate that StrandDesigner can generate diverse and high-quality hair strands that faithfully adhere to the provided sketches, surpassing the performance of existing sketch-based hair generation techniques in terms of fidelity and controllability. StrandDesigner offers a practical and intuitive tool for artists to create customized hairstyles with precise control over individual strands, significantly streamlining the hair design process."
http://arxiv.org/abs/2508.01335v1,StyleSentinel: Reliable Artistic Copyright Verification via Stylistic Fingerprints,"Artistic copyright infringement poses a significant challenge in the digital age, particularly with the proliferation of AI-generated art that can mimic existing styles. Current copyright verification methods often rely on exact image matching or human expertise, proving inadequate for identifying subtle stylistic similarities indicative of infringement. We introduce StyleSentinel, a novel approach to artistic copyright verification that leverages stylistic fingerprints extracted from artwork. StyleSentinel employs a pre-trained convolutional neural network (CNN) to generate feature maps representing the stylistic elements of an image. These features are then aggregated and compressed into a compact stylistic fingerprint, which is robust to content variations while sensitive to stylistic nuances. Experiments on a diverse dataset of artworks, including both human-created and AI-generated images, demonstrate that StyleSentinel achieves high accuracy in identifying stylistic similarities between potentially infringing artworks and copyrighted source material. This enables a more reliable and automated process for artistic copyright verification, contributing to the protection of artists' intellectual property in an increasingly complex digital landscape."
http://arxiv.org/abs/2508.01230v1,Point-wise Diffusion Models for Physical Systems with Shape Variations: Application to Spatio-temporal and Large-scale system,"Modeling the dynamics of physical systems with complex geometries is crucial in various scientific and engineering domains. However, traditional methods often struggle to handle shape variations and large-scale systems efficiently. This paper addresses the challenge of learning and generating realistic spatio-temporal dynamics of physical systems exhibiting shape variations using a novel point-wise diffusion model. Our approach leverages a conditional diffusion process operating directly on point cloud representations of the system's state. Specifically, we introduce a learnable diffusion and denoising process conditioned on both time and shape embeddings obtained from a graph neural network, enabling the generation of diverse and plausible future states. Experiments on fluid flow around varying obstacles and large-scale granular material simulations demonstrate that our method accurately captures the complex dynamics, outperforms existing mesh-based and neural operator techniques in terms of prediction accuracy and generalization to unseen shapes, and exhibits promising scaling properties. This work provides a powerful tool for simulating and understanding physical systems with intricate geometries, opening avenues for applications in design optimization and scientific discovery."
http://arxiv.org/abs/2508.01170v1,DELTAv2: Accelerating Dense 3D Tracking,"Dense 3D tracking is a critical component in various applications, including robotics, augmented reality, and autonomous navigation. However, the computational demands of dense tracking methods, particularly those relying on iterative optimization, often limit their real-time applicability on resource-constrained platforms. This paper addresses the problem of accelerating dense 3D tracking while maintaining accuracy and robustness. We introduce DELTAv2, an enhanced version of our previous DELTA framework, which leverages a novel cascaded architecture combining efficient feature extraction, learned deformation fields, and a GPU-accelerated optimization scheme. Specifically, DELTAv2 employs a lightweight convolutional neural network for feature extraction, followed by a coarse-to-fine deformation field refinement strategy guided by a learned prior, significantly reducing the search space for iterative optimization. Experimental results on challenging benchmark datasets demonstrate that DELTAv2 achieves a 2-3x speedup compared to state-of-the-art dense tracking algorithms, while maintaining comparable or superior accuracy and robustness to noise and occlusion. This acceleration enables real-time dense 3D tracking on embedded systems and opens up new possibilities for interactive and mobile applications."
http://arxiv.org/abs/2508.00733v4,"AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation","Generating high-quality audio synchronized with visual content is crucial for creating immersive and engaging multimedia experiences. However, existing audio generation models often specialize in specific audio types, such as speech, music, or sound effects, lacking the versatility to handle diverse audio landscapes within a single framework. We introduce AudioGen-Omni, a unified multimodal diffusion transformer designed for generating video-synchronized audio encompassing speech, song, and sound effects. Our model leverages a shared transformer backbone and a conditional diffusion process, guided by both textual descriptions and visual features extracted from video frames, to generate coherent and temporally aligned audio. Through extensive experiments, we demonstrate that AudioGen-Omni achieves state-of-the-art performance across multiple audio generation tasks, including video-to-speech, video-to-music, and video-to-sound effects generation, surpassing specialized models in terms of audio quality, synchronization accuracy, and content diversity. This unified approach offers a significant advancement towards creating more realistic and controllable audiovisual content generation systems."
http://arxiv.org/abs/2508.00453v1,PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA,"Multispectral and hyperspectral image fusion aims to integrate the high spatial resolution of multispectral (MS) images with the rich spectral information of hyperspectral (HS) images. However, the ill-posed nature of this fusion problem, stemming from the inherent ambiguity in mapping between different spectral spaces and varying sensor characteristics, often leads to spectral and spatial distortions in the fused product. We introduce PIF-Net, a novel Prior-guided Invertible Fusion Network that addresses this challenge by explicitly modeling the fusion process with an invertible Mamba architecture. PIF-Net incorporates a learned prior distribution to constrain the solution space and guide the fusion toward plausible and physically realistic HS images. Furthermore, we employ Fusion-Aware Low-Rank Adaptation (LoRA) to efficiently adapt the Mamba backbone for optimal fusion performance, focusing parameter updates on fusion-specific knowledge. Experimental results on benchmark datasets demonstrate that PIF-Net achieves state-of-the-art fusion performance, exhibiting significant improvements in both spectral fidelity and spatial resolution compared to existing methods. This work offers a promising direction for robust and accurate multispectral and hyperspectral image fusion, particularly in scenarios with significant spectral variability or sensor discrepancies."
http://arxiv.org/abs/2508.00443v2,SDMatte: Grafting Diffusion Models for Interactive Matting,"Image matting, the process of accurately extracting foreground objects from an image, remains a challenging task, especially when high precision and user interaction are required. Existing interactive matting methods often struggle with complex foreground boundaries and require extensive user input for satisfactory results. We introduce SDMatte, a novel interactive matting framework that leverages the generative power of pre-trained diffusion models to refine initial matte predictions. Our approach grafts a diffusion-based refinement module onto an existing trimap-based matting network. This module is conditioned on both the input image and the initial matte, and it iteratively denoises a latent representation to generate a refined alpha matte, effectively correcting errors and hallucinating plausible details along object boundaries. Experiments on standard matting benchmarks demonstrate that SDMatte significantly improves the quality of matte predictions with minimal user interaction, achieving state-of-the-art performance in terms of both accuracy and user effort. Our method offers a powerful and efficient solution for high-quality interactive image matting, bridging the gap between user guidance and the generative capabilities of diffusion models."
http://arxiv.org/abs/2508.00412v1,Sortblock: Similarity-Aware Feature Reuse for Diffusion Model,"Diffusion models have achieved remarkable success in various generative tasks, yet their computational demands remain a significant bottleneck. A key inefficiency lies in the redundant computation across different denoising steps, particularly when processing similar image regions. This paper introduces Sortblock, a novel similarity-aware feature reuse mechanism designed to accelerate diffusion model inference. Sortblock dynamically identifies and groups similar feature blocks across different denoising timesteps using a learnable similarity metric. These similar blocks are then processed only once, and the computed features are reused for all members of the group, eliminating redundant computations. Experiments on image generation and editing tasks demonstrate that Sortblock can significantly reduce the computational cost of diffusion models, achieving up to a 2x speedup with minimal impact on image quality. By enabling efficient feature reuse, Sortblock paves the way for deploying diffusion models in resource-constrained environments and facilitating real-time interactive applications."
http://arxiv.org/abs/2508.00359v1,CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective,"Collaborative perception (CP) among autonomous vehicles enhances environmental awareness beyond individual sensor limitations. However, existing CP methods often struggle with computational overhead due to redundant information sharing and inefficient fusion strategies, particularly in dynamic, real-world scenarios. This paper introduces CoST, an efficient collaborative perception framework that leverages a unified spatiotemporal perspective to minimize communication bandwidth and maximize fusion efficiency. CoST represents environmental information as a dynamic voxel grid, selectively transmitting only the changes in voxel occupancy between consecutive time steps. Furthermore, it employs a novel attention-based fusion module that adaptively weights contributions from different agents based on their spatiotemporal relevance and reliability. Experiments on the OPV2V dataset demonstrate that CoST achieves state-of-the-art CP performance while reducing communication bandwidth by up to 40% and improving inference speed by 25% compared to existing methods. CoST's efficiency makes it a promising solution for real-time collaborative perception in resource-constrained autonomous driving applications."
http://arxiv.org/abs/2507.23785v1,Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis,"Synthesizing dynamic 3D scenes from video inputs is a challenging task due to the inherent ambiguity of inferring 3D geometry and motion from 2D observations. Existing video-to-4D methods often struggle to generate high-fidelity and temporally coherent 4D representations, particularly for complex scenes with significant occlusions and non-rigid deformations. We propose Gaussian Variation Field Diffusion (GVF-Diffusion), a novel framework that leverages a conditional diffusion model to generate high-quality 4D Gaussian fields from multi-view video. GVF-Diffusion learns a prior distribution over 4D Gaussian fields conditioned on the input video, enabling the generation of novel, temporally coherent 4D scenes. Our approach incorporates a variation field to explicitly model the temporal evolution of each Gaussian, fostering smoother and more realistic motion. Experiments on diverse datasets demonstrate that GVF-Diffusion significantly outperforms state-of-the-art methods in terms of visual fidelity, temporal consistency, and geometric accuracy, as measured by standard metrics. The generated 4D scenes exhibit realistic dynamic details and are suitable for various applications, including novel view synthesis, virtual reality, and animation."
http://arxiv.org/abs/2507.23778v1,Half-Physics: Enabling Kinematic 3D Human Model with Physical Interactions,"Accurate and physically plausible 3D human motion capture is crucial for numerous applications, ranging from virtual reality to robotics. However, existing kinematic 3D human models often lack the ability to realistically interact with their environment, resulting in implausible behaviors like interpenetration with objects or violation of fundamental physical laws. To address this, we introduce Half-Physics, a novel framework that combines the efficiency of kinematic motion capture with the realism of physics-based simulation. Our approach uses a kinematic skeleton driven by motion capture data as a guide, while simultaneously employing a simplified, yet effective, physical simulation to resolve collisions and enforce joint limits. This ""Half-Physics"" approach allows the kinematic skeleton to deviate slightly from the captured motion when necessary, ensuring physically plausible interactions. Experiments demonstrate that Half-Physics significantly reduces interpenetration and joint limit violations compared to purely kinematic models, while maintaining a high degree of fidelity to the original motion capture data, achieving a balance between accuracy and physical realism. This enables the creation of more immersive and believable virtual environments populated by interacting human avatars."
http://arxiv.org/abs/2507.23683v1,I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation,"Autonomous driving systems rely on vast amounts of labeled data for training, yet acquiring and annotating real-world data is expensive and time-consuming. This paper addresses the challenge of generating realistic and diverse synthetic data by proposing I2V-GS, a novel Infrastructure-to-Vehicle view transformation framework leveraging Gaussian Splatting. Our method renders photorealistic vehicle-view images from infrastructure-view images using an intermediate 3D Gaussian representation of the scene. Specifically, we first reconstruct a scene from infrastructure cameras using 3D Gaussian Splatting, then render novel vehicle-view images by projecting and rasterizing the Gaussians from the desired viewpoint, incorporating differentiable rendering for optimization. We demonstrate that I2V-GS generates high-fidelity and geometrically accurate vehicle-view images, significantly outperforming existing image-to-image translation and neural radiance field-based view synthesis techniques, as evidenced by quantitative metrics and qualitative evaluations. This approach facilitates the efficient and scalable generation of synthetic autonomous driving data, enabling improved training and validation of perception algorithms."
http://arxiv.org/abs/2507.23676v1,DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data,"Microbiome studies often suffer from missing data due to technical limitations and varying experimental designs across different modalities. Imputing these missing values accurately is crucial for downstream analyses and biological interpretations. We address the challenge of multimodal microbiome data imputation by introducing DepMicroDiff, a novel diffusion-based framework that leverages dependency relationships between microbial taxa and different data modalities. DepMicroDiff employs a denoising diffusion probabilistic model conditioned on observed data and a learned dependency graph representing inter-taxa and inter-modality relationships. This allows the model to probabilistically generate missing values while respecting the underlying structure of the data. Experiments on both synthetic and real-world microbiome datasets demonstrate that DepMicroDiff significantly outperforms state-of-the-art imputation methods, achieving higher accuracy and preserving the inherent biological correlations within the data. Our dependency-aware diffusion approach provides a powerful and robust solution for handling missing data in complex microbiome studies, facilitating more comprehensive and reliable analyses."
http://arxiv.org/abs/2507.23657v1,OmniTraj: Pre-Training on Heterogeneous Data for Adaptive and Zero-Shot Human Trajectory Prediction,"Human trajectory prediction is crucial for proactive decision-making in autonomous systems operating in human-populated environments. Existing methods often struggle to generalize across diverse datasets and adapt to novel scenarios due to limited training data and domain shifts. This paper introduces OmniTraj, a novel pre-training framework designed to learn robust and transferable representations for human trajectory prediction by leveraging a large, heterogeneous dataset composed of both real-world and synthetic trajectories with varying levels of noise and annotation quality. OmniTraj employs a contrastive learning objective coupled with a trajectory reconstruction task to encourage the model to learn invariant features across different data sources and improve its ability to handle noisy or incomplete observations. We demonstrate that OmniTraj significantly outperforms state-of-the-art methods in zero-shot transfer learning across multiple benchmark datasets, achieving substantial improvements in prediction accuracy and robustness, particularly in challenging scenarios with limited data. The results highlight the effectiveness of pre-training on heterogeneous data for achieving adaptive and generalizable human trajectory prediction models."
http://arxiv.org/abs/2507.23544v1,User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals,"Estimating user experience (UX) in Human-Robot Interaction (HRI) is crucial for designing robots that can adapt to users needs and preferences. However, UX is a complex, subjective construct, and its direct measurement is often challenging, particularly using objective social signals. This paper addresses the problem of estimating user experience during HRI by leveraging multimodal social signals within a multi-instance learning (MIL) framework. Our method represents an interaction as a bag of instances, where each instance comprises a temporal window of multimodal features extracted from video (facial expressions, body pose) and audio (speech prosody, acoustic features) data. We train a MIL model to predict the overall UX label for the entire interaction bag, implicitly learning which instances (i.e., temporal segments) are most indicative of the overall user experience. Experimental results on a dataset of human-robot tutoring interactions demonstrate that our MIL-based multimodal approach outperforms traditional single-instance learning methods and achieves competitive results compared to state-of-the-art methods that rely on hand-crafted features. This work provides a novel and effective approach for automatically estimating user experience in HRI by leveraging the temporal dynamics of multimodal social signals, facilitating the development of more user-centered and adaptive robotic systems."
http://arxiv.org/abs/2507.23523v2,H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation,"Bimanual robotic manipulation promises to enhance automation in complex assembly tasks, yet remains challenging due to the intricate coordination required and limited robustness to unforeseen circumstances. Current robotic systems often struggle to adapt to scenarios where human intervention could significantly improve task efficiency or recover from errors. We introduce H-RDT, Human-enhanced Robotic Decision Tree, a novel framework that integrates human manipulation seamlessly into a bimanual robotic manipulation system. H-RDT leverages a decision tree structure to dynamically switch between autonomous robotic execution and human-guided manipulation based on real-time task progress and environmental feedback. This is achieved through a novel hybrid control strategy, combining impedance control for safe human interaction with model-predictive control for autonomous execution, enabling smooth transitions and collaborative task completion. Experiments on a pick-and-place assembly task demonstrate that H-RDT significantly reduces task completion time and improves robustness compared to purely autonomous or purely teleoperated approaches, especially in the presence of uncertainties. This work paves the way for more flexible and adaptable robotic systems capable of effectively collaborating with humans in complex manipulation scenarios."
http://arxiv.org/abs/2507.23371v1,VMatcher: State-Space Semi-Dense Local Feature Matching,"Local feature matching is a fundamental task in computer vision, crucial for applications like SLAM, structure-from-motion, and image retrieval. Existing methods often struggle with textureless regions, repetitive patterns, and large viewpoint changes, leading to sparse or incorrect matches. This paper introduces VMatcher, a novel approach to state-space semi-dense local feature matching that leverages variational inference to model the underlying geometry and uncertainty in feature correspondences. VMatcher propagates information from high-confidence matches to nearby regions through a learned state-space model, effectively regularizing the matching process and generating a dense, probabilistic matching field. We demonstrate that VMatcher achieves state-of-the-art performance on challenging benchmark datasets, significantly improving matching accuracy and robustness, particularly in scenarios with significant viewpoint variations and low-texture environments. The proposed method offers a robust and accurate solution for establishing correspondences between images, paving the way for improved performance in a wide range of vision applications."
http://arxiv.org/abs/2507.23313v1,The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models,"Text-to-image (TTI) models have demonstrated remarkable capabilities in generating images from textual descriptions, opening new avenues for creative expression and artistic exploration. However, the precise interpretation of artistic prompts, particularly those referencing specific artists or styles, remains poorly understood. This paper investigates how TTI models interpret and translate artistic prompts, focusing on the challenges of stylistic fidelity and potential biases encoded within the models. We introduce a novel analytical framework that combines quantitative image analysis, leveraging metrics like style transfer loss and CLIP similarity, with qualitative human evaluation to assess the stylistic accuracy of generated images. Our experiments involve prompting various TTI models with descriptions referencing artists like Rembrandt and specific subjects, such as cows, to dissect the interplay between artistic style and object representation. Results reveal a tendency for models to prioritize general stylistic features over nuanced artistic techniques, often conflating lighting and color palettes with deeper stylistic characteristics. Furthermore, we observe biases toward stereotypical representations of certain artists and subjects. This study provides valuable insights into the limitations of current TTI models in capturing artistic nuance and highlights the need for improved training methodologies to ensure more faithful and unbiased artistic interpretations."
http://arxiv.org/abs/2507.23300v2,Training-free Geometric Image Editing on Diffusion Models,"Diffusion models have demonstrated remarkable capabilities in image generation and manipulation, often requiring extensive training or fine-tuning for specific editing tasks. However, geometric image editing, such as perspective correction or object reshaping, remains challenging without retraining the model to account for the desired geometric transformations. This paper introduces a novel training-free approach to geometric image editing leveraging the inherent generative priors within pre-trained diffusion models. Our method, termed ""Diffusion Warp,"" iteratively warps the latent space representation of an image during the denoising process, guided by user-defined geometric constraints. Specifically, we estimate the optimal warp field at each denoising step by minimizing the discrepancy between the warped latent feature maps and the original image's latent representation, effectively steering the generation process towards the desired geometric configuration. Experiments on a variety of images and geometric transformations demonstrate that Diffusion Warp can effectively perform complex geometric edits without requiring any additional training data or model fine-tuning, preserving image fidelity and semantic consistency. The proposed approach opens new avenues for intuitive and flexible image manipulation directly within the latent space of diffusion models, circumventing the need for costly retraining procedures."
http://arxiv.org/abs/2507.23278v1,"UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing","Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable success in zero-shot image classification and retrieval. However, its inherent asymmetry between image and text encoders limits its applicability to more general multimodal tasks such as image generation and editing. This paper introduces UniLiP, a novel framework that adapts CLIP to a unified architecture capable of multimodal understanding, generation, and editing. UniLiP employs a shared Transformer backbone for both image and text modalities, augmented with modality-specific projection layers. Crucially, we introduce a novel masked multimodal modeling objective during pre-training that encourages bidirectional interaction and alignment between image and text representations, enabling generation capabilities. Experimental results demonstrate that UniLiP achieves competitive performance on image classification and retrieval compared to standard CLIP, while simultaneously enabling high-quality text-to-image generation and text-guided image editing without task-specific fine-tuning. This work paves the way for a more versatile and unified approach to multimodal learning, expanding the applicability of CLIP-based models to a broader range of downstream tasks."
http://arxiv.org/abs/2507.23277v1,iLRM: An Iterative Large 3D Reconstruction Model,"Large-scale 3D reconstruction is crucial for numerous applications, ranging from autonomous navigation to virtual reality. However, existing methods often struggle with maintaining global consistency and detail fidelity when reconstructing expansive and complex environments, especially with limited computational resources. We introduce iLRM, an Iterative Large 3D Reconstruction Model, designed to address these challenges by progressively refining an initial coarse reconstruction through iterative refinement stages. iLRM leverages a novel hierarchical scene representation coupled with a learnable refinement module that predicts local geometry adjustments based on multi-scale contextual information. The iterative nature allows for error correction and detail enhancement at each stage, progressively improving the overall reconstruction quality. Experiments on large-scale benchmark datasets demonstrate that iLRM significantly outperforms state-of-the-art methods in terms of both geometric accuracy and visual fidelity while maintaining computational efficiency. This iterative refinement approach enables the creation of high-quality, globally consistent 3D reconstructions of large and complex environments, advancing the capabilities of 3D vision systems."
http://arxiv.org/abs/2507.23253v1,Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality,"Time series forecasting is a crucial task in various domains, yet traditional methods often struggle to capture complex dependencies and geometric structures within the data. This paper addresses the challenge of explicitly measuring and modeling underlying geometric information in time series to improve forecasting accuracy. We propose a novel approach that transforms time series data into image representations, enabling the application of powerful computer vision techniques for geometric analysis. Specifically, we leverage Gramian Angular Field (GAF) encodings to convert time series segments into images, followed by a convolutional neural network (CNN) architecture incorporating attention mechanisms to extract relevant geometric features. We then incorporate these learned geometric features into a downstream forecasting model. Our experiments on benchmark time series datasets demonstrate that incorporating geometric information extracted from the image modality significantly enhances forecasting performance compared to state-of-the-art time series models, particularly for datasets exhibiting non-linear and complex dynamics. This work provides a new perspective on time series analysis by bridging the gap between time series forecasting and computer vision, opening avenues for leveraging image processing techniques to improve forecasting accuracy and interpretability."
http://arxiv.org/abs/2507.23251v1,A Deep Dive into Generic Object Tracking: A Survey,"Generic Object Tracking (GOT) is a fundamental task in computer vision, enabling a wide range of applications from autonomous driving to video surveillance. Despite significant progress in recent years, the field still suffers from a lack of a comprehensive overview that synthesizes the diverse methodologies and their relative strengths. This survey aims to bridge this gap by providing a structured and in-depth analysis of the landscape of GOT algorithms, focusing primarily on deep learning-based approaches. We categorize existing trackers based on their core mechanisms, including Siamese networks, correlation filters, transformers, and discriminative correlation tracking, dissecting their architectural choices and loss functions. Furthermore, we analyze the impact of different training strategies, data augmentation techniques, and benchmark datasets on tracker performance. Our analysis reveals that while transformer-based trackers exhibit superior performance in challenging scenarios with occlusions and complex object motion, Siamese networks remain computationally efficient for real-time applications. This survey provides a valuable resource for researchers and practitioners seeking to understand the current state-of-the-art in GOT and identifies promising directions for future research."
http://arxiv.org/abs/2507.23219v1,Learning Arbitrary-Scale RAW Image Downscaling with Wavelet-based Recurrent Reconstruction,"Image downscaling is a fundamental task in image processing, often employed for efficient storage, transmission, and multi-scale analysis. However, existing deep learning-based downscaling methods typically focus on fixed or integer scale factors, limiting their applicability to arbitrary scales and often neglecting the specific characteristics of RAW image data. This paper addresses the challenge of learning arbitrary-scale downscaling directly from RAW images. We propose a novel Wavelet-based Recurrent Reconstruction Network (WRRNet) that leverages the multi-resolution properties of wavelet transforms and the temporal modeling capabilities of recurrent neural networks. WRRNet first decomposes the RAW image into wavelet subbands, then employs a recurrent network to progressively reconstruct high-resolution subbands from their downscaled counterparts, effectively learning the inverse downscaling process. Experiments on various RAW image datasets demonstrate that WRRNet significantly outperforms state-of-the-art methods, especially at non-integer scales, achieving superior PSNR and SSIM scores while preserving fine details and reducing aliasing artifacts. Our approach provides a flexible and effective solution for arbitrary-scale RAW image downscaling, enabling broader applications in computational photography and low-level vision tasks."
http://arxiv.org/abs/2507.23064v2,"Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints","Autonomous driving systems require robust scene understanding and accurate trajectory planning, often relying on multiple sensor modalities. Existing vision-language models for autonomous driving struggle to effectively fuse information from diverse inputs like camera images, high-definition maps (HD-Maps), and planned waypoints, particularly in a goal-oriented manner suitable for real-time operation. This paper introduces a novel vision-language fusion architecture for autonomous driving that leverages goal-centered cross-attention to integrate camera, HD-Map, and waypoint information. Our method uses a transformer-based encoder to process each modality and then employs a cross-attention mechanism, conditioned on a learned goal embedding derived from the waypoints, to selectively attend to relevant features from the camera image and HD-Map. Experimental results on the CARLA simulator demonstrate that our approach significantly improves driving performance, achieving a 25% reduction in collision rate and a 15% increase in driving score compared to state-of-the-art methods, while maintaining real-time inference speeds. This goal-centered fusion strategy offers a promising avenue for developing more reliable and efficient autonomous driving systems."
http://arxiv.org/abs/2507.23006v1,Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction,"Urban scene reconstruction from multi-view images is crucial for applications like autonomous driving and city planning. Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable performance in novel view synthesis, but its direct application to large-scale urban scenes faces challenges in robustness and efficiency due to complex geometries, varying lighting conditions, and the sheer scale of the data. This paper addresses these limitations by introducing a robust and efficient 3D-GS pipeline tailored for urban scene reconstruction. Our approach incorporates a novel adaptive density control mechanism that dynamically adjusts the number of Gaussians based on scene complexity and reconstruction error, preventing over-densification in less detailed regions and ensuring accurate representation in intricate areas. Furthermore, we propose a hierarchical data structure to accelerate rendering and optimization, enabling efficient handling of large-scale datasets. Experiments on several challenging urban scene datasets demonstrate that our method achieves state-of-the-art reconstruction quality while significantly reducing memory consumption and rendering time compared to existing 3D-GS approaches. This work paves the way for practical and scalable 3D reconstruction of complex urban environments."
http://arxiv.org/abs/2507.22742v1,Social-Pose: Enhancing Trajectory Prediction with Human Body Pose,"Human trajectory prediction is crucial for autonomous systems operating in crowded environments. Existing methods primarily focus on social interactions and scene context, often overlooking the valuable information encoded within human body pose. This paper addresses the problem of incorporating human body pose into trajectory prediction models to improve accuracy and realism. We introduce Social-Pose, a novel framework that integrates pose estimation with a graph neural network (GNN)-based trajectory predictor. Social-Pose first extracts pose features from video frames using a pre-trained pose estimator. These features are then incorporated into the GNN, allowing the model to reason about the intentions and movement capabilities of individuals based on their body language. Experiments on benchmark datasets demonstrate that Social-Pose achieves state-of-the-art performance, significantly reducing prediction error and improving the realism of generated trajectories, particularly in scenarios involving complex human actions. This work highlights the importance of considering human body pose for accurate and realistic trajectory prediction, opening new avenues for research in socially aware autonomous systems."
http://arxiv.org/abs/2507.22480v1,Estimating 2D Camera Motion with Hybrid Motion Basis,"Estimating camera motion is a fundamental task in computer vision, enabling applications such as simultaneous localization and mapping (SLAM) and visual odometry. This paper addresses the problem of robust and accurate 2D camera motion estimation from video sequences, particularly in scenarios with significant parallax and scene depth variations. We propose a novel approach that utilizes a hybrid motion basis, combining learned data-driven bases with analytical bases derived from parametric motion models. Specifically, we employ a convolutional autoencoder to learn a compact representation of common motion patterns in image sequences and integrate these learned bases with traditional affine motion models. This hybrid representation allows for capturing both global and local motion characteristics effectively. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of accuracy and robustness, outperforming existing methods particularly in challenging scenarios with significant scene depth variations and non-rigid object motion. This hybrid motion basis provides a powerful and adaptable framework for robust camera motion estimation, leading to improved performance in a wide range of vision applications."
http://arxiv.org/abs/2507.22420v1,Eyepiece-free pupil-optimized holographic near-eye displays,"Holographic near-eye displays (NEDs) promise compact and lightweight augmented reality (AR) and virtual reality (VR) experiences by reconstructing 3D scenes close to the eye. However, conventional holographic NEDs often suffer from limited field of view and the need for a physical eyepiece, hindering their practical application. This paper addresses the challenge of creating eyepiece-free holographic NEDs with a large eyebox that dynamically adapts to the user's pupil position. We propose an optimized holographic display architecture that combines a spatial light modulator (SLM) with a custom-designed diffractive optical element (DOE) to steer and shape the reconstructed wavefront. The DOE is optimized to create multiple holographic reconstructions at different locations, effectively enlarging the eyebox. Furthermore, we introduce a pupil tracking system that allows for real-time adjustment of the holographic reconstruction, ensuring the rendered image is always aligned with the user's pupil. Experimental results demonstrate a significant increase in the eyebox size compared to conventional holographic NEDs, while maintaining high image quality and resolution. This eyepiece-free, pupil-optimized approach paves the way for more comfortable and immersive holographic AR/VR experiences."
http://arxiv.org/abs/2507.22398v1,On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in tasks requiring joint reasoning over visual and textual data. However, the robustness of these models to adversarial attacks, particularly those crafted in the frequency domain, remains largely unexplored. This paper investigates the vulnerability of state-of-the-art VLMs to imperceptible, targeted adversarial perturbations applied to images in the frequency domain. We propose a novel attack strategy that leverages a gradient-based optimization to craft perturbations specifically targeting the discrete cosine transform (DCT) coefficients of input images, while maintaining a low perceptual distance in the spatial domain. Through extensive experiments on various VLMs, including CLIP and ALIGN, across image classification and image-text retrieval tasks, we demonstrate a significant drop in performance under our targeted frequency-domain attacks. These findings highlight a critical weakness in the robustness of VLMs, suggesting a need for developing more resilient architectures and training strategies that are less susceptible to frequency-domain adversarial manipulations."
http://arxiv.org/abs/2507.22393v1,Gems: Group Emotion Profiling Through Multimodal Situational Understanding,"Understanding group emotions in social gatherings is crucial for applications like social robotics, personalized advertising, and crowd management. However, existing methods often focus on individual emotion recognition, neglecting the influence of social context and multimodal cues that shape collective emotional states. This paper addresses the challenge of accurately profiling group emotions by leveraging multimodal situational understanding. We introduce GEMS, a novel framework that integrates visual, audio, and textual information to infer group-level emotional profiles. GEMS employs a hierarchical attention mechanism to weigh the contributions of individual expressions, environmental context, and conversational content towards the overall group emotion. Furthermore, we incorporate a knowledge graph to represent social relationships and contextual factors, enabling the model to reason about the situational dynamics influencing collective feelings. Experimental results on a newly curated dataset of social gatherings demonstrate that GEMS significantly outperforms state-of-the-art methods in group emotion recognition, achieving a 15% improvement in F1-score. This work provides a significant step towards building socially intelligent systems capable of understanding and responding to the nuanced emotional dynamics of human groups."
http://arxiv.org/abs/2507.22378v1,Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding,"Decoding task-evoked brain activity from functional magnetic resonance imaging (fMRI) data is crucial for understanding cognitive processes, but is often limited by the size and variability of available task-specific datasets. This work addresses the challenge of learning robust and generalizable representations from large-scale resting-state fMRI data to improve the decoding of task-evoked brain activity. We propose a novel transfer learning framework that leverages a deep convolutional autoencoder trained on a massive, multi-site resting-state fMRI dataset to extract whole-brain representations. These learned representations are then transferred and fine-tuned on smaller, task-specific fMRI datasets for decoding cognitive states. Our results demonstrate that transferring representations from large-scale resting-state data significantly improves the accuracy and generalization performance of task-evoked brain activity decoding compared to training models from scratch or using traditional feature extraction methods, particularly in scenarios with limited task-specific data. This approach offers a powerful strategy for leveraging the abundance of resting-state fMRI data to enhance the analysis and interpretation of task-related neural activity."
http://arxiv.org/abs/2507.21858v1,Low-Cost Test-Time Adaptation for Robust Video Editing,"Video editing models, trained on large datasets, often struggle with domain shifts encountered in real-world scenarios, leading to artifacts and inconsistencies in edited outputs. This paper addresses the problem of adapting pre-trained video editing models to new, unseen video domains at test time without requiring extensive fine-tuning or access to source domain data. We propose a novel low-cost test-time adaptation (TTA) framework based on self-supervised learning and cycle consistency. Our approach leverages the inherent temporal redundancy in videos to generate pseudo-labels and train a lightweight adaptation module to align the feature distributions of the source and target domains, while simultaneously enforcing cycle consistency between the original and edited videos to preserve content integrity. Experiments on diverse video editing tasks, including video inpainting and style transfer, demonstrate that our TTA method significantly improves the robustness and visual quality of edited videos compared to existing domain adaptation techniques, while requiring minimal computational overhead. Our approach offers a practical solution for deploying robust video editing models in dynamic and unpredictable environments."
http://arxiv.org/abs/2507.21567v1,RelMap: Enhancing Online Map Construction with Class-Aware Spatial Relation and Semantic Priors,"Online map construction is a fundamental task for autonomous navigation and scene understanding. However, existing methods often struggle with noisy sensor data and lack semantic awareness, leading to inaccurate and incomplete maps. This paper addresses the challenge of building robust and semantically meaningful maps in real-time by leveraging spatial relationships between objects and semantic priors. We introduce RelMap, a novel framework that integrates class-aware spatial relation constraints into the mapping process. RelMap utilizes a graph-based representation where nodes represent objects and edges encode probabilistic spatial relationships learned from a pre-trained object detector and a spatial relation classifier. These relationships, combined with semantic priors from the object classes, regularize the map optimization, leading to more accurate object pose estimation and reduced map drift. Experimental results on benchmark datasets demonstrate that RelMap significantly improves the accuracy and completeness of online maps compared to state-of-the-art methods, particularly in cluttered environments. RelMap offers a promising approach for building more reliable and informative maps for autonomous systems operating in complex real-world scenarios."
http://arxiv.org/abs/2507.21530v1,Suppressing Gradient Conflict for Generalizable Deepfake Detection,"Deepfake technology poses a significant threat due to its potential for malicious manipulation and dissemination of misinformation. Current deepfake detection methods often suffer from poor generalization ability when faced with unseen manipulation techniques or datasets. This limitation stems from the presence of gradient conflict during training, where different features within an image contribute inconsistently to the classification decision, hindering the model's ability to learn robust and generalizable representations. To address this, we propose a novel Gradient Conflict Suppression (GCS) framework, which explicitly minimizes gradient conflict by introducing a regularization term that encourages consistency in feature contributions across different regions of an image. Furthermore, GCS incorporates a feature disentanglement module to separate manipulation-related features from content-related features, further improving the model's focus on relevant cues. Extensive experiments on multiple benchmark datasets demonstrate that GCS significantly outperforms state-of-the-art methods in terms of generalization ability, achieving substantial improvements in cross-dataset and cross-manipulation detection scenarios. Our work offers a crucial step towards building more reliable and robust deepfake detection systems applicable to real-world scenarios."
http://arxiv.org/abs/2507.20953v1,Mask-Free Audio-driven Talking Face Generation for Enhanced Visual Quality and Identity Preservation,"Audio-driven talking face generation aims to synthesize realistic and synchronized lip movements of a person given only an audio clip. Current mask-based approaches, while effective, often suffer from artifacts around the mouth region and struggle to maintain high-fidelity identity preservation due to the independent processing of the lip area. To address these limitations, we propose a novel mask-free audio-driven talking face generation framework that leverages a 3D morphable model (3DMM) prior for enhanced visual quality and identity preservation. Our method employs a two-stage approach: first, an audio encoder predicts 3DMM parameters directly from the audio, capturing global facial movements; second, a refinement network, conditioned on the predicted 3DMM parameters and the input face image, synthesizes the final video frames using a generative adversarial network (GAN) architecture trained with identity-preserving losses. Extensive experiments demonstrate that our mask-free approach achieves state-of-the-art performance in terms of visual quality, lip synchronization accuracy, and identity preservation, surpassing existing mask-based methods. This work offers a significant advancement in generating realistic and personalized talking face videos directly from audio, with implications for virtual avatars, teleconferencing, and entertainment applications."
http://arxiv.org/abs/2507.20860v1,Ensemble Foreground Management for Unsupervised Object Discovery,"Unsupervised object discovery aims to identify and segment salient objects in images without relying on manual annotations. A major challenge lies in effectively distinguishing foreground objects from complex backgrounds in the absence of explicit supervision, often leading to noisy or incomplete object proposals. We address this problem by introducing an ensemble foreground management framework that leverages the complementary strengths of multiple unsupervised foreground estimation techniques. Our method constructs a diverse set of initial foreground proposals using different saliency detection and background modeling approaches. These proposals are then refined and integrated through a novel iterative process involving mutual consistency checking and adaptive weighting based on their performance in subsequent object discovery stages. Specifically, we employ a self-supervised learning strategy to evaluate the quality of each proposal within the ensemble. Experimental results on challenging benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art unsupervised object discovery methods, achieving substantial gains in both segmentation accuracy and object localization precision. This highlights the effectiveness of ensemble-based foreground management in improving the robustness and accuracy of unsupervised object discovery."
http://arxiv.org/abs/2507.20800v2,LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations,"The Spotted Lanternfly (SLF) poses a significant threat to agriculture and forestry in several regions, necessitating effective monitoring and control strategies. Current methods for identifying and suppressing SLF populations are often labor-intensive and lack the scalability required for widespread deployment. To address this challenge, we introduce LanternNet, a novel hub-and-spoke system leveraging computer vision and edge computing for automated SLF detection and targeted intervention. LanternNet comprises a central processing hub connected to multiple distributed spoke devices equipped with cameras and insecticidal dispensers. These spokes employ a lightweight convolutional neural network, optimized for edge deployment, to identify SLF presence in real-time. Upon detection, the system autonomously dispenses a measured dose of insecticide, minimizing environmental impact. In field experiments, LanternNet demonstrated a 92% SLF detection accuracy and significantly reduced SLF populations within a treated area compared to control zones. LanternNet offers a scalable and environmentally conscious approach to SLF management, potentially mitigating the devastating impact of this invasive species."
http://arxiv.org/abs/2507.20763v1,KASportsFormer: Kinematic Anatomy Enhanced Transformer for 3D Human Pose Estimation on Short Sports Scene Video,"Human pose estimation from video is crucial for understanding and analyzing human actions, particularly in dynamic sports scenarios. However, accurately capturing 3D human pose from short, fast-paced sports videos remains challenging due to motion blur, occlusions, and the complexity of athletic movements. This paper introduces KASportsFormer, a novel kinematic anatomy enhanced transformer network designed for robust 3D human pose estimation in short sports videos. KASportsFormer integrates kinematic anatomical constraints directly into the transformer architecture via a novel Kinematic Structure Attention (KSA) module. KSA leverages bone length ratios and joint angle limits to guide the attention mechanism, promoting physically plausible pose predictions. Furthermore, the model incorporates a temporal context encoder to capture motion dynamics within the short video clips. Experiments on challenging sports datasets, including Volleyball and Badminton, demonstrate that KASportsFormer achieves state-of-the-art performance, significantly improving pose estimation accuracy, especially in scenarios with significant occlusions and rapid movements. The proposed method offers a robust and accurate solution for analyzing human movement in complex sports videos, enabling a wide range of applications, including performance analysis and automated sports coaching."
http://arxiv.org/abs/2507.20757v1,Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry,"Estimating the shape and contents of opaque containers remains a significant challenge in computer vision, hindering applications in robotics, manufacturing, and quality control. This paper addresses the problem of reconstructing the internal geometry of opaque liquid-filled containers using external observations. We propose a novel approach based on speckle vibrometry, where we excite the container with acoustic vibrations and analyze the resulting speckle patterns on its surface. By learning a mapping between the frequency response of the speckle patterns and the internal liquid volume, utilizing a deep convolutional neural network trained on simulated data, we can infer the liquid's shape and fill level. Our results demonstrate accurate reconstruction of various liquid shapes and fill levels within opaque containers, achieving a mean absolute error of less than 5% in volume estimation. This non-invasive and contactless technique offers a promising solution for inspecting the contents of opaque containers without requiring direct visual access, opening doors for automated inspection and quality control processes."
http://arxiv.org/abs/2508.03720v1,Outlier Detection Algorithm for Circle Fitting,"Circle fitting is a fundamental task in computer vision with applications ranging from object recognition to industrial inspection. However, real-world data often contains outliers that can significantly degrade the accuracy of traditional circle fitting algorithms. This paper addresses the problem of robust circle fitting in the presence of a high percentage of outliers. We propose a novel outlier detection algorithm that iteratively refines a circle estimate by combining a robust M-estimator with a data-driven outlier rejection threshold. The method first estimates an initial circle using a robust least-squares technique. Subsequently, it adaptively adjusts the outlier rejection threshold based on the residual distribution, effectively removing points that deviate significantly from the current circle estimate. Experiments on both synthetic and real-world datasets demonstrate that the proposed algorithm achieves superior performance compared to state-of-the-art methods, particularly in scenarios with high outlier ratios, yielding a significant improvement in circle fitting accuracy and robustness. This makes it a valuable tool for applications where reliable circle fitting is crucial despite the presence of noisy data."
http://arxiv.org/abs/2507.20198v3,"When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios","Large language models (LLMs) are increasingly employed in multimodal tasks, demanding the efficient processing of long-context inputs spanning images, videos, and audios. However, the quadratic complexity of attention mechanisms in transformers poses a significant bottleneck when dealing with lengthy token sequences derived from these modalities, leading to increased computational costs and memory requirements. This survey addresses the critical need for effective token compression techniques tailored for multimodal long-context data. We systematically categorize and analyze existing compression strategies, including token pruning, token merging, learned token selection, and hierarchical approaches, evaluating their applicability and performance across diverse multimodal scenarios. We further investigate modality-specific adaptations and cross-modal synergies in token compression. Our analysis reveals that learned token selection and hierarchical methods often achieve superior compression rates with minimal performance degradation compared to simpler pruning techniques. This survey provides a comprehensive overview of the current landscape of multimodal long-context token compression, offering valuable insights and future directions for researchers and practitioners seeking to optimize LLMs for multimodal applications."
http://arxiv.org/abs/2508.02177v1,Deep classification algorithm for De-identification of DICOM medical images,"Medical imaging, particularly DICOM images, contains protected health information (PHI) that necessitates de-identification before research or educational use. Manual de-identification is time-consuming and prone to error, while existing automated methods often struggle with complex layouts and variable data placement within DICOM files. This paper addresses the challenge of reliably and efficiently identifying and removing PHI from DICOM images using a novel deep learning-based classification algorithm. Our method leverages a convolutional neural network (CNN) architecture trained on a large, diverse dataset of DICOM images with annotated PHI regions. The CNN is specifically designed to classify image patches as either containing PHI or not, enabling precise localization and subsequent removal or redaction of sensitive information. Experimental results demonstrate that our proposed algorithm achieves a high F1-score of 0.96 on a held-out test set, significantly outperforming traditional rule-based methods and demonstrating robustness across different modalities and image types. This automated and accurate de-identification pipeline facilitates the wider sharing and utilization of medical imaging data for research advancements while ensuring patient privacy."
http://arxiv.org/abs/2507.20126v1,An Automated Deep Segmentation and Spatial-Statistics Approach for Post-Blast Rock Fragmentation Assessment,"Rock fragmentation assessment is crucial for optimizing blasting operations in mining and construction, influencing downstream processes like loading, hauling, and crushing. Manual fragmentation analysis is time-consuming, subjective, and prone to inconsistencies. This paper introduces an automated deep learning and spatial-statistics approach for efficient and objective post-blast rock fragmentation assessment. Our method employs a Mask R-CNN deep learning model, trained on a novel dataset of blast rock images, to accurately segment individual rock fragments. Subsequently, we integrate spatial statistics, including Ripley's K-function and pair correlation function, to characterize the spatial distribution and clustering patterns of the segmented fragments. Experimental results demonstrate that our approach achieves high segmentation accuracy (mean Average Precision of 0.85) and provides valuable insights into fragmentation size distribution and spatial arrangement. This automated methodology offers a significant improvement over traditional methods, enabling rapid and reliable feedback for blast design optimization and operational efficiency."
http://arxiv.org/abs/2507.19917v1,A mini-batch training strategy for deep subspace clustering networks,"Deep subspace clustering (DSC) networks have shown promising results in clustering high-dimensional data by integrating representation learning and subspace clustering. However, training these networks on large datasets poses a significant computational challenge due to the computationally intensive self-expressiveness layer which requires processing the entire data in each iteration. To address this, we propose a novel mini-batch training strategy for DSC networks that leverages stochastic optimization and efficient memory management. Specifically, we formulate a mini-batch self-expressiveness loss that approximates the full-batch loss while significantly reducing memory requirements and computational cost. Furthermore, we introduce a dynamic sampling technique to ensure that each mini-batch contains representative samples from different subspaces, improving the robustness and convergence of the training process. Experimental results on several benchmark datasets demonstrate that our mini-batch training strategy achieves comparable or even better clustering performance compared to the full-batch training approach, while significantly reducing training time and memory consumption. This work enables the application of DSC networks to large-scale datasets, broadening their applicability and impact."
http://arxiv.org/abs/2507.19682v1,DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning,"Multimodal data is increasingly prevalent across diverse fields, offering complementary perspectives on underlying phenomena. However, effectively integrating information from multiple modalities to extract both shared and modality-specific insights remains a significant challenge. This paper introduces DeepJIVE, a novel deep learning framework for Joint and Individual Variation Explained (JIVE) that leverages neural networks to uncover latent structures within multimodal datasets. DeepJIVE employs a modular architecture comprising modality-specific encoders, a shared latent space, and modality-specific reconstruction decoders. The model is trained end-to-end with a loss function that encourages the separation of joint and individual variation while ensuring accurate reconstruction of the original data. We demonstrate the effectiveness of DeepJIVE on both synthetic and real-world datasets, including a neuroimaging dataset, showcasing its ability to identify meaningful shared and individual features that capture complex relationships between modalities. Our results demonstrate that DeepJIVE outperforms existing JIVE methods and other deep learning-based multimodal integration techniques in terms of both reconstruction accuracy and the ability to disentangle joint and individual sources of variation. DeepJIVE provides a powerful tool for exploring and understanding complex multimodal data, facilitating new discoveries in various domains."
http://arxiv.org/abs/2507.18788v1,Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning,"Image captioning, the task of automatically generating textual descriptions of images, has seen significant progress driven by deep learning. However, current approaches often struggle to capture fine-grained details and generate captions that accurately reflect the visual content in a comprehensive and iterative manner. This paper introduces an iterative deep learning framework for image captioning, designed to progressively refine caption generation through a feedback loop. Our model leverages a novel attention mechanism within a recurrent neural network (RNN) to initially generate a coarse caption. This initial caption is then fed back into the model, guiding a subsequent attention-based refinement stage that focuses on previously overlooked visual elements. We demonstrate significant improvements on the COCO benchmark, achieving a CIDEr score of 135.2, surpassing state-of-the-art methods by capturing more nuanced visual relationships and generating more informative and contextually relevant captions. This iterative refinement strategy presents a powerful paradigm for enhancing the accuracy and detail of automatically generated image descriptions."
http://arxiv.org/abs/2507.18407v1,DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation,"Medical image segmentation is a crucial step in computer-aided diagnosis and treatment planning. Accurate segmentation, however, remains challenging due to the complex anatomical structures, low contrast, and high variability present in medical images. This paper addresses the problem of effectively capturing and integrating multi-scale contextual information while simultaneously separating foreground objects from complex backgrounds in medical image segmentation. We propose a novel Deep Connectivity Feature Fusion Separation Network (DCFFSNet) that incorporates a densely connected feature fusion module to aggregate hierarchical features from different network depths, enhancing the representation of both local and global contexts. Furthermore, a spatial attention-guided separation module is introduced to suppress irrelevant background information and refine object boundaries. Experimental results on multiple medical image segmentation datasets, including the ISIC 2018 skin lesion dataset and the Synapse multi-organ segmentation dataset, demonstrate that DCFFSNet achieves state-of-the-art performance, outperforming existing methods in terms of segmentation accuracy and robustness. The proposed DCFFSNet offers a promising solution for improving the precision and reliability of medical image segmentation, paving the way for more accurate clinical applications."
http://arxiv.org/abs/2507.18133v1,Deep Learning for Glioblastoma Morpho-pathological Features Identification: A BraTS-Pathology Challenge Solution,"Glioblastoma (GBM) is the most aggressive type of brain tumor, characterized by significant morphological heterogeneity. Accurate identification of key morpho-pathological features from histopathological images is crucial for improved diagnosis and prognostication. This paper addresses the challenging task of automatically identifying and classifying these features in digitized whole slide images (WSIs) of GBM, as presented in the BraTS-Pathology challenge. We propose a deep learning pipeline leveraging a multi-resolution approach. First, WSIs are tiled into smaller patches at multiple magnifications. Then, a convolutional neural network (CNN) ensemble, incorporating EfficientNet and ResNet architectures, is trained on these patches to predict the presence of specific morpho-pathological features, including microvascular proliferation (MVP), pseudopalisading necrosis (PN), and cellularity. Our method achieved competitive performance in the BraTS-Pathology 2023 challenge, demonstrating a mean Dice score of 0.78 across all feature classes on the validation set. This work highlights the potential of deep learning to assist pathologists in the objective and efficient identification of critical GBM features, ultimately contributing to improved patient care."
http://arxiv.org/abs/2507.17971v2,Benchmarking of Deep Learning Methods for Generic MRI Multi-Organ Abdominal Segmentation,"Accurate and automated segmentation of abdominal organs from Magnetic Resonance Imaging (MRI) is crucial for various clinical applications, including computer-aided diagnosis, treatment planning, and quantitative image analysis. However, achieving robust and generalizable multi-organ segmentation remains challenging due to the inherent variability in organ shape, size, location, and image contrast across different MRI protocols and patient populations. This paper presents a comprehensive benchmark of state-of-the-art deep learning methods for generic MRI multi-organ abdominal segmentation, focusing on evaluating their performance and generalizability across diverse datasets. We investigated several popular architectures, including U-Net, V-Net, and attention-based models, and implemented a standardized training and evaluation pipeline using publicly available datasets and evaluation metrics. Our results demonstrate that while certain architectures exhibit superior performance on specific organs, no single method consistently outperforms others across all organs and datasets. Furthermore, we identify key factors influencing segmentation accuracy, such as network depth, receptive field size, and the use of attention mechanisms. This benchmark provides valuable insights into the strengths and weaknesses of different deep learning approaches for multi-organ abdominal segmentation, facilitating the development of more robust and generalizable automated tools for clinical practice and research."
http://arxiv.org/abs/2507.16779v1,"Improving U-Net Confidence on TEM Image Data with L2-Regularization, Transfer Learning, and Deep Fine-Tuning","Transmission electron microscopy (TEM) provides critical insights into cellular and material structures at the nanoscale. However, automated segmentation of TEM images remains challenging, particularly in achieving high confidence predictions for downstream analysis. This paper addresses the problem of improving the reliability and accuracy of U-Net segmentation for TEM image data, focusing on enhancing prediction confidence. We propose a novel approach combining L2-regularization during training, transfer learning from a pre-trained model on natural images (ImageNet), and a deep fine-tuning strategy specifically tailored for TEM datasets. L2-regularization helps prevent overfitting and improve generalization, while transfer learning provides a strong initialization for the U-Net, and deep fine-tuning adapts the pre-trained network to the nuances of TEM imagery, focusing on refining the later layers for optimal performance. Our results demonstrate significant improvements in both segmentation accuracy (Dice score increase of 5-8% on tested datasets) and prediction confidence, as measured by the entropy of the predicted probability maps, compared to U-Nets trained from scratch or with only shallow fine-tuning. These improvements in U-Net confidence and accuracy are crucial for reliable automated analysis and interpretation of TEM data in various scientific domains."
http://arxiv.org/abs/2507.16065v2,Handcrafted vs. Deep Radiomics vs. Fusion vs. Deep Learning: A Comprehensive Review of Machine Learning -Based Cancer Outcome Prediction in PET and SPECT Imaging,"Positron Emission Tomography (PET) and Single-Photon Emission Computed Tomography (SPECT) imaging provide valuable functional information for cancer diagnosis and prognosis. Machine learning-based radiomics, which extracts quantitative features from medical images, has emerged as a promising approach for predicting cancer outcomes. However, a comprehensive comparison of different machine learning strategies, including handcrafted radiomics, deep radiomics (features learned directly from deep learning models), fusion strategies combining both, and end-to-end deep learning, remains lacking. This review systematically analyzes studies employing these four distinct machine learning paradigms for cancer outcome prediction using PET and SPECT imaging data. We categorize studies based on cancer type, imaging modality, feature extraction method (handcrafted or deep), machine learning algorithm, and performance metrics. Our analysis reveals that while handcrafted radiomics provides interpretable features, deep radiomics and fusion strategies often achieve superior predictive performance, particularly when combined with sophisticated deep learning classifiers, and end-to-end deep learning approaches show promise but require large datasets. This review highlights the strengths and weaknesses of each approach, providing a valuable resource for researchers aiming to develop robust and accurate machine learning models for cancer outcome prediction in nuclear medicine."
http://arxiv.org/abs/2507.15987v1,Semantic-Aware Gaussian Process Calibration with Structured Layerwise Kernels for Deep Neural Networks,"Deep neural networks (DNNs) often exhibit overconfidence in their predictions, leading to poorly calibrated probabilities and unreliable decision-making in safety-critical applications. Existing calibration methods frequently overlook the rich semantic information encoded within DNN feature spaces and struggle to capture complex, layer-specific uncertainties. This paper introduces a novel semantic-aware Gaussian Process (GP) calibration framework with structured layerwise kernels designed to address these limitations. Our approach leverages semantic segmentation masks to guide the GP calibration process, allowing the model to learn distinct calibration functions for different semantic regions within the input image. Furthermore, we employ structured layerwise kernels that decompose the GP covariance function into a series of layer-specific components, enabling the model to capture the unique uncertainty characteristics of each layer in the DNN. Experimental results on benchmark semantic segmentation datasets demonstrate that our method significantly improves calibration performance compared to state-of-the-art techniques, while maintaining competitive accuracy. The proposed framework provides a principled and effective approach for enhancing the reliability of DNNs by explicitly modeling semantic and layerwise uncertainties."
http://arxiv.org/abs/2507.15496v1,Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images,"LiDAR-visual odometry is a crucial task for autonomous navigation in complex environments. However, the sparsity of LiDAR point clouds and the challenges of visual odometry under poor illumination or texture limit the robustness and accuracy of existing approaches. This paper addresses the problem of improving LiDAR-visual odometry performance in challenging scenarios by leveraging dense depth maps generated from sparse LiDAR points and images. We propose a novel deep learning framework that first fuses sparse LiDAR point clouds and monocular images to generate a dense depth map using a conditional generative adversarial network. This dense depth map then guides a deep neural network to estimate the relative pose between consecutive frames, effectively bridging the gap between sparse LiDAR data and rich visual information. Experimental results on the KITTI dataset demonstrate that our method significantly outperforms state-of-the-art LiDAR-visual odometry methods, particularly in scenarios with sparse LiDAR points and poor illumination. Our approach provides a more robust and accurate solution for autonomous navigation in challenging environments, enhancing the reliability of robot perception and control."
http://arxiv.org/abs/2507.15193v2,A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT,"Deep learning models have shown promise in automated medical image segmentation, including the detection of pheochromocytomas, rare tumors of the adrenal glands. However, the relatively small size and variable location of these tumors within the abdomen pose a significant challenge for accurate and robust segmentation. This paper investigates the incorporation of anatomical priors into a deep learning framework to improve pheochromocytoma segmentation in abdominal CT images. We propose a multi-stage approach, first employing a coarse segmentation network to identify the adrenal glands based on their spatial relationships with surrounding organs, followed by a refined segmentation network that leverages the adrenal gland localization to precisely delineate the tumor. Anatomical priors are integrated through a combination of loss function weighting and spatial attention mechanisms that bias the network towards plausible tumor locations within the adrenal gland. Experimental results on a dataset of 150 CT scans demonstrate that the proposed method significantly outperforms baseline segmentation models, achieving a Dice score improvement of 8% and a reduction in false positive rate of 15%. This study highlights the effectiveness of integrating anatomical knowledge into deep learning models for improved medical image segmentation, particularly in challenging scenarios with small and variably located structures."
http://arxiv.org/abs/2507.15078v1,PET Image Reconstruction Using Deep Diffusion Image Prior,"Positron Emission Tomography (PET) image reconstruction is an ill-posed inverse problem, often requiring strong regularization to compensate for noise and limited data. Traditional regularization methods can introduce biases and limit spatial resolution. To address this, we propose a novel PET image reconstruction framework leveraging a deep diffusion image prior. Our method employs a pre-trained, unconditional diffusion model to constrain the solution space of the reconstruction, effectively guiding the optimization process towards realistic and high-quality PET images. Specifically, we integrate the diffusion model as a regularizer within an iterative reconstruction algorithm, where the gradient of the diffusion model steers the evolving image towards regions of high probability under the learned data distribution. Experiments on simulated and real clinical PET data demonstrate that our approach significantly improves image quality, reduces noise, and enhances lesion detectability compared to conventional reconstruction techniques and other deep learning-based methods. This work establishes the potential of diffusion models as powerful image priors for solving challenging inverse problems in medical imaging, offering a promising avenue for improved diagnostic accuracy and quantitative analysis in PET imaging."
http://arxiv.org/abs/2507.14932v1,Probabilistic smooth attention for deep multiple instance learning in medical imaging,"Multiple Instance Learning (MIL) is crucial for weakly supervised medical image analysis, where only bag-level labels are available. However, existing deep MIL methods often suffer from noisy instance representations and unstable attention mechanisms, hindering accurate bag classification. This paper addresses the challenge of learning robust and reliable instance-level attention weights in deep MIL models for medical imaging. We propose a novel probabilistic smooth attention mechanism that leverages a Beta distribution to model the uncertainty in instance importance. This allows for smoother attention weight assignments by considering a distribution over potential attention values, mitigating the impact of outlier instances and promoting more stable learning. Furthermore, we introduce a regularization term that encourages diversity in the attention weights, preventing the model from focusing on only a few instances. Experimental results on benchmark medical imaging datasets demonstrate that our proposed method achieves significant improvements in classification accuracy and outperforms state-of-the-art MIL approaches. The proposed probabilistic smooth attention mechanism offers a more robust and reliable approach for deep MIL in medical imaging, leading to improved diagnostic performance and interpretability."
http://arxiv.org/abs/2507.14093v1,Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment,"Scoliosis, a three-dimensional deformity of the spine, requires accurate and reliable assessment for effective clinical management. While deep learning models have shown promise in automating scoliosis assessment from radiographs, their generalizability across diverse clinical settings remains underexplored. This study addresses the critical need for multi-centre validation of a previously developed deep learning model for automated Cobb angle measurement and vertebral localization in scoliosis radiographs. We evaluated the model's performance on independent datasets from three distinct medical centres, encompassing variations in patient demographics, radiographic protocols, and image quality. Our model leverages a convolutional neural network (CNN) architecture trained on a large, curated dataset and fine-tuned for improved robustness. The results demonstrate that the model maintains high accuracy and precision across all three centres, achieving a mean absolute error (MAE) of less than 5 degrees for Cobb angle measurement and strong agreement in vertebral localization compared to expert manual annotations. These findings provide compelling evidence for the clinical applicability and generalizability of deep learning in scoliosis assessment, paving the way for wider adoption and improved patient care."
http://arxiv.org/abs/2507.14046v1,D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging,"Pulmonary impedance imaging (PII) is a radiation-free technique for monitoring regional ventilation distribution, offering valuable insights into lung function. However, reconstructing high-resolution 3D time-sequence PII from limited and noisy boundary measurements remains a significant challenge, leading to ill-posed inverse problems. We introduce D2IP, a novel Deep Dynamic Image Prior framework for 3D time-sequence PII reconstruction. D2IP leverages a recurrent neural network, specifically a ConvGRU, to learn a dynamic prior from a training dataset of realistic pulmonary impedance variations. This learned prior is then incorporated into an iterative reconstruction algorithm, effectively regularizing the solution space and guiding the reconstruction process towards physiologically plausible solutions. Experiments using simulated and phantom data demonstrate that D2IP significantly outperforms conventional static regularization techniques, achieving improved spatial resolution and temporal fidelity in reconstructed impedance changes. This advancement holds promise for enhancing the accuracy and clinical utility of PII in diagnosing and monitoring respiratory diseases."
http://arxiv.org/abs/2507.13782v1,Converting T1-weighted MRI from 3T to 7T quality using deep learning,"High-field magnetic resonance imaging (MRI) at 7T offers improved signal-to-noise ratio and contrast compared to 3T, enabling more detailed visualization of brain structures. However, 7T MRI is less accessible and more expensive than 3T MRI. This paper addresses the problem of synthesizing high-quality 7T-like T1-weighted MRI images from readily available 3T data. We propose a novel deep learning framework based on a 3D conditional generative adversarial network (cGAN) with a perceptual loss function. The generator network learns to map 3T images to corresponding 7T-like images, while the discriminator distinguishes between real 7T images and generated images. The perceptual loss, calculated using a pre-trained VGG network, encourages the generated images to preserve high-level structural information present in real 7T images. Quantitative evaluation using PSNR and SSIM metrics demonstrates that our method significantly improves image quality compared to baseline interpolation techniques and other GAN-based approaches. Qualitative results further confirm the enhanced anatomical detail and contrast in the synthesized 7T-like images. This work offers a cost-effective solution for enhancing the quality of routinely acquired 3T MRI data, facilitating improved clinical diagnosis and research studies."
http://arxiv.org/abs/2507.13527v1,SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM,"Conductive Atomic Force Microscopy (C-AFM) is a powerful technique for characterizing the electrical properties of 2D materials like MoS$_2$ at the nanoscale. However, acquiring dense C-AFM datasets is time-consuming, limiting its applicability for high-throughput material screening and defect analysis. We address this limitation by introducing SparseC-AFM, a deep learning method that accurately reconstructs high-resolution C-AFM images from sparse measurements. Our method leverages a convolutional neural network (CNN) architecture trained on synthetically generated C-AFM data mimicking MoS$_2$ characteristics, incorporating physically informed data augmentation strategies to enhance robustness. The network learns to interpolate and extrapolate the current map, effectively filling in the gaps between sparsely sampled points. We demonstrate that SparseC-AFM achieves state-of-the-art performance in reconstructing high-resolution C-AFM images from sparse data, reducing the required acquisition time by up to an order of magnitude while maintaining high fidelity. This significantly accelerates the characterization of MoS$_2$ and other 2D materials, facilitating faster discovery and optimization of novel electronic devices."
http://arxiv.org/abs/2507.13458v1,Domain-randomized deep learning for neuroimage analysis,"Deep learning models have shown promise in neuroimage analysis, but their generalization ability is often limited by domain shift between training and testing datasets due to variations in acquisition protocols, scanner manufacturers, and subject populations. This paper addresses the challenge of improving the robustness of deep learning models for neuroimage analysis across different domains. We propose a domain-randomized training strategy that involves augmenting the training data with a wide range of synthetic variations mimicking potential domain shifts encountered in real-world neuroimaging data. Specifically, we randomly vary image intensity distributions, noise levels, contrast, and geometric transformations during training, forcing the model to learn features that are invariant to these domain-specific characteristics. We evaluate our approach on multiple neuroimaging tasks, including brain age prediction and Alzheimer's disease classification, using publicly available datasets with known domain variations. The results demonstrate that domain-randomized training significantly improves the generalization performance of deep learning models on unseen datasets, outperforming models trained with standard data augmentation techniques. This approach offers a practical and effective strategy for developing robust and generalizable deep learning models for neuroimage analysis, facilitating their deployment across diverse clinical settings."
http://arxiv.org/abs/2507.13106v1,Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI Images and Lung Maturity Evaluation for Fetal Growth Restriction,"Fetal lung maturity assessment is crucial for managing pregnancies complicated by fetal growth restriction (FGR), where premature delivery is often necessary. Accurate fetal lung segmentation from diffusion-weighted MRI (DWI) is a prerequisite for quantitative analysis of lung development, but manual segmentation is time-consuming and prone to inter-observer variability. This paper addresses the challenge of automated and accurate fetal lung segmentation in DWI images for improved lung maturity evaluation in FGR pregnancies. We propose a novel deep learning framework based on a 3D U-Net architecture, incorporating attention mechanisms and spatial pyramid pooling to enhance feature extraction and segmentation accuracy, specifically tailored for the low signal-to-noise ratio and motion artifacts common in fetal DWI. Furthermore, we derive quantitative DWI metrics from the segmented lungs, correlating them with gestational age and clinical indicators of lung maturity. Experimental results on a dataset of FGR pregnancies demonstrate that our method achieves significantly higher Dice scores and lower Hausdorff distances compared to existing segmentation techniques, enabling more reliable estimation of lung maturity indices. This automated and accurate segmentation pipeline facilitates objective and quantitative assessment of fetal lung development, potentially improving clinical decision-making in managing FGR pregnancies and optimizing neonatal outcomes."
http://arxiv.org/abs/2507.12939v1,A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image,"Landslides pose significant threats to human lives and infrastructure, particularly in mountainous regions. Accurate and timely identification of landslides from remote sensing imagery is crucial for hazard assessment and mitigation efforts. This paper addresses the challenge of robust and efficient landslide classification from remote sensing data, which is often hampered by spectral variability, complex terrain, and the presence of confounding factors. We propose a novel deep-learning framework that combines a convolutional neural network (CNN) for feature extraction with a recurrent neural network (RNN) for incorporating spatial context. Specifically, we employ a U-Net architecture pre-trained on a large-scale image dataset to extract hierarchical features from remote sensing images, followed by a bidirectional LSTM to model the spatial dependencies between neighboring pixels. Experimental results on a diverse set of remote sensing datasets demonstrate that our proposed framework achieves state-of-the-art performance in landslide classification, outperforming traditional machine learning algorithms and other deep learning approaches. This improved classification accuracy contributes to more effective landslide hazard management and risk reduction strategies."
http://arxiv.org/abs/2507.12869v2,WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding,"Person re-identification (Re-ID) traditionally relies on visual cues, limiting its applicability in scenarios with poor lighting or occlusions. Emerging research explores device-free Re-ID using Wi-Fi channel state information (CSI), offering privacy-preserving and environment-agnostic identification. However, existing Wi-Fi Re-ID methods often struggle to effectively extract discriminative features from noisy and high-dimensional CSI data. We introduce WhoFi, a novel deep learning framework for person Re-ID based on Wi-Fi CSI encoding. WhoFi employs a dedicated CSI pre-processing module to mitigate noise and enhance signal stability. Subsequently, a Siamese network architecture with a novel attention-based feature fusion mechanism learns robust person embeddings from the encoded CSI sequences. Experimental results on benchmark datasets demonstrate that WhoFi achieves state-of-the-art performance, outperforming existing Wi-Fi Re-ID methods by a significant margin. This research highlights the potential of leveraging Wi-Fi signals for robust and privacy-conscious person re-identification in challenging environments."
http://arxiv.org/abs/2507.13408v1,A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs,"Shoulder fractures are a common orthopedic injury, requiring timely and accurate diagnosis for effective treatment. However, manual radiographic interpretation can be time-consuming and prone to errors, especially in high-volume clinical settings. This paper addresses the challenge of automating shoulder fracture detection in clinical radiographs by proposing a novel deep learning-based ensemble system. Our approach leverages a diverse set of pre-trained convolutional neural networks (CNNs), including ResNet50, DenseNet121, and EfficientNetB0, fine-tuned on a large dataset of shoulder radiographs with fracture annotations. The outputs of these individual models are then aggregated using a weighted averaging ensemble strategy, optimized through a Bayesian optimization process to maximize performance. Experimental results demonstrate that our ensemble system achieves a superior performance with an AUC of 0.96 and a sensitivity of 0.92 at 90% specificity, outperforming individual CNN models and existing state-of-the-art methods. This automated system offers a significant improvement in diagnostic accuracy and efficiency, potentially leading to faster and more effective treatment of shoulder fractures."
http://arxiv.org/abs/2507.12092v1,Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis,"Accurate segmentation of cortical lesions (CLs) in magnetic resonance imaging (MRI) is crucial for understanding multiple sclerosis (MS) pathology and progression. However, CL segmentation remains challenging due to their subtle appearance and variable characteristics, leading to inconsistent performance of deep learning (DL) methods across different datasets and scanner configurations. This work benchmarks several state-of-the-art DL segmentation architectures, including 2D, 3D, and hybrid 2.5D convolutional neural networks (CNNs), on a multi-center MS MRI dataset with manual CL annotations. We further investigate the explainability of these models using gradient-based saliency maps and occlusion sensitivity analysis to identify image features driving their predictions, focusing on understanding failure modes and potential biases. Our results demonstrate that a 3D U-Net architecture achieves the best overall performance, with a Dice score of 0.62, while highlighting significant variability in performance across different imaging sites. The explainability analysis reveals that models often rely on periventricular hyperintensities and image artifacts, suggesting avenues for improvement through targeted data augmentation and regularization techniques. This comprehensive evaluation provides critical insights into the capabilities and limitations of DL for CL segmentation, paving the way for more robust and reliable automated analysis in MS research and clinical practice."
http://arxiv.org/abs/2507.11936v4,A Survey of Deep Learning for Geometry Problem Solving,"Geometric problem solving, encompassing tasks like geometric theorem proving and diagram understanding, has traditionally relied on symbolic reasoning and rule-based systems. However, recent advancements in deep learning have opened new avenues for tackling these challenging problems, offering the potential to learn geometric relationships and infer solutions directly from visual and textual inputs. This survey provides a comprehensive overview of deep learning techniques applied to geometry problem solving, focusing on architectures, learning paradigms, and datasets. We categorize existing approaches based on their input modalities (diagrams, text, or both) and the type of geometric reasoning they employ (deductive, inductive, or abductive). Furthermore, we analyze the strengths and limitations of different deep learning models, including graph neural networks, transformers, and convolutional neural networks, in capturing geometric properties and relationships. Our analysis reveals a growing trend towards end-to-end trainable models capable of integrating visual and textual information for more robust and accurate geometric reasoning. Finally, we identify key challenges and future research directions, such as improving generalization capabilities, handling noisy or incomplete data, and developing more interpretable models for geometry problem solving. This work serves as a valuable resource for researchers and practitioners interested in leveraging deep learning to advance the field of automated geometric reasoning."
http://arxiv.org/abs/2507.13383v1,Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models,"Text-to-image (T2I) models have demonstrated remarkable capabilities in generating photorealistic images from textual descriptions. However, these models inherit biases and safety concerns from their training data, and critically, the definition of ""safety"" itself is subjective and varies across individuals and communities. This paper addresses the problem of aligning T2I models with diverse, pluralistic notions of safety. We introduce Deep DIVE (Diverse Interpretations and Values Elicitation), a novel dataset comprising 15,000 prompts, each annotated with safety ratings from multiple annotators representing diverse demographic backgrounds and viewpoints. These ratings capture nuanced perspectives on potential harms, moving beyond simple binary classifications. We demonstrate the utility of Deep DIVE by fine-tuning existing T2I models with a reinforcement learning framework that incorporates these pluralistic safety signals as rewards, resulting in models that generate images better aligned with the safety preferences of specific demographic groups while maintaining overall image quality. Our results show a significant reduction in perceived harmful content across diverse perspectives, as evaluated through human studies. Deep DIVE provides a crucial resource for developing T2I models that respect and reflect the diversity of human values and safety concerns."
http://arxiv.org/abs/2507.11461v1,Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent,"Poisson imaging is a fundamental task in various scientific and engineering applications, often requiring the solution of challenging inverse problems due to noise and ill-posedness. Existing deep learning approaches often involve unrolled iterative algorithms, which can be computationally expensive and memory intensive, especially for high-resolution images. This paper proposes a novel framework leveraging Deep Equilibrium (DEQ) models for solving Poisson imaging inverse problems via a mirror descent optimization scheme. Specifically, we formulate the inverse problem as a minimization of a Poisson negative log-likelihood regularized by a learned prior, and then design a DEQ architecture whose equilibrium point corresponds to the solution obtained by a learned mirror descent algorithm. This allows us to implicitly define the solution of the inverse problem through a single forward pass of the DEQ model, drastically reducing computational cost and memory footprint. Experiments on simulated and real-world Poisson imaging datasets demonstrate that our approach achieves comparable or superior performance to state-of-the-art unrolled methods, while significantly reducing computational complexity and memory requirements. The proposed framework provides a scalable and efficient deep learning approach for solving challenging Poisson imaging inverse problems, paving the way for real-time applications and deployment on resource-constrained devices."
http://arxiv.org/abs/2507.10143v1,Deep Recurrence for Dynamical Segmentation Models,"Dynamical segmentation models offer a powerful framework for video understanding by jointly inferring object segmentations and their temporal evolution. However, traditional approaches often struggle to capture long-range dependencies and complex temporal dynamics due to limitations in their temporal modeling capacity. This paper addresses the challenge of effectively modeling long-range temporal dependencies in dynamical segmentation models. We propose a novel deep recurrent architecture integrated within a variational framework for dynamical segmentation. Our approach leverages recurrent neural networks with gated recurrent units (GRUs) to propagate information across time, enabling the model to learn complex temporal relationships between segmentation masks and latent dynamics. Furthermore, we introduce a novel loss function that encourages temporal consistency and reduces segmentation flickering. Experimental results on benchmark video segmentation datasets, including SegTrack v2 and DAVIS, demonstrate that our approach significantly outperforms existing dynamical segmentation models and achieves state-of-the-art performance in terms of segmentation accuracy and temporal stability. Our deep recurrent architecture offers a more robust and effective approach to dynamical segmentation, leading to improved video understanding capabilities."
http://arxiv.org/abs/2507.09627v1,Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices,"Reconfigurable Intelligent Surfaces (RIS) promise significant performance gains in extremely large-scale multiple-input multiple-output (XL-MIMO) systems. However, accurate channel estimation remains a critical challenge, especially when deploying computationally intensive algorithms on resource-limited edge devices. This paper addresses the problem of efficient channel estimation for RIS-aided XL-MIMO systems suitable for deployment on edge devices with limited computational resources. We propose a lightweight deep learning-based channel estimation network, termed LiteRIS-Net, that leverages a cascaded architecture of efficient convolutional blocks and knowledge distillation techniques to minimize model complexity while preserving accuracy. Specifically, we design a compact student network trained using the output of a larger, more complex teacher network trained offline with extensive data, enabling LiteRIS-Net to learn complex channel characteristics with significantly fewer parameters. Simulation results demonstrate that LiteRIS-Net achieves comparable channel estimation accuracy to state-of-the-art methods with a substantial reduction in computational cost and memory footprint, making it feasible for real-time implementation on edge devices. This work offers a practical and efficient solution for channel estimation in RIS-aided XL-MIMO systems, facilitating their deployment in resource-constrained environments."
http://arxiv.org/abs/2507.09609v1,I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models,"Phase retrieval (PR) is the problem of reconstructing an image from its magnitude-only Fourier transform measurements, a fundamental challenge in various imaging modalities. Traditional iterative PR algorithms often struggle with noise and limited data, leading to slow convergence and suboptimal reconstructions. We address these limitations by introducing I2I-PR, a novel deep iterative refinement framework for phase retrieval leveraging image-to-image diffusion models. I2I-PR iteratively refines an initial PR estimate by incorporating learned priors from a pre-trained diffusion model. Specifically, we utilize a conditional diffusion model trained to map noisy PR estimates to cleaner image reconstructions, guided by the consistency of the Fourier magnitude measurements. Our experiments on benchmark datasets demonstrate that I2I-PR significantly outperforms state-of-the-art PR algorithms, achieving higher reconstruction accuracy and robustness to noise, particularly in challenging low-data regimes. This work highlights the potential of diffusion models to enhance classical inverse problems, offering a powerful tool for high-quality image reconstruction in phase-limited scenarios."
http://arxiv.org/abs/2507.09305v3,DAA*: Deep Angular A Star for Image-based Path Planning,"Image-based path planning is crucial for autonomous navigation in environments where only visual information is available. Traditional path planning algorithms often struggle with high-dimensional image spaces and can produce paths with suboptimal heading angles, leading to inefficient or infeasible robot trajectories. This paper addresses the problem of generating smooth and efficient image-based paths by incorporating angular information directly into the search process. We introduce Deep Angular A* (DAA*), a novel path planning algorithm that integrates a learned cost function, derived from a Convolutional Neural Network trained to predict traversability, with an angularly-aware A* search. DAA* discretizes the configuration space into position and heading angle pairs, allowing the A* search to explicitly optimize for smooth angular transitions. Experimental results demonstrate that DAA* generates paths with significantly reduced heading angle changes and shorter path lengths compared to existing image-based path planning methods, while maintaining comparable computational efficiency. This improved path quality enhances the feasibility and efficiency of robot navigation in complex visual environments."
http://arxiv.org/abs/2507.08404v1,Deep Hashing with Semantic Hash Centers for Image Retrieval,"Hashing techniques have become increasingly popular for large-scale image retrieval due to their efficiency in storage and search. However, many existing deep hashing methods struggle to effectively capture semantic information, leading to suboptimal retrieval performance. This paper introduces a novel deep hashing approach, Deep Hashing with Semantic Hash Centers (DHSHC), designed to learn discriminative hash codes by explicitly incorporating semantic information into the hashing process. DHSHC learns semantic hash centers in the feature space, representing distinct semantic categories. The learned hash codes are then encouraged to be close to the hash center corresponding to their semantic label, thereby improving the semantic consistency of the hash codes. Extensive experiments on several benchmark datasets, including CIFAR-10, NUS-WIDE, and COCO, demonstrate that DHSHC outperforms state-of-the-art deep hashing methods, achieving significant improvements in retrieval accuracy. The proposed method provides a promising direction for learning more effective and semantically meaningful hash codes for large-scale image retrieval."
http://arxiv.org/abs/2507.08096v1,An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images,"Building height estimation is crucial for urban planning, disaster management, and 3D city modeling. Obtaining accurate building heights from single Synthetic Aperture Radar (SAR) images remains a challenging task due to speckle noise, geometric distortions, and the lack of direct height information. This paper addresses the problem of accurately estimating building heights from single SAR images by leveraging an object-based deep learning framework. Our proposed method integrates building footprint delineation with a deep convolutional neural network (CNN) designed for height regression. First, building footprints are extracted using a semantic segmentation network trained on SAR imagery. Then, these segmented building instances are fed into a dedicated CNN architecture that learns to predict building heights based on the SAR backscatter characteristics within the object boundaries. Experimental results on a diverse SAR dataset demonstrate that our object-based approach significantly improves height estimation accuracy compared to pixel-based methods, achieving a root mean squared error (RMSE) reduction of up to 25%. The proposed framework offers a robust and efficient solution for large-scale building height mapping from single SAR images, contributing to more comprehensive and accurate urban environment monitoring."
http://arxiv.org/abs/2507.07839v1,MeD-3D: A Multimodal Deep Learning Framework for Precise Recurrence Prediction in Clear Cell Renal Cell Carcinoma (ccRCC),"Clear cell renal cell carcinoma (ccRCC) is a prevalent malignancy with variable recurrence patterns even after nephrectomy. Accurate prediction of recurrence is crucial for personalized patient management, yet current clinical and pathological features often lack sufficient precision. This paper addresses the challenge of improving recurrence prediction in ccRCC by leveraging multimodal data integration. We propose MeD-3D, a novel multimodal deep learning framework that integrates histopathological whole slide images (WSIs) with clinical and genomic data. MeD-3D employs a 3D convolutional neural network to extract spatially-aware features from WSIs, which are then fused with clinical and genomic information through a multi-layer perceptron. Evaluated on a retrospective cohort of ccRCC patients, MeD-3D significantly outperformed models relying solely on clinical parameters, achieving an improved C-index of 0.78 for recurrence prediction. Our findings demonstrate the potential of multimodal deep learning to enhance risk stratification and guide treatment decisions in ccRCC."
http://arxiv.org/abs/2507.07757v1,Deep Learning based 3D Volume Correlation for Additive Manufacturing Using High-Resolution Industrial X-ray Computed Tomography,"Additive manufacturing (AM) processes are susceptible to various defects that can compromise the structural integrity of fabricated parts. High-resolution industrial X-ray computed tomography (XCT) provides a non-destructive means to inspect AM parts, enabling the identification of defects and the validation of the manufacturing process. However, accurately correlating 3D volumes from XCT scans acquired at different stages of the AM process or after deformation remains a challenging task due to noise, artifacts, and significant geometric variations. We propose a novel deep learning framework for 3D volume correlation based on a Siamese network architecture, trained to learn robust feature representations from XCT volumes. Our network incorporates a 3D convolutional neural network (CNN) to extract features, followed by a correlation layer to estimate the displacement field between the volumes. A spatial transformer network (STN) is then used to register the volumes based on the estimated displacement field. We demonstrate the effectiveness of our approach on a dataset of AM parts with simulated and real defects, showing a significant improvement in registration accuracy compared to traditional intensity-based registration methods. Specifically, our method reduces the registration error by an average of 40% compared to the commonly used iterative closest point (ICP) algorithm. This deep learning-based 3D volume correlation method enables more accurate defect detection and process monitoring in AM, contributing to enhanced quality control and improved part performance."
http://arxiv.org/abs/2507.07638v1,Bridging the gap in FER: addressing age bias in deep learning,"Facial Expression Recognition (FER) has witnessed significant progress due to deep learning; however, the performance of FER models often degrades when deployed on faces with different demographic attributes than those present in the training data. This paper addresses the critical issue of age bias in deep learning-based FER systems, where models trained on predominantly young adults exhibit reduced accuracy when analyzing expressions of older individuals. We propose a novel Age-Aware Feature Decomposition and Recomposition (AAFDR) framework. This framework decomposes facial features into age-invariant and age-specific components using adversarial learning and subsequently recomposes them to generate augmented feature representations that mitigate the impact of age bias. Specifically, we introduce an age discriminator that encourages the feature extractor to learn age-invariant features, while age-specific feature generators learn to reconstruct age-related information. Experimental results on several benchmark FER datasets, including those with varied age distributions, demonstrate that our AAFDR framework significantly improves the generalization performance of FER models across different age groups, achieving state-of-the-art results in cross-age FER scenarios. This work offers a practical solution for developing more robust and equitable FER systems applicable across diverse age demographics."
http://arxiv.org/abs/2507.06966v1,Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy,"Deep learning-based deformable image registration has shown promise in medical image analysis, particularly in applications like MR-guided radiotherapy. However, domain shifts between training and testing data, arising from variations in patient anatomy or imaging protocols, can significantly degrade registration accuracy. This paper addresses the challenge of robust multi-domain registration in the context of prostate MR images by introducing a segmentation-regularized training strategy. Our method incorporates segmentation masks of anatomical structures, predicted by a separate network, as a spatial regularization term within the registration loss function. This regularization encourages the registration network to align corresponding anatomical regions across different domains, thereby improving the overall deformation field accuracy. We evaluate our approach on a dataset comprising MR images from multiple institutions and demonstrate a significant improvement in Dice scores for prostate and surrounding organs compared to baseline registration methods trained without segmentation regularization. This enhanced registration accuracy has the potential to improve the precision and efficacy of MR-guided prostate cancer radiotherapy."
http://arxiv.org/abs/2507.06011v2,ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge,"Deep learning inference at the edge offers low latency and enhanced privacy, but resource-constrained edge devices face challenges in meeting the computational demands of modern deep neural networks. This paper addresses the problem of minimizing energy consumption during inference of deep learning models deployed across multiple edge devices with inter-device communication. We propose ECORE, an energy-conscious optimized routing framework that dynamically partitions and distributes the computational workload of a deep learning model across available edge devices. ECORE leverages a novel energy consumption model that considers both computation and communication costs, and employs a graph-based optimization algorithm to determine the optimal routing strategy. Experimental results using several benchmark deep learning models and edge device configurations demonstrate that ECORE achieves up to 35% reduction in total energy consumption compared to existing layer-by-layer and data-parallel partitioning strategies, while maintaining acceptable latency. This work offers a practical solution for deploying complex deep learning models on energy-limited edge devices, paving the way for more sustainable and efficient edge intelligence."
http://arxiv.org/abs/2507.05451v1,Self-supervised Deep Learning for Denoising in Ultrasound Microvascular Imaging,"Ultrasound microvascular imaging (UMI) enables visualization of microvessels without contrast agents, but is inherently susceptible to significant noise contamination, hindering accurate diagnosis and quantitative analysis. Traditional denoising methods often struggle to effectively remove complex, signal-dependent noise in UMI while preserving subtle microvascular structures. We propose a novel self-supervised deep learning framework for UMI denoising, leveraging the structural redundancy within the UMI data itself. Our approach trains a deep convolutional neural network to predict a clean image from its noisy counterpart, using a carefully designed loss function that enforces noise reduction while preserving image details, and avoids the need for paired clean/noisy training data. Experimental results on both simulated and in vivo UMI data demonstrate that our method outperforms conventional denoising techniques, achieving superior noise reduction and improved visualization of microvascular networks. This self-supervised denoising framework holds significant promise for enhancing the quality and reliability of UMI, facilitating improved clinical diagnosis and research applications."
http://arxiv.org/abs/2507.05393v1,Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration,"Underwater image enhancement is critical for a variety of marine applications, yet the inherent challenges of light absorption and scattering result in poor visibility and color distortion. Existing deep learning-based methods often rely solely on objective metrics for training, neglecting the nuances of human perception in assessing image quality. To address this, we propose a novel deep learning framework that integrates subjective image quality assessment into the training process for underwater image enhancement. Our network architecture, based on a U-Net structure, is trained using a composite loss function that combines traditional objective metrics (e.g., PSNR, SSIM) with a learned subjective quality score predicted by a separately trained quality assessment network. This subjective network is trained on a large dataset of underwater images with corresponding human ratings of perceived quality. Experimental results on several benchmark datasets demonstrate that our approach significantly improves both objective metrics and subjective visual quality compared to state-of-the-art methods, leading to more visually pleasing and informative underwater images. The integration of subjective quality assessment provides a more perceptually aligned approach to underwater image enhancement, benefiting downstream tasks such as marine robotics and ecological monitoring."
http://arxiv.org/abs/2507.05029v1,Estimating Object Physical Properties from RGB-D Vision and Depth Robot Sensors Using Deep Learning,"Estimating the physical properties of objects is crucial for robotic manipulation and interaction within unstructured environments. Accurately determining parameters such as mass, center of mass, and friction coefficient from visual and depth information remains a significant challenge. This paper introduces a novel deep learning framework that estimates object physical properties by fusing RGB-D vision data with depth sensor readings acquired from a robotic arm. Our method employs a multi-modal convolutional neural network (CNN) architecture that processes RGB images and registered depth maps, coupled with a graph neural network (GNN) to incorporate geometric information extracted from the depth robot sensor during interaction. The GNN leverages the relationships between contact points and robot joint configurations to refine the property estimations. Experimental results on a diverse dataset of household objects demonstrate that our approach achieves state-of-the-art performance in estimating mass, center of mass, and friction coefficient, significantly outperforming vision-only or depth-only methods. These advancements enable robots to better understand and interact with their surroundings, paving the way for more robust and adaptable robotic manipulation strategies."
http://arxiv.org/abs/2507.04947v1,DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer,"Masked autoregressive models have shown promise in image generation, but their computational cost remains a significant barrier, especially at high resolutions. This limitation stems from the large number of tokens needed to represent images and the sequential nature of the autoregressive process. We address this challenge by introducing DC-AR, a novel masked autoregressive image generation framework employing a Deep Compression Hybrid Tokenizer. Our tokenizer combines vector quantization with a learned compression network to achieve a compact and semantically rich representation of images. This compressed representation significantly reduces the sequence length for the autoregressive model, leading to faster generation speeds. Experiments on benchmark datasets demonstrate that DC-AR achieves comparable or superior image quality compared to existing autoregressive models while reducing inference time by up to 4x. DC-AR presents a practical and efficient approach for high-resolution image generation with autoregressive models, paving the way for broader applications in creative content creation and data augmentation."
http://arxiv.org/abs/2507.04684v1,SPIDER: Structure-Preferential Implicit Deep Network for Biplanar X-ray Reconstruction,"Biplanar X-ray imaging is a crucial technique for 3D bone reconstruction, offering lower radiation exposure compared to CT scans. However, current methods often struggle with noisy X-ray images and rely on explicit surface representations, hindering accurate and robust reconstruction. We address the problem of generating high-fidelity 3D bone reconstructions from biplanar X-ray images, particularly in the presence of image noise and complex bone structures. To this end, we introduce SPIDER: a Structure-Preferential Implicit Deep Network that leverages a novel implicit neural representation guided by structural priors. SPIDER incorporates a learned shape embedding space and a structure-aware deformation field, which explicitly encourages the network to learn and enforce bone-like structures during reconstruction. Our experiments on synthetic and real X-ray datasets demonstrate that SPIDER significantly outperforms state-of-the-art methods in terms of reconstruction accuracy, robustness to noise, and preservation of fine-grained bone details. This improved reconstruction quality has the potential to enhance clinical diagnosis and treatment planning in orthopedic applications."
http://arxiv.org/abs/2507.04622v1,A Deep Unfolding Framework for Diffractive Snapshot Spectral Imaging,"Diffractive snapshot spectral imagers offer the potential for compact and efficient hyperspectral imaging by encoding spectral information into a single 2D image using a diffractive optical element. However, decoding the spectral datacube from this single snapshot is a severely ill-posed inverse problem, often relying on computationally expensive iterative reconstruction algorithms with limited performance, especially in low signal-to-noise ratio scenarios. This paper addresses the challenge of achieving high-quality and efficient spectral reconstruction from diffractive snapshot spectral images. We propose a novel deep unfolding framework that explicitly incorporates the physics of diffraction and the image formation process into a learnable neural network architecture. This framework unfolds an iterative proximal gradient descent algorithm, replacing hand-crafted regularizers with learnable convolutional neural networks to improve reconstruction accuracy and robustness. Experimental results on both synthetic and real-world datasets demonstrate that our method significantly outperforms traditional iterative reconstruction algorithms and other deep learning-based approaches in terms of reconstruction quality, speed, and noise resilience. This work provides a powerful and efficient solution for spectral reconstruction in diffractive snapshot spectral imaging, paving the way for its wider adoption in various applications."
http://arxiv.org/abs/2507.04495v1,README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model,"Digital watermarking offers a promising avenue for embedding digital signatures directly within multimedia content, enabling authentication and integrity verification. However, existing methods often lack robustness against common image processing operations and are susceptible to errors introduced during the embedding and extraction processes, leading to signature corruption. This paper introduces README, a Robust Error-Aware Digital Signature framework, leveraging a deep learning-based watermarking model trained to explicitly account for and mitigate potential errors. Our approach utilizes a convolutional neural network architecture for both embedding and extraction, incorporating an error-aware loss function that penalizes signature distortions and encourages resilience to noise and distortions. We further integrate a robust error correction code within the signature itself, allowing for reconstruction of the original signature even with partial or corrupted extractions. Experimental results demonstrate that README significantly outperforms state-of-the-art watermarking techniques in terms of signature recovery accuracy under various attack scenarios, including JPEG compression, Gaussian noise addition, and geometric transformations. This robust and error-aware framework provides a more reliable solution for digital signature embedding, ensuring data integrity and authenticity in real-world applications."
http://arxiv.org/abs/2507.04465v1,"Visual Hand Gesture Recognition with Deep Learning: A Comprehensive Review of Methods, Datasets, Challenges and Future Research Directions","Visual hand gesture recognition (HGR) has emerged as a crucial modality for human-computer interaction, enabling intuitive and natural interfaces across diverse applications. However, the inherent complexities of hand articulations, varying illumination conditions, and cluttered backgrounds pose significant challenges to robust and accurate gesture recognition. This paper presents a comprehensive review of deep learning-based methods for visual HGR, systematically analyzing a wide range of architectures, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformer-based models, and their specific applications in gesture recognition. We categorize and evaluate publicly available datasets, highlighting their strengths and limitations in representing real-world scenarios. Furthermore, we delve into the key challenges facing the field, such as view-point invariance, real-time processing, and generalization to unseen users and environments. The review identifies promising future research directions, including the exploration of self-supervised learning, multi-modal fusion, and the development of more robust and explainable deep learning models. This work provides a valuable resource for researchers and practitioners, offering a holistic understanding of the state-of-the-art in deep learning-based visual HGR and paving the way for future advancements in this dynamic field."
http://arxiv.org/abs/2507.04410v1,Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models,"Multimedia verification, the task of ascertaining the trustworthiness of online content, is increasingly critical in combating the spread of misinformation. Existing methods often struggle to effectively integrate diverse modalities and perform comprehensive, contextualized reasoning necessary for robust verification. We address this challenge by introducing a novel framework: Multi-Agent Deep Research Multimodal Large Language Models (MADReM-LLM). This approach leverages a team of specialized agents, each equipped with a fine-tuned multimodal Large Language Model, to independently investigate different aspects of a claim, such as source credibility, visual consistency, and textual coherence. These agents then collaboratively synthesize their findings, resolving conflicts and generating a final verification report. Experimental results on a benchmark dataset of multimedia claims demonstrate that MADReM-LLM significantly outperforms state-of-the-art methods in accuracy and explainability, achieving a 15% improvement in F1-score. This framework offers a more robust and transparent approach to multimedia verification, paving the way for more reliable automated fact-checking systems."
http://arxiv.org/abs/2507.04269v1,Efficient Training of Deep Networks using Guided Spectral Data Selection: A Step Toward Learning What You Need,"Deep neural networks often require extensive training on large, diverse datasets, leading to significant computational costs and energy consumption. A common practice is to uniformly sample data during training, which may include redundant or less informative samples, hindering efficient learning. This paper addresses the problem of identifying and prioritizing the most informative samples during training to accelerate convergence and improve generalization performance. We introduce Guided Spectral Data Selection (GSDS), a novel approach that leverages spectral analysis of feature embeddings to estimate the informativeness of data points. GSDS employs a Gaussian Mixture Model (GMM) in the spectral domain to cluster embeddings and guides data selection based on cluster density and proximity to cluster boundaries, favoring under-represented and challenging samples. Experiments on benchmark datasets demonstrate that GSDS significantly reduces training time while achieving comparable or superior accuracy compared to uniform sampling and other data selection strategies. This efficient training paradigm offers a pathway toward developing more sustainable and resource-conscious deep learning models."
http://arxiv.org/abs/2507.04252v1,Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images,"Accurate and rapid diagnosis of COVID-19 is crucial for effective disease management and control. While RT-PCR remains the gold standard, lung computed tomography (CT) imaging offers a complementary diagnostic tool, especially in resource-limited settings or when facing PCR testing delays. This paper addresses the challenge of achieving high accuracy and efficiency in COVID-19 diagnosis from lung CT scans. We propose a novel deep-learning framework that integrates a 3D convolutional neural network (CNN) for feature extraction with an attention mechanism to highlight salient regions indicative of COVID-19 infection. Specifically, we employ a ResNet3D backbone pre-trained on a large-scale medical image dataset, followed by a spatial attention module to focus on areas exhibiting ground-glass opacities, consolidation, and other characteristic lesions. Our method achieves a sensitivity of 96.2% and a specificity of 94.8% on a multi-center dataset of over 1,000 lung CT scans, outperforming existing state-of-the-art methods. These results demonstrate the potential of our deep-learning-assisted approach to provide a highly accurate and reliable tool for COVID-19 diagnosis, enabling faster clinical decision-making and improved patient outcomes."
http://arxiv.org/abs/2507.03937v1,EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems,"Speckle noise is an inherent artifact in ultrasound imaging, significantly degrading image quality and hindering accurate clinical diagnosis, particularly in portable systems with limited processing power. This paper addresses the challenge of real-time speckle reduction and image enhancement in portable ultrasound devices, where computational constraints limit the applicability of complex deep learning models. We propose EdgeSRIE, a hybrid deep learning framework that combines a lightweight convolutional neural network (CNN) for initial speckle reduction with an edge-preserving guided filter for subsequent image enhancement. The CNN, optimized for low-latency inference on edge devices, suppresses speckle while preserving important anatomical features. The guided filter then refines the image by sharpening edges and enhancing contrast, leveraging the CNN's output as a guidance map. Experimental results on both simulated and real ultrasound data demonstrate that EdgeSRIE achieves comparable or superior performance to state-of-the-art despeckling methods in terms of PSNR, SSIM, and contrast-to-noise ratio, while maintaining a processing speed suitable for real-time applications on portable ultrasound systems. EdgeSRIE offers a practical and effective solution for enhancing ultrasound image quality in resource-constrained environments, ultimately improving diagnostic accuracy and clinical workflow."
http://arxiv.org/abs/2507.03558v2,An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography (CT) Images,"Brain stroke is a leading cause of long-term disability and death, necessitating rapid and accurate diagnosis for effective treatment. However, manual analysis of Computed Tomography (CT) images for stroke detection is time-consuming and prone to inter-observer variability. This paper addresses the challenge of efficient and accurate automated stroke diagnosis from CT scans. We propose a novel deep learning framework, StrokeNet, which incorporates a lightweight convolutional neural network (CNN) architecture specifically designed to minimize computational cost while maximizing feature extraction relevant to stroke identification. StrokeNet utilizes a cascade of efficient convolutional blocks with channel attention mechanisms to enhance the representation of subtle stroke lesions. Furthermore, we incorporate a custom loss function that balances sensitivity and specificity, crucial for clinical applications. Experimental results on a large, multi-center CT image dataset demonstrate that StrokeNet achieves state-of-the-art performance in stroke detection, with an AUC of 0.96 and a 3x reduction in inference time compared to existing deep learning models. This efficient and accurate framework offers a valuable tool for radiologists, enabling faster and more reliable stroke diagnosis, ultimately leading to improved patient outcomes."
http://arxiv.org/abs/2507.02519v1,IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning,"White shrimp ( *Penaeus vannamei*) aquaculture is a globally significant industry, and accurate biometrical analysis is crucial for monitoring growth, health, and overall production efficiency. Manual measurement of shrimp biometrics is a time-consuming, laborious, and error-prone process, hindering large-scale data collection and analysis. This paper introduces IMASHRIMP, an automated computer vision and deep learning pipeline for efficient and accurate biometrical analysis of white shrimp from laboratory images. IMASHRIMP employs a Mask R-CNN model for shrimp instance segmentation, followed by a series of image processing techniques to extract key biometrical features, including body length, carapace length, and body width. The system was trained and validated on a dataset of 500 laboratory images of white shrimp, achieving a mean Average Precision (mAP) of 0.95 for shrimp detection and segmentation, and a high correlation (R > 0.98) between automated measurements and manual measurements for all biometrical features. IMASHRIMP offers a rapid, objective, and scalable solution for shrimp biometrical analysis, enabling more effective aquaculture management and research."
http://arxiv.org/abs/2507.02517v1,Detecting Multiple Diseases in Multiple Crops Using Deep Learning,"Plant diseases pose a significant threat to global food security, leading to substantial yield losses and economic hardship for farmers. Accurate and timely disease detection is crucial for effective disease management; however, traditional methods are often time-consuming and require expert knowledge. This paper addresses the challenge of simultaneously detecting and classifying multiple diseases across a diverse range of crops using a single deep learning model. We propose a novel multi-task learning framework based on a convolutional neural network (CNN) architecture, incorporating attention mechanisms to focus on relevant regions within plant images and employing a shared feature extraction backbone coupled with disease-specific classification heads. Our model is trained and evaluated on a large, publicly available dataset of plant images, demonstrating significant improvements in both disease detection accuracy and generalization performance compared to existing single-disease and single-crop approaches, achieving an average F1-score of 0.87 across all crops and diseases. This research offers a scalable and efficient solution for automated plant disease diagnosis, paving the way for precision agriculture and improved crop management practices."
http://arxiv.org/abs/2507.02416v1,Determination Of Structural Cracks Using Deep Learning Frameworks,"Structural health monitoring is crucial for ensuring the safety and longevity of civil infrastructure. Traditional crack detection methods are often time-consuming, labor-intensive, and subjective. This paper addresses the problem of automating and improving the accuracy of structural crack detection by leveraging deep learning techniques. We propose a novel framework that integrates a convolutional neural network (CNN) for robust feature extraction with a region proposal network (RPN) for precise crack localization. The CNN, pre-trained on a large-scale image dataset and fine-tuned with crack-specific data, learns discriminative features indicative of crack presence. The RPN then uses these features to generate candidate crack regions, which are subsequently refined and classified. Experiments on a diverse dataset of concrete bridge images demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in terms of detection accuracy (mAP of 0.85) and reducing false positive rates by 15%. This automated and highly accurate crack detection framework offers a significant advancement for efficient and reliable structural health monitoring."
http://arxiv.org/abs/2507.02367v1,A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\left[^{18}\text{F}\right]$FDG PET imaging,"Dynamic Positron Emission Tomography (PET) imaging with $\left[^{18}\text{F}\right]$FDG is a crucial tool for studying glucose metabolism in vivo, particularly in small animal research. Accurate quantification of metabolic rates relies on precise knowledge of the arterial input function (AIF), which is often challenging to obtain non-invasively and reliably, especially in small animals. This work addresses the problem of accurately and robustly predicting the AIF directly from dynamic PET image data, circumventing the need for invasive blood sampling or manual region-of-interest (ROI) delineation. We propose a novel deep learning model based on a 3D convolutional neural network (CNN) architecture, incorporating attention mechanisms and trained using a combination of simulated and real animal data with data augmentation techniques to improve generalization. The network learns to extract relevant spatio-temporal features from the dynamic PET volumes and directly predicts the AIF. Evaluated on both simulated and real rat $\left[^{18}\text{F}\right]$FDG PET datasets, the proposed method demonstrates significantly improved AIF prediction accuracy compared to conventional image-derived methods, exhibiting lower root mean squared error and higher correlation with reference AIFs obtained from arterial blood sampling. This robust and versatile deep learning model offers a powerful tool for non-invasive and accurate quantification of dynamic PET studies, facilitating preclinical research and reducing the burden associated with invasive procedures."
http://arxiv.org/abs/2507.01912v1,3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP,"Accurate 3D reconstruction of orchards is crucial for precision agriculture applications, enabling tasks such as yield estimation and automated pruning. However, significant variations in orchard appearance between dormant (leaf-off) and canopy (leaf-on) seasons pose a challenge for consistent and comprehensive 3D modeling. This paper addresses the problem of generating a complete and accurate 3D orchard model by fusing point clouds acquired during both dormant and canopy seasons. Our method leverages deep learning for semantic segmentation of individual trees in point clouds from both seasons, followed by efficient registration using a modified Fast Global Iterative Closest Point (Fast GICP) algorithm incorporating semantic information to improve alignment accuracy. Finally, we fuse the registered point clouds to create a comprehensive 3D model. Experimental results demonstrate that our approach significantly improves registration accuracy compared to standard Fast GICP, with a reduction in registration error of up to 40%. The fused 3D models provide a rich and complete representation of the orchard structure, enabling more robust and reliable downstream agricultural applications."
http://arxiv.org/abs/2507.01590v1,Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring,"Autonomous AI surveillance systems are increasingly relevant for applications ranging from elder care to security monitoring. However, current systems often lack the cognitive and behavioral understanding necessary for nuanced and reliable assessment of human activity. This paper addresses the challenge of creating an AI surveillance system capable of autonomously monitoring and interpreting complex human behaviors by leveraging multimodal deep learning. We propose a novel architecture, the Multimodal Cognitive-Behavioral Network (MCBN), which integrates video and audio streams through a hierarchical attention mechanism. MCBN first extracts features using modality-specific deep neural networks, then fuses these features using a cross-modal attention module to emphasize relevant information, and finally classifies behaviors using a recurrent neural network to capture temporal dependencies. Experimental results on a newly curated dataset of in-home activities demonstrate that MCBN outperforms state-of-the-art methods in behavior recognition accuracy, achieving a 15% improvement in F1-score, particularly in detecting subtle cognitive changes. This research provides a significant step towards developing intelligent surveillance systems capable of providing proactive and personalized support."
http://arxiv.org/abs/2507.01502v1,Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images,"Accurate tree crown delineation from satellite imagery is crucial for forest inventory, ecological monitoring, and sustainable resource management. However, achieving high accuracy remains challenging due to spectral variability, complex canopy structures, and limitations of both traditional and deep learning approaches when applied in isolation. This paper addresses the problem of improving tree crown detection accuracy by synergistically integrating traditional image processing techniques with deep learning methodologies. Our proposed method employs a two-stage approach: first, a marker-controlled watershed segmentation algorithm, informed by spectral indices and morphological operations, generates initial crown candidates. Subsequently, a convolutional neural network (CNN), trained on a large dataset of manually annotated tree crowns, refines these candidates by classifying them as true or false detections, thereby removing false positives and improving boundary precision. Experiments conducted on high-resolution satellite imagery demonstrate that our integrated approach significantly outperforms both the traditional watershed segmentation and a standalone CNN, achieving an average F1-score improvement of 15% and 8% respectively. The proposed framework offers a robust and efficient solution for accurate tree crown detection, enabling improved forest monitoring and management practices."
http://arxiv.org/abs/2507.01494v2,Crop Pest Classification Using Deep Learning Techniques: A Review,"Crop pests pose a significant threat to global food security, causing substantial yield losses and economic damage. Accurate and timely pest identification is crucial for effective pest management strategies. This review addresses the challenge of efficiently and accurately classifying crop pests by examining the application of deep learning techniques. We systematically analyze a wide range of deep learning models, including Convolutional Neural Networks (CNNs) such as ResNet, Inception, and EfficientNet, as well as more recent architectures like Transformers, focusing on their performance metrics (accuracy, precision, recall, F1-score) when applied to pest classification tasks using publicly available datasets. Our review highlights the strengths and weaknesses of different deep learning approaches, emphasizing the impact of factors like dataset size, image resolution, and model complexity on classification accuracy. The analysis reveals that CNN-based architectures, particularly those pre-trained on large datasets, generally achieve high accuracy in pest classification, with some Transformer-based models showing promising results, especially when dealing with complex image backgrounds or subtle pest features. This comprehensive review provides valuable insights for researchers and practitioners seeking to leverage deep learning for automated and precise crop pest identification, ultimately contributing to improved agricultural practices and sustainable food production."
http://arxiv.org/abs/2507.01279v1,Classification based deep learning models for lung cancer and disease using medical images,"Lung cancer remains a leading cause of mortality worldwide, with early and accurate diagnosis being crucial for improved patient outcomes. The effective analysis of medical images, such as CT scans and X-rays, is paramount for identifying cancerous nodules and other lung diseases. This paper addresses the challenge of developing a robust and automated system for classifying lung cancer and related diseases directly from medical images. We propose a novel deep learning framework that leverages a hybrid architecture combining Convolutional Neural Networks (CNNs) for feature extraction and Recurrent Neural Networks (RNNs) for contextual understanding of the 3D image volumes. Furthermore, we incorporate attention mechanisms to highlight salient regions within the images, guiding the model to focus on diagnostically relevant features. Experimental results on a large, publicly available dataset demonstrate that our proposed method achieves state-of-the-art performance, surpassing existing approaches in terms of accuracy, sensitivity, and specificity in classifying various lung pathologies. This advancement offers a significant step towards developing computer-aided diagnostic tools that can assist radiologists in making faster and more accurate diagnoses, ultimately improving patient care and survival rates."
http://arxiv.org/abs/2507.01123v1,Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions,"Landslides pose a significant threat to infrastructure and human lives, particularly in mountainous regions. Accurate and timely landslide detection and mapping are crucial for risk assessment and disaster management. This paper addresses the challenge of developing a robust and generalizable deep learning model for landslide detection that can effectively utilize multi-source satellite data across diverse geographic regions, mitigating limitations of single-source and region-specific approaches. We propose a novel deep learning framework incorporating a multi-modal fusion strategy to integrate optical (Sentinel-2) and radar (Sentinel-1) satellite imagery, along with digital elevation model (DEM) derivatives. Specifically, we employ a U-Net based architecture with attention mechanisms to extract relevant features from each data source and fuse them at multiple scales. The model is trained and validated on a geographically diverse dataset comprising landslide inventories from several countries, including Nepal, Italy, and Japan. Our experiments demonstrate that the proposed method achieves state-of-the-art performance in landslide detection, with an average F1-score of 0.85, outperforming single-source and traditional machine learning approaches. This research provides a valuable tool for automated and efficient landslide mapping, contributing to improved disaster preparedness and mitigation efforts globally."
http://arxiv.org/abs/2507.00903v1,Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection,"Cardiovascular Magnetic Resonance (CMR) is a crucial non-invasive imaging modality for assessing cardiac structure and function, often utilizing T1 and T2 mapping to quantify tissue characteristics. Manual segmentation of cardiac structures in these maps is time-consuming and prone to inter-observer variability, hindering efficient and reliable disease diagnosis. This paper addresses the challenge of automating cardiac segmentation in T1 and T2 maps to improve diagnostic efficiency. We propose a novel deep learning framework employing a cascaded U-Net architecture, where the first U-Net performs coarse segmentation of the left ventricle (LV), right ventricle (RV), and myocardium, and the second U-Net refines these initial segmentations using a region-of-interest approach. Furthermore, we incorporate attention mechanisms to enhance feature representation and improve segmentation accuracy, particularly at the boundaries of cardiac structures. Experimental results on a multi-center dataset demonstrate that our method achieves state-of-the-art performance with Dice scores of 0.93, 0.90, and 0.88 for LV, RV, and myocardium, respectively, significantly outperforming existing methods and reducing segmentation time. This automated segmentation pipeline facilitates efficient and objective analysis of cardiac MRI maps, enabling faster and more reliable disease detection."
http://arxiv.org/abs/2507.00852v1,Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting,"Flexible manufacturing demands robust and adaptable vision systems for object recognition, particularly in scenarios where traditional tray-based presentation is impractical. This paper addresses the challenge of detecting individual components directly from cluttered scenes in tray-free environments under variable lighting conditions, a common occurrence in dynamic manufacturing settings. We propose a novel deep learning architecture, ComponentNet, which combines a modified Mask R-CNN with an attention mechanism specifically designed to filter out illumination-induced artifacts and enhance feature extraction for individual components. ComponentNet is trained on a synthetically generated dataset with realistic lighting variations and occlusions, augmented with real-world data via transfer learning. Experimental results demonstrate that ComponentNet achieves a significant improvement in Average Precision (AP) of 15% compared to standard Mask R-CNN and other state-of-the-art object detection methods when evaluated on a challenging real-world dataset of randomly placed components under varying lighting. This advancement enables more reliable and efficient component detection, paving the way for increased automation and flexibility in manufacturing processes."
http://arxiv.org/abs/2507.00845v1,Do Echo Top Heights Improve Deep Learning Nowcasts?,"Accurate short-term precipitation forecasting, or nowcasting, is crucial for mitigating risks associated with severe weather events. Deep learning models have shown promise in precipitation nowcasting, often utilizing radar reflectivity data as input. This work investigates whether incorporating echo top height (ETH) data, a measure of the vertical extent of precipitation, alongside radar reflectivity improves the performance of deep learning nowcasting models. We propose a novel architecture that integrates ETH data as an additional input channel to a Convolutional LSTM (ConvLSTM) network, allowing the model to learn spatiotemporal correlations between reflectivity and ETH. We train and evaluate our model on a large dataset of radar observations from the Multi-Radar Multi-Sensor (MRMS) system, comparing performance against a ConvLSTM baseline trained solely on reflectivity. Our results demonstrate that incorporating ETH data leads to a significant improvement in nowcasting accuracy, particularly for intense precipitation events, as measured by the Critical Success Index (CSI) and Heidke Skill Score (HSS). This study highlights the value of leveraging multi-parameter radar data to enhance deep learning-based precipitation nowcasting."
http://arxiv.org/abs/2507.00832v1,Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection,"Deep learning has shown promise for intracranial aneurysm (IA) detection on computed tomography angiography (CTA) images, but false positive rates remain a significant barrier to clinical adoption. This study addresses the challenge of reducing false positives in deep learning-based IA detection by incorporating anatomical knowledge into a post-processing pipeline. We propose a novel anatomy-based post-processing (ABPP) method that leverages automatically segmented brain vasculature to identify and filter out false positive detections occurring outside anatomically plausible aneurysm locations. The ABPP pipeline incorporates a 3D U-Net for vessel segmentation, followed by spatial filtering of IA detections based on their proximity to segmented vessels and exclusion of detections in regions inconsistent with aneurysm formation. Experimental results on a large, multi-center CTA dataset demonstrate a significant reduction in false positives by 32% while maintaining a high sensitivity of 90% for IA detection, leading to a substantial improvement in overall diagnostic accuracy. This anatomy-informed post-processing strategy offers a practical approach to enhance the reliability and interpretability of deep learning systems for IA detection, facilitating their integration into clinical workflows."
http://arxiv.org/abs/2507.00582v2,Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models,"Iterative closest point (ICP) and its variants have been the cornerstone of rigid registration for decades, yet they often struggle with noisy, incomplete, or outlier-ridden data. Recent learning-based methods offer robustness but frequently sacrifice interpretability and generalizability due to their reliance on large training datasets and black-box architectures. This paper addresses the gap between classical and learning-based registration by introducing a novel deep equilibrium model (DEM) framework. Our approach unrolls the classical iterative registration process within a DEM architecture, enabling the learned components to refine and enhance each step of the optimization without losing the inherent structure and guarantees of the underlying algorithm. Specifically, we learn feature embeddings and robust correspondence weighting functions within the DEM, allowing for implicit differentiation and efficient training. We demonstrate that our method outperforms both classical ICP variants and existing learning-based approaches on several challenging datasets, including synthetic point clouds with significant noise and real-world LiDAR scans, achieving improved registration accuracy and robustness. This framework provides a principled way to incorporate learnable components into classical algorithms, paving the way for more interpretable and generalizable solutions in geometric computer vision."
http://arxiv.org/abs/2507.00373v3,Customizable ROI-Based Deep Image Compression,"Deep image compression has shown promise in achieving high compression ratios while maintaining image quality. However, existing methods often lack the flexibility to prioritize specific regions of interest (ROIs) within an image, leading to suboptimal compression performance when certain areas are more important than others. This paper introduces a customizable ROI-based deep image compression framework that allows users to define and prioritize ROIs, enabling targeted allocation of bitrate to perceptually important regions. Our method incorporates a novel attention mechanism guided by user-defined ROI masks, modulating the feature maps extracted by the encoder to emphasize ROIs before quantization and entropy coding. Furthermore, a rate-distortion loss function is designed to penalize distortion within ROIs more heavily, encouraging higher fidelity reconstruction in these areas. Experiments on diverse image datasets demonstrate that our approach achieves significant improvements in visual quality within ROIs compared to state-of-the-art image compression techniques, while maintaining competitive overall compression performance. This customizable ROI-based approach offers a practical solution for applications where selective image quality preservation is crucial, such as medical imaging and surveillance systems."
http://arxiv.org/abs/2507.00182v2,Graph-Based Deep Learning for Component Segmentation of Maize Plants,"Accurate and automated segmentation of individual components in maize plants (e.g., stalk, leaves, ears) is crucial for high-throughput phenotyping and precision agriculture. However, complex plant architectures, occlusions, and variations in illumination pose significant challenges for existing segmentation techniques. This paper addresses the problem of robust and fine-grained component segmentation of maize plants from 3D point cloud data. We propose a novel graph-based deep learning framework that leverages both geometric and spectral information for improved segmentation. Our method constructs a graph representation of the point cloud, where nodes represent individual points and edges capture spatial relationships. A graph convolutional network (GCN) is then trained to predict semantic labels for each node, incorporating both local geometric features and global spectral embeddings learned through a spectral attention mechanism. We evaluate our method on a challenging dataset of maize plant point clouds, demonstrating significant improvements in segmentation accuracy compared to state-of-the-art point cloud segmentation methods. The proposed graph-based deep learning framework offers a robust and accurate solution for component segmentation in maize plants, enabling more effective plant phenotyping and agricultural management."
http://arxiv.org/abs/2506.23916v1,Three-dimensional end-to-end deep learning for brain MRI analysis,"Brain MRI analysis is crucial for diagnosing and monitoring various neurological disorders. However, traditional methods often rely on handcrafted features and complex pipelines, limiting their efficiency and generalization capabilities. This paper addresses the challenge of developing a streamlined and automated approach for brain MRI analysis by proposing a novel three-dimensional end-to-end deep learning framework. Our method directly processes raw 3D MRI volumes using a custom-designed convolutional neural network architecture, incorporating spatial attention mechanisms to focus on salient anatomical regions. The network is trained end-to-end to perform tasks such as brain age prediction and Alzheimer's disease classification, eliminating the need for intermediate segmentation or feature extraction steps. Experimental results on large publicly available datasets demonstrate that our approach achieves state-of-the-art performance, surpassing existing methods in both accuracy and computational efficiency. This end-to-end 3D deep learning framework offers a promising avenue for advancing automated brain MRI analysis in clinical settings."
http://arxiv.org/abs/2506.23721v1,Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound,"Real-time kidney imaging is crucial for various clinical procedures, including biopsies and nephrostomy, where accurate localization and measurement are paramount. However, manual kidney segmentation in ultrasound (US) images is time-consuming and prone to inter-observer variability, hindering efficient clinical workflows. This paper addresses the challenge of automating real-time kidney segmentation and measurement in US images for integration with augmented reality (AR) guidance. We propose a deep learning-based semantic segmentation framework utilizing a lightweight U-Net architecture optimized for real-time performance. The network is trained on a large dataset of US kidney images with expert annotations and further refined using transfer learning techniques and data augmentation strategies. We integrated the segmentation output into an AR environment, enabling visualization of kidney boundaries and measurements directly overlaid onto the patient during US scanning. Our results demonstrate accurate and robust kidney segmentation with an average Dice score of 92.3% and a processing speed of over 30 frames per second on a standard GPU, facilitating real-time AR-assisted guidance. This automated segmentation pipeline offers a significant improvement in efficiency and accuracy for kidney interventions, paving the way for enhanced clinical decision-making and improved patient outcomes."
http://arxiv.org/abs/2506.23537v2,AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm,"High Dynamic Range (HDR) imaging aims to capture scenes with extreme luminance variations, often requiring merging multiple Low Dynamic Range (LDR) images. Traditional HDR reconstruction methods often suffer from misalignment artifacts and struggle to effectively fuse information across different exposures, especially in complex scenes. This paper addresses the challenge of robust and accurate HDR reconstruction by proposing AFUNet, a novel deep unfolding network that leverages cross-iterative alignment and fusion synergy. AFUNet unfolds the iterative optimization process of a traditional HDR reconstruction algorithm into a learnable deep network, incorporating explicit alignment modules and adaptive fusion blocks within each stage. Crucially, information from both the aligned LDR features and the intermediate HDR estimates are propagated across iterations, enabling a synergistic refinement process that corrects for residual misalignments and progressively fuses information from different exposures. Experimental results on benchmark datasets demonstrate that AFUNet achieves state-of-the-art performance, significantly outperforming existing methods in terms of PSNR, SSIM, and visual quality, particularly in challenging scenes with significant motion and complex lighting. AFUNet offers a robust and efficient solution for HDR reconstruction, advancing the state-of-the-art and enabling high-quality HDR imaging in real-world scenarios."
http://arxiv.org/abs/2506.23030v1,VisionScores -- A system-segmented image score dataset for deep learning tasks,"Image quality assessment plays a crucial role in various computer vision applications, yet existing datasets often lack fine-grained, spatially-aware quality annotations. This paper introduces VisionScores, a novel image dataset featuring human-annotated quality scores for individual segments within images. Unlike global image quality scores, VisionScores provides localized assessments by leveraging a pre-trained segmentation model to divide images into semantically meaningful regions. Each segment is then independently scored by multiple human raters, capturing variations in perceived quality across different image regions. We demonstrate the utility of VisionScores by training deep learning models to predict segment-level quality scores, achieving significantly improved performance compared to models trained solely on global image scores. The dataset enables the development of more nuanced and spatially-aware image quality assessment algorithms, ultimately leading to enhanced performance in downstream computer vision tasks."
http://arxiv.org/abs/2506.23016v1,Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks,"Mild Cognitive Impairment (MCI) represents a transitional stage between normal aging and dementia, often characterized by subtle cognitive decline detectable through neuropsychological assessments. Early and accurate diagnosis of MCI is crucial for timely intervention and potential disease-modifying therapies. This paper addresses the challenge of improving MCI diagnosis by leveraging eye movement patterns and image content during visual memory encoding and retrieval tasks. We propose a novel deep learning framework that integrates convolutional neural networks (CNNs) for image feature extraction with recurrent neural networks (RNNs), specifically LSTMs, to model the temporal dynamics of eye movements. The CNN extracts semantic features from the images used in the visual memory task, while the LSTM learns to classify eye-tracking sequences based on fixations, saccades, and dwell times on specific image regions. We achieved a classification accuracy of 87.5% in distinguishing MCI patients from healthy controls, demonstrating a significant improvement over traditional machine learning methods that rely on hand-engineered features. These findings highlight the potential of deep learning to uncover subtle, yet informative, biomarkers for MCI diagnosis from multimodal data streams derived from visual memory tasks."
http://arxiv.org/abs/2506.22939v1,Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data,"Scene categorization in remote sensing imagery is crucial for various applications, including urban planning, environmental monitoring, and disaster management. However, the inherent complexity of high-resolution remote sensing data, characterized by significant intra-class variability and subtle inter-class differences, poses a significant challenge for accurate scene classification. This paper introduces a novel deep learning method, termed Multi-Scale Attention Fusion Network (MSAFN), specifically designed for robust scene categorization in remote sensing data. MSAFN leverages a convolutional neural network backbone coupled with a multi-scale attention module to capture contextual information at varying granularities. Further, a novel fusion mechanism is employed to effectively integrate these multi-scale features, enabling the network to learn discriminative representations that are invariant to scale variations and object orientations. Experimental results on benchmark datasets, including the UC Merced Land Use dataset and the WHU-RS19 dataset, demonstrate that MSAFN achieves state-of-the-art performance, surpassing existing methods by a significant margin in terms of overall accuracy and per-class F1-score. The proposed MSAFN provides a powerful and effective solution for automated scene categorization, facilitating improved analysis and interpretation of remote sensing data."
http://arxiv.org/abs/2506.22850v1,DMD-Net: Deep Mesh Denoising Network,"Surface mesh denoising is a fundamental task in computer graphics and geometry processing, aiming to remove noise while preserving important geometric features. Traditional mesh denoising methods often struggle with complex noise distributions and sharp feature preservation, leading to over-smoothed or under-denoised results. To address these limitations, we propose DMD-Net, a novel Deep Mesh Denoising Network leveraging intrinsic mesh geometry. DMD-Net operates directly on the mesh structure, employing a graph convolutional network architecture to learn denoising filters. Specifically, we introduce a novel feature extraction module based on geodesic distances and dihedral angles, capturing both local and non-local geometric context. Furthermore, we incorporate a feature aggregation module that adaptively weights and combines features from multiple scales to enhance denoising performance across varying noise levels and feature sizes. Experimental results on benchmark datasets demonstrate that DMD-Net significantly outperforms state-of-the-art mesh denoising algorithms, achieving higher accuracy and better feature preservation. This work presents a robust and effective deep learning approach for mesh denoising, offering a promising alternative to traditional methods."
http://arxiv.org/abs/2506.22749v1,Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds,"Large-scale colored point clouds are increasingly prevalent in various applications, including autonomous driving, robotics, and virtual reality. However, acquiring high-density point clouds with rich attribute information remains challenging due to hardware limitations and data acquisition costs. This paper addresses the problem of jointly up-sampling both the geometry and attributes of sparse colored point clouds in a large-scale setting. We propose a novel deep learning framework, termed Joint Geometry and Attribute Upsampling Network (JGAUN), which leverages a multi-branch architecture to simultaneously learn geometry and attribute features from local point neighborhoods. JGAUN incorporates a geometry-aware attention mechanism to adaptively weight the contributions of neighboring points for geometry reconstruction, while a shared feature space facilitates the transfer of information between geometry and attribute up-sampling branches. Experiments on large-scale datasets demonstrate that JGAUN significantly outperforms state-of-the-art point cloud up-sampling methods in terms of both geometry accuracy and attribute fidelity, while maintaining computational efficiency. This work offers a practical solution for generating high-quality, dense colored point clouds from sparse inputs, enabling improved performance in downstream tasks."
http://arxiv.org/abs/2506.22338v1,A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake,"Synthetic Aperture Radar (SAR) data offers a crucial advantage for rapid disaster response due to its all-weather, day-and-night imaging capabilities. However, extracting reliable and detailed building damage information from SAR imagery remains a significant challenge, particularly in densely built urban environments affected by earthquakes. This paper addresses the problem of automated and accurate building damage assessment following the 2023 Turkiye earthquake, leveraging Very High Resolution (VHR) SAR data and integrating it with readily available geospatial information. We propose a novel deep learning framework that combines a multi-temporal SAR change detection module with a geospatial feature encoding module. The SAR module utilizes a Siamese U-Net architecture to identify changes between pre- and post-event SAR images, while the geospatial module incorporates building footprints and height information. The outputs are then fused and processed by a downstream classifier to predict damage levels. Experimental results on a severely affected area demonstrate the framework's ability to achieve state-of-the-art performance in building damage classification, with an overall accuracy exceeding 85% and a significant improvement in F1-score for the critically damaged class compared to existing methods. This framework offers a valuable tool for rapid and reliable damage assessment, supporting efficient disaster relief and recovery efforts."
http://arxiv.org/abs/2506.22532v1,High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning,"Three-dimensional (3D) cine imaging, capturing dynamic volumetric data over time, is crucial for understanding complex biological processes. However, achieving high spatial resolution, isotropic voxels, and rapid acquisition speeds simultaneously remains a significant challenge, particularly for live specimens. This paper addresses the problem of reconstructing high-resolution isotropic 3D cine volumes from a limited number of 2D real-time image sequences. Our method concatenates multiple synchronized 2D real-time image streams acquired from different viewing angles. We then employ a deep learning framework trained on synthetic data to perform automated segmentation and to reconstruct a high-resolution isotropic 3D volume at each time point, effectively overcoming limitations in optical resolution and enabling accurate volumetric representation. We demonstrate the efficacy of our approach by imaging beating cardiomyocytes, achieving a 5x improvement in axial resolution compared to the native optical resolution and enabling accurate cell segmentation over time. This method provides a powerful tool for studying dynamic 3D biological phenomena with unprecedented detail and automation."
http://arxiv.org/abs/2506.22222v1,Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections,"Type B aortic dissection (TBAD) is a life-threatening condition requiring precise diagnosis and monitoring through medical imaging. Accurate segmentation of the true lumen (TL), false lumen (FL), and aortic wall in computed tomography angiography (CTA) scans is crucial for treatment planning and follow-up, but manual segmentation is time-consuming and prone to inter-observer variability. This paper addresses the challenge of automating TBAD segmentation by leveraging advanced deep learning techniques to improve accuracy and efficiency. We propose a novel multi-stage framework incorporating a 3D U-Net architecture with attention mechanisms for initial segmentation, followed by a conditional generative adversarial network (cGAN) for refinement and boundary correction. The cGAN is conditioned on both the input CTA image and the initial segmentation, enabling it to learn complex anatomical features and produce highly accurate segmentations. Experimental results on a clinically realistic dataset of TBAD CTA scans demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in terms of Dice score, Hausdorff distance, and segmentation time. This automated segmentation framework has the potential to significantly improve the efficiency and consistency of TBAD diagnosis and management."
http://arxiv.org/abs/2506.22216v1,ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning,"Low-light image enhancement is crucial for improving the visibility and perception of images captured in challenging lighting conditions. However, existing methods often struggle to achieve personalized enhancement, failing to cater to individual image characteristics and user preferences. This paper introduces ReF-LLE, a novel personalized low-light enhancement framework leveraging Reference-Guided Deep Reinforcement Learning. Our approach employs a deep reinforcement learning agent trained to sequentially adjust image enhancement parameters based on both the low-light input and a user-provided reference image exhibiting desired aesthetic qualities. The agent learns to optimize a reward function that balances perceptual quality, similarity to the reference image, and fidelity to the original content, ensuring a personalized and visually pleasing enhancement. Extensive experiments on benchmark datasets demonstrate that ReF-LLE achieves superior performance compared to state-of-the-art methods in terms of both quantitative metrics and subjective visual quality, generating enhanced images that closely resemble the artistic style of the reference image. This work provides a significant advancement in personalized low-light image enhancement by seamlessly integrating user preferences into the enhancement process."
http://arxiv.org/abs/2506.22041v1,Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning,"White matter lesions (WMLs) are commonly observed in magnetic resonance imaging (MRI) of aging brains and are associated with various neurological disorders. Accurate and automated WML localization is crucial for efficient diagnosis and disease monitoring, yet current methods often struggle with scalability and robustness across diverse datasets. This paper addresses the challenge of developing a WML localization method that is both scalable to large datasets and robust to variations in image acquisition parameters and lesion characteristics. We propose a novel multimodal deep learning framework that leverages complementary information from T1-weighted, T2-weighted, and FLAIR MRI sequences. Our architecture incorporates a 3D U-Net backbone enhanced with attention mechanisms to focus on lesion-relevant features and a multi-task learning strategy to simultaneously predict lesion probability maps and lesion burden scores. Experimental results on a large multi-center dataset demonstrate that our approach achieves state-of-the-art performance in WML segmentation, exhibiting improved Dice scores and reduced false positive rates compared to existing methods. The proposed framework offers a practical and reliable solution for automated WML localization, facilitating large-scale neuroimaging studies and clinical applications."
http://arxiv.org/abs/2506.21945v1,SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images,"Semantic segmentation of fine-resolution remotely sensed images is crucial for various applications, including urban planning, environmental monitoring, and disaster management. However, the high spatial resolution and complex spectral characteristics of these images pose significant challenges for accurate and efficient segmentation. This paper addresses the problem of effectively capturing both local and global contextual information necessary for precise semantic labeling in such complex scenes. We propose SDRNET, a novel Stacked Deep Residual Network architecture specifically designed for fine-resolution remote sensing image segmentation. SDRNET leverages a multi-stage stacking of residual blocks to progressively extract hierarchical features while incorporating attention mechanisms to enhance feature representation. Furthermore, we utilize dilated convolutions within the residual blocks to expand the receptive field without sacrificing spatial resolution. Experimental results on benchmark datasets demonstrate that SDRNET achieves state-of-the-art performance, outperforming existing methods in terms of overall accuracy and intersection-over-union (IoU) metrics. SDRNET provides a robust and accurate solution for semantic segmentation, advancing the capabilities of remote sensing image analysis for various real-world applications."
http://arxiv.org/abs/2506.21891v1,DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025,"Understanding and reasoning about video content remains a significant challenge in computer vision, particularly in the context of long, untrimmed videos with complex activities. This paper addresses the problem of efficiently exploring and retrieving relevant segments within large video datasets, as posed by the CVRR (Contextual Video Retrieval and Reasoning) challenge at CVPR 2025. We introduce DIVE: Deep-search Iterative Video Exploration, a novel framework that iteratively refines its search strategy based on contextual understanding and user feedback (simulated in the challenge). DIVE leverages a combination of transformer-based video encoders to extract rich visual and textual features, a dynamic search space pruning mechanism to focus on promising regions, and a reinforcement learning agent to optimize the exploration policy for maximizing retrieval accuracy within a limited query budget. Our experiments on the CVRR benchmark demonstrate that DIVE achieves state-of-the-art performance, significantly improving the recall of relevant video segments compared to baseline methods while maintaining computational efficiency. These results highlight the potential of iterative exploration strategies guided by deep learning for efficient and accurate video understanding and retrieval."
http://arxiv.org/abs/2506.21770v1,Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images,"Glaucoma, a leading cause of irreversible blindness, often progresses asymptomatically in its early stages, making timely detection crucial for preventing vision loss. This paper addresses the challenge of early glaucoma detection by leveraging deep learning techniques on fundus images, aiming to improve diagnostic accuracy and accessibility. We propose a novel multi-dataset training strategy using a convolutional neural network (CNN) architecture. The model is trained on a combination of publicly available glaucoma datasets, employing data augmentation and transfer learning to enhance generalization and robustness across diverse image characteristics and labeling conventions. Specifically, we fine-tune a pre-trained ResNet-50 model using a weighted cross-entropy loss function to address class imbalance in the datasets. Experimental results demonstrate that our multi-dataset approach achieves superior performance compared to models trained on individual datasets, reaching an AUC of 0.92 and a sensitivity of 88% at a specificity of 85% on an independent test set. This research offers a valuable contribution towards developing automated, reliable, and widely applicable glaucoma screening tools, facilitating early intervention and potentially mitigating the global burden of glaucoma-related blindness."
http://arxiv.org/abs/2506.21444v2,Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation,"Mitotic figure assessment is crucial for cancer diagnosis and prognosis, with atypical mitoses indicating potential genomic instability and aggressive tumor behavior. However, accurate classification of atypical versus normal mitotic figures remains challenging, particularly due to subtle morphological differences and inter-observer variability. This paper addresses the problem of robust atypical mitosis classification by benchmarking state-of-the-art deep learning models and vision foundation models (VFMs) using a rigorous cross-dataset evaluation protocol. We fine-tune several convolutional neural networks (CNNs), including ResNet and EfficientNet variants, and explore the transfer learning capabilities of VFMs like DINOv2 and CLIP. Our evaluation encompasses diverse mitotic figure datasets with varying staining protocols and image resolutions, assessing performance on both in-distribution and out-of-distribution scenarios. Results demonstrate that while specialized CNNs achieve high accuracy within their training distribution, VFMs exhibit superior generalization capabilities and robustness to domain shifts, achieving a relative performance increase of up to 15% in cross-dataset evaluations. These findings highlight the potential of VFMs for building more reliable and generalizable automated mitosis classification systems, ultimately improving the accuracy and efficiency of cancer diagnosis."
http://arxiv.org/abs/2506.21151v1,Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels,"Late Gadolinium Enhancement Cardiac Magnetic Resonance Imaging (LGE-CMR) is the gold standard for assessing myocardial scar, crucial for diagnosis and prognosis of various heart conditions. However, accurate scar segmentation is challenging due to the variability in scar appearance and the presence of noise in manual annotations, which significantly impacts the performance of deep learning models. This paper addresses the problem of robust myocardial scar segmentation in LGE-CMR images when training with noisy labels. We propose a novel deep learning framework that integrates a label correction module based on a Gaussian Mixture Model (GMM) and a spatial attention mechanism into a 3D U-Net architecture. The GMM identifies and corrects potentially mislabeled voxels by estimating the probability of each voxel belonging to the scar or background class, while the spatial attention refines the segmentation by focusing on relevant anatomical regions. Our experimental results on a large LGE-CMR dataset demonstrate that the proposed method significantly outperforms state-of-the-art segmentation techniques, achieving improved Dice scores and reduced Hausdorff distances even in the presence of substantial label noise. This approach offers a promising solution for reliable and automated scar quantification, facilitating improved clinical workflows and patient care."
http://arxiv.org/abs/2506.20464v1,A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners,"Rock bolts are critical safety components in underground mines, ensuring structural stability. However, their accurate and automated identification in complex 3D point clouds acquired via mobile laser scanners remains a significant challenge due to noise, varying point densities, and the presence of other mining infrastructure. This paper presents a novel deep learning approach for robust rock bolt identification in such environments. Our method employs a PointNet++ architecture modified to incorporate contextual information through a customized feature aggregation module. This module leverages local and global point cloud characteristics to enhance the discriminative power of the network for distinguishing rock bolts from surrounding clutter. We trained and validated our approach on a large dataset of real-world underground mine point clouds, achieving a mean average precision (mAP) of 92.5% and significantly outperforming traditional geometric-based methods. This accurate and automated rock bolt identification system enables efficient mine safety inspections and structural health monitoring, reducing manual labor and improving overall operational safety."
http://arxiv.org/abs/2506.20407v2,Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images,"Gestational age (GA) estimation is crucial for monitoring fetal development and predicting delivery outcomes. While ultrasound imaging is routinely used for this purpose, accurate GA estimation remains challenging due to inter-observer variability and subjective measurements. This paper addresses the problem of improving GA estimation accuracy by leveraging both radiomic features and deep learning representations extracted from fetal ultrasound images. We propose a novel fusion approach that combines handcrafted radiomic features, capturing subtle textural and morphological characteristics, with deep features learned by a pre-trained convolutional neural network fine-tuned on a large fetal ultrasound dataset. The fused feature vector is then used to train a regression model for GA prediction. Experimental results on a large, multi-center dataset demonstrate that our method achieves significantly lower mean absolute error (MAE) and root mean squared error (RMSE) compared to methods relying solely on radiomic features or deep learning representations. This hybrid approach provides a robust and accurate method for GA estimation, potentially improving prenatal care and reducing adverse perinatal outcomes."
http://arxiv.org/abs/2506.22498v1,ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction,"Bed-exit events pose significant fall risks for patients in healthcare settings, necessitating proactive monitoring and prediction. Existing bed-exit prediction systems often rely on video or wearable sensors, which can raise privacy concerns or be cumbersome for patients. This paper addresses the challenge of early and accurate bed-exit prediction using only time-series image representations derived from load signals captured by bed sensors. We propose ViFusionTST, a novel deep fusion architecture that leverages Vision Transformers (ViT) to extract spatial features from time-series images representing load distribution patterns and temporal self-attention mechanisms to capture the evolution of these patterns over time. ViFusionTST employs an early fusion strategy to combine information from multiple load sensors, enabling a holistic understanding of patient movement. Experimental results on a real-world dataset demonstrate that ViFusionTST achieves state-of-the-art performance in early bed-exit prediction, significantly outperforming existing methods in terms of precision and recall. This privacy-preserving and non-obtrusive approach offers a valuable tool for enhancing patient safety and reducing fall-related incidents in healthcare environments."
http://arxiv.org/abs/2506.20152v1,Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration,"Structured pruning is an effective technique for reducing the computational cost and memory footprint of deep neural networks, facilitating their deployment on resource-constrained devices. However, determining the optimal pruning criterion for a given network and task remains a challenging, often manually-intensive process. This paper addresses the problem of automatically selecting the most effective structured pruning criterion from a set of candidates, based on its impact on the network's loss function. We propose a novel loss-aware criterion selection framework that leverages a differentiable approximation of the pruning process to estimate the sensitivity of the network's loss to the removal of different structures based on various pruning criteria. This allows us to train a selector network to predict the best criterion for each layer, optimizing for minimal performance degradation after pruning. Experiments on various benchmark datasets and network architectures demonstrate that our method consistently identifies pruning criteria that lead to significantly higher accuracy compared to using a single, hand-selected criterion across all layers, achieving up to 2% improvement in top-1 accuracy on ImageNet with ResNet-50 at 50% sparsity. Our approach provides a principled and automated way to optimize structured pruning, leading to more efficient and accurate deep neural networks for deployment in resource-limited environments."
http://arxiv.org/abs/2506.19167v1,A Deep Learning Based Method for Fast Registration of Cardiac Magnetic Resonance Images,"Cardiac Magnetic Resonance (CMR) imaging is a crucial tool for assessing cardiac function and diagnosing cardiovascular diseases. Accurate registration of CMR images, aligning images acquired at different time points or orientations, is essential for quantitative analysis and motion tracking. However, traditional registration methods are often computationally expensive and time-consuming, hindering their application in real-time clinical settings. This paper addresses the challenge of achieving fast and accurate CMR image registration by proposing a deep learning-based approach. Our method employs a convolutional neural network (CNN) trained to directly predict the deformation field between a moving and a fixed image. Specifically, we utilize a U-Net architecture with spatial transformer networks to learn the non-linear mapping, enabling efficient and accurate image alignment. We demonstrate that our deep learning model achieves comparable registration accuracy to conventional iterative methods while significantly reducing computation time, achieving an average speedup of over 100x. This fast and accurate registration method has the potential to improve the efficiency and accessibility of CMR image analysis in clinical practice."
http://arxiv.org/abs/2506.18731v1,Deep CNN Face Matchers Inherently Support Revocable Biometric Templates,"Deep Convolutional Neural Networks (CNNs) have achieved remarkable performance in face recognition, often surpassing traditional methods. However, the vulnerability of face recognition systems to presentation attacks and the inherent immutability of facial biometrics raise serious privacy concerns. This paper addresses the problem of designing revocable biometric templates from deep CNN face matchers without sacrificing accuracy or requiring retraining. We propose a novel framework that leverages the learned feature space of a pre-trained CNN to generate revocable templates. Our approach involves applying a non-invertible transformation parameterized by a user-specific key to the extracted feature vectors, effectively altering the biometric representation while preserving discriminative information. Experiments on benchmark datasets demonstrate that our method maintains near state-of-the-art verification performance even after multiple template revocations, and significantly outperforms existing revocable template techniques. This work provides a practical and efficient solution for enhancing the security and privacy of face recognition systems by enabling template revocability without compromising accuracy."
http://arxiv.org/abs/2506.18679v2,MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation,"Active contour models offer a flexible framework for medical image segmentation, but their performance is highly dependent on initialization and parameter tuning, often requiring significant manual intervention. This paper addresses the challenge of automating and optimizing active contour evolution by leveraging multi-agent deep reinforcement learning (MARL). We introduce MARL-MambaContour, a novel approach where multiple agents, each controlling a segment of the active contour, collaboratively refine the contour's shape through individual actions informed by a shared Mamba-based perception module. This perception module processes local image patches and contour geometry, enabling each agent to learn an optimal policy for contour adjustment. Experiments on cardiac and brain MRI datasets demonstrate that MARL-MambaContour achieves significant improvements in segmentation accuracy and robustness compared to traditional active contour methods and single-agent reinforcement learning approaches, exhibiting superior Dice scores and reduced sensitivity to initialization. Our method offers a fully automated and adaptable framework for active contour optimization, paving the way for more efficient and reliable medical image segmentation pipelines."
http://arxiv.org/abs/2506.18474v1,A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation,"Semantic segmentation with deep convolutional neural networks (DCNNs) has achieved remarkable success in various applications. However, real-world datasets often exhibit severe class imbalance, where certain classes are significantly under-represented, leading to biased models and poor segmentation performance for minority classes. To address this issue, we propose a novel class balancing strategy integrated within a DCNN framework specifically designed for imbalanced data segmentation. Our approach, termed ""Class-Aware Feature Re-weighting and Augmentation Network (CAFRAN)"", dynamically re-weights feature maps based on class prevalence during training and incorporates a novel augmentation technique that focuses on generating synthetic samples for under-represented classes within the feature space. Specifically, CAFRAN learns class-specific attention maps to emphasize minority class features and uses a conditional generative adversarial network (cGAN) to augment these features, ensuring diversity and preventing overfitting. Experiments on benchmark datasets with significant class imbalance demonstrate that CAFRAN significantly outperforms existing state-of-the-art methods, achieving substantial improvements in Intersection-over-Union (IoU) scores for minority classes while maintaining competitive performance on majority classes. This research provides a promising solution for robust and accurate semantic segmentation in the presence of severe class imbalance, enhancing the applicability of DCNNs in real-world scenarios."
http://arxiv.org/abs/2506.18284v1,Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset,"Endoscopic image classification is crucial for computer-aided diagnosis of gastrointestinal diseases; however, most existing methods assume a closed-set scenario where all possible classes are known during training. This assumption limits their applicability in real-world clinical settings where novel, unseen pathologies may be encountered. This paper addresses the challenge of open set recognition (OSR) in endoscopic image classification, specifically focusing on the task of identifying both known and unknown classes within the Kvasir dataset. We propose a novel deep learning approach combining a pre-trained EfficientNet-B0 backbone with a Gaussian Mixture Model (GMM) to model the feature distribution of known classes in the embedding space. During inference, a likelihood ratio test based on the GMM is used to differentiate between known and unknown samples. Our experiments demonstrate that the proposed method achieves a significant improvement in OSR performance compared to traditional softmax-based classifiers, with an Area Under the ROC Curve (AUC) of 92.3% for detecting unknown samples while maintaining competitive accuracy on known classes. This demonstrates the potential of our approach to enhance the robustness and reliability of endoscopic image analysis systems in realistic clinical environments by effectively handling unseen pathologies."
http://arxiv.org/abs/2506.18209v1,Deep Learning-based Alignment Measurement in Knee Radiographs,"Knee osteoarthritis (OA) is a prevalent musculoskeletal disease, and accurate alignment measurement in radiographs is crucial for diagnosis and treatment planning. However, manual measurement is time-consuming and subject to inter-observer variability. This paper addresses the problem of automating precise and robust alignment measurement in knee radiographs using deep learning. We propose a novel end-to-end deep learning framework that directly predicts anatomical landmarks relevant to alignment angles, such as the hip center, knee center, and ankle center, from raw radiographic images. The network architecture incorporates a stacked hourglass network for robust landmark detection, followed by a regression module to estimate the mechanical axis angle (MAA) and other relevant alignment parameters. Experimental results on a large dataset of knee radiographs demonstrate that our method achieves state-of-the-art accuracy in landmark localization and alignment angle estimation, approaching the performance of expert radiologists with significantly reduced processing time. This automated approach offers a practical and reliable solution for large-scale screening and monitoring of knee OA, improving diagnostic accuracy and clinical workflow efficiency."
http://arxiv.org/abs/2506.18069v2,Unfolding the Past: A Comprehensive Deep Learning Approach to Analyzing Incunabula Pages,"Incunabula, the earliest printed books, offer invaluable insights into the transition from manuscript to print culture, yet their fragile condition and diverse layouts present significant challenges for automated analysis. This paper addresses the problem of efficiently and accurately extracting and interpreting textual and visual information from digitized incunabula pages. We introduce a novel deep learning pipeline, ""Unfolding the Past,"" which integrates a custom-trained object detection model for layout analysis, a robust optical character recognition (OCR) module fine-tuned on historical fonts, and a graph neural network (GNN) to capture relationships between detected elements. The object detection model segments text blocks, illustrations, and decorative elements, while the OCR module transcribes text with high accuracy, even in the presence of noise and varying font styles. The GNN then leverages spatial relationships and semantic context to correct OCR errors and infer page structure. Our experiments on a diverse dataset of incunabula pages demonstrate significant improvements in layout analysis accuracy (mAP of 0.85) and OCR performance (character error rate reduction of 20% compared to off-the-shelf solutions). This comprehensive approach enables large-scale automated analysis of incunabula, facilitating new avenues for research in book history, typography, and the cultural impact of early printing."
http://arxiv.org/abs/2506.18060v1,Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes,"Accurate estimation of wheat spike morphology is crucial for high-throughput phenotyping and genetic studies related to yield improvement. Existing methods for 3D spike reconstruction often rely on specialized hardware or struggle with the complex, self-occluding structure of wheat spikes. This paper addresses the challenge of estimating detailed 3D morphological traits of wheat spikes directly from multi-view RGB images. We propose a novel deep learning framework based on a supervised Long Short-Term Memory (LSTM) network, which processes sequentially arranged multi-view images to predict a 3D point cloud representation of the spike. The LSTM architecture learns temporal dependencies between views, effectively integrating information from different perspectives, and is trained with a combination of Chamfer distance and normal consistency loss to ensure accurate and geometrically plausible reconstructions. Experimental results on a diverse dataset of wheat spikes demonstrate that our method achieves state-of-the-art performance in 3D reconstruction accuracy compared to existing multi-view stereo approaches, exhibiting a significant improvement in capturing fine-grained morphological details. This approach offers a cost-effective and accurate solution for high-throughput phenotyping, enabling more efficient breeding strategies for enhanced wheat production."
http://arxiv.org/abs/2506.16735v1,3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting,"Hyperspectral image (HSI) inpainting aims to reconstruct missing or corrupted spectral information, which is crucial for various remote sensing applications. Traditional HSI inpainting methods often struggle to capture the complex spectral-spatial correlations present in high-dimensional HSI data, particularly when dealing with large contiguous missing regions. To address this limitation, we propose 3DeepRep, a novel 3D deep low-rank tensor representation learning framework for HSI inpainting. 3DeepRep leverages a 3D convolutional neural network (CNN) to extract high-level features from the incomplete HSI cube, followed by a tensor low-rank approximation to effectively capture the underlying global structure and reduce noise. Specifically, we employ a Tucker decomposition-inspired architecture within the CNN to enforce low-rank constraints on the learned feature tensors, promoting spectral-spatial consistency. Experimental results on benchmark HSI datasets demonstrate that 3DeepRep significantly outperforms state-of-the-art methods in terms of quantitative metrics (PSNR, SSIM, SAM) and visual quality, particularly in scenarios with large missing regions. This research provides a powerful and efficient approach for HSI inpainting, enabling improved performance in downstream tasks such as classification and target detection."
http://arxiv.org/abs/2506.16418v1,Efficient Transformations in Deep Learning Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, but their computational cost and energy consumption remain significant bottlenecks, especially for deployment on resource-constrained devices. This paper addresses the challenge of improving the efficiency of CNNs by focusing on optimizing the transformations within convolutional layers, which are the most computationally intensive components. We propose a novel framework, Transformation Decomposition and Selection (TDS), that decomposes complex convolutional transformations into a sequence of simpler, learnable transformations. TDS leverages a reinforcement learning agent to dynamically select the optimal sequence of transformations for each layer and input, balancing accuracy and computational cost. Experiments on standard image classification datasets, including CIFAR-10 and ImageNet, demonstrate that TDS achieves comparable or superior accuracy to existing CNN architectures while significantly reducing the number of floating-point operations (FLOPs) by up to 40%. These results highlight the potential of TDS to create more efficient and deployable CNNs, enabling wider adoption of deep learning in resource-limited environments."
http://arxiv.org/abs/2506.16353v1,MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval,"Deep hashing has emerged as a powerful technique for large-scale image retrieval due to its efficiency in storage and search. However, existing deep hashing methods often struggle to effectively capture long-range dependencies and global contextual information, limiting their performance on complex visual data. To address this limitation, we propose MambaHash, a novel visual state space deep hashing model leveraging the Mamba architecture, known for its selective state space modeling capabilities. MambaHash employs a hierarchical Mamba-based encoder to extract robust and context-aware image features, followed by a quantization layer to generate compact binary hash codes. A tailored loss function, combining pairwise ranking and quantization loss, is designed to optimize the hashing network for accurate similarity preservation. Extensive experiments on benchmark datasets, including CIFAR-10, NUS-WIDE, and ImageNet, demonstrate that MambaHash significantly outperforms state-of-the-art deep hashing methods in terms of retrieval accuracy and efficiency. The proposed MambaHash offers a promising solution for efficient and accurate large-scale image retrieval by effectively integrating state space modeling into deep hashing frameworks."
http://arxiv.org/abs/2506.15908v1,Pediatric Pancreas Segmentation from MRI Scans with Deep Learning,"Pediatric pancreatic diseases, such as pancreatitis and cystic fibrosis, often require accurate assessment of pancreatic morphology and volume via Magnetic Resonance Imaging (MRI). Manual segmentation of the pediatric pancreas from MRI scans is a time-consuming and subjective task, presenting a significant bottleneck in clinical workflows and hindering quantitative analysis. This paper proposes a novel deep learning framework for automated pediatric pancreas segmentation from MRI scans. Our approach utilizes a 3D U-Net architecture incorporating attention mechanisms and deep supervision to enhance feature extraction and segmentation accuracy, specifically addressing the challenges posed by the relatively small size and variable shape of the pediatric pancreas. We trained and evaluated our model on a multi-center dataset of pediatric abdominal MRI scans, achieving a Dice score of 82.3% and an Average Symmetric Surface Distance (ASSD) of 1.2 mm, significantly outperforming existing atlas-based and conventional segmentation techniques. This automated, accurate segmentation method has the potential to improve diagnostic accuracy, facilitate quantitative research, and ultimately enhance the clinical management of pediatric pancreatic diseases."
http://arxiv.org/abs/2506.15806v1,Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving,"Autonomous driving requires robust 3D scene understanding for safe navigation and collision avoidance. Explicit 3D representations, like point clouds or meshes, are often computationally expensive and memory intensive, hindering real-time performance. This paper addresses the challenge of efficiently reconstructing 3D scenes from monocular images for enhanced collision understanding in autonomous driving scenarios. We propose a novel deep learning framework that leverages implicit neural representations to directly predict signed distance functions (SDFs) from single-view images. Our network, trained on synthetic and real-world datasets, learns to implicitly encode the 3D geometry of the scene, enabling efficient and continuous surface reconstruction. Experimental results demonstrate that our method achieves comparable accuracy to explicit reconstruction techniques while significantly reducing memory footprint and inference time. Furthermore, we show that the learned SDF representation facilitates accurate collision prediction and avoidance, leading to improved autonomous driving performance in simulated environments. This work offers a promising approach for real-time 3D scene understanding, contributing to safer and more efficient autonomous navigation."
http://arxiv.org/abs/2506.15182v1,Classification of Multi-Parametric Body MRI Series Using Deep Learning,"Multi-parametric body Magnetic Resonance Imaging (MRI) provides comprehensive information for disease diagnosis and staging. However, the manual interpretation of these multi-sequence datasets is time-consuming and susceptible to inter-observer variability. This paper addresses the problem of automated classification of body MRI series directly from raw image data, eliminating the need for manual feature engineering or segmentation. We propose a novel deep learning framework based on a 3D convolutional neural network (CNN) that simultaneously learns spatial features within each MRI sequence and integrates information across multiple parametric series. Specifically, we employ a multi-stream architecture where each stream processes a distinct MRI sequence, followed by a fusion module to combine the learned representations for final classification. We evaluated our approach on a large clinical dataset of abdominal MRI scans for liver lesion classification, achieving an average AUC of 0.92, significantly outperforming traditional machine learning methods and demonstrating comparable performance to expert radiologists. This automated classification pipeline has the potential to improve diagnostic accuracy, reduce reporting times, and facilitate large-scale population screening."
http://arxiv.org/abs/2506.14524v1,Integrating Radiomics with Deep Learning Enhances Multiple Sclerosis Lesion Delineation,"Accurate and reliable delineation of multiple sclerosis (MS) lesions from magnetic resonance imaging (MRI) is crucial for monitoring disease progression and evaluating treatment efficacy. Manual lesion segmentation is time-consuming and prone to inter-rater variability. This paper addresses the challenge of improving the accuracy and robustness of automated MS lesion segmentation by integrating radiomic features with a deep learning framework. We propose a novel approach that combines a 3D U-Net architecture with radiomic features extracted from the lesion candidates predicted by the U-Net. Specifically, the radiomic features are incorporated into a secondary classification stage to refine the initial U-Net predictions. We evaluated our method on a multi-center dataset of MS patients and demonstrate a significant improvement in Dice score and lesion-wise sensitivity compared to the U-Net baseline and other state-of-the-art methods. The integration of radiomics with deep learning provides a more comprehensive and accurate approach to MS lesion segmentation, potentially leading to improved clinical decision-making and personalized treatment strategies."
http://arxiv.org/abs/2506.14367v1,DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI,"Accurate and efficient brain disease classification is crucial for timely diagnosis and treatment planning. Existing deep learning methods often lack explainability and struggle with the complexities of multi-class differentiation in neuroimaging data. This paper introduces DGG-XNet, a novel hybrid deep learning framework for multi-class brain disease classification that integrates a DenseNet backbone with a Guided Grad-CAM (Grad-CAM++) explainability module. DGG-XNet leverages the feature extraction capabilities of DenseNet to capture intricate patterns in MRI images. Subsequently, Grad-CAM++ is employed to generate fine-grained, class-discriminative heatmaps, highlighting the image regions most influential in the classification decision. We evaluate DGG-XNet on a large multi-class brain disease dataset, demonstrating superior classification accuracy (e.g., achieving an average F1-score of 94.5%) compared to state-of-the-art methods, while also providing interpretable visualizations that enhance clinical trust and understanding. The proposed framework offers a valuable tool for radiologists and clinicians by providing both accurate diagnoses and visual explanations, thereby improving the efficiency and reliability of brain disease diagnosis."
http://arxiv.org/abs/2506.13897v3,DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding,"Human activity understanding (HAU) from point clouds is crucial for various applications, yet remains challenging due to viewpoint variations and the inherent sparsity of point cloud data. Existing methods often rely solely on point cloud geometry, neglecting valuable information from readily available multi-modal sources like skeleton data, inertial measurement units (IMUs), and textual descriptions. This paper introduces DeSPITE, a novel framework for contrastive deep learning of multi-modal embeddings to enhance point cloud HAU. DeSPITE leverages a shared embedding space where point cloud features are aligned with corresponding skeleton, IMU, and text modalities using a contrastive loss. Specifically, we employ modality-specific encoders to extract features, followed by a cross-modal attention mechanism to capture intricate relationships between point cloud and other modalities. Our experiments on benchmark datasets demonstrate that DeSPITE significantly outperforms state-of-the-art point cloud-based HAU methods, achieving notable improvements in classification accuracy and robustness to noise. These results highlight the effectiveness of our contrastive multi-modal learning approach for enriching point cloud representations and advancing the field of human activity understanding."
http://arxiv.org/abs/2506.13484v1,Deep Diffusion Models and Unsupervised Hyperspectral Unmixing for Realistic Abundance Map Synthesis,"Hyperspectral unmixing (HSU) aims to decompose hyperspectral images into constituent pure spectral signatures (endmembers) and their corresponding fractional abundances. Generating realistic abundance maps for synthetic hyperspectral data is challenging, often relying on simplistic parametric models or requiring extensive manual annotation. This paper addresses the problem of synthesizing realistic abundance maps in an unsupervised manner for the purpose of generating high-quality synthetic hyperspectral data. We propose a novel framework that leverages deep diffusion models conditioned on endmember signatures to generate abundance maps. Specifically, we train a diffusion model on real abundance maps obtained via unsupervised HSU applied to real hyperspectral imagery. This allows the diffusion model to learn the complex spatial correlations and statistical properties inherent in real-world abundance distributions. Our experiments demonstrate that abundance maps synthesized using our approach exhibit significantly improved realism compared to those generated by traditional methods, as evaluated by quantitative metrics and visual inspection. This leads to more realistic synthetic hyperspectral data, which can be crucial for training and benchmarking HSU algorithms. The proposed method offers a powerful and unsupervised approach for generating realistic abundance maps, facilitating the development and evaluation of hyperspectral image analysis techniques."
http://arxiv.org/abs/2506.13457v1,Deep Learning-Based Multi-Object Tracking: A Comprehensive Survey from Foundations to State-of-the-Art,"Multi-object tracking (MOT) is a crucial task in computer vision, enabling a wide range of applications from autonomous driving to video surveillance. However, the inherent challenges of data association, occlusions, and complex object interactions necessitate robust and accurate tracking algorithms. This paper addresses the critical need for a comprehensive understanding of deep learning-based MOT methods, which have revolutionized the field in recent years. We present a structured survey that systematically categorizes and analyzes existing deep learning approaches for MOT, spanning from fundamental tracking-by-detection paradigms to end-to-end trainable architectures. The survey covers key aspects such as feature extraction, association strategies, re-identification techniques, and the utilization of different deep learning architectures (e.g., CNNs, RNNs, Transformers). Furthermore, we provide a detailed performance comparison of state-of-the-art methods on benchmark datasets, highlighting their strengths and limitations. The analysis reveals that deep learning-based MOT algorithms consistently outperform traditional methods, particularly in crowded scenes and under challenging conditions, showcasing the significant advancements driven by deep learning. This survey serves as a valuable resource for researchers and practitioners, providing a clear roadmap for navigating the rapidly evolving landscape of deep learning-based multi-object tracking."
http://arxiv.org/abs/2506.13201v1,A Comprehensive Survey on Deep Learning Solutions for 3D Flood Mapping,"Accurate and timely flood mapping is crucial for effective disaster management and mitigation. Recent advancements in deep learning have opened new avenues for automated and efficient 3D flood mapping using diverse data sources. However, the landscape of deep learning techniques applied to this specific problem remains fragmented and lacks a comprehensive overview. This survey addresses this gap by providing a structured and critical analysis of deep learning solutions for 3D flood mapping. We categorize existing methods based on input data modalities (e.g., optical imagery, SAR, LiDAR, DEMs), network architectures (e.g., CNNs, RNNs, transformers), and learning paradigms (e.g., supervised, semi-supervised, unsupervised). Furthermore, we analyze the performance of these methods across different geographic regions and flood types, highlighting their strengths and limitations. Our review reveals the increasing adoption of convolutional neural networks for feature extraction from multi-source data and the growing interest in generative models for flood depth estimation and uncertainty quantification. We also identify key challenges and future research directions, including the need for robust models that generalize across diverse environmental conditions and the development of efficient methods for handling large-scale datasets. This survey serves as a valuable resource for researchers and practitioners seeking to leverage deep learning for improved flood mapping and risk assessment."
http://arxiv.org/abs/2506.13089v1,"SuperPoint-SLAM3: Augmenting ORB-SLAM3 with Deep Features, Adaptive NMS, and Learning-Based Loop Closure","Simultaneous Localization and Mapping (SLAM) is a fundamental problem in robotics and computer vision, with ORB-SLAM3 establishing itself as a robust and accurate visual SLAM system. However, its reliance on hand-crafted ORB features can limit performance in challenging environments with texture-less regions or significant viewpoint changes. This paper introduces SuperPoint-SLAM3, an augmentation of ORB-SLAM3 that incorporates learned features, adaptive Non-Maximum Suppression (NMS), and a learning-based loop closure module to address these limitations. Specifically, we replace ORB features with SuperPoint features for tracking and mapping, employing an adaptive NMS strategy to dynamically adjust feature density based on image content. Furthermore, we introduce a novel loop closure module that leverages a learned place recognition network to improve loop detection accuracy and robustness. Experimental results on public datasets demonstrate that SuperPoint-SLAM3 achieves superior performance compared to ORB-SLAM3, particularly in environments with limited texture and significant illumination variations, while maintaining comparable computational efficiency. This work showcases the potential of integrating deep learning-based features and techniques to enhance the robustness and accuracy of state-of-the-art visual SLAM systems."
http://arxiv.org/abs/2506.13032v1,AS400-DET: Detection using Deep Learning Model for IBM i (AS/400),"The IBM i (AS/400) platform remains a critical infrastructure for many enterprises, yet its integration with modern AI-driven applications remains limited. This paper addresses the challenge of enabling object detection capabilities directly within the IBM i environment, specifically focusing on scenarios where real-time image analysis can enhance existing business processes. We propose AS400-DET, a deep learning-based object detection framework optimized for deployment on the IBM i. The framework leverages a modified YOLOv5 architecture, fine-tuned using a transfer learning approach with synthetic data generated to mimic common IBM i-related imagery, such as inventory shelves and production lines. Furthermore, we implemented a custom ONNX runtime interpreter tailored for the IBM i's Power Systems architecture to minimize dependencies on external libraries. Experimental results demonstrate that AS400-DET achieves a mean average precision (mAP) of 0.75 on our synthetic dataset and maintains a frame rate of 15 FPS on a Power9 processor, demonstrating its feasibility for real-time applications. This work provides a practical pathway for integrating modern deep learning object detection capabilities with legacy IBM i systems, unlocking new opportunities for automation and process optimization."
http://arxiv.org/abs/2506.12885v3,"Model-Agnostic, Temperature-Informed Sampling Enhances Cross-Year Crop Mapping with Deep Learning","Accurate and timely crop mapping is crucial for agricultural monitoring and resource management, yet deep learning models often struggle to generalize across different years due to variations in environmental conditions and agricultural practices. This paper addresses the challenge of maintaining high crop mapping accuracy when deploying models trained on one year's data to subsequent years. We propose a novel model-agnostic, temperature-informed sampling strategy that leverages the softmax temperature of a pre-trained deep learning model to identify and selectively sample informative data points from the target year for fine-tuning. By adaptively adjusting the sampling probability based on the model's uncertainty, our method prioritizes data points that are most likely to improve generalization. Experiments on multi-year Sentinel-2 imagery from agricultural regions demonstrate that our approach consistently outperforms both traditional fine-tuning and other active learning strategies, achieving an average increase of 5-8% in overall accuracy on unseen years. This temperature-informed sampling method provides a practical and efficient solution for adapting deep learning models to changing agricultural landscapes, enhancing the reliability of cross-year crop mapping."
http://arxiv.org/abs/2506.12798v1,Predicting Genetic Mutations from Single-Cell Bone Marrow Images in Acute Myeloid Leukemia Using Noise-Robust Deep Learning Models,"Acute Myeloid Leukemia (AML) is a heterogeneous hematological malignancy driven by diverse genetic mutations, impacting treatment strategies and patient prognosis. Identifying these mutations typically requires invasive and time-consuming genomic sequencing, creating a need for faster, image-based diagnostic tools. This paper addresses the challenge of predicting specific genetic mutations in AML directly from single-cell images of bone marrow aspirates. We propose a noise-robust deep learning framework utilizing a contrastive learning pre-training strategy, followed by fine-tuning with a mutation prediction task. The contrastive learning stage learns robust feature representations by mitigating the impact of image artifacts and variations inherent in single-cell microscopy. We further incorporate an attention mechanism to highlight relevant cellular regions for improved prediction accuracy. Experimental results on a large dataset of single-cell images from AML patients demonstrate our model achieves significantly improved prediction performance compared to standard convolutional neural networks, particularly for mutations with subtle morphological manifestations. This work demonstrates the potential of deep learning for rapid, non-invasive genetic mutation prediction in AML, paving the way for personalized treatment strategies and improved patient outcomes."
http://arxiv.org/abs/2506.12766v1,Probing Deep into Temporal Profile Makes the Infrared Small Target Detector Much Better,"Infrared small target detection (IRSTD) plays a crucial role in various applications, including early warning systems and precision guidance. However, detecting dim and tiny targets amidst complex backgrounds remains a significant challenge. This paper addresses the limitations of existing IRSTD methods that often fail to fully exploit the subtle temporal variations inherent in real-world infrared sequences. We propose a novel deep learning architecture that explicitly models and integrates long-range temporal dependencies within the infrared sequence. Our approach utilizes a multi-scale temporal convolutional network (MSTCN) to extract hierarchical temporal features, followed by a recurrent neural network (RNN) with attention mechanism to adaptively weigh and fuse these features, effectively capturing the target's temporal profile. Experimental results on publicly available datasets demonstrate that our method achieves state-of-the-art performance, significantly improving the detection rate and reducing false alarms compared to existing approaches. This research highlights the importance of deep temporal modeling for robust and accurate infrared small target detection."
http://arxiv.org/abs/2506.14834v1,Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection,"Diabetic Retinopathy (DR) is a leading cause of blindness worldwide, making early and accurate detection crucial. Deploying deep learning models for automated DR screening on edge devices offers the potential for accessible and timely diagnosis, especially in resource-constrained settings. However, the computational demands of deep learning pose a significant challenge for deployment on resource-limited edge devices. This paper investigates the feasibility and performance of deploying and evaluating multiple pre-trained deep learning models for DR detection on edge platforms. We explore model compression techniques, including pruning and quantization, to optimize several state-of-the-art convolutional neural networks (CNNs) for edge deployment. Furthermore, we evaluate the trade-off between model accuracy, inference speed, and resource utilization across different edge devices using a publicly available DR dataset. Our results demonstrate that compressed models achieve comparable accuracy to their full-sized counterparts while significantly reducing inference time and memory footprint, enabling real-time DR detection on edge devices. This work provides valuable insights into the practical considerations and performance characteristics of deploying deep learning-based DR screening tools on edge platforms, paving the way for wider adoption of AI-powered healthcare solutions in underserved communities."
http://arxiv.org/abs/2506.12492v1,Comparative Analysis of Deep Learning Strategies for Hypertensive Retinopathy Detection from Fundus Images: From Scratch and Pre-trained Models,"Hypertensive Retinopathy (HR), a common microvascular complication of hypertension, can lead to severe vision impairment if left untreated. Early detection through fundus image analysis is crucial for effective management. This paper addresses the challenge of accurately and efficiently detecting HR from fundus images using deep learning techniques. We comparatively analyze the performance of convolutional neural networks (CNNs) trained from scratch and those leveraging transfer learning from pre-trained models, specifically VGG16, ResNet50, and InceptionV3. The models were trained and evaluated on a publicly available dataset of fundus images, with data augmentation strategies employed to mitigate overfitting. Our results demonstrate that pre-trained models, particularly ResNet50 fine-tuned for HR detection, achieved superior performance compared to models trained from scratch, attaining an AUC of 0.92 and an accuracy of 87%. These findings highlight the efficacy of transfer learning for HR detection, providing a valuable tool for automated and efficient screening in clinical settings, particularly where labeled data is limited."
http://arxiv.org/abs/2506.12363v1,Hierarchical Deep Feature Fusion and Ensemble Learning for Enhanced Brain Tumor MRI Classification,"Accurate classification of brain tumor Magnetic Resonance Imaging (MRI) is crucial for effective diagnosis and treatment planning. However, the subtle and complex characteristics of tumor heterogeneity pose a significant challenge for automated classification systems. This paper addresses the problem of improving brain tumor MRI classification accuracy by leveraging the complementary strengths of multi-scale deep features and ensemble learning. Our proposed method, Hierarchical Deep Feature Fusion and Ensemble Learning (HDFEL), first extracts deep features from MRI slices using pre-trained Convolutional Neural Networks (CNNs) at multiple scales, capturing both global and local tumor characteristics. These features are then hierarchically fused to create a comprehensive feature representation. Finally, an ensemble learning framework, combining multiple classifiers trained on different subsets of the fused features, is employed to improve robustness and generalization. Experimental results on the benchmark BraTS 2020 dataset demonstrate that HDFEL achieves a significant improvement in classification accuracy compared to state-of-the-art methods, with an average Dice score of 0.88 and accuracy of 92.3%. This enhanced classification performance has the potential to significantly aid clinicians in making more informed decisions for brain tumor management."
http://arxiv.org/abs/2507.23143v1,X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention,"Neural motion reenactment aims to transfer the motion of a driving video to a source actor, creating realistic and controllable animations. Existing methods often struggle with preserving the source actor's identity and generating expressive motions, particularly when dealing with large pose variations or complex facial expressions. This paper introduces X-NeMo, an Expressive Neural Motion reenactment framework via Disentangled Latent Attention, designed to address these limitations. X-NeMo employs a novel architecture that disentangles motion and appearance features in the latent space using a variational autoencoder. A disentangled attention mechanism then selectively attends to the relevant motion features while preserving the source actor's identity, enabling the generation of nuanced and expressive motions. Experimental results on benchmark datasets demonstrate that X-NeMo achieves state-of-the-art performance in terms of visual quality, identity preservation, and motion accuracy, surpassing existing methods in generating realistic and controllable reenactments. X-NeMo represents a significant advancement in neural motion reenactment, offering a powerful tool for creating high-quality animated content."
http://arxiv.org/abs/2507.22404v1,MINR: Implicit Neural Representations with Masked Image Modelling,"Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing images and other signals as continuous functions, offering benefits like resolution independence and memory efficiency. However, training INRs often requires dense supervision, limiting their applicability in scenarios with sparse or incomplete data. We address this limitation by introducing Masked Image Modelling for Neural Radiance fields (MINR), a novel approach that leverages masked autoencoding to learn robust INR representations from partially observed images. MINR trains an INR to reconstruct masked regions of an image, forcing the network to learn holistic image understanding and internalize contextual information. We employ a transformer-based architecture to predict the masked pixels from the unmasked regions, guiding the INR optimization. Experiments on standard image datasets demonstrate that MINR significantly improves INR performance in scenarios with limited supervision and noisy data, achieving state-of-the-art results in image reconstruction and novel view synthesis from sparse inputs. This work demonstrates the potential of self-supervised learning techniques to enhance the robustness and applicability of INRs, opening new avenues for image representation and manipulation."
http://arxiv.org/abs/2507.20746v1,AR-LIF: Adaptive reset leaky-integrate and fire neuron for spiking neural networks,"Spiking Neural Networks (SNNs), inspired by biological neural computation, offer potential advantages in energy efficiency and temporal processing compared to traditional Artificial Neural Networks (ANNs). However, training deep SNNs remains a significant challenge, often requiring conversion from pre-trained ANNs or complex surrogate gradient methods. This paper addresses the limitations of existing spiking neuron models, particularly the fixed reset mechanism in Leaky-Integrate and Fire (LIF) neurons, which hinders efficient learning and adaptation to varying input patterns. We introduce the Adaptive Reset LIF (AR-LIF) neuron, which dynamically adjusts its reset potential based on the neuron's membrane potential and a learnable parameter. This adaptive reset mechanism allows for more nuanced control over neuronal excitability and enables the network to learn more efficient spiking representations. Experimental results on benchmark datasets, including CIFAR-10 and CIFAR-100, demonstrate that SNNs equipped with AR-LIF neurons achieve significantly improved accuracy and faster convergence compared to traditional LIF neurons and other state-of-the-art SNN training methods. The AR-LIF neuron provides a more biologically plausible and computationally effective building block for training high-performance SNNs."
http://arxiv.org/abs/2507.20200v1,Neural Shell Texture Splatting: More Details and Fewer Primitives,"Neural rendering techniques have shown impressive results in novel view synthesis, often relying on volumetric representations or explicit 3D meshes. However, these methods can be computationally expensive or require intricate mesh parameterizations. This paper addresses the challenge of efficient and high-quality novel view synthesis from sparse input views without relying on dense volumetric grids or detailed meshes. We introduce Neural Shell Texture Splatting (NSTS), an improved differentiable rendering pipeline that represents the scene as a collection of layered, textured shells. NSTS learns a neural texture and depth offset for each shell, allowing for view-dependent refinement of the geometry and appearance. Crucially, we replace the traditional splatting primitives with a novel differentiable warping and blending operation, enabling the use of fewer, larger shells and reducing computational overhead. We demonstrate that NSTS achieves state-of-the-art results on benchmark datasets while requiring significantly fewer primitives than previous shell-based approaches, leading to faster training and rendering times. This demonstrates the potential of NSTS for efficient and high-quality novel view synthesis in various applications."
http://arxiv.org/abs/2507.19474v1,DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations,"Simultaneous Localization and Mapping (SLAM) is a fundamental problem in robotics and computer vision, enabling autonomous agents to navigate and interact with their environment. Existing RGB-D SLAM systems often struggle in texture-less environments or under significant illumination changes, hindering robust and accurate map building. This paper introduces DINO-SLAM, a novel RGB-D SLAM system that leverages self-supervised DINO features to enhance both tracking and mapping. Our approach incorporates DINO visual features into the front-end tracking module, improving robustness against visual ambiguities and enabling accurate pose estimation even in challenging scenarios. Furthermore, DINO features are integrated into the back-end mapping module to guide the reconstruction of both neural implicit surfaces and explicit TSDF representations, leading to more complete and detailed 3D models. We demonstrate through extensive experiments on benchmark datasets that DINO-SLAM significantly improves the accuracy and robustness of pose estimation and map reconstruction compared to state-of-the-art RGB-D SLAM systems, particularly in scenarios with limited texture or varying illumination. DINO-SLAM provides a powerful framework for building robust and accurate SLAM systems by effectively harnessing the representational power of self-supervised visual features."
http://arxiv.org/abs/2507.18060v1,BokehDiff: Neural Lens Blur with One-Step Diffusion,"Creating realistic and controllable lens blur, or bokeh, remains a challenging problem in image manipulation and computer graphics. Existing methods often struggle with complex scene geometry, requiring significant computational resources or user interaction. We introduce BokehDiff, a novel approach for synthesizing high-quality, spatially-varying lens blur using a one-step diffusion process conditioned on a single all-in-focus image and a user-specified depth map. Our method leverages a pre-trained diffusion model to directly denoise an initial noisy image into a bokeh-rendered image, guided by a learned modulation network that adapts the diffusion process based on the provided depth information. Experiments on synthetic and real-world datasets demonstrate that BokehDiff achieves state-of-the-art performance in terms of visual quality and fidelity to the target depth, while significantly reducing computational cost compared to iterative or physically-based rendering techniques. BokehDiff offers a practical and efficient solution for artistic image manipulation and depth-aware image editing, enabling realistic lens blur effects with minimal user effort."
http://arxiv.org/abs/2507.18031v1,ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks,"Deepfake image generation is rapidly advancing, posing a significant threat to information integrity. Detecting these manipulated images remains a challenge, particularly in explaining the reasoning behind a classification. This paper introduces ViGText, a novel deepfake detection framework that leverages vision-language models (VLMs) and graph neural networks (GNNs) to not only classify deepfakes but also provide textual explanations. ViGText first employs a VLM to extract visual features and generate descriptive captions highlighting potential manipulation artifacts. A GNN then analyzes the relationships between these textual descriptions and visual regions, learning to identify inconsistencies indicative of deepfake generation. Experiments on benchmark datasets demonstrate that ViGText achieves state-of-the-art detection performance while providing human-interpretable explanations for its decisions, improving trust and transparency. This approach offers a crucial step towards developing more robust and explainable deepfake detection systems, fostering greater public awareness and resilience against misinformation."
http://arxiv.org/abs/2507.17351v1,Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, and recent advancements have extended them to semantic NeRFs, enabling scene understanding alongside rendering. However, training semantic NeRFs typically requires dense, per-pixel semantic annotations, which are expensive and time-consuming to acquire. This paper addresses the problem of label-efficient training of semantic NeRFs by leveraging active learning techniques to strategically select the most informative views for annotation. Our proposed method integrates uncertainty-based sampling, specifically utilizing predictive entropy and mutual information, to identify regions where the semantic NeRF model exhibits low confidence or high disagreement. We then employ a view selection strategy that prioritizes views containing these uncertain regions, thereby maximizing the information gain from each newly labeled view. Experimental results on benchmark datasets demonstrate that our active learning approach significantly reduces the annotation cost, achieving comparable or even superior semantic segmentation accuracy with substantially fewer labeled views compared to passive learning baselines. This work highlights the potential of active learning to alleviate the annotation bottleneck in semantic NeRF training, enabling the development of more practical and scalable 3D scene understanding systems."
http://arxiv.org/abs/2507.16278v1,"Understanding Generalization, Robustness, and Interpretability in Low-Capacity Neural Networks","Deep neural networks excel in various computer vision tasks, but their complexity often hinders understanding their generalization, robustness, and interpretability. This paper addresses the challenge of disentangling these properties in low-capacity neural networks, providing a more tractable setting for analysis. We propose a methodology combining empirical analysis with theoretical tools, focusing on networks with a limited number of parameters trained on benchmark datasets. Our approach involves systematically varying network architecture, training data characteristics, and regularization techniques, while simultaneously tracking generalization error, adversarial vulnerability, and feature representations. We observed that specific regularization strategies, such as weight decay and dropout, significantly improve generalization and robustness in low-capacity networks, albeit at the cost of potentially less interpretable feature representations. Furthermore, we demonstrate a strong correlation between the flatness of the loss landscape and the network's robustness to adversarial perturbations. These findings offer valuable insights into the fundamental trade-offs between generalization, robustness, and interpretability in neural networks, paving the way for designing more reliable and understandable vision systems."
http://arxiv.org/abs/2507.15686v1,LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression,"Point cloud geometry compression is crucial for efficient storage and transmission of 3D data. Traditional point cloud compression techniques often introduce quantization errors, leading to information loss. This paper addresses the problem of lossless point cloud geometry compression by leveraging the power of Implicit Neural Representations (INRs). We introduce LINR-PCGC, a novel framework that represents point cloud geometry as a continuous function learned by a neural network. Our approach first transforms the point cloud into a space-filling curve to establish a sequential order, then employs a lossless entropy coding scheme on the network's weights after training. This process allows us to perfectly reconstruct the original point cloud from the compressed representation. Experimental results demonstrate that LINR-PCGC achieves competitive compression ratios compared to state-of-the-art lossless point cloud codecs, while guaranteeing perfect reconstruction accuracy. The proposed method offers a promising avenue for lossless point cloud compression, enabling applications demanding high fidelity and data integrity."
http://arxiv.org/abs/2507.15035v1,OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography,"Breast ultrasound computed tomography (USCT) offers a promising non-ionizing alternative to mammography for breast cancer screening, leveraging wave-based imaging to reconstruct tissue properties. However, computationally intensive reconstruction algorithms hinder real-time clinical translation. This work investigates the potential of neural operators to accelerate USCT image reconstruction by directly mapping wavefield data to tissue property maps. We introduce OpenBreastUS, a comprehensive benchmark dataset derived from finite-difference time-domain simulations mimicking realistic breast USCT acquisitions, featuring diverse breast densities and lesion characteristics. We then evaluate the performance of several prominent neural operator architectures, including the Fourier Neural Operator (FNO) and DeepONet, trained to predict sound speed distributions from simulated time-of-flight data. Our results demonstrate that neural operators can achieve reconstruction speeds several orders of magnitude faster than traditional iterative methods, while maintaining acceptable image quality, particularly for identifying larger lesions. This study highlights the potential of neural operators for real-time USCT reconstruction and provides a valuable resource for future research in learned wave imaging."
http://arxiv.org/abs/2507.14793v1,Flow Equivariant Recurrent Neural Networks,"Recurrent Neural Networks (RNNs) are powerful tools for processing sequential data, but often lack explicit mechanisms for handling symmetries present in the underlying dynamics. This limitation hinders their ability to generalize across different reference frames or viewpoints, particularly in tasks involving physical systems governed by flow fields. We introduce Flow Equivariant Recurrent Neural Networks (FERNNs), a novel architecture that enforces equivariance to diffeomorphisms of the input space within the recurrent cell. FERNNs leverage a combination of vector and scalar fields to represent the hidden state, and employ equivariant neural networks to propagate information through time while preserving the flow structure. Experiments on a range of tasks, including fluid flow prediction and trajectory forecasting in dynamical systems, demonstrate that FERNNs achieve significantly improved generalization performance and sample efficiency compared to standard RNNs and other equivariant architectures. This highlights the importance of incorporating flow equivariance into recurrent models for learning robust representations of dynamic systems."
http://arxiv.org/abs/2507.13929v1,TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views,"Neural Radiance Fields (NeRFs) have shown remarkable success in novel view synthesis, but their reliance on dense, calibrated multi-view images limits their applicability in scenarios with sparse input views and varying scene dynamics. This paper addresses the challenge of building generalizable NeRFs that can extrapolate across time from few-shot input views of dynamic scenes. We introduce TimeNeRF, a novel framework that disentangles static scene geometry from dynamic appearance and motion using a time-conditioned deformation field and a canonical radiance field. A temporal encoder learns a latent representation of time, which is then used to condition the deformation field, allowing the model to predict the pose of points in the scene at different time instances. Experimental results on synthetic and real-world dynamic scenes demonstrate that TimeNeRF significantly outperforms existing methods in few-shot novel view synthesis and temporal extrapolation, achieving state-of-the-art performance in terms of image quality and temporal consistency. TimeNeRF provides a crucial step toward building robust and generalizable 4D scene representations from limited and temporally sparse observations."
http://arxiv.org/abs/2507.13595v1,NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision,"Neural implicit surfaces, represented by Signed Distance Functions (SDFs), have shown remarkable capabilities in 3D shape reconstruction and representation. However, training SDFs typically requires accurate and noise-free supervision, which is often unavailable in real-world scenarios due to sensor limitations or data acquisition complexities. This paper addresses the challenge of learning accurate and clean SDF representations from noisy supervision data. We introduce NoiseSDF2NoiseSDF, a novel framework that leverages a two-stage training process. First, a preliminary SDF is learned directly from the noisy data. Then, we employ a noise-aware refinement strategy, where a second SDF network is trained to predict the *residual* SDF, conditioned on the noisy input and the output of the first network. This residual learning is guided by a noise model, allowing the network to effectively disentangle noise from the underlying clean geometry. Experimental results on both synthetic and real-world noisy datasets demonstrate that NoiseSDF2NoiseSDF significantly outperforms existing methods in terms of reconstruction accuracy and robustness to noise. This work provides a promising approach for learning high-quality 3D representations from imperfect and readily available data."
http://arxiv.org/abs/2507.13485v1,Neural Architecture Search with Mixed Bio-inspired Learning Rules,"Neural Architecture Search (NAS) automates the design of neural networks, offering the potential to surpass human-engineered architectures. However, existing NAS algorithms often rely on computationally expensive reinforcement learning or evolutionary algorithms, or gradient-based methods that struggle with discrete search spaces. This paper addresses the challenge of efficient and effective NAS by introducing a novel search strategy inspired by the principles of biological learning. Our approach, termed NAS-BioMix, leverages a mixed learning rule paradigm, combining Hebbian learning for structural exploration with Spike-Timing-Dependent Plasticity (STDP) for performance-driven refinement. Specifically, we use Hebbian learning to probabilistically generate candidate architectures based on the co-activation of network components, and then apply STDP-inspired rules to fine-tune the architecture based on the observed performance on a validation set. We demonstrate NAS-BioMix on image classification tasks using CIFAR-10 and ImageNet datasets. Results show that NAS-BioMix achieves comparable or superior performance to state-of-the-art NAS methods while significantly reducing the computational cost associated with architecture search. This bio-inspired approach offers a promising new direction for efficient and scalable neural architecture search."
http://arxiv.org/abs/2507.12953v1,cIDIR: Conditioned Implicit Neural Representation for Regularized Deformable Image Registration,"Deformable image registration is a crucial task in medical imaging and computer vision, aiming to find a spatial transformation that aligns two images. Traditional registration methods often struggle with large deformations and require careful regularization to avoid unrealistic transformations. We address these challenges by introducing cIDIR, a novel Conditioned Implicit Neural Representation for Regularized Deformable Image Registration. cIDIR represents the deformation field as a continuous function parameterized by a neural network, conditioned on both the moving and fixed images. This implicit representation allows for inherent smoothness and sub-pixel accuracy. Moreover, we incorporate a learnable regularization term directly into the network architecture, enabling adaptive control over the deformation field's smoothness and complexity based on the image content. Experiments on a variety of image registration tasks, including medical image alignment and facial expression transfer, demonstrate that cIDIR achieves state-of-the-art performance, outperforming existing methods in terms of registration accuracy and deformation field smoothness. cIDIR offers a powerful and flexible framework for deformable image registration, paving the way for more robust and accurate image analysis."
http://arxiv.org/abs/2507.12714v2,NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement,"Modeling the geometric diversity of leaves is crucial for applications in botany, agriculture, and computer graphics. However, existing methods often struggle to capture the complex, nuanced shapes and deformations exhibited by real-world leaves, and often lack explicit control over these attributes. This paper introduces NeuraLeaf, a novel neural parametric model for representing leaf geometry with explicit disentanglement of shape and deformation. NeuraLeaf utilizes a deep neural network to learn a latent space representation of leaf shapes, conditioned on a small set of interpretable parameters controlling key morphological features like length, width, and aspect ratio. A separate deformation network then warps the base shape based on learned deformation codes, allowing for realistic variations such as bending, twisting, and surface undulations. We demonstrate that NeuraLeaf can accurately reconstruct diverse leaf shapes from various plant species, achieving superior performance compared to state-of-the-art generative models in terms of reconstruction accuracy and shape parameter control. Furthermore, we show the model's ability to generate novel, plausible leaf geometries by manipulating the disentangled shape and deformation parameters, opening avenues for data augmentation and virtual plant modeling. The proposed framework provides a powerful tool for analyzing and synthesizing realistic leaf geometries with unprecedented control and interpretability."
http://arxiv.org/abs/2507.12600v1,HairFormer: Transformer-Based Dynamic Neural Hair Simulation,"Dynamic hair simulation is crucial for realistic character animation in computer graphics, but traditional physics-based methods are computationally expensive and challenging to control. Data-driven approaches offer alternatives, yet often struggle to capture the complex, long-range dependencies inherent in hair dynamics. We address the problem of efficiently and accurately simulating dynamic hair motion by introducing HairFormer, a novel transformer-based architecture specifically designed for this task. HairFormer leverages self-attention mechanisms to model interactions between hair strands across significant distances, enabling it to capture global hair behavior. Furthermore, we incorporate a dynamic graph representation of the hair structure that evolves over time, allowing the network to adapt to changing hair configurations. Experiments demonstrate that HairFormer outperforms state-of-the-art methods in terms of both simulation accuracy and computational efficiency, achieving more realistic and visually appealing hair dynamics. This work provides a powerful new tool for artists and animators seeking to create compelling and believable characters with dynamic hair."
http://arxiv.org/abs/2507.12138v1,Neural Human Pose Prior,"Human pose estimation is a fundamental task in computer vision, crucial for understanding human behavior in images and videos. However, accurate pose estimation remains challenging due to occlusions, variations in clothing, and complex articulated motion. This paper addresses the problem of enforcing realistic and physically plausible human poses by introducing a novel neural human pose prior. Our method learns a latent space representation of valid human poses using a variational autoencoder (VAE) trained on a large-scale motion capture dataset. We then integrate this learned prior into existing pose estimation frameworks by projecting predicted poses into the learned latent space and decoding them back into joint coordinates, effectively regularizing the pose estimates. Experiments on benchmark datasets demonstrate that incorporating our neural pose prior significantly improves the accuracy and robustness of state-of-the-art pose estimation models, particularly in challenging scenarios with occlusions and noisy detections. This approach offers a powerful and generalizable method for improving the quality of human pose estimation across various applications."
http://arxiv.org/abs/2507.12489v1,Physically Based Neural LiDAR Resimulation,"LiDAR simulation plays a crucial role in the development and evaluation of autonomous systems, enabling cost-effective and safe testing of perception algorithms. However, existing LiDAR simulators often struggle to accurately replicate the complex physical phenomena that govern LiDAR point cloud generation, leading to a domain gap between simulated and real-world data. This paper addresses the challenge of generating realistic LiDAR point clouds by introducing a novel physically based neural LiDAR resimulation framework. Our approach leverages a differentiable rendering pipeline informed by the LiDAR sensor equation to model the interaction of laser beams with the environment. We then train a neural network to refine the initial physically based simulation, learning to compensate for inaccuracies in the scene representation and sensor model. Experiments on real-world datasets demonstrate that our method significantly improves the realism of simulated LiDAR point clouds, reducing the domain gap and leading to improved performance of perception algorithms trained on simulated data. This work provides a powerful tool for generating high-fidelity synthetic LiDAR data, facilitating the development and validation of robust autonomous systems."
http://arxiv.org/abs/2507.10637v2,A Simple Baseline for Stable and Plastic Neural Networks,"Continual learning aims to train neural networks on a sequence of tasks without forgetting previously learned knowledge. However, striking a balance between stability (maintaining old knowledge) and plasticity (acquiring new knowledge) remains a significant challenge. This paper addresses the problem of catastrophic forgetting in continual learning by introducing a surprisingly simple yet effective baseline approach. Our method, termed ""Gradient Projection with Random Subspace"" (GPRS), leverages gradient projection to minimize interference with past tasks while simultaneously enabling learning in a randomly selected subspace of the parameter space for each new task. GPRS projects the gradient of the current task onto the null space of the gradients of previous tasks, and restricts the update to a random subset of the network's weights. We demonstrate through extensive experiments on standard continual learning benchmarks (e.g., Split MNIST, CIFAR-10, CIFAR-100) that GPRS achieves competitive or superior performance compared to more complex state-of-the-art methods, particularly in high-plasticity regimes and with limited computational overhead. This highlights the potential of simple, well-regularized approaches for addressing the stability-plasticity dilemma in continual learning."
http://arxiv.org/abs/2507.10623v1,Flows and Diffusions on the Neural Manifold,"Deep neural networks learn complex, high-dimensional representations that can be interpreted as residing on a low-dimensional manifold. Understanding the geometry and dynamics of this neural manifold is crucial for tasks such as generalization, robustness, and interpretability. This paper addresses the problem of characterizing and manipulating data distributions on the neural manifold by introducing a novel framework that combines concepts from optimal transport (OT) and diffusion models. Our approach leverages neural network activations to define a data-dependent metric tensor, enabling the computation of geodesic flows and diffusion processes directly on the learned manifold. We then formulate a diffusion model conditioned on these flows, allowing for controlled generation and manipulation of data points along geodesics. Experimental results on image and text datasets demonstrate the effectiveness of our method in generating realistic samples, performing semantic interpolation, and enhancing model robustness to adversarial perturbations. This work provides a new perspective on understanding and manipulating neural representations by explicitly considering the underlying manifold structure."
http://arxiv.org/abs/2507.09513v1,Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding,"Animal behavioral analysis is crucial for understanding neural mechanisms underlying cognition and disease. However, annotating animal behavior in video is time-consuming and often subjective, hindering the development of robust and generalizable models. This paper addresses the challenge of learning effective video representations for animal behavior analysis with limited labeled data. We propose a self-supervised pretraining strategy for Vision Transformers (ViTs) using a novel combination of masked autoencoding and contrastive learning objectives, specifically tailored for behavioral video data. Our method, termed Behavior-Specific Pretrained Transformer (BSPT), leverages temporal context and motion cues inherent in behavioral videos to guide representation learning. BSPT is pretrained on a large, unlabeled dataset of rodent behavior videos and then fine-tuned on downstream tasks, including behavior classification, pose estimation, and neural encoding prediction. We demonstrate that BSPT significantly outperforms existing self-supervised pretraining methods and achieves state-of-the-art performance on multiple benchmark datasets, exhibiting strong generalization across different behavioral tasks and animal species. This work provides a powerful and efficient framework for learning representations from unlabeled behavioral videos, ultimately accelerating research in neuroscience and animal behavior."
http://arxiv.org/abs/2507.11549v2,A Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search,"Deformable Transformers have shown promise in various vision tasks due to their ability to focus on relevant image regions with deformable attention. However, their high memory consumption, particularly with high-resolution inputs and deep architectures, limits their deployment on resource-constrained devices. To address this, we propose a memory-efficient framework for Deformable Transformers guided by Neural Architecture Search (NAS). Our approach, coined MemNAS-Deformable, simultaneously searches for optimal deformable attention configurations and network architectures to minimize memory footprint while preserving performance. Specifically, we introduce a differentiable memory cost model integrated into the NAS objective, guiding the search towards architectures with reduced memory access. Experiments on image classification and object detection demonstrate that MemNAS-Deformable achieves comparable or superior performance to manually designed Deformable Transformers with up to 40% reduction in memory usage. This enables the deployment of high-performing Deformable Transformers on devices with limited resources, broadening their applicability in real-world scenarios."
http://arxiv.org/abs/2507.09269v1,Cross Knowledge Distillation between Artificial and Spiking Neural Networks,"Spiking Neural Networks (SNNs) offer potential advantages in energy efficiency and temporal processing compared to traditional Artificial Neural Networks (ANNs), making them attractive for neuromorphic computing. However, training deep and complex SNNs remains a significant challenge due to the non-differentiable nature of spike events. We address this problem by introducing a novel cross-knowledge distillation (CKD) framework that leverages the strengths of both ANNs and SNNs. Our method involves training a high-performing ANN teacher and then transferring its knowledge to an SNN student through a combination of feature-based and response-based distillation losses. Specifically, we minimize the difference between intermediate feature maps and output probabilities of the ANN and SNN, encouraging the SNN to mimic the ANN's learned representations and decision boundaries. Experiments on benchmark datasets, including CIFAR-10 and CIFAR-100, demonstrate that our CKD approach significantly improves the accuracy of SNNs, achieving state-of-the-art performance compared to other SNN training methods. This work provides a promising avenue for developing high-performance and energy-efficient SNNs by effectively utilizing the knowledge embedded in well-trained ANNs."
http://arxiv.org/abs/2507.13372v1,Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks,"Breast cancer is a leading cause of cancer-related deaths among women, highlighting the critical need for improved early detection methods. Existing computer-aided diagnosis systems often struggle with subtle image features and the integration of multi-modal data. This paper addresses these limitations by proposing a novel framework that combines Vision Transformers (ViTs) and Graph Neural Networks (GNNs) for enhanced breast cancer detection. Our approach first leverages ViTs to extract robust visual features from mammograms, capturing both local and global contexts. Subsequently, a GNN is employed to model the relationships between different regions of interest identified by the ViT, incorporating clinical metadata as node features to improve diagnostic accuracy. Evaluated on a large dataset of mammograms with associated clinical data, our method achieves a significant improvement in AUC and sensitivity compared to state-of-the-art methods. This hybrid approach offers a promising avenue for developing more accurate and reliable breast cancer detection systems, potentially leading to earlier diagnosis and improved patient outcomes."
http://arxiv.org/abs/2507.08800v1,NeuralOS: Towards Simulating Operating Systems via Neural Generative Models,"Operating systems (OS) are complex software systems managing hardware resources and providing foundational services for applications. Simulating OS behavior accurately is crucial for research, development, and education, yet traditional simulation methods often struggle to capture the intricate, emergent dynamics arising from the interplay of numerous components. We address the challenge of creating a data-driven OS simulation by introducing NeuralOS, a novel framework employing neural generative models to learn and reproduce OS execution traces. NeuralOS leverages a transformer-based architecture conditioned on system calls and resource states to predict subsequent system activities, effectively learning a probabilistic model of OS behavior. We demonstrate that NeuralOS can generate realistic OS execution traces, closely mimicking the statistical properties of real-world OS logs, including inter-arrival times of system calls and resource utilization patterns. This approach offers a powerful new paradigm for OS simulation, enabling researchers to explore complex system interactions and evaluate novel OS designs without relying on hand-crafted models or resource-intensive traditional simulators."
http://arxiv.org/abs/2507.08776v2,CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, achieving impressive realism but demanding significant computational resources for training and rendering. A key bottleneck lies in the dense sampling of 3D space required to accurately represent scene geometry and appearance. We address this challenge by introducing CLiFT, a novel approach that leverages compressive light-field tokens for compute-efficient and adaptive neural rendering. CLiFT learns a compact, view-dependent codebook of light-field tokens representing local scene radiance variations. During rendering, these tokens are adaptively selected and aggregated based on the viewing direction and scene content, enabling efficient reconstruction of the radiance field. Experiments demonstrate that CLiFT achieves comparable or superior rendering quality to state-of-the-art NeRF methods while significantly reducing computational costs, with up to 4x speedup in rendering time and 3x reduction in memory footprint. CLiFT's efficiency and adaptability pave the way for deploying high-quality neural rendering on resource-constrained platforms and enabling real-time interactive experiences."
http://arxiv.org/abs/2507.08494v1,Unified People Tracking with Graph Neural Networks,"Multi-object tracking (MOT) is a fundamental task in computer vision, crucial for applications like autonomous driving and video surveillance. However, effectively tracking people in crowded scenes with frequent occlusions and complex interactions remains a challenge. This paper addresses the problem of achieving robust and consistent people tracking by explicitly modeling inter-person relationships and long-range dependencies. We propose a novel unified tracking framework that leverages Graph Neural Networks (GNNs) to learn and propagate information between people detections across time. Our approach constructs a dynamic graph where nodes represent detections and edges encode spatial, temporal, and appearance similarities. The GNN iteratively refines node embeddings by aggregating information from neighboring nodes, enabling the model to reason about occlusions and re-identify individuals even after prolonged absences. Experimental results on challenging MOT datasets, including MOT17 and MOT20, demonstrate that our method achieves state-of-the-art performance, significantly reducing identity switches and improving overall tracking accuracy. Our unified GNN-based approach offers a powerful and generalizable solution for robust people tracking in complex environments."
http://arxiv.org/abs/2507.07734v1,EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks,"Event cameras offer significant advantages over traditional frame-based cameras in dynamic environments due to their high temporal resolution and low latency. However, effectively leveraging the asynchronous and sparse nature of event data for action recognition remains a challenge, particularly in achieving early recognition with minimal latency. This paper introduces EEvAct, an early event-based action recognition framework that utilizes high-rate two-stream Spiking Neural Networks (SNNs) to process event data. EEvAct employs a novel temporal encoding scheme to convert event streams into high-rate spike trains, preserving fine-grained temporal information. These spike trains are then fed into two parallel SNN streams, one processing raw event polarities and the other processing optical flow extracted directly from events. We demonstrate that EEvAct achieves state-of-the-art early action recognition performance on the DVS128 Gesture and ASL-DVS datasets, exhibiting significant improvements in latency and accuracy compared to existing event-based and frame-based approaches. EEvAct's ability to perform rapid and accurate action recognition from event data unlocks possibilities for real-time applications in robotics, autonomous driving, and human-computer interaction."
http://arxiv.org/abs/2507.06719v1,A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding,"Open-vocabulary 3D visual grounding aims to localize objects in a 3D scene based on free-form textual descriptions, enabling flexible human-robot interaction and scene understanding. However, effectively bridging the gap between language and 3D space, especially for complex spatial relationships expressed in natural language, remains a significant challenge. We introduce a novel neural representation framework that leverages Large Language Models (LLMs) to enhance spatial reasoning for open-vocabulary 3D visual grounding. Our approach first encodes the 3D scene into a neural scene representation that captures both geometric and semantic information. Then, we employ an LLM to parse the input text description, extract relevant object entities and their spatial relationships, and generate structured spatial reasoning prompts that guide the grounding process. Finally, a cross-modal fusion module integrates the scene representation and the LLM-generated spatial reasoning cues to predict the target object's location. Experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance, significantly improving the accuracy and robustness of 3D visual grounding, particularly for descriptions involving intricate spatial relationships. This work highlights the potential of integrating LLMs with neural scene representations to unlock more sophisticated spatial reasoning capabilities in 3D vision tasks."
http://arxiv.org/abs/2507.05952v1,High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes,"Neural surface reconstruction has shown remarkable progress in recent years, enabling high-quality 3D models from multi-view images. However, existing methods often struggle with generalization to unseen scenes and can be computationally expensive due to dense feature processing. This paper addresses the challenge of achieving both high-fidelity reconstruction and strong generalization ability with limited computational resources. We introduce a novel approach that leverages sparse feature volumes, constructed by adaptively sampling and aggregating image features based on view selection and geometric priors. A neural implicit surface is then learned from these sparse feature volumes using a hierarchical architecture that allows for multi-scale feature integration and efficient memory management. Our experiments on benchmark datasets demonstrate that our method achieves state-of-the-art reconstruction accuracy while maintaining superior generalization performance compared to existing dense and sparse neural reconstruction techniques. This work presents a significant step towards practical and scalable neural surface reconstruction for real-world applications."
http://arxiv.org/abs/2507.06269v2,BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields,"Neural Signed Distance Fields (SDFs) have shown remarkable performance in representing complex 3D geometries. However, accurately quantifying the uncertainty associated with these learned representations, particularly near the object surface, remains a significant challenge. We address this problem by introducing BayesSDF, a novel approach that incorporates a Bayesian framework to estimate surface-based Laplacian uncertainty for neural SDFs. BayesSDF leverages the connection between the Laplacian of the SDF and the surface curvature to derive a principled uncertainty estimate directly on the reconstructed geometry. Specifically, we model the SDF network weights as Gaussian distributions and employ a Laplace approximation to efficiently estimate the posterior distribution over the SDF function. This allows us to compute the variance of the Laplacian, providing a localized measure of uncertainty reflecting the confidence in the surface reconstruction. Experiments on synthetic and real-world datasets demonstrate that BayesSDF provides more accurate and reliable uncertainty estimates compared to existing methods, particularly in regions with high curvature or noisy data. Our approach enables more robust and reliable applications of neural SDFs in tasks such as 3D reconstruction, shape completion, and downstream robotic manipulation tasks."
http://arxiv.org/abs/2507.08841v2,Zero-Shot Neural Architecture Search with Weighted Response Correlation,"Neural Architecture Search (NAS) automates the design of deep learning models, yet many approaches require extensive training and evaluation, limiting their applicability. Zero-shot NAS aims to predict architecture performance without training, offering a more efficient alternative. However, existing zero-shot proxies often struggle to accurately reflect the true performance of architectures across diverse datasets and search spaces. We propose a novel zero-shot NAS method, Weighted Response Correlation (WRC), that leverages the correlation between layer responses to estimate architecture performance. WRC analyzes the activation patterns of different layers within an architecture and assigns weights based on the information content of each layer's response, emphasizing the most informative features. By correlating these weighted responses, WRC provides a robust and efficient proxy for architecture evaluation. Experiments on NAS-Bench-201 and DARTS search spaces demonstrate that WRC achieves state-of-the-art zero-shot prediction accuracy, exhibiting a significantly higher correlation with true architecture performance compared to existing methods. This improved correlation enables more effective architecture selection, paving the way for faster and more generalizable NAS."
http://arxiv.org/abs/2507.05397v1,Neural-Driven Image Editing,"Image editing is a fundamental task in computer vision and graphics, often relying on manual manipulation or predefined filters. Current methods often lack intuitive control and require significant expertise to achieve desired artistic effects. This paper addresses the challenge of creating a neural-driven image editing framework that allows users to manipulate images based on high-level semantic guidance. Our approach leverages a generative adversarial network (GAN) pre-trained on a large image dataset, coupled with a novel attention mechanism that allows users to specify regions of interest and desired semantic changes through natural language or example images. By fine-tuning the GAN's latent space using the attention-guided guidance, we achieve localized and semantically coherent image edits. Experimental results demonstrate that our method enables intuitive and effective image manipulation, producing high-quality results that surpass existing approaches in terms of realism and user control. This work paves the way for more accessible and creative image editing tools, bridging the gap between human intention and machine execution."
http://arxiv.org/abs/2507.05249v1,Physics-Guided Dual Implicit Neural Representations for Source Separation,"Source separation aims to decompose a mixture signal into its constituent sources, a long-standing challenge in signal processing and computer vision. Existing deep learning methods often struggle to generalize to unseen mixtures due to their reliance on large, labeled datasets and limited incorporation of physical mixing models. We address this limitation by introducing a physics-guided dual implicit neural representation for source separation. Our method leverages the wave equation to model sound propagation and employs two implicit neural representations: one to represent the source signals as functions of space and time, and another to represent the mixing coefficients. By incorporating the wave equation as a regularization term during training, we constrain the learned representations to adhere to physical principles. Experiments on both synthetic and real-world datasets demonstrate that our approach achieves state-of-the-art performance in source separation, particularly in scenarios with limited training data and complex acoustic environments. This work demonstrates the benefit of physics-informed learning for improving the robustness and generalization ability of source separation models."
http://arxiv.org/abs/2507.05191v1,Neuralocks: Real-Time Dynamic Neural Hair Simulation,"Realistic hair simulation is crucial for creating believable digital characters in various applications, from films to virtual avatars. However, achieving real-time performance with complex hair dynamics remains a significant challenge. We introduce Neuralocks, a novel neural hair simulation framework designed for real-time dynamic hair animation. Our approach leverages a graph neural network (GNN) to learn the underlying physics of hair strand interactions and external forces. The GNN directly predicts hair strand positions in the next time step, conditioned on the current state and external forces, such as wind and collisions. We train the network on a large dataset of offline, high-fidelity simulations, enabling it to generalize to unseen hairstyles and dynamic scenarios. Experimental results demonstrate that Neuralocks achieves significantly faster simulation times compared to traditional physics-based methods, while maintaining comparable visual quality and realistic dynamic behavior. This allows for interactive manipulation and real-time rendering of complex hairstyles, opening new possibilities for virtual character creation and interactive experiences."
http://arxiv.org/abs/2507.05190v1,QMoE: A Quantum Mixture of Experts Framework for Scalable Quantum Neural Networks,"Quantum Neural Networks (QNNs) hold promise for surpassing classical machine learning in specific tasks, but face significant challenges in scalability and expressivity. Existing QNN architectures often struggle to efficiently utilize increasing numbers of qubits and parameters, limiting their applicability to complex problems. This paper introduces QMoE, a Quantum Mixture of Experts framework designed to address these limitations by enabling scalable and modular QNN architectures. QMoE leverages a classical gating network to dynamically route quantum data samples to specialized quantum ""expert"" networks, each trained on a subset of the input space. This allows for increased model capacity without exponentially increasing circuit depth, and enables parallel computation across experts. We demonstrate the efficacy of QMoE on image classification and quantum chemistry datasets, achieving significant improvements in accuracy and training efficiency compared to monolithic QNNs and classical MoE equivalents with comparable parameter counts. QMoE provides a novel and practical approach for building scalable and high-performing QNNs, paving the way for tackling more complex quantum machine learning problems."
http://arxiv.org/abs/2507.05315v1,Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces,"Soft tissue deformation prediction is crucial for surgical simulation, planning, and robotic-assisted surgery. Accurately estimating both the deformation and the forces involved remains a significant challenge due to the complex, nonlinear material properties of soft tissues and the computational cost of traditional methods like Finite Element Analysis (FEA). This paper introduces a novel Conditional Graph Neural Network (CGNN) architecture to predict soft tissue deformation and interaction forces under various loading conditions. Our CGNN represents the soft tissue as a graph, with nodes representing material points and edges representing connections between them. The network learns to propagate information between nodes based on applied forces and material properties, conditioned on boundary conditions and external loads. The model incorporates learnable edge features to capture material heterogeneity. We demonstrate that our CGNN achieves comparable accuracy to FEA simulations while offering orders of magnitude faster computation times on benchmark datasets of soft tissue compression and incision. This approach provides a computationally efficient and accurate alternative to traditional methods, enabling real-time simulation and interactive applications in surgical training and planning."
http://arxiv.org/abs/2507.04725v1,Unleashing the Power of Neural Collapse: Consistent Supervised-Unsupervised Alignment for Generalized Category Discovery,"Generalized Category Discovery (GCD) aims to simultaneously cluster unlabeled data and classify labeled data, where the unlabeled data potentially contains both seen and unseen categories. A significant challenge in GCD lies in bridging the gap between supervised learning on labeled data and unsupervised learning on unlabeled data. We address this challenge by leveraging the phenomenon of Neural Collapse (NC), which describes the tendency of learned features in deep networks to collapse to class means and exhibit equiangular tight frame (ETF) properties during the late stages of training. Our method, Consistent Supervised-Unsupervised Alignment (CSUA), encourages NC in both labeled and unlabeled data by explicitly aligning the class means of labeled data with the cluster centroids of unlabeled data, and promoting ETF properties within both sets. CSUA further incorporates a novel consistency regularization term that ensures consistent cluster assignments across different augmentations of the unlabeled data, improving robustness and cluster quality. Experiments on standard GCD benchmarks demonstrate that CSUA significantly outperforms existing state-of-the-art methods, achieving substantial gains in both clustering accuracy and classification accuracy, particularly in scenarios with a large number of unseen categories. This work highlights the potential of leveraging NC principles to achieve effective and consistent alignment between supervised and unsupervised learning paradigms for challenging open-world recognition tasks."
http://arxiv.org/abs/2507.04671v1,DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and Continuous Adaptation,"Neural Architecture Search (NAS) has demonstrated remarkable success in automating the design of deep learning models. However, the high computational cost associated with evaluating numerous architectures remains a significant bottleneck, hindering its applicability in resource-constrained environments. This paper addresses the challenge of resource-efficient NAS by proposing DANCE: Data-Aware and Continuous Adaptation for Neural Architecture Search. DANCE leverages a differentiable search space and introduces a novel data-aware sampling strategy that prioritizes architectures based on their performance on informative data subsets, thereby reducing the need for full dataset evaluations. Furthermore, DANCE incorporates a continuous adaptation mechanism that dynamically adjusts the search space during the optimization process, focusing on promising architectural regions and accelerating convergence. Experimental results on image classification benchmarks demonstrate that DANCE achieves state-of-the-art performance compared to existing NAS methods while significantly reducing the computational cost, requiring up to 5x less GPU hours. DANCE offers a practical and efficient approach to NAS, enabling the discovery of high-performing architectures even with limited computational resources."
http://arxiv.org/abs/2507.04456v1,BiVM: Accurate Binarized Neural Network for Efficient Video Matting,"Video matting, a crucial task in video editing and post-production, often demands significant computational resources due to the high resolution and temporal coherence requirements. Existing deep learning based video matting methods, while achieving high accuracy, are computationally expensive, hindering their deployment on resource-constrained devices. This paper introduces BiVM, a novel Binarized Neural Network (BNN) architecture designed specifically for efficient and accurate video matting. BiVM leverages a spatio-temporal attention mechanism within a recurrent framework, enabling the network to effectively capture both spatial details and temporal dependencies crucial for accurate alpha matte extraction. Furthermore, we introduce a customized binarization strategy that minimizes information loss during the quantization process, resulting in improved performance compared to standard binarization techniques. Experimental results on standard video matting benchmarks demonstrate that BiVM achieves comparable accuracy to state-of-the-art full-precision methods while significantly reducing computational cost and memory footprint. BiVM offers a practical solution for real-time video matting on edge devices, facilitating wider adoption in various applications."
http://arxiv.org/abs/2507.04408v1,A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields,"Neural Radiance Fields (NeRF) have shown impressive results in novel view synthesis, but often suffer from overfitting to the training views, leading to artifacts and reduced generalization, especially with sparse input views. This paper addresses the problem of inconsistent or biased sampling during NeRF training, which exacerbates overfitting and hinders the learning of a consistent 3D scene representation. We introduce a view-consistent sampling strategy that leverages epipolar geometry to guide sample selection along corresponding rays across different views. Our method biases sampling towards regions of high photometric consistency, thereby encouraging the network to learn a smoother and more accurate radiance field. Experimental results on benchmark datasets demonstrate that our approach significantly improves the quality of novel view synthesis, particularly in scenarios with limited training data, outperforming state-of-the-art regularization techniques. The proposed view-consistent sampling method provides a simple yet effective way to regularize NeRF training and enhance its robustness to overfitting."
http://arxiv.org/abs/2507.03765v1,Efficient Event-Based Semantic Segmentation via Exploiting Frame-Event Fusion: A Hybrid Neural Network Approach,"Event cameras offer advantages over traditional frame-based cameras in high-speed and high dynamic range scenarios, but their sparse and asynchronous nature poses challenges for semantic segmentation. This paper addresses the problem of efficiently performing semantic segmentation using event data by effectively fusing it with complementary information from concurrently captured frames. We propose a novel hybrid neural network architecture, termed Frame-Event Fusion Network (FEF-Net), that leverages both frame-based and event-based representations. FEF-Net employs a dual-branch encoder to extract features from frames and events separately, followed by a cross-modal attention mechanism that adaptively fuses these features at multiple scales. Specifically, event features are used to guide the attention over frame features, highlighting regions of interest and improving the accuracy of segmentation. Experimental results on benchmark datasets demonstrate that FEF-Net achieves state-of-the-art segmentation accuracy with significantly reduced computational cost compared to event-only methods and outperforms existing frame-event fusion techniques, highlighting the benefits of our efficient fusion strategy. This work contributes to the development of robust and efficient vision systems for challenging environments where traditional cameras struggle."
http://arxiv.org/abs/2507.03504v2,Information-Bottleneck Driven Binary Neural Network for Change Detection,"Change detection aims to identify differences in multi-temporal images of the same scene, a critical task for various applications, including environmental monitoring and disaster management. However, deploying deep learning-based change detection models in resource-constrained environments remains challenging due to their high computational cost and memory footprint. To address this, we propose a novel Information-Bottleneck Driven Binary Neural Network (IB-BNN) for change detection. Our method leverages the information bottleneck (IB) principle to guide the binarization process, encouraging the network to retain only the most relevant information necessary for change detection while discarding redundant features. Specifically, we introduce a differentiable approximation of the IB Lagrangian and incorporate it as a regularization term during training, promoting feature compression and robustness to noise. Experimental results on benchmark change detection datasets demonstrate that our IB-BNN achieves comparable or even superior performance to full-precision and other binarized networks, while significantly reducing memory usage and computational complexity. This makes our approach highly suitable for deployment on edge devices and in scenarios with limited resources, paving the way for real-time and efficient change detection applications."
http://arxiv.org/abs/2507.03094v1,Neural Dynamic Modes: Computational Imaging of Dynamical Systems from Sparse Observations,"Dynamical systems are prevalent in scientific and engineering domains, yet their full state is often inaccessible due to physical constraints or limitations of sensing technology. This paper addresses the challenge of inferring the dynamics of a system from sparse and potentially noisy observations using a novel computational imaging framework. We introduce Neural Dynamic Modes (NDM), which leverages neural operators to learn a Koopman-based representation of the underlying dynamics, coupled with a differentiable observation model. NDM learns a latent space where the dynamics are approximately linear and time-invariant, enabling accurate prediction and extrapolation from limited data. We demonstrate NDM's performance on several challenging tasks, including fluid flow estimation from sparse velocity measurements and reconstruction of chaotic systems from partial observations, achieving superior accuracy and robustness compared to traditional methods and existing neural operator approaches. The proposed framework offers a powerful tool for analyzing and predicting the behavior of complex dynamical systems from incomplete data, with broad applicability in fields such as fluid mechanics, climate modeling, and robotics."
http://arxiv.org/abs/2507.02494v1,MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations,"Scientific simulations generate massive, high-dimensional datasets, posing significant challenges for storage, analysis, and visualization. Implicit Neural Representations (INRs) offer a compact and continuous representation alternative to traditional discrete storage, but training individual INRs for each simulation variable is computationally expensive, especially for multivariate data. This paper introduces MC-INR, a novel framework for efficiently encoding multivariate scientific simulation data using meta-learning and clustered implicit neural representations. MC-INR leverages meta-learning to train a shared initialization for INRs across multiple simulation variables, enabling rapid adaptation to new variables with minimal fine-tuning. Furthermore, we employ a clustering strategy to group variables with similar spatial characteristics, allowing for the sharing of INR parameters within each cluster, thereby further reducing computational cost and improving representation accuracy. Experiments on diverse scientific datasets, including climate modeling and fluid dynamics simulations, demonstrate that MC-INR achieves significant speedups in training time and substantial reductions in model size compared to training individual INRs, while maintaining comparable or superior reconstruction accuracy. MC-INR provides a practical and scalable solution for representing and analyzing large-scale multivariate scientific simulation data."
http://arxiv.org/abs/2507.02443v1,Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic,"Accurate and rapid detection of fruits in agricultural settings is crucial for automated harvesting, yield estimation, and disease monitoring. However, robust detection of red grapes in vineyards presents challenges due to variations in lighting, occlusion, and complex backgrounds. This paper addresses the problem of achieving real-time, high-accuracy detection of red grapes within vineyard imagery using computationally efficient methods suitable for embedded deployment. We propose a custom-designed artificial neural network (ANN) architecture optimized for implementation within the programmable logic of a Field-Programmable Gate Array (FPGA). The network leverages a streamlined convolutional structure coupled with fixed-point quantization to minimize resource utilization and maximize throughput. Furthermore, we employ hardware acceleration techniques, including loop unrolling and pipelining, to exploit the inherent parallelism of the FPGA architecture. Experimental results on a challenging dataset of vineyard images demonstrate that our FPGA-accelerated ANN achieves a grape detection accuracy comparable to state-of-the-art methods while exhibiting a significant speedup, enabling real-time processing at frame rates exceeding 30 frames per second. This work paves the way for the development of efficient and deployable computer vision systems for precision agriculture."
http://arxiv.org/abs/2507.02349v1,Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection,"Cerebrovascular landmark detection is crucial for various neuroimaging applications, including surgical planning, image registration, and disease diagnosis. Manual annotation of these landmarks is time-consuming and prone to inter-observer variability. This paper addresses the challenge of automating cerebrovascular landmark detection in 3D angiographic images. We propose a novel two-step neural network architecture. The first step employs a 3D U-Net to segment the cerebrovascular tree, effectively reducing the search space for landmark localization. The second step utilizes a dedicated landmark detection network, conditioned on the vascular segmentation, to predict the 3D coordinates of each landmark. This network leverages a heatmap regression approach to improve localization accuracy. Experiments on a large clinical dataset demonstrate that our method achieves state-of-the-art performance, with a mean landmark localization error significantly lower than existing methods. Our automated landmark detection framework facilitates efficient and reliable analysis of cerebrovascular anatomy, paving the way for improved clinical workflows and more precise neuroimaging studies."
http://arxiv.org/abs/2507.02322v1,Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model,"Rice leaf diseases pose a significant threat to global food security, causing substantial yield losses. Accurate and timely disease identification is crucial for effective disease management. This paper addresses the challenge of developing a robust and efficient rice leaf disease recognition system by comparatively evaluating two distinct neural network-based approaches: a feature-based model utilizing handcrafted features and a direct imaging model employing convolutional neural networks (CNNs). The feature-based model incorporates techniques like color histogram analysis and texture analysis using Gray-Level Co-occurrence Matrices (GLCM) as input to a multi-layer perceptron. Conversely, the direct imaging model leverages pre-trained CNN architectures, specifically VGG16 and ResNet50, fine-tuned for direct classification from leaf images. Experimental results on a publicly available dataset demonstrate that the direct imaging model, particularly the fine-tuned ResNet50, outperforms the feature-based approach, achieving an average classification accuracy of 96.5% compared to 89.2% for the feature-based model. These findings highlight the potential of deep learning-based direct imaging models for accurate and efficient rice leaf disease recognition, offering a valuable tool for precision agriculture and sustainable rice production."
http://arxiv.org/abs/2507.01559v1,How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks,"Continual learning aims to enable neural networks to learn new tasks sequentially without forgetting previously acquired knowledge. However, catastrophic forgetting remains a significant obstacle, particularly influenced by the interplay between weight updates driven by optimizers and mechanisms for mitigating interference, such as weight resampling. This work investigates how different optimizers and weight resampling strategies impact the learning dynamics and forgetting behavior in continual learning scenarios. We perform a comprehensive analysis by combining several popular optimizers (e.g., SGD, Adam) with various weight resampling techniques, proposing a novel metric to quantify the degree of ""weight rejuvenation"" achieved by resampling. Our experiments across diverse image classification benchmarks demonstrate that the choice of optimizer significantly influences the effectiveness of weight resampling, with optimizers possessing adaptive learning rates often exhibiting reduced benefits from resampling due to their inherent ability to navigate complex loss landscapes. Furthermore, we find that carefully tuned resampling rates, guided by our rejuvenation metric, can lead to substantial improvements in both forward transfer and mitigation of catastrophic forgetting. These findings provide valuable insights into the complex interaction between optimization and memory maintenance in continual learning, paving the way for more robust and efficient continual learning algorithms."
http://arxiv.org/abs/2507.01182v1,Rapid Salient Object Detection with Difference Convolutional Neural Networks,"Salient object detection aims to identify and segment the most visually distinctive objects in an image, playing a critical role in various computer vision applications. Existing deep learning-based salient object detection methods often rely on complex architectures and computationally expensive operations, hindering their application in real-time scenarios. This paper introduces a novel Difference Convolutional Neural Network (DCNet) architecture for rapid and accurate salient object detection. DCNet leverages difference convolutions, which efficiently capture contextual information by explicitly modeling the relationships between neighboring features, thereby reducing computational complexity. Furthermore, we incorporate a multi-scale feature aggregation module to effectively integrate contextual information from different levels of the network. Experimental results on several benchmark datasets demonstrate that DCNet achieves state-of-the-art performance in terms of both accuracy and speed, outperforming existing methods while maintaining a significantly lower computational cost. DCNet's efficiency and accuracy make it a promising solution for real-time salient object detection in resource-constrained environments."
http://arxiv.org/abs/2507.00969v1,Surgical Neural Radiance Fields from One Image,"Neural Radiance Fields (NeRFs) have shown remarkable capabilities in novel view synthesis, but typically require multiple calibrated images for training. Reconstructing surgical scenes, particularly for minimally invasive procedures, presents unique challenges due to limited viewpoints and the difficulty of obtaining accurate camera poses. This paper addresses the problem of reconstructing a surgical scene as a NeRF from a single endoscopic image. We propose a novel framework that leverages a pre-trained, generic 3D shape prior to regularize the NeRF training process. Our approach incorporates a differentiable rendering loss based on the input image and a shape prior loss that encourages the NeRF geometry to align with the prior. This allows for effective training of a NeRF from a single view by providing crucial geometric constraints. Experiments on synthetic and real surgical data demonstrate that our method can generate plausible 3D reconstructions and enable novel view synthesis from a single image, outperforming existing single-view NeRF approaches. This work opens the door to reconstructing detailed surgical environments with minimal data, facilitating downstream applications such as surgical training and augmented reality guidance."
http://arxiv.org/abs/2507.00937v1,RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles,"Radar sensors offer a robust sensing modality for unmanned ground vehicles (UGVs), particularly in adverse weather conditions. However, radar point clouds are inherently sparse and noisy, posing challenges for downstream tasks like object detection and scene understanding. This paper addresses the problem of enhancing radar point cloud representations to improve the accuracy and robustness of perception systems on UGVs. We introduce RaGNNarok, a light-weight Graph Neural Network (GNN) architecture specifically designed for processing radar data. RaGNNarok leverages a novel edge construction strategy based on radar signal characteristics and a simplified graph convolution operation to efficiently propagate information between neighboring points. Experimental results on a real-world radar dataset demonstrate that RaGNNarok significantly improves point cloud quality, leading to a 15% increase in object detection accuracy compared to processing raw radar data and outperforms existing GNN-based point cloud enhancement methods with fewer parameters and lower computational cost. This work provides a practical and effective solution for enhancing radar perception on resource-constrained UGV platforms."
http://arxiv.org/abs/2507.00743v1,Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery,"Optical Coherence Tomography (OCT) is a crucial imaging modality for diagnosing and monitoring retinal diseases, including Epiretinal Membranes (ERM). Accurate classification of ERM subtypes and surgical approaches using OCT scans remains challenging due to subtle variations in retinal morphology and the presence of noise. This paper introduces a novel Tunable Wavelet Unit based Convolutional Neural Network (TWU-CNN) to enhance OCT analysis for classifying the type of surgical intervention required for ERM treatment. The TWU-CNN integrates learnable wavelet decomposition and reconstruction layers within the convolutional architecture, enabling the network to adaptively extract multi-scale features and suppress noise inherent in OCT images. Specifically, the wavelet units are incorporated into the initial convolutional layers to decompose the input OCT image into different frequency subbands, followed by convolutional layers to extract features from each subband, and finally reconstructed to produce enhanced feature maps. Experimental results on a dataset of OCT scans demonstrate that the TWU-CNN achieves a significant improvement in classification accuracy compared to standard CNN architectures and other state-of-the-art methods, exhibiting a 5-8% increase in F1-score. The proposed method provides a valuable tool for clinicians in pre-operative planning, leading to more effective and personalized surgical interventions for ERM."
http://arxiv.org/abs/2507.00739v1,Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, but often lack explicit mechanisms for multi-resolution analysis and frequency decomposition. Many existing wavelet-based CNN augmentations employ fixed wavelet bases or learnable filters without strict wavelet properties, limiting their adaptability and interpretability. This paper introduces a novel Biorthogonal Tunable Wavelet Unit (BTWU) integrated within a CNN architecture, leveraging the lifting scheme for efficient and invertible wavelet transforms. The BTWU learns optimal biorthogonal wavelet filters directly from data through tunable lifting coefficients, enabling flexible adaptation to specific image characteristics and task requirements. Furthermore, we enforce near-perfect reconstruction constraints during training to maintain signal fidelity and prevent information loss. Experimental results on image classification and image denoising benchmarks demonstrate that the proposed BTWU-CNN outperforms conventional CNNs and other wavelet-integrated networks, achieving improved accuracy and robustness while maintaining computational efficiency. Our method provides a principled and effective approach to incorporate wavelet analysis within CNNs, offering enhanced representational power and interpretability for various computer vision applications."
http://arxiv.org/abs/2507.00476v1,FreNBRDF: A Frequency-Rectified Neural Material Representation,"Neural BRDF representations have shown promise in capturing complex material appearance, but often struggle to accurately reproduce high-frequency details and sharp specularities, resulting in blurry or oversmoothed renderings. This limitation stems from the inherent difficulty of neural networks in representing signals with rapid variations, especially when trained on limited data. We introduce FreNBRDF, a novel frequency-rectified neural BRDF representation that explicitly decomposes the BRDF into a low-frequency base and a high-frequency residual. The base component is modeled with a standard neural network, while the residual component is represented using a frequency-domain parameterization based on the Discrete Cosine Transform (DCT), allowing for targeted learning of high-frequency details. We further introduce a rectification loss that encourages the neural network to capture the broad BRDF structure, simplifying the learning process for the DCT coefficients. Experiments on both synthetic and real-world BRDF datasets demonstrate that FreNBRDF significantly improves the accuracy of material appearance reproduction, especially in capturing sharp specular highlights and fine-grained texture details, achieving state-of-the-art performance compared to existing neural BRDF models. FreNBRDF offers a practical and effective solution for high-fidelity material representation, enabling more realistic and compelling rendering in various applications."
http://arxiv.org/abs/2507.02979v1,Iterative Misclassification Error Training (IMET): An Optimized Neural Network Training Technique for Image Classification,"Deep neural networks have achieved remarkable success in image classification; however, their performance is often hampered by noisy labels and the tendency to overfit on easily classified examples. We address the problem of optimizing neural network training to improve robustness against mislabeled data and focus learning on challenging examples. We propose Iterative Misclassification Error Training (IMET), a novel training technique that iteratively identifies potentially misclassified samples based on prediction consistency across epochs and adaptively adjusts their training weights. Specifically, IMET assigns lower weights to samples exhibiting inconsistent predictions, effectively down-weighting potential mislabeled data and prioritizing learning from consistently misclassified, and therefore more challenging, examples. Extensive experiments on benchmark datasets with both synthetic and real-world label noise demonstrate that IMET consistently outperforms state-of-the-art methods, achieving significant improvements in classification accuracy, particularly under high noise levels. IMET offers a robust and effective approach to training neural networks for image classification, enhancing performance in the presence of noisy labels and improving generalization."
http://arxiv.org/abs/2506.24127v1,How to Design and Train Your Implicit Neural Representation for Video Compression,"Implicit Neural Representations (INRs) have emerged as a promising alternative to traditional discrete representations for various computer vision tasks, including image and video compression. However, the effective design and training of INRs for video compression remain a significant challenge due to the high dimensionality and temporal coherence inherent in video data. This paper addresses the problem of optimizing INR architectures and training methodologies specifically for efficient video compression. We propose a novel framework that combines a carefully designed INR architecture with a hierarchical training strategy. Our architecture leverages a hypernetwork to generate the weights of a coordinate-based MLP, enabling efficient encoding of video frames. The hierarchical training strategy involves progressively increasing the complexity of the INR, starting with a coarse representation and refining it iteratively. Experimental results demonstrate that our approach achieves competitive compression ratios compared to state-of-the-art video codecs, while maintaining high reconstruction quality and enabling continuous resolution decoding. This work provides a valuable contribution towards practical and efficient video compression using implicit neural representations."
http://arxiv.org/abs/2506.23854v1,HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity,"Neural implicit surfaces have shown remarkable capabilities in reconstructing 3D shapes from multi-view images. However, their performance degrades significantly in regions with low texture or high reflectivity, resulting in geometric ambiguities and artifacts. To address this limitation, we introduce HiNeuS: a novel neural surface reconstruction approach that explicitly mitigates these ambiguities by incorporating a learned, view-dependent shading field and a robust geometry regularization term. Our method learns to predict per-point shading variations conditioned on the viewing direction, effectively disentangling albedo and lighting effects. Furthermore, we introduce a total variation-based regularizer on the signed distance function (SDF) gradients, encouraging piecewise smooth surfaces and discouraging noisy reconstructions in ambiguous regions. Experiments on both synthetic and real-world datasets demonstrate that HiNeuS significantly improves the fidelity of reconstructed surfaces, particularly in challenging scenarios with low texture and reflective surfaces, outperforming state-of-the-art neural implicit surface reconstruction methods. HiNeuS offers a robust and accurate solution for 3D reconstruction, expanding the applicability of neural implicit surfaces to a wider range of real-world scenes."
http://arxiv.org/abs/2506.23717v1,Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation,"Spiking Neural Networks (SNNs) offer potential advantages in energy efficiency for neuromorphic hardware due to their event-driven nature. However, achieving competitive accuracy with deep learning models often requires complex architectures and long simulation times, hindering practical deployment. This paper addresses the challenge of optimizing SNN performance by dynamically allocating bit precision to different network components. We introduce an adaptive bit allocation scheme that adjusts the bit-width of neuronal membrane potentials and synaptic weights based on their sensitivity to overall network accuracy. This is achieved through a differentiable proxy metric estimating the impact of quantization on neuron activation patterns, enabling gradient-based optimization of bit allocation. Our experiments on benchmark datasets such as CIFAR-10 and CIFAR-100 demonstrate that our adaptive bit allocation significantly reduces the bit operations required while maintaining comparable or even improved accuracy compared to fixed-precision SNNs. This approach paves the way for more efficient and deployable SNNs, bridging the gap between theoretical advantages and practical applications."
http://arxiv.org/abs/2506.23236v1,"VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions","Accurate and efficient 3D human body representations are crucial for simulating realistic interactions within virtual environments. While parametric models like SMPL offer controllable body poses, they often lack the geometric detail necessary for precise contact and collision handling. We address this limitation by introducing VolumetricSMPL, a novel neural volumetric body model that combines the parametric control of SMPL with the geometric richness of neural implicit representations. Our approach leverages a learned deformation field conditioned on SMPL parameters to map a canonical implicit volume to the posed body shape. This allows us to efficiently query occupancy and signed distance values for collision detection and contact force computation. Furthermore, we introduce a differentiable rendering pipeline enabling direct optimization of VolumetricSMPL parameters from multi-view images. Experiments demonstrate that VolumetricSMPL achieves significantly improved geometric accuracy compared to standard SMPL, resulting in more realistic interactions and stable contact simulations. Our model offers a practical and efficient solution for incorporating detailed human body representations into interactive virtual environments and physics simulations."
http://arxiv.org/abs/2506.23004v1,A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks,"Visible Light Communication (VLC) using smartphone cameras presents a promising alternative for indoor localization and data transmission. However, accurate frame identification and synchronization are crucial challenges due to the rolling shutter effect, motion blur, and varying ambient light conditions inherent in smartphone cameras. This paper addresses the problem of robust and efficient frame identification and synchronization in smartphone-based VLC systems. We propose a novel technique employing a Convolutional Neural Network (CNN) designed to directly learn temporal features from sequences of captured frames. The CNN architecture incorporates a custom loss function that penalizes misaligned frame sequences, promoting accurate identification of the VLC signal and precise synchronization. Experimental results demonstrate that our method achieves a significant improvement in frame identification accuracy compared to traditional signal processing techniques, with a reduction in synchronization error of up to 40% under challenging lighting and motion conditions. This robust frame identification and synchronization technique significantly enhances the reliability and performance of smartphone-based VLC systems, paving the way for practical deployment in real-world applications."
http://arxiv.org/abs/2506.22899v1,Neural Cellular Automata: From Cells to Pixels,"Cellular Automata (CA) are discrete dynamical systems that have been used to model complex phenomena in various fields, from physics to biology. However, traditional CA lack the representational power necessary for complex image generation and manipulation tasks. This paper addresses the problem of learning CA rules capable of generating and maintaining complex images directly from pixel data, without relying on pre-defined feature representations. We introduce Neural Cellular Automata (NCA), a differentiable parameterization of CA rules using neural networks. The NCA learns to update cell states based on their local neighborhood, enabling the emergence of global patterns and behaviors from local interactions. Our experiments demonstrate that NCAs can be trained to grow target images from a single seed cell, robustly regenerate damaged images, and adapt to morphological changes. Furthermore, we show NCAs can solve challenging texture synthesis tasks, achieving results competitive with state-of-the-art generative models. This work provides a novel framework for image synthesis and manipulation, offering a new perspective on emergent computation and self-organizing systems in computer vision."
http://arxiv.org/abs/2506.22803v2,Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding,"Understanding the inner workings of neural networks remains a significant challenge, particularly when deploying these models in sensitive domains requiring human oversight. This paper addresses the problem of enhancing human understanding of black-box neural network decisions by introducing a novel Concept Bottleneck Model (CBM) intervention framework. Our approach integrates a CBM into an existing black-box model, allowing humans to interact with the network by directly manipulating high-level concepts. Specifically, we train a CBM to predict intermediate concepts from input data, then use these concept predictions as input features to a modified version of the original black-box model. By allowing users to modify the CBM's concept predictions, we enable them to explore the impact of individual concepts on the final prediction of the black-box model. Experimental results on image classification and medical diagnosis datasets demonstrate that our intervention framework significantly improves human accuracy in predicting model outputs and identifying influential concepts, compared to baseline methods that offer limited interpretability. This work provides a crucial step towards building more transparent and trustworthy AI systems by facilitating meaningful human-AI collaboration and enabling users to understand the reasoning behind complex model decisions."
http://arxiv.org/abs/2506.22156v1,Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction,"Markov Random Fields (MRFs) are widely used in image processing and computer vision for tasks such as image segmentation and map reconstruction. However, iterative inference algorithms for MRF optimization, such as belief propagation, are computationally intensive, particularly when integrated with deep neural networks for parameter learning and inference. This paper addresses the challenge of accelerating the training phase of neural networks used for MRF-based map reconstruction. We propose a novel hardware architecture implemented on a Field-Programmable Gate Array (FPGA) tailored for ultra-fast neural network training. Our architecture leverages pipelining and parallel processing to accelerate both the forward and backward propagation steps, incorporating custom floating-point units optimized for the specific computational characteristics of the network. Experimental results demonstrate a significant speedup of up to 25x compared to a high-performance CPU implementation, while maintaining comparable accuracy in map reconstruction. This hardware acceleration enables real-time or near-real-time training of neural networks for MRF map reconstruction, facilitating the development of adaptive and efficient vision systems."
http://arxiv.org/abs/2506.22134v1,Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization,"Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing continuous signals and shapes, often outperforming discrete representations in terms of memory efficiency and resolution scalability. However, standard INRs can suffer from overfitting and poor generalization, especially when representing complex data or learned from limited observations. To address this, we propose a novel low-rank INR framework that leverages Schatten-p quasi-norm regularization on the weight matrices of the neural network, promoting low-rank structures within the network and reducing the model's effective capacity. Simultaneously, we introduce a Jacobian regularization term that encourages smoothness in the learned representation, preventing high-frequency artifacts and improving generalization. We approximate the non-convex Schatten-p quasi-norm using a differentiable relaxation, enabling efficient optimization via standard backpropagation. Experimental results on various tasks, including image and shape reconstruction, demonstrate that our approach achieves superior performance compared to state-of-the-art INR methods, exhibiting improved reconstruction accuracy and robustness to noise. This work provides a principled framework for learning compact and generalizable INRs, paving the way for efficient representation and manipulation of complex visual data."
http://arxiv.org/abs/2506.21884v2,UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, enabling photorealistic rendering from sparse input images. However, NeRF struggles to accurately represent scenes with complex material mixtures and spectral variations, as it typically models radiance with a single RGB color value at each point in space. To address this limitation, we propose UnMix-NeRF, a novel approach that integrates spectral unmixing techniques into the NeRF framework. UnMix-NeRF decomposes the radiance field into a set of spectrally distinct material components, represented by learned spectral signatures and their corresponding volume densities. During rendering, the color at each pixel is synthesized by linearly combining the spectral signatures weighted by their densities along the ray. Experiments on synthetic and real-world hyperspectral datasets demonstrate that UnMix-NeRF achieves significantly improved reconstruction accuracy and material disentanglement compared to standard NeRF and other spectral NeRF variants, particularly in scenes with mixed materials. This approach provides a powerful tool for analyzing and rendering scenes with complex spectral properties, opening new avenues for applications in remote sensing, material science, and realistic rendering."
http://arxiv.org/abs/2506.21537v1,ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers,"Residual Neural Networks (ResNets) have revolutionized deep learning by enabling the training of extremely deep architectures, achieving state-of-the-art performance in various computer vision tasks. However, their computational demands remain a significant bottleneck. This paper addresses the challenge of implementing and accelerating ResNets using analog Rydberg atom quantum computers, which offer potential for significant speedups in specific computational tasks. We introduce ResQ, a novel framework that maps the core operations of ResNet blocks, including linear transformations and non-linear activations, onto Rydberg atom quantum hardware. ResQ leverages variational quantum algorithms and optimized encoding schemes to efficiently represent and process image data within the quantum system. Our simulations demonstrate that ResQ can successfully implement ResNet blocks with competitive accuracy compared to classical counterparts for small image classification problems, while requiring significantly fewer computational resources in terms of gate count and circuit depth. This work paves the way for exploring the potential of quantum computing in accelerating deep learning models for computer vision and other computationally intensive applications."
http://arxiv.org/abs/2506.21349v3,Generalizable Neural Electromagnetic Inverse Scattering,"Electromagnetic inverse scattering aims to reconstruct the properties of an object from scattered electromagnetic fields, finding applications in medical imaging, non-destructive testing, and security screening. However, traditional iterative methods are computationally expensive and struggle with complex geometries and high-contrast materials, while deep learning approaches often lack generalization to unseen data distributions. This paper addresses the challenge of achieving generalizable electromagnetic inverse scattering using neural networks. We propose a novel physics-informed neural network architecture that incorporates electromagnetic scattering physics as a regularization term directly into the loss function, alongside a data-driven term. Crucially, our network is trained on a diverse, synthetically generated dataset spanning a wide range of object shapes, sizes, and material properties, enhanced with data augmentation techniques. Experimental results demonstrate that our approach significantly outperforms existing data-driven methods in terms of reconstruction accuracy and robustness, particularly when applied to objects with characteristics outside the training distribution. This work paves the way for real-time, generalizable electromagnetic imaging in diverse and challenging scenarios."
http://arxiv.org/abs/2506.20638v1,Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects,"Accurate attitude estimation and 3D reconstruction of non-cooperative space objects are critical for various space situational awareness applications, including active debris removal and on-orbit servicing. However, the lack of prior knowledge and the presence of significant pose variations make these tasks challenging, especially when relying solely on optical sensor data. This paper addresses the problem of jointly estimating the attitude and reconstructing the 3D shape of non-cooperative space objects from a sequence of monocular images. We propose a novel neural network architecture that integrates a differentiable pose estimator with a neural radiance field (NeRF)-based 3D reconstruction module. The pose estimator leverages a recurrent neural network to enforce temporal consistency and refine attitude estimates across the image sequence, while the NeRF module simultaneously learns a 3D representation of the object and optimizes pose parameters through volume rendering. We demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets, significantly improving the accuracy of attitude estimation and the quality of 3D reconstruction compared to existing approaches. The joint optimization framework enables robust and accurate perception of space objects, paving the way for autonomous robotic missions in complex space environments."
http://arxiv.org/abs/2506.20355v1,"Practical insights on the effect of different encodings, anstze and measurements in quantum and hybrid convolutional neural networks","Quantum Convolutional Neural Networks (QCNNs) offer a promising avenue for leveraging quantum computation to enhance image processing tasks, yet their practical utility hinges on the careful selection of encoding methods, quantum circuit architectures (anstze), and measurement strategies. This paper addresses the critical need for empirical understanding of how these choices impact the performance of both pure and hybrid QCNNs in image classification. We systematically investigate the effects of various data encodings (amplitude, angle, and qubit reuse), parameterized quantum circuit anstze (hardware-efficient, strongly entangling, and problem-inspired), and measurement schemes (single-qubit Pauli measurements and global measurements) on benchmark image datasets. Our findings reveal that the interplay between encoding strategy and ansatz complexity significantly influences model expressivity and trainability, with specific combinations yielding superior accuracy and generalization. Notably, we observe that while more complex anstze can achieve higher accuracy with appropriate encodings, they are also more susceptible to noise and require careful regularization. These insights provide practical guidance for designing and implementing effective QCNNs, facilitating their application in real-world computer vision problems."
http://arxiv.org/abs/2506.19491v1,Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications,"Neural 3D reconstruction techniques have shown promise in various computer vision applications, offering the potential for efficient and accurate 3D model generation. However, their performance in the context of small Unmanned Aerial Vehicle (UAV)-based applications, characterized by limited computational resources and challenging environmental conditions, remains relatively unexplored. This paper addresses the critical gap in understanding the applicability and limitations of neural 3D reconstruction methods for generating accurate and reliable 3D models from UAV-acquired imagery. We conduct a comprehensive experimental evaluation of several state-of-the-art neural reconstruction pipelines, adapting them for efficient execution on edge computing platforms suitable for small UAVs. Our evaluation considers metrics such as reconstruction accuracy (measured by Chamfer Distance and IoU), memory footprint, and processing time, across diverse real-world datasets captured by a small UAV. We find that while some neural methods can achieve comparable accuracy to traditional Structure-from-Motion (SfM) techniques under ideal conditions, their robustness significantly degrades in scenarios with low-texture surfaces or varying illumination, and their computational demands remain a challenge for real-time UAV applications. This work provides valuable insights into the practical considerations for deploying neural 3D reconstruction on small UAVs and highlights areas for future research to improve their robustness and efficiency."
http://arxiv.org/abs/2506.19465v1,Stylized Structural Patterns for Improved Neural Network Pre-training,"Self-supervised pre-training has become a crucial technique for learning robust visual representations, often relying on contrastive learning or masked image modeling. However, existing methods primarily focus on pixel-level or feature-level reconstruction, neglecting the explicit encoding of underlying structural information within images. This work introduces a novel pre-training strategy, Stylized Structural Patterns (SSP), designed to enhance neural network understanding of object structure. SSP involves extracting edge maps from training images and then stylizing these edges with varying textures and colors, creating a diverse set of structural representations. These stylized edge maps are then used as targets for a reconstruction task during pre-training, forcing the network to learn relationships between visual appearance and underlying structure. Experiments on ImageNet classification and object detection benchmarks demonstrate that networks pre-trained with SSP exhibit improved performance compared to traditional pre-training methods, especially in low-data regimes and when fine-tuning on datasets with significant domain shifts. This suggests that explicitly encoding structural information during pre-training leads to more generalizable and robust visual representations."
http://arxiv.org/abs/2506.19051v1,NIC-RobustBench: A Comprehensive Open-Source Toolkit for Neural Image Compression and Robustness Analysis,"Neural Image Compression (NIC) has demonstrated promising performance in terms of rate-distortion trade-offs, but its robustness against common image corruptions and adversarial attacks remains largely unexplored. This paper addresses the critical gap in evaluating and improving the robustness of NIC models. We introduce NIC-RobustBench, a comprehensive, open-source toolkit designed for standardized benchmarking and analysis of NIC robustness. Our toolkit encompasses a wide range of pre-trained NIC models, including learned transform coding and autoencoder-based architectures, alongside a diverse suite of corruption types (e.g., noise, blur, weather) and adversarial attacks (e.g., FGSM, PGD). Furthermore, NIC-RobustBench provides tools for evaluating the performance of various defense strategies against these perturbations. Experimental results using NIC-RobustBench reveal significant vulnerabilities in existing NIC models, demonstrating substantial performance degradation under common image corruptions and adversarial attacks, even at low perturbation levels. NIC-RobustBench facilitates reproducible research and development of robust NIC algorithms, paving the way for reliable deployment of NIC in real-world applications."
http://arxiv.org/abs/2506.18720v1,Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI,"Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a crucial tool for breast cancer diagnosis, relying on the temporal evolution of contrast agent uptake. However, accurately modeling this complex process, particularly the non-linear diffusion and interaction within heterogeneous breast tissue, remains a challenge. We address the problem of developing a spatiotemporally accurate model for contrast enhancement in breast MRI. Our approach introduces Temporal Neural Cellular Automata (TNCA), a novel deep learning framework extending traditional NCAs by incorporating explicit temporal dependencies. TNCA learns local update rules based on neighboring cell states and a temporal memory, allowing it to dynamically simulate contrast agent diffusion and uptake over time. We demonstrate that TNCA can accurately reproduce realistic contrast enhancement patterns observed in real DCE-MRI data, achieving superior performance compared to standard CNN-based and LSTM-based approaches in predicting future contrast distributions. This framework offers a powerful and interpretable tool for understanding and simulating contrast enhancement dynamics, potentially leading to improved diagnostic accuracy and personalized treatment planning in breast cancer."
http://arxiv.org/abs/2506.18678v1,MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation,"Collaborative Simultaneous Localization and Mapping (SLAM) enables multiple agents to collectively build a consistent map of an environment, improving robustness and efficiency compared to single-agent systems. However, existing collaborative SLAM systems often struggle with representing complex scenes and efficiently fusing information from multiple agents with potentially noisy or incomplete observations. This paper introduces MCN-SLAM, a novel multi-agent collaborative neural SLAM system that leverages a hybrid implicit neural scene representation. MCN-SLAM employs a combination of a global Neural Radiance Field (NeRF) for scene reconstruction and local Signed Distance Functions (SDFs) for detailed geometry capture and agent-specific tracking. A distributed optimization framework is introduced to fuse local SDF representations into the global NeRF in a decentralized manner, while mitigating the impact of noisy agent poses. Experimental results on both synthetic and real-world datasets demonstrate that MCN-SLAM achieves superior mapping accuracy and robustness compared to state-of-the-art collaborative SLAM systems, particularly in challenging environments with limited overlap between agent viewpoints. MCN-SLAM provides a promising approach for building high-fidelity, collaborative maps in complex and dynamic environments."
http://arxiv.org/abs/2507.02901v2,Online Continual Learning via Spiking Neural Networks with Sleep Enhanced Latent Replay,"Continual learning aims to enable artificial neural networks to learn new tasks sequentially without forgetting previously learned knowledge. However, catastrophic forgetting remains a significant challenge, especially in resource-constrained and biologically plausible models like Spiking Neural Networks (SNNs). We address this challenge by introducing a novel online continual learning framework for SNNs that leverages sleep-enhanced latent replay. Our approach integrates a self-organizing latent space to represent and replay past experiences, coupled with a sleep-like mechanism that periodically consolidates learned representations and mitigates interference between tasks. Specifically, during sleep phases, we introduce targeted noise to the latent space, promoting exploration of previously learned manifolds and strengthening synaptic connections crucial for past task performance. Experimental results on established continual learning benchmarks, including permuted MNIST and CIFAR-10, demonstrate that our method significantly outperforms existing SNN-based continual learning approaches, achieving higher accuracy and reduced forgetting. This work advances the development of energy-efficient and biologically inspired continual learning systems capable of adapting to dynamic environments."
http://arxiv.org/abs/2506.17996v1,Fast Neural Inverse Kinematics on Human Body Motions,"Inverse kinematics (IK) is fundamental for controlling human body motions in various applications such as animation, robotics, and virtual reality. However, traditional IK solvers often struggle to meet the real-time requirements of interactive systems, especially with high-dimensional human skeletons. This paper addresses the challenge of achieving fast and accurate IK solutions for human body motions by leveraging neural networks. We propose a novel neural IK architecture consisting of a cascaded network that predicts joint angles directly from end-effector positions and a learned prior on plausible human poses. The network is trained on a large motion capture dataset and incorporates regularization techniques to ensure smooth and natural-looking movements. Our experiments demonstrate that the proposed method achieves significantly faster computation times compared to traditional iterative solvers, while maintaining comparable or superior accuracy in reproducing target end-effector positions. Furthermore, our method exhibits robustness to out-of-distribution poses. The speed and accuracy of our neural IK solver enable real-time control and manipulation of human avatars in interactive environments."
http://arxiv.org/abs/2506.17191v1,Facial Landmark Visualization and Emotion Recognition Through Neural Networks,"Facial landmark detection and emotion recognition are fundamental tasks in computer vision, enabling a wide range of applications from human-computer interaction to behavioral analysis. Accurately recognizing emotions from facial expressions remains challenging due to variations in pose, illumination, and individual expression styles. This paper introduces a novel approach integrating facial landmark visualization with a deep neural network for improved emotion recognition. Our method first employs a convolutional neural network (CNN) to detect and visualize 68 facial landmarks. These landmarks are then utilized as spatial features, which are combined with raw pixel data and fed into a multi-branch CNN architecture designed to extract both local and global contextual information for emotion classification. Experimental results on benchmark datasets, including FER-2013 and AffectNet, demonstrate that our proposed method achieves state-of-the-art performance, surpassing existing approaches by a significant margin in terms of recognition accuracy. This highlights the effectiveness of integrating landmark visualization as a spatial feature for robust emotion recognition, paving the way for more accurate and reliable emotion-aware systems."
http://arxiv.org/abs/2506.17165v1,Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network,"Brain tumor classification from magnetic resonance imaging (MRI) is crucial for timely diagnosis and treatment planning. Imbalanced datasets, particularly with limited representation of rare tumor types, pose a significant challenge to training robust Convolutional Neural Network (CNN) classifiers. This paper addresses the problem of improving brain tumor classification accuracy in the presence of imbalanced MRI datasets using a novel Generative Adversarial Network (GAN)-augmentation strategy. We introduce a Proportional Sensitivity GAN (PS-GAN) that generates synthetic MRI images, focusing on augmenting under-represented tumor classes proportionally to their sensitivity scores derived from a preliminary CNN classification. This sensitivity-aware generation ensures that the augmented data effectively improves the classifier's ability to distinguish between different tumor types. Experiments on the BraTS 2021 dataset demonstrate that our PS-GAN augmented CNN achieves a statistically significant improvement in Dice score (average increase of 3%) and overall accuracy (average increase of 2%) compared to baseline CNNs and CNNs augmented with traditional GANs or other data augmentation techniques. The proposed method offers a practical and effective approach to enhance the performance of brain tumor classification systems, particularly in scenarios with limited and imbalanced data."
http://arxiv.org/abs/2506.16773v1,Infrared and Visible Image Fusion Based on Implicit Neural Representations,"Infrared and visible image fusion aims to integrate complementary information from different modalities, enhancing scene understanding in various applications. Existing fusion methods often struggle with preserving fine details and effectively transferring thermal radiation information, particularly in complex scenes. To address these limitations, we propose a novel infrared and visible image fusion framework based on Implicit Neural Representations (INRs). Our approach represents both infrared and visible images as continuous functions parameterized by multi-layer perceptrons (MLPs), enabling a more flexible and high-resolution representation compared to discrete pixel grids. The fusion process is achieved by adaptively weighting the INR representations of the source images, guided by a learned fusion network that considers both local image features and global contextual information. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of both quantitative metrics and visual quality, effectively transferring thermal information while preserving structural details and reducing artifacts. This INR-based fusion technique offers a promising direction for developing more robust and effective multi-modal image fusion algorithms."
http://arxiv.org/abs/2506.16627v1,FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models,"Neural Signed Distance Functions (SDFs) have emerged as a powerful representation for modeling 3D shapes, demonstrating remarkable capabilities in representing complex geometries. However, representing sharp features and planar surfaces, prevalent in CAD models, remains a challenge, often resulting in over-smoothed surfaces and loss of fine details. This paper addresses the problem of efficiently and effectively regularizing the curvature of neural SDFs to better represent CAD models with sharp features and planar regions. We introduce FlatCAD, a novel training scheme that incorporates a targeted curvature regularization loss, specifically designed to penalize high curvature in planar regions and encourage sharp edges. This loss leverages a differentiable curvature estimation based on the SDF's Hessian and its gradient, allowing for efficient optimization. Experimental results on a diverse set of CAD models demonstrate that FlatCAD significantly improves the geometric accuracy and feature preservation compared to existing neural SDF methods, achieving lower Chamfer distance and Hausdorff distance while maintaining comparable training times. FlatCAD provides a practical and effective approach for representing CAD models with neural SDFs, enabling downstream applications requiring high-fidelity geometric reconstruction."
http://arxiv.org/abs/2506.16331v1,Transparency Techniques for Neural Networks trained on Writer Identification and Writer Verification,"Writer identification and verification are crucial tasks in document analysis, often relying on complex neural network models to capture subtle handwriting nuances. However, the black-box nature of these models hinders understanding of which features are most discriminative for identifying or verifying authorship. This paper addresses the challenge of enhancing the transparency of neural networks trained for writer identification and verification tasks. We explore and adapt several post-hoc explainability techniques, including Grad-CAM, Layer-wise Relevance Propagation (LRP), and attention visualization, to highlight salient regions in handwriting samples that influence the model's predictions. By visualizing these regions, we gain insights into the features the network prioritizes, such as specific stroke patterns, letter formations, and global text characteristics. Our experiments on benchmark datasets demonstrate that these transparency techniques effectively reveal meaningful handwriting features used by the network, and furthermore, that feature importance varies depending on the writer and the specific network architecture. This work contributes to building trust in neural network-based writer identification and verification systems by providing interpretable explanations of their decision-making processes."
http://arxiv.org/abs/2506.16210v2,From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction,"Anisotropic Magnetic Resonance Imaging (MRI) accelerates data acquisition by undersampling k-space along specific directions, leading to artifacts that complicate downstream analysis. Traditional reconstruction methods often struggle with motion artifacts, which further degrade image quality, particularly in highly anisotropic acquisitions. To address this, we propose a novel reconstruction framework leveraging implicit neural representations (INRs) that progressively refine the image from a coarse initial estimate to a continuous, high-resolution representation. Our method, termed CoCo-INR, first encodes the undersampled k-space data into a coarse volumetric representation using a shallow INR. Subsequently, a series of refinement modules, each consisting of a deeper INR, progressively incorporates k-space consistency and motion correction via a learned displacement field. Experiments on in-vivo cardiac MRI data demonstrate that CoCo-INR significantly outperforms state-of-the-art reconstruction techniques, reducing motion artifacts and improving image sharpness, as measured by PSNR and SSIM. The proposed approach provides a robust and efficient solution for high-quality anisotropic MRI reconstruction, paving the way for improved clinical diagnosis and quantitative imaging."
http://arxiv.org/abs/2506.16186v1,Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis,"Traffic accident detection and analysis are crucial for improving road safety and urban planning. Existing methods often struggle with limited and imbalanced datasets, hindering the performance of deep learning models. This paper addresses the challenge of enhancing traffic accident detection and analysis by integrating Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNNs). Our proposed framework utilizes a GAN to generate synthetic accident images, augmenting the original dataset and mitigating the class imbalance problem. These generated images, along with the real images, are then fed into a CNN-based classifier for accident detection. Furthermore, we employ Grad-CAM to visualize the regions in the images that the CNN focuses on, enabling detailed analysis of accident-related features. Experimental results on a real-world traffic accident dataset demonstrate that our integrated GAN-CNN approach significantly improves accident detection accuracy compared to standalone CNN models and other state-of-the-art methods, achieving a relative improvement of 15% in F1-score. This research provides a robust and interpretable framework for traffic accident detection, contributing to the development of more effective road safety systems."
http://arxiv.org/abs/2506.15815v2,GratNet: A Photorealistic Neural Shader for Diffractive Surfaces,"Diffractive optical elements (DOEs) are increasingly used to generate complex light patterns for applications ranging from augmented reality displays to advanced microscopy. However, accurately rendering photorealistic images of scenes containing DOEs remains a significant challenge due to the complex light transport phenomena involved. We address the problem of generating realistic renderings of diffractive surfaces by introducing GratNet, a novel neural shader specifically designed to model the appearance of DOEs. GratNet learns a spatially varying BRDF that accounts for the unique diffraction characteristics of the surface. It takes as input the viewing direction, lighting direction, and local grating parameters (period and orientation) and outputs the reflected radiance. We train GratNet on synthetic data generated using a physics-based diffraction model and demonstrate its ability to reproduce complex diffraction patterns with high fidelity. GratNet significantly outperforms traditional BRDF models and other neural rendering techniques in terms of accuracy and visual realism, enabling the creation of compelling visualizations of scenes containing diffractive surfaces. This provides a critical tool for design and evaluation of optical systems incorporating DOEs, ultimately accelerating their adoption in diverse applications."
http://arxiv.org/abs/2506.15680v1,Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos,"Learning accurate and robust deformable object models from video remains a challenging problem in computer vision, particularly from readily available RGB-D data. This paper addresses the problem of learning temporally coherent and physically plausible 3D deformable object representations directly from RGB-D video sequences. We introduce Particle-Grid Neural Dynamics (PGND), a novel approach that combines the strengths of particle-based representations, grid-based neural implicit functions, and neural dynamics. PGND represents the object as a set of interacting particles, each associated with a local grid-based neural implicit function that defines the object's surface. A neural dynamics model governs the particle interactions and motion, learned directly from the observed RGB-D data using a differentiable physics engine. Experiments on both synthetic and real-world datasets demonstrate that PGND can accurately reconstruct and track deformable objects, outperforming existing methods in terms of reconstruction accuracy, temporal coherence, and robustness to noise. The proposed method enables more realistic and interpretable deformable object modeling for applications in robotics, augmented reality, and physics simulation."
http://arxiv.org/abs/2506.15276v1,MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion,"Neural Radiance Fields (NeRFs) have shown promise for novel view synthesis, but extending them to dynamic scenes and videos remains challenging due to the increased complexity and computational demands. Existing dynamic NeRFs often struggle with capturing fine-grained details and handling complex motions in long videos. To address these limitations, we introduce MSNeRV, a novel Neural Video Representation with Multi-Scale Feature Fusion. MSNeRV employs a hierarchical architecture that extracts features at multiple spatial and temporal scales and fuses them within a NeRF framework. Specifically, we leverage 3D convolutional networks for feature extraction and incorporate a novel attention mechanism to adaptively weight and combine features from different scales based on their relevance to the current query point. Experiments on various dynamic scene datasets demonstrate that MSNeRV achieves state-of-the-art performance in terms of reconstruction quality and novel view synthesis, particularly in handling complex motions and preserving fine details. This work significantly advances the capabilities of neural video representations for realistic and high-fidelity dynamic scene modeling."
http://arxiv.org/abs/2506.15258v2,Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference,"Chest X-rays are a crucial tool for diagnosing various pulmonary diseases, but accessing and processing them raises significant privacy concerns due to the sensitive patient information they contain. This paper addresses the problem of performing chest X-ray classification without revealing the raw image data or the trained model parameters. We propose a novel privacy-preserving framework that leverages variational autoencoders (VAEs) to encode chest X-rays into a lower-dimensional latent space, followed by a classifier trained on these latent representations. Critically, the classification inference is performed using homomorphic encryption (HE), allowing computations on encrypted latent vectors without decryption. Our experimental results demonstrate that classification accuracy on encrypted latent representations is comparable to that achieved with plaintext data, with minimal performance degradation. This approach allows for accurate chest X-ray classification while ensuring patient data and model privacy, paving the way for secure and collaborative medical image analysis."
http://arxiv.org/abs/2506.15242v2,RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories,"Neural Radiance Fields (NeRFs) have demonstrated impressive capabilities in novel view synthesis, but their performance is highly dependent on accurate camera pose estimation. Existing NeRF-based reconstruction methods often struggle with complex camera trajectories and noisy initial pose estimates, leading to blurry or distorted reconstructions. This paper introduces RA-NeRF, a robust framework for NeRF reconstruction that jointly optimizes for scene geometry and camera poses, even under complex trajectories. RA-NeRF leverages a robust pose refinement module incorporating a differentiable pose parameterization coupled with a learned uncertainty weighting to mitigate the impact of outlier correspondences. Furthermore, we introduce a novel annealing strategy for pose optimization, gradually increasing the influence of the NeRF rendering loss to avoid local minima and stabilize pose estimation. Experiments on synthetic and real-world datasets with challenging camera motions demonstrate that RA-NeRF significantly outperforms state-of-the-art methods in both reconstruction quality and pose accuracy, particularly when initialized with inaccurate poses. Our approach enables high-fidelity NeRF reconstruction from challenging datasets, expanding the applicability of NeRFs to more complex and uncontrolled environments."
http://arxiv.org/abs/2506.14667v2,DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification,"Neural Architecture Search (NAS) has shown great promise in automatically designing high-performance deep learning models for image classification. However, most NAS algorithms treat the training data as a static entity, neglecting the potential benefits of dynamically adapting the training set composition during the search process. This paper addresses the problem of inefficient data utilization in NAS by proposing DDS-NAS, a novel framework that integrates Dynamic Data Selection (DDS) within the NAS loop. DDS-NAS employs an on-line hard example mining strategy to prioritize challenging samples during architecture evaluation, forcing candidate architectures to learn from the most informative data points. Specifically, a light-weight predictor network estimates the difficulty of each sample, guiding the selection of a subset of hard examples for training each architecture. Experiments on CIFAR-10 and ImageNet demonstrate that DDS-NAS consistently outperforms state-of-the-art NAS methods, achieving comparable or superior accuracy with significantly reduced search costs. The proposed approach highlights the importance of data-aware NAS and opens new avenues for developing more efficient and effective automated model design strategies."
http://arxiv.org/abs/2506.14350v1,FGA-NN: Film Grain Analysis Neural Network,"Film grain, an inherent characteristic of analog film, significantly impacts the visual quality and aesthetic of scanned film archives. Removing or reducing film grain is crucial for preservation, restoration, and downstream analysis, yet existing methods often struggle to differentiate grain from fine image details, leading to over-smoothing and loss of valuable information. This paper introduces FGA-NN, a Film Grain Analysis Neural Network designed to accurately model and analyze film grain characteristics within scanned film frames. FGA-NN employs a multi-scale convolutional architecture coupled with a novel grain attention mechanism. This mechanism learns to selectively focus on grain-like structures while suppressing genuine image details, enabling precise grain estimation and subsequent removal. Experimental results on a diverse dataset of scanned film demonstrate that FGA-NN outperforms state-of-the-art denoising algorithms in terms of both quantitative metrics (PSNR, SSIM) and perceptual quality, preserving fine details while effectively suppressing film grain. FGA-NN offers a significant advancement in film restoration by providing a powerful tool for accurate and perceptually pleasing film grain removal."
http://arxiv.org/abs/2506.14856v1,Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction,"3D reconstruction from multiple views is a fundamental problem in computer vision, crucial for applications ranging from robotics to augmented reality. A key challenge lies in efficiently selecting the next best viewpoints to maximize reconstruction quality while minimizing acquisition cost. This paper addresses the problem of active view selection in 3D reconstruction by leveraging neural uncertainty estimation to guide the viewpoint selection process. Our method employs a neural network to predict pixel-wise depth and associated uncertainty maps from existing reconstructions. These uncertainty maps are then used to inform an active view selection strategy that prioritizes viewpoints likely to reduce overall reconstruction uncertainty, focusing on regions with high uncertainty and favorable viewing angles. Experiments on both synthetic and real-world datasets demonstrate that our approach significantly improves reconstruction accuracy and completeness compared to existing active view selection methods and passive data acquisition strategies, particularly in challenging scenarios with occlusions and texture-less surfaces. This work offers a principled and efficient approach to active 3D reconstruction, paving the way for more robust and scalable 3D scanning systems."
http://arxiv.org/abs/2506.14176v1,One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification,"Neural Architecture Search (NAS) has shown great promise in automating the design of deep learning models, yet its application to pathological image classification remains limited due to the high computational cost of training numerous candidate architectures, especially with limited data. This paper addresses the challenge of efficient NAS for pathological image classification in a one-shot setting. We propose a novel NAS framework that leverages network similarity to guide the initialization of candidate architectures within a supernet. Specifically, we pre-train a supernet and then, instead of random initialization for each candidate architecture, we initialize their weights based on the similarity of their graph structure to previously evaluated, high-performing architectures within the supernets search space. This network similarity-directed initialization allows for faster convergence and more accurate performance estimation during the one-shot NAS process. Experimental results on multiple pathological image datasets demonstrate that our method achieves state-of-the-art classification accuracy while significantly reducing the computational cost compared to existing NAS approaches and hand-crafted architectures. This work provides a practical and efficient solution for deploying customized deep learning models for pathological image analysis, accelerating the development of automated diagnostic tools."
http://arxiv.org/abs/2506.14846v1,Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks; however, their performance is highly dependent on architectural choices, particularly the kernel size and feature map dimension in convolutional layers. Determining the optimal kernel size and dimension remains a challenging problem, often relying on computationally expensive manual tuning or grid search. This paper proposes an architecture optimization approach leveraging Neural Architecture Search (NAS) with a tailored search space focusing on kernel size and output channel dimension within predefined bounds. We employ a differentiable NAS strategy, specifically DARTS, modified to efficiently explore the search space while maintaining computational feasibility. The search process incorporates a novel regularization term that encourages sparsity in both kernel size and channel dimension selections, leading to more compact and efficient architectures. Experiments on CIFAR-10 and ImageNet demonstrate that our method discovers CNN architectures that outperform manually designed and other NAS-derived networks with comparable or fewer parameters. The optimized architectures achieve a top-1 accuracy of X% on ImageNet, representing a Y% improvement over baseline models with similar complexity. This work provides a practical and effective approach for automating the design of efficient CNN architectures by simultaneously optimizing kernel size and feature map dimension, leading to improved performance and reduced computational cost."
http://arxiv.org/abs/2506.13506v1,Stimulus Motion Perception Studies Imply Specific Neural Computations in Human Visual Stabilization,"Human visual stabilization, the ability to maintain a stable percept of the world despite self-motion, is crucial for interacting with our environment. While the behavioral outcomes of visual stabilization are well-documented, the specific neural computations underlying the processing of stimulus motion during self-motion remain poorly understood. This paper investigates how the human visual system integrates retinal and extra-retinal signals to differentiate between object motion and self-induced image motion, focusing on the computational implications of observed perceptual biases. We conducted a series of psychophysical experiments where participants judged the direction of motion of a visual stimulus presented during simulated self-motion. By systematically varying the stimulus speed, direction, and the characteristics of the simulated self-motion, we derived computational models that best explain the observed perceptual biases. Our results demonstrate that human observers exhibit a systematic underestimation of stimulus motion congruent with self-motion and an overestimation of motion opposing it, suggesting a non-linear, gain-modulated integration of retinal and extra-retinal signals. These findings support a model where the visual system actively predicts and compensates for self-induced image motion through specialized neural computations, thereby providing insights into the mechanisms underlying stable visual perception."
http://arxiv.org/abs/2506.13195v1,ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs,"Single-view 3D reconstruction of anatomical structures from 2D radiographs remains a challenging task in medical imaging, particularly in dentistry where panoramic radiographs offer a comprehensive view of the oral cavity. This paper addresses the problem of accurately reconstructing the 3D geometry of oral anatomy, specifically teeth and jawbones, from a single panoramic radiograph. We propose ViT-NeBLa, a novel hybrid framework that combines a Vision Transformer (ViT) for robust feature extraction with a Neural Beer-Lambert (NeBLa) rendering module to model X-ray attenuation. The ViT learns global context and feature representations from the panoramic radiograph, which are then used to parameterize the NeBLa module for differentiable rendering of the 3D oral anatomy. NeBLa leverages the Beer-Lambert law to model X-ray attenuation, enabling the reconstruction of 3D volume densities from the 2D input. Experimental results on a synthetic dataset and real clinical panoramic radiographs demonstrate that ViT-NeBLa achieves state-of-the-art performance in single-view 3D reconstruction of oral anatomy, outperforming existing methods in terms of accuracy and robustness. This work offers a promising avenue for automated 3D modeling of oral structures from readily available panoramic radiographs, potentially improving diagnostic and treatment planning workflows in dentistry."
http://arxiv.org/abs/2506.13050v1,NeuVAS: Neural Implicit Surfaces for Variational Shape Modeling,"Variational shape modeling provides a powerful framework for creating and manipulating 3D shapes by optimizing an energy functional. However, traditional methods often rely on explicit mesh representations, which can be computationally expensive and topologically restrictive. This paper addresses the challenge of integrating the flexibility and efficiency of neural implicit surfaces into variational shape modeling. We introduce NeuVAS, a novel approach that represents shapes as neural implicit surfaces and formulates variational problems directly in the implicit domain. NeuVAS leverages differentiable rendering and implicit differentiation to compute gradients of energy functionals with respect to the neural network parameters, enabling efficient optimization. We demonstrate NeuVAS on various shape modeling tasks, including surface fairing, shape interpolation, and constrained shape optimization. Experimental results show that NeuVAS achieves comparable or superior performance to traditional mesh-based methods, while offering greater flexibility in handling topological changes and reducing computational cost. NeuVAS opens new avenues for designing and manipulating 3D shapes with complex geometries and topologies using the power of neural implicit representations."
http://arxiv.org/abs/2506.12896v2,Structure-Preserving Patch Decoding for Efficient Neural Video Representation,"Neural video representations have shown promise in compressing and generating video content, but often struggle with maintaining fine-grained structural details and computational efficiency during decoding. This paper addresses the challenge of efficiently decoding high-fidelity video frames from a learned latent representation while preserving crucial structural information. We introduce Structure-Preserving Patch Decoding (SPPD), a novel framework that leverages a spatially-aware patch selection mechanism and a structure-guided reconstruction network. SPPD identifies and prioritizes salient patches within the latent representation based on structural significance, enabling targeted decoding efforts. These selected patches are then processed by a reconstruction network trained to explicitly preserve structural integrity, minimizing distortions and artifacts. Experiments on benchmark video datasets demonstrate that SPPD achieves comparable or superior reconstruction quality to state-of-the-art methods with significantly reduced computational overhead during decoding. Our approach offers a compelling solution for efficient and high-fidelity neural video representation, enabling real-time video generation and compression applications."
http://arxiv.org/abs/2506.12706v1,NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in various downstream tasks, but their robustness against adversarial attacks remains a significant concern. Existing prompt tuning methods often struggle to defend against sophisticated adversarial perturbations due to their limited capacity to adapt to diverse attack strategies. We address this limitation by introducing NAP-Tuning, a novel Neural Augmented Prompt Tuning framework for improving the adversarial robustness of VLMs. NAP-Tuning dynamically augments the learnable prompt with neural features extracted from intermediate layers of the VLM itself. This allows the prompt to adaptively incorporate contextual information relevant to the specific input and potential adversarial perturbations. Experimental results on benchmark datasets demonstrate that NAP-Tuning significantly enhances adversarial robustness compared to state-of-the-art prompt tuning and fine-tuning methods, achieving substantial improvements in accuracy under various white-box and black-box attacks. This work highlights the potential of neural augmentation within prompt tuning to create more resilient and reliable VLMs for real-world applications."
http://arxiv.org/abs/2506.12693v1,Zero-shot denoising via neural compression: Theoretical and algorithmic framework,"Image denoising is a fundamental task in computer vision, often relying on supervised learning with extensive training data. However, acquiring clean training data is often impractical, motivating the need for zero-shot denoising methods. This paper addresses the challenge of zero-shot image denoising by leveraging the principles of neural compression. We propose a novel theoretical and algorithmic framework that views denoising as an implicit compression task, hypothesizing that a well-compressed image representation inherently discards noise. Our method trains an autoencoder network to compress noisy images, explicitly penalizing the complexity of the latent representation using an information bottleneck inspired regularizer. We then reconstruct the denoised image from this compressed latent space. Experimental results demonstrate that our method achieves competitive performance compared to state-of-the-art zero-shot denoising techniques across various noise levels and image datasets, without requiring any pre-training or clean image examples. This work provides a new perspective on denoising by connecting it to neural compression, opening avenues for developing more robust and generalizable denoising algorithms."
http://arxiv.org/abs/2506.12456v2,Demographics-Informed Neural Network for Multi-Modal Spatiotemporal forecasting of Urban Growth and Travel Patterns Using Satellite Imagery,"Urban growth and travel patterns are intrinsically linked, impacting resource allocation, infrastructure planning, and overall quality of life in cities. Accurate forecasting of these dynamics is crucial, yet remains challenging due to complex interdependencies and the influence of various socio-economic factors. This paper addresses the limitations of existing spatiotemporal forecasting models by incorporating demographic information into a multi-modal neural network architecture for predicting urban growth and travel demand. Our proposed Demographics-Informed Neural Network (DINN) leverages satellite imagery, historical travel data, and granular demographic features, processed through separate convolutional and recurrent modules, then fused using an attention mechanism to capture the nuanced relationships between these modalities. We evaluate DINN on a large-scale dataset encompassing multiple metropolitan areas, demonstrating significant improvements in prediction accuracy compared to state-of-the-art baselines, particularly in regions experiencing rapid demographic shifts. The integration of demographic insights offers a more comprehensive and accurate understanding of urban dynamics, enabling proactive and equitable urban planning strategies."
http://arxiv.org/abs/2506.12007v1,SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution Shifts,"Neural surrogates, trained to predict the outcome of computationally expensive processes, offer a promising avenue for rapid design space exploration. However, their performance often degrades significantly when deployed on data exhibiting distribution shifts relative to the training set. This paper introduces SIMSHIFT, a novel benchmark designed to systematically evaluate the robustness of neural surrogates under various types of distribution shifts encountered in scientific and engineering applications. SIMSHIFT comprises three distinct tasks: airfoil drag prediction, molecular property prediction, and computational fluid dynamics simulation, each with carefully curated datasets exhibiting controlled shifts in input features and underlying physical parameters. We evaluate several state-of-the-art domain adaptation techniques on SIMSHIFT, demonstrating that existing methods struggle to effectively mitigate performance degradation across different shift types and tasks. Our analysis reveals the critical need for developing surrogate models that are intrinsically robust to distribution shifts, facilitating reliable and efficient design optimization in real-world applications."
http://arxiv.org/abs/2506.11574v1,Camera-based method for the detection of lifted truck axles using convolutional neural networks,"Commercial vehicles often utilize liftable axles to improve fuel efficiency and maneuverability when operating under light load conditions. However, accurately determining the axle configuration of a moving truck presents a challenge for automated tolling, weight enforcement, and traffic monitoring systems. This paper addresses the problem of automatically detecting lifted axles on trucks using a camera-based system. We propose a novel approach that leverages convolutional neural networks (CNNs) to analyze images of truck axles captured from roadside cameras. Specifically, we train a custom CNN architecture, incorporating attention mechanisms, to classify individual axles as either deployed or lifted. The network is trained on a large, diverse dataset of truck images annotated with axle state information. Our experimental results demonstrate a high accuracy in axle state classification, achieving an average precision of 96% on a held-out test set, even under varying lighting and weather conditions. This camera-based method offers a cost-effective and non-intrusive solution for accurately identifying truck axle configurations, enabling improved efficiency and fairness in transportation management systems."
http://arxiv.org/abs/2506.10463v1,Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization,"Neural network quantization is an essential technique for deploying deep learning models on resource-constrained devices, but often suffers from significant accuracy degradation. This paper addresses the problem of sensitivity to weight initialization in quantized neural networks, where standard initialization schemes designed for full-precision models can lead to suboptimal performance after quantization. We propose a novel weight initialization strategy, termed ""Quantization-Aware Initialization"" (QAI), that leverages the statistics of the quantization process to guide the initial weight distribution. QAI approximates the expected distribution of quantized weights by incorporating the quantization range and number of quantization levels into the initialization function. Empirical evaluations on various image classification benchmarks, including CIFAR-10 and ImageNet, demonstrate that QAI consistently improves the accuracy of quantized models compared to standard initialization methods like Kaiming and Xavier, particularly at lower bitwidths (e.g., 4-bit and 8-bit). These results highlight the importance of considering quantization during the initialization phase and offer a simple yet effective technique for improving the performance of quantized neural networks."
http://arxiv.org/abs/2506.10407v1,Semi-Tensor-Product Based Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) have achieved remarkable success in diverse computer vision tasks, owing to their ability to learn hierarchical representations from data. However, traditional CNNs typically operate on data represented as tensors with fixed dimensions, limiting their flexibility in handling data with varying structures and complexities. This paper addresses the challenge of adapting CNNs to process tensor data with potentially inconsistent or variable dimensions, thereby enabling more robust and efficient feature extraction. We propose Semi-Tensor-Product based Convolutional Neural Networks (STP-CNNs), which leverage the semi-tensor product (STP) to unify tensor multiplications and convolutions across different tensor dimensions. STP-CNNs replace standard convolutional layers with STP-based convolutional layers, allowing for direct processing of tensors with different sizes and modalities. Experimental results on benchmark datasets demonstrate that STP-CNNs achieve competitive or superior performance compared to traditional CNNs, particularly when dealing with data exhibiting dimensional heterogeneity. The proposed STP-CNN framework offers a novel and versatile approach to enhance the adaptability and performance of CNNs in various computer vision applications."
http://arxiv.org/abs/2506.09949v1,Sampling Theory for Super-Resolution with Implicit Neural Representations,"Implicit Neural Representations (INRs) have shown promise in representing continuous signals, leading to impressive performance in super-resolution tasks. However, a comprehensive theoretical understanding of how sampling density affects the performance of INR-based super-resolution remains limited. This paper addresses the question: given a target super-resolution factor, what is the minimum sampling density required to accurately reconstruct high-resolution images using INRs? We develop a novel sampling theory for INR-based super-resolution by analyzing the spectral properties of the learned representation and establishing a connection between the required sampling rate and the network architecture. Our analysis reveals that the effective bandwidth of the INR is directly related to the network's depth and width, and that violating a derived Nyquist-like sampling condition leads to aliasing artifacts in the reconstructed image. Experiments on benchmark datasets demonstrate that our theoretical predictions accurately reflect the empirical performance of INR-based super-resolution, providing guidance for selecting appropriate sampling densities. This work provides a theoretical foundation for INR-based super-resolution, enabling more efficient and principled design of sampling strategies and network architectures."
http://arxiv.org/abs/2506.09695v2,Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model,"Alzheimer's Disease (AD) poses a significant global health challenge, demanding accurate and early diagnosis for effective intervention. Existing deep learning approaches for AD diagnosis often suffer from high computational costs and limited interpretability, hindering their practical deployment in resource-constrained environments. This paper addresses the need for a lightweight and interpretable model by proposing a novel Spiking Neural Network (SNN) architecture for AD diagnosis based on structural MRI data. Our approach leverages a convolutional SNN (ConvSNN) optimized for efficient feature extraction, followed by a temporal encoding scheme that captures the dynamic progression of AD-related biomarkers. Furthermore, we introduce a layer-wise relevance propagation (LRP) technique tailored for SNNs, enabling the identification of salient brain regions contributing to the diagnostic decision. Experimental results on the ADNI dataset demonstrate that our model achieves comparable accuracy to state-of-the-art deep learning methods while significantly reducing computational complexity and providing valuable insights into the underlying neuropathology. This lightweight and interpretable SNN offers a promising avenue for practical and explainable AD diagnosis, facilitating earlier and more targeted clinical interventions."
http://arxiv.org/abs/2506.09668v1,CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain,"Accurate spatio-temporal modeling of the perinatal brain is crucial for understanding typical development and identifying early biomarkers of neurological disorders. Existing methods often struggle to capture the complex, multi-modal dynamics of this period due to limitations in registration accuracy and the curse of dimensionality when handling longitudinal data. We introduce CINeMA, a Conditional Implicit Neural Multi-Modal Atlas, which learns a continuous spatio-temporal representation of the perinatal brain from multi-modal MRI data. CINeMA leverages a conditional implicit neural representation conditioned on gestational age to model the evolving brain anatomy and contrasts across modalities. A diffeomorphic registration module ensures accurate alignment of input scans, enabling the atlas to learn a smooth and consistent trajectory of brain development. We demonstrate that CINeMA achieves state-of-the-art performance in reconstruction accuracy and cross-modal synthesis compared to existing atlas-based and deep learning methods on a large cohort of perinatal brain MRI scans. This framework provides a powerful tool for characterizing normative brain development and detecting deviations indicative of neurological risk."
http://arxiv.org/abs/2506.11146v1,HQFNN: A Compact Quantum-Fuzzy Neural Network for Accurate Image Classification,"Quantum machine learning (QML) holds the promise of exponential speedups for certain computational tasks, while fuzzy logic offers robust handling of uncertainty. However, realizing the full potential of QML often requires significant quantum resources, and integrating it effectively with fuzzy systems remains a challenge. This paper addresses the need for a compact and accurate QML model suitable for image classification, particularly in resource-constrained environments. We propose HQFNN, a hybrid quantum-fuzzy neural network that leverages a parameterized quantum circuit (PQC) for feature extraction and a fuzzy inference system for classification. The PQC is designed to minimize qubit requirements while maximizing feature discriminability, and the fuzzy system allows for flexible decision boundaries based on learned membership functions. Experimental results on benchmark image datasets demonstrate that HQFNN achieves competitive classification accuracy compared to classical and other hybrid models, while requiring significantly fewer qubits than other quantum-based approaches. HQFNN offers a promising pathway towards practical and efficient quantum-enhanced image classification."
http://arxiv.org/abs/2506.11139v1,Grids Often Outperform Implicit Neural Representations,"Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals and shapes, offering continuous and memory-efficient representations. However, their training can be computationally expensive and their performance, particularly in high-frequency regions, is often limited by spectral bias. This paper investigates the performance gap between INRs and traditional grid-based representations, questioning the assumption that INRs invariably offer superior reconstruction quality and compactness. We propose a carefully controlled comparative study, focusing on tasks such as image and shape representation, and introduce a modified grid-based approach incorporating learnable interpolation kernels to enhance its representational capacity. Our experiments demonstrate that, contrary to common belief, optimized grid-based methods frequently achieve comparable or superior reconstruction quality to INRs, while maintaining competitive memory footprints and significantly faster training times. These findings highlight the enduring effectiveness of grid-based methods and suggest that the benefits of INRs may be overstated in certain applications, prompting a re-evaluation of their widespread adoption."
http://arxiv.org/abs/2506.09100v1,Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction,"Quantitative MRI (qMRI) provides valuable biomarkers for characterizing tissue properties, but its high dimensionality and long acquisition times pose significant challenges for clinical translation. Existing reconstruction methods often rely on supervised learning or strong regularization, limiting their applicability in scenarios with limited training data or complex anatomical structures. We address the problem of unsupervised high-dimensional qMRI reconstruction by introducing a novel Low-Rank Augmented Implicit Neural Representation (LRA-INR). Our approach represents the high-dimensional qMRI volume as a continuous function parameterized by a neural network, augmented with a low-rank decomposition to capture global correlations and reduce the network's learning burden. Furthermore, we incorporate self-supervised regularization terms based on data consistency and anatomical priors directly into the loss function, enabling reconstruction without ground truth data. Experiments on simulated and in-vivo qMRI data demonstrate that LRA-INR achieves superior reconstruction accuracy and noise reduction compared to state-of-the-art unsupervised methods, particularly in scenarios with high undersampling. This enables faster and more robust qMRI acquisition and processing, facilitating broader clinical applications."
http://arxiv.org/abs/2506.08619v1,A Probability-guided Sampler for Neural Implicit Surface Rendering,"Neural implicit surfaces have emerged as a powerful representation for high-quality 3D reconstruction and novel view synthesis. However, efficient and accurate rendering of these surfaces remains challenging due to the need for dense and uniform sampling along camera rays, often leading to redundant computations in empty space and under-sampling near complex geometric details. We address this inefficiency by introducing a probability-guided sampler that adaptively adjusts the sampling density based on the estimated probability of intersecting the implicit surface. Our method leverages a lightweight neural network to predict a per-ray probability distribution, informed by learned geometric priors and view-dependent effects. This distribution is then used to guide the sampling process, concentrating samples near the predicted surface location while reducing unnecessary computations elsewhere. Experiments on benchmark datasets demonstrate that our approach significantly reduces the number of samples required for high-quality rendering, leading to substantial improvements in rendering speed and memory efficiency compared to state-of-the-art methods, without sacrificing accuracy. This efficient sampling strategy unlocks the potential for real-time rendering and large-scale scene reconstruction using neural implicit surfaces."
http://arxiv.org/abs/2506.08562v1,Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection,"Class Incremental Object Detection (CIOD) aims to learn new object categories without forgetting previously learned ones, a challenging task due to catastrophic forgetting and imbalanced learning. A major impediment to mitigating forgetting in CIOD is the phenomenon of neural collapse, where feature representations of different classes converge, particularly within the classifier. This paper introduces a novel Hierarchical Neural Collapse Detection Transformer (H-NCDT) designed to explicitly detect and alleviate neural collapse at different levels of feature abstraction in CIOD. Our H-NCDT employs a transformer-based architecture to analyze the similarity structure of feature representations within and across classes at both global image and local object levels. By identifying collapsing features, we introduce a targeted regularization strategy that encourages feature diversity and separation, thereby preserving previously learned knowledge while effectively learning new classes. Experimental results on benchmark CIOD datasets demonstrate that our H-NCDT significantly outperforms existing state-of-the-art methods in terms of both novel and base class performance, reducing forgetting and improving overall detection accuracy. This work provides a new perspective on addressing catastrophic forgetting in CIOD by explicitly detecting and mitigating neural collapse, paving the way for more robust and scalable incremental object detection systems."
http://arxiv.org/abs/2506.08183v1,A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation,"Accurate measurement of rodent eye movements is crucial for studying visual perception, neurological disorders, and drug efficacy. Existing methods often suffer from limitations in precision, robustness, and automation, especially when dealing with noisy or low-resolution video data. This paper introduces a novel system for accurate tracking and video recording of rodent eye movements that leverages convolutional neural networks (CNNs) for biomedical image segmentation. Our system employs a custom-designed miniature head-mounted camera and infrared illumination to capture high-resolution videos of the rodent eye. A U-Net based CNN is trained to segment the pupil and corneal reflection from each video frame, enabling precise calculation of eye position and movement. We demonstrate that our CNN-based segmentation achieves significantly higher accuracy compared to traditional thresholding and ellipse fitting methods, with a mean Intersection over Union (IoU) of 0.92 and a reduction in tracking error by 40%. This automated and accurate system provides a valuable tool for researchers investigating the neural mechanisms underlying visual behavior and developing new therapies for vision-related disorders."
http://arxiv.org/abs/2506.08163v2,SpINRv2: Implicit Neural Representation for Passband FMCW Radars,"Frequency-Modulated Continuous-Wave (FMCW) radars are crucial for various applications, including autonomous driving and robotics, due to their ability to provide accurate range and velocity information. However, traditional FMCW radar processing often results in sparse and noisy point clouds, limiting their direct applicability in complex scene understanding tasks. This paper addresses the challenge of reconstructing high-resolution, continuous representations from passband FMCW radar data, effectively overcoming the limitations imposed by discrete signal processing techniques. We introduce SPINRv2, an Implicit Neural Representation (INR) tailored for passband FMCW radar signals. SPINRv2 leverages a multi-layer perceptron (MLP) network to learn a continuous mapping from spatial coordinates to radar signal amplitudes, conditioned on the radars sweep parameters. To enhance performance, we incorporate a novel spectral regularization term during training, encouraging the network to learn physically plausible signal characteristics. Experimental results on both simulated and real-world radar datasets demonstrate that SPINRv2 significantly outperforms existing methods in terms of reconstruction quality and noise reduction, achieving state-of-the-art performance in representing complex radar scenes. SPINRv2 offers a powerful framework for leveraging the full potential of FMCW radar data, paving the way for improved perception and decision-making in various radar-based applications."
http://arxiv.org/abs/2506.07932v1,Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor,"Recent advances in 3D generative models have shown remarkable capabilities in synthesizing complex and realistic 3D shapes. However, the inherent high dimensionality of 3D data leads to models with large parameter spaces, raising questions about their true generative capacity versus their ability to simply memorize and compress the training data. This paper investigates the hypothesis that many current 3D generative models, particularly those based on implicit neural representations, are effectively acting as extreme neural compressors, achieving high fidelity reconstruction through memorization rather than genuine novel synthesis. We introduce Squeeze3D, a framework to systematically analyze the compression capabilities of 3D generative models by measuring their information bottleneck. Squeeze3D involves progressively reducing the latent space dimensionality and observing the corresponding drop in reconstruction quality, enabling us to quantify the trade-off between compression and fidelity. Our experiments on various state-of-the-art 3D generative models demonstrate that they exhibit surprisingly high compression ratios while maintaining acceptable reconstruction accuracy, suggesting a significant portion of their parameters are dedicated to memorization rather than learning a true generative distribution. This finding highlights the need for novel evaluation metrics and architectural designs that explicitly encourage generalization and discourage memorization in 3D generative modeling, ultimately paving the way for models with improved generative capabilities."
http://arxiv.org/abs/2506.07737v2,SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding,"Monocular 3D object detection is crucial for autonomous navigation and scene understanding, yet it remains challenging due to the inherent ambiguity of projecting 3D information onto a 2D image. Existing deep learning methods often rely on computationally expensive floating-point operations and struggle with energy efficiency. To address these limitations, we propose SpikeSMOKE, a novel spiking neural network (SNN) architecture for monocular 3D object detection. SpikeSMOKE incorporates a cross-scale gated coding mechanism that selectively integrates multi-scale features extracted by convolutional SNN layers, enhancing the network's ability to perceive objects at varying distances and occlusions. Furthermore, we introduce a temporal encoding scheme optimized for representing depth information within the spike domain, enabling more accurate 3D bounding box regression. Experimental results on the KITTI dataset demonstrate that SpikeSMOKE achieves competitive 3D detection accuracy compared to state-of-the-art deep learning methods, while exhibiting significantly reduced energy consumption due to the event-driven nature of SNNs. This work paves the way for deploying efficient and accurate 3D perception systems on resource-constrained platforms."
http://arxiv.org/abs/2506.07735v1,Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning,"Neural Architecture Search (NAS) has achieved remarkable progress in automating the design of deep learning architectures. However, effectively representing neural architectures remains a crucial challenge for efficient and generalizable NAS. This paper addresses the problem of learning expressive and informative representations of neural architectures by exploring a novel combination of language embedding techniques and dynamic graph structures. We propose a Language-Embedded Dynamic Graph Neural Network (LED-GNN) that first leverages a pre-trained language model to encode the textual descriptions of architectural components into a semantic embedding space. These embeddings are then used as node features in a dynamic graph, where graph structure and node features are iteratively refined through a message-passing mechanism conditioned on the architecture's performance feedback. Experiments on established NAS benchmarks, including NAS-Bench-201 and DARTS search space, demonstrate that LED-GNN achieves superior performance compared to state-of-the-art architecture representation learning methods, particularly in zero-shot NAS scenarios and few-shot architecture ranking. This work highlights the potential of integrating natural language understanding with graph-based representation learning for more effective neural architecture search and understanding."
http://arxiv.org/abs/2506.07720v1,ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks,"Spiking Neural Networks (SNNs) offer the potential for ultra-low power computation by leveraging event-driven processing. However, the energy efficiency of SNNs is often limited by the energy cost associated with synaptic operations and memory access, especially when using high-precision weights and activations. This paper addresses the challenge of reducing the computational cost and memory footprint of SNNs without significantly compromising accuracy. We introduce ReverB-SNN, a novel approach that reverses a single bit in both the weights and activations during inference. Specifically, we focus on reversing the least significant bit (LSB) to introduce controlled noise, effectively functioning as a stochastic regularization technique. We demonstrate that, surprisingly, this seemingly detrimental operation can improve generalization and robustness, while simultaneously enabling aggressive quantization and reducing synaptic operations. Our experiments on benchmark datasets such as MNIST, Fashion-MNIST, and CIFAR-10 show that ReverB-SNN achieves comparable or even superior accuracy to conventional SNNs with full-precision weights and activations, particularly under low-bit quantization. This work offers a simple yet effective strategy for optimizing SNNs for efficient hardware deployment by leveraging the benefits of bit reversal, opening new avenues for energy-efficient AI."
http://arxiv.org/abs/2506.07709v1,Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding,"Video compression efficiency is crucial for efficient storage and transmission of visual data. Traditional video codecs often struggle to effectively capture complex motion and subtle temporal dependencies, particularly in B-frames. This paper addresses the challenge of improving B-frame coding efficiency in neural video codecs by introducing a novel approach for fine-grained motion compression and selective temporal fusion. Our method decomposes motion vectors into multiple frequency bands and selectively compresses each band based on its perceptual importance, achieving higher compression ratios without significant visual degradation. Furthermore, we propose a selective temporal fusion module that adaptively weights information from past and future reference frames based on local motion characteristics, allowing for more accurate frame reconstruction. Experimental results on standard video datasets demonstrate that our approach achieves significant bitrate savings compared to state-of-the-art neural video codecs, while maintaining comparable or superior perceptual quality. This work provides a promising direction for developing more efficient and visually pleasing neural video coding solutions."
http://arxiv.org/abs/2506.07188v1,Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks,"Post-training neural network modification aims to adapt pre-trained models to new tasks or datasets with minimal computational overhead. However, existing methods often struggle to effectively leverage the rich feature representations learned during pre-training, leading to suboptimal performance when adapting to significantly different downstream tasks. This paper addresses the challenge of efficiently propagating task-specific information back through the pre-trained feature hierarchy to refine and adapt these features. We introduce Hierarchical Feature-level Reverse Propagation (HF-RP), a novel approach that injects task-specific gradients directly into intermediate feature maps of the pre-trained network during a lightweight fine-tuning stage. HF-RP utilizes a trainable adapter module at each layer to modulate the pre-trained features with the reverse-propagated gradients, allowing for targeted adaptation of feature representations at different levels of abstraction. Experiments on various image classification benchmarks demonstrate that HF-RP consistently outperforms state-of-the-art post-training adaptation methods, achieving significant accuracy gains while maintaining computational efficiency. The proposed method provides a promising avenue for rapidly deploying pre-trained models to new tasks with enhanced performance by effectively harnessing the power of hierarchical feature adaptation."
http://arxiv.org/abs/2506.07069v1,Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization,"3D Gaussian Splatting (3D-GS) has emerged as a leading technique for novel view synthesis, offering state-of-the-art rendering quality and competitive training times. However, real-time rendering of 3D-GS scenes, especially for high-resolution outputs or complex scenes, remains computationally intensive due to the costly sorting and rasterization steps required to determine visibility and project Gaussians onto the image plane. This paper introduces a novel approach to accelerate 3D-GS rendering by leveraging neural sorting and axis-oriented rasterization. Our method employs a lightweight neural network to predict a coarse depth ordering of Gaussians, enabling efficient bucket sorting and significantly reducing the number of pairwise depth comparisons. Furthermore, we introduce a custom rasterizer that exploits axis-aligned bounding boxes for Gaussians, minimizing redundant pixel shading calculations. Experimental results demonstrate that our approach achieves a substantial speedup of up to 2x compared to existing state-of-the-art rasterizers, while maintaining comparable rendering quality across various benchmark datasets. This acceleration enables real-time rendering of high-fidelity 3D-GS scenes on consumer-grade hardware, paving the way for more interactive and immersive experiences."
http://arxiv.org/abs/2506.06780v1,Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations,"Predicting the future evolution of rotations in 3D space, represented by the Special Orthogonal group SO(3), is crucial in various applications, including robotics, autonomous navigation, and computer animation. Existing methods often struggle to accurately capture the continuous-time dynamics of SO(3) data, particularly when dealing with noisy or irregularly sampled observations. This paper introduces Savitzky-Golay Neural Controlled Differential Equations (SG-NCDEs) for continuous-time SO(3) forecasting. Our approach leverages Neural CDEs to model the latent dynamics of SO(3) trajectories, while incorporating a Savitzky-Golay filter within the neural network architecture to denoise the input signal and estimate higher-order derivatives. This allows the model to learn smoother and more accurate representations of the underlying continuous-time process. We demonstrate superior forecasting performance compared to state-of-the-art methods on both synthetic and real-world SO(3) datasets, particularly in scenarios with high noise levels and irregular sampling. The proposed SG-NCDE framework provides a robust and accurate approach for continuous-time SO(3) forecasting, enabling improved performance in downstream applications that rely on accurate rotation prediction."
http://arxiv.org/abs/2506.08043v1,Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers,"Soft tissue manipulation with multiple robotic graspers is crucial for various surgical and industrial applications, demanding accurate and real-time deformation modeling. Existing methods often struggle to balance computational efficiency and realistic representation of complex material properties, particularly under multiple contact constraints. This paper introduces the Neural-Augmented Kelvinlet (NAK), a novel framework for real-time soft tissue deformation that combines the physical interpretability of Kelvinlet models with the learning capacity of neural networks. NAK employs a pre-computed Kelvinlet basis to represent the linear elastic response of the tissue, while a lightweight neural network learns to correct for non-linearities and complex boundary conditions arising from multiple graspers. This allows for efficient computation by leveraging linear superposition within the Kelvinlet space, with neural correction providing high-fidelity deformation approximations. Experiments demonstrate that NAK achieves significantly improved accuracy compared to traditional Kelvinlet methods and faster computation times than finite element methods, while accurately modeling complex deformations under multiple grasping forces. The proposed framework offers a practical solution for real-time soft tissue manipulation in robotic surgery and automation."
http://arxiv.org/abs/2506.06271v1,BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading,"Creating photorealistic and relightable digital human avatars remains a significant challenge in computer vision and graphics. Existing methods often struggle to disentangle geometry, albedo, and lighting, leading to artifacts under novel illumination. We address this problem by introducing BecomingLit, a novel approach for generating relightable Gaussian avatars with hybrid neural shading. Our method leverages 3D Gaussian Splatting for efficient and high-quality rendering, augmented with a hybrid shading model that combines a physically-based BRDF with a learned neural shading component. Specifically, we predict per-Gaussian BRDF parameters and a neural residual that captures complex lighting effects and unmodeled geometry. This hybrid approach allows us to retain the interpretability and robustness of physically-based rendering while also incorporating the flexibility of neural networks to model intricate details. Experiments on synthetic and real-world datasets demonstrate that BecomingLit achieves state-of-the-art relighting accuracy and visual fidelity, surpassing existing neural rendering techniques. This advancement facilitates the creation of more realistic and controllable digital humans for applications in virtual reality, augmented reality, and telepresence."
http://arxiv.org/abs/2506.06412v1,NeurNCD: Novel Class Discovery via Implicit Neural Representation,"Novel Class Discovery (NCD) aims to identify unseen classes in unlabeled data using knowledge learned from labeled data. Existing NCD methods often rely on hand-crafted feature engineering or pre-defined clustering algorithms, which can be sub-optimal and lack adaptability to diverse datasets. This paper introduces NeurNCD, a novel approach to NCD leveraging Implicit Neural Representations (INRs). NeurNCD represents each instance as a continuous function learned by an INR, effectively encoding both the feature representation and the underlying data manifold. By optimizing a clustering objective directly in the INR parameter space, we encourage the emergence of distinct clusters corresponding to novel classes, guided by a contrastive loss that leverages information from the labeled data. Experimental results on benchmark datasets demonstrate that NeurNCD achieves state-of-the-art performance in NCD, outperforming existing methods by a significant margin in terms of clustering accuracy and NMI score. NeurNCD's ability to learn adaptable feature representations and directly optimize clustering within the INR parameter space provides a powerful and generalizable framework for discovering novel classes."
http://arxiv.org/abs/2506.05869v1,Loss Functions for Predictor-based Neural Architecture Search,"Neural Architecture Search (NAS) aims to automate the design of high-performing neural networks, often relying on expensive training and evaluation cycles. Predictor-based NAS mitigates this cost by training a surrogate model to predict the performance of architectures, guiding the search process. However, the choice of loss function used to train the predictor significantly impacts its accuracy and, consequently, the effectiveness of the NAS process. This paper investigates the influence of different loss functions on the performance of predictors in predictor-based NAS. We systematically evaluate various loss functions, including L1, L2, Huber, and ranking-based losses, focusing on their ability to accurately predict the relative performance of architectures across diverse search spaces. Our experiments on established NAS benchmarks demonstrate that ranking-based losses, particularly a novel margin-based ranking loss, consistently outperform traditional regression losses in terms of predictor accuracy and the quality of architectures discovered by the NAS algorithm. These results highlight the importance of aligning the predictor's training objective with the ultimate goal of accurately ranking architectures for efficient NAS. This work provides valuable insights for designing more effective and efficient predictor-based NAS algorithms."
http://arxiv.org/abs/2506.05679v1,Integer Binary-Range Alignment Neuron for Spiking Neural Networks,"Spiking Neural Networks (SNNs) offer potential advantages in energy efficiency due to their event-driven and sparse communication nature, particularly when implemented on neuromorphic hardware. However, the inherent non-differentiability of spike events poses significant challenges for training deep SNNs, often requiring complex approximation techniques or conversion from pre-trained Artificial Neural Networks (ANNs). This paper addresses the challenge of directly training deep SNNs with high accuracy and minimal computational overhead. We introduce the Integer Binary-Range Alignment Neuron (IBRA-Neuron), a novel spiking neuron model that aligns the integer membrane potential with a binary firing range. This alignment enforces a stricter constraint during backpropagation, enabling more stable and efficient training. Furthermore, the IBRA-Neuron utilizes integer-based operations for both forward and backward passes, eliminating the need for floating-point computations. Experiments on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet demonstrate that SNNs equipped with IBRA-Neurons achieve state-of-the-art accuracy compared to other directly trained SNNs, while significantly reducing computational complexity. The IBRA-Neuron provides a pathway towards deploying high-performance, energy-efficient SNNs on resource-constrained platforms."
http://arxiv.org/abs/2506.05633v3,Noninvasive precision modulation of high-level neural population activity via natural vision perturbations,"Understanding how specific neural populations encode and process complex visual information remains a central challenge in neuroscience. Current methods for manipulating neural activity often rely on invasive techniques, limiting their applicability and ethical considerations in human studies. This paper addresses the problem of non-invasively and precisely modulating high-level neural population activity related to object recognition and scene understanding. We propose a novel framework leveraging subtle, perceptually-informed perturbations of natural images, optimized using gradient-based methods against a deep neural network proxy of target neural populations. By iteratively adjusting image features within a constrained perceptual space, we generate stimuli that selectively enhance or suppress activity in targeted network layers, thereby inferring corresponding effects on hypothesized neural populations. We demonstrate that our method can effectively modulate high-level semantic representations, leading to predictable shifts in behavioral responses and providing insights into the causal role of specific neural populations in visual perception. This work introduces a powerful, non-invasive tool for investigating the neural basis of visual cognition and offers a promising avenue for developing targeted interventions for visual processing deficits."
http://arxiv.org/abs/2506.05488v1,Implicit Neural Representation for Video Restoration,"Video restoration aims to recover high-quality video frames from degraded observations suffering from blur, noise, and downsampling. Existing video restoration methods typically rely on explicit representations of video frames as discrete pixel grids, leading to computational bottlenecks and limitations in handling arbitrary resolutions and continuous temporal information. We propose a novel video restoration framework leveraging Implicit Neural Representations (INRs) to represent videos as continuous functions mapping spatio-temporal coordinates to RGB values. Our method utilizes a hypernetwork to generate the weights of a coordinate-based Multi-Layer Perceptron (MLP), conditioned on the degraded input video. By optimizing the MLP parameters to fit the degraded video while enforcing priors learned from high-quality video data, we can effectively restore high-resolution video frames. Experimental results on benchmark datasets demonstrate that our INR-based approach achieves competitive or superior performance compared to state-of-the-art explicit methods, particularly in scenarios involving large upscaling factors and complex degradations. This work highlights the potential of INRs for video restoration, offering a memory-efficient and resolution-agnostic alternative to traditional pixel-based representations."
http://arxiv.org/abs/2506.05487v1,A Neural Network Model of Spatial and Feature-Based Attention,"Attention mechanisms are crucial for enabling visual systems to focus computational resources on relevant image regions and features. Existing neural network models often treat spatial and feature-based attention as separate processes or rely on complex, hand-crafted architectures. This paper addresses the need for a unified and learnable model capable of simultaneously capturing both spatial and feature-based attentional biases in a computationally efficient manner. We propose a novel neural network architecture, the Spatial-Feature Attention Network (SFANet), which integrates spatial attention modules and feature attention modules within a recurrent framework. SFANet leverages convolutional LSTM layers to iteratively refine attention maps in both spatial and feature domains, allowing for dynamic adaptation based on the evolving representation of the input image. We demonstrate that SFANet achieves state-of-the-art performance on several benchmark datasets for image classification and object detection, outperforming existing attention models with fewer parameters and lower computational cost. The proposed SFANet offers a powerful and efficient approach to modeling attention, paving the way for more robust and interpretable vision systems."
http://arxiv.org/abs/2506.05347v1,Neural Inverse Rendering from Propagating Light,"Inverse rendering aims to recover scene properties such as geometry, material, and lighting from images. Traditional inverse rendering methods often struggle with complex light transport phenomena, particularly when light propagates through participating media, leading to inaccurate estimations. This paper addresses the challenge of neural inverse rendering in scenes with propagating light, where volumetric scattering and absorption significantly alter the appearance. We introduce a novel neural inverse rendering framework that leverages a differentiable rendering engine capable of simulating propagating light and a neural network to jointly optimize scene geometry, spatially varying BRDFs, and volumetric scattering parameters. Our method iteratively refines these parameters by minimizing the discrepancy between rendered and observed images using gradient descent. Experimental results on synthetic and real-world datasets demonstrate that our approach significantly improves the accuracy of recovered scene properties and produces photorealistic renderings compared to existing inverse rendering techniques, especially in scenes with complex light transport. This work paves the way for more accurate and robust scene understanding in challenging visual environments."
http://arxiv.org/abs/2506.05169v1,Through-the-Wall Radar Human Activity Recognition WITHOUT Using Neural Networks,"Through-the-wall radar human activity recognition (HAR) offers significant potential for applications in security, healthcare, and smart homes. However, many existing approaches rely on computationally expensive and data-hungry deep learning models, limiting their deployment in resource-constrained environments or scenarios with limited training data. This paper addresses the challenge of performing accurate through-the-wall radar HAR without relying on neural networks. We propose a novel method based on micro-Doppler signatures extracted from radar data, followed by a feature engineering pipeline that incorporates time-frequency analysis and statistical descriptors. These features are then fed into a Support Vector Machine (SVM) classifier, optimized using a grid search approach. Our experiments using a publicly available dataset demonstrate that the proposed method achieves competitive accuracy compared to deep learning-based approaches, while requiring significantly less computational resources and training data. These results highlight the potential of classical machine learning techniques for efficient and effective through-the-wall radar HAR."
http://arxiv.org/abs/2506.05011v1,UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting,"Unmanned Aerial Vehicles (UAVs) offer unique perspectives for capturing dynamic human activities in outdoor environments. However, generating novel views of these dynamic scenes from sparse UAV viewpoints remains a significant challenge due to complex occlusions, varying lighting conditions, and the inherent difficulty in modeling deformable human motion. We introduce UAV4D, a novel dynamic neural rendering framework that leverages 3D Gaussian Splatting to reconstruct and render high-fidelity, time-varying human-centric scenes captured by UAVs. Our method incorporates a deformation field learned through a lightweight neural network conditioned on time and Gaussian position, enabling accurate tracking of human movement and dynamic scene changes. Furthermore, we introduce a novel regularization term based on optical flow consistency to improve the robustness of deformation learning, especially in areas with sparse observations. Experiments on a newly collected UAV dataset demonstrate that UAV4D significantly outperforms existing dynamic neural rendering techniques, achieving state-of-the-art performance in terms of visual quality and reconstruction accuracy. This work opens new avenues for immersive visualization and analysis of human activities captured from aerial perspectives."
http://arxiv.org/abs/2506.04526v3,EECD-Net: Energy-Efficient Crack Detection with Spiking Neural Networks and Gated Attention,"Crack detection in infrastructure is crucial for safety and maintenance, but current deep learning methods often demand significant computational resources. This work addresses the need for energy-efficient crack detection by introducing EECD-Net, a novel Spiking Neural Network (SNN) architecture incorporating gated attention mechanisms. EECD-Net leverages the inherent energy efficiency of SNNs, processing information through sparse, event-driven spike trains. We propose a novel gated attention module integrated within the SNN layers to selectively emphasize relevant crack features and suppress noise, enhancing detection accuracy with minimal computational overhead. Experimental results on benchmark crack datasets demonstrate that EECD-Net achieves competitive crack detection performance compared to traditional deep learning models, while significantly reducing energy consumption by up to 60%. This makes EECD-Net a promising solution for real-time, resource-constrained crack detection applications, facilitating widespread deployment on edge devices."
http://arxiv.org/abs/2506.04121v1,A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks,"Medical image segmentation is a crucial task in computer-aided diagnosis, enabling precise delineation of anatomical structures and lesions for quantitative analysis and treatment planning. However, manual segmentation is time-consuming and prone to inter-observer variability, motivating the development of automated solutions. This paper addresses the challenge of robust and accurate medical image segmentation across diverse modalities and anatomical regions. We present a comprehensive study evaluating the performance of various deep neural network architectures, including U-Net, V-Net, and attention-based models, on a diverse dataset encompassing CT, MRI, and ultrasound images. We further investigate the impact of different loss functions, data augmentation techniques, and transfer learning strategies on segmentation accuracy and generalization ability. Our experimental results demonstrate that the attention-augmented U-Net architecture, combined with a hybrid loss function incorporating Dice loss and cross-entropy, achieves state-of-the-art performance on multiple benchmark datasets, significantly improving segmentation accuracy compared to baseline models. These findings highlight the potential of deep learning to revolutionize medical image analysis, paving the way for more efficient and reliable clinical workflows."
http://arxiv.org/abs/2506.03890v1,Identifying Alzheimer's Disease Prediction Strategies of Convolutional Neural Network Classifiers using R2* Maps and Spectral Clustering,"Alzheimer's Disease (AD) is a progressive neurodegenerative disorder, and early diagnosis is crucial for effective intervention. Convolutional Neural Networks (CNNs) have shown promise in predicting AD from neuroimaging data, but understanding the specific image features driving these predictions remains a challenge. This study addresses the problem of identifying and characterizing the prediction strategies employed by CNN classifiers trained on R2* maps derived from Quantitative Susceptibility Mapping (QSM) data to predict AD. We propose a method that utilizes layer-wise relevance propagation to generate relevance maps highlighting image regions influential in CNN decisions. These relevance maps are then analyzed using spectral clustering to identify distinct clusters of prediction patterns, representing different strategies employed by the CNN. We found that CNNs primarily rely on regions within the hippocampus, amygdala, and precuneus for AD prediction, and spectral clustering revealed distinct subgroups of CNNs that emphasized different combinations of these regions. This work provides a novel approach to interpretability in AD prediction using CNNs, offering insights into the neurobiological underpinnings of the learned models and potentially revealing novel biomarkers for AD."
http://arxiv.org/abs/2506.03571v1,DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network,"Object detection is a fundamental task in computer vision, often relying on bounding box regression for precise localization. However, accurately predicting bounding boxes for complex or occluded objects remains a challenge due to limitations in capturing contextual relationships. This paper introduces DiagNet, a novel object detection framework that leverages Graph Neural Networks (GNNs) to enforce diagonal constraints on the adjacency matrix, thereby improving the modeling of object part relationships. DiagNet constructs a graph where nodes represent object proposals and edges encode their relationships. The core innovation lies in a specialized GNN layer that penalizes deviations from a diagonal adjacency matrix, implicitly encouraging the network to learn representations where spatially adjacent proposals are strongly connected, and distant proposals are weakly connected. This diagonal constraint is enforced through a novel loss function that minimizes the difference between the learned adjacency matrix and a target diagonal matrix. Experiments on benchmark datasets like COCO demonstrate that DiagNet achieves significant improvements in Average Precision (AP), particularly for objects with complex shapes and occlusions, outperforming state-of-the-art GNN-based object detection methods. DiagNet offers a principled approach to incorporating spatial context into object detection, leading to more robust and accurate localization."
http://arxiv.org/abs/2506.03538v1,Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting,"Neural Radiance Fields (NeRFs) have demonstrated impressive capabilities in novel view synthesis, yet their performance degrades significantly when applied to unconstrained, real-world scenes with imperfect camera poses and lighting variations. This paper addresses the challenge of robust neural rendering in the wild by introducing Asymmetric Dual 3D Gaussian Splatting (AD-GS), a novel approach that enhances the representational power and optimization stability of 3D Gaussian Splatting (3D-GS) for challenging real-world scenarios. AD-GS employs two sets of 3D Gaussians with asymmetric properties: one representing the scene's surface and the other capturing volumetric effects and handling noisy geometry. These Gaussians are optimized jointly with a robust loss function designed to mitigate the impact of inaccurate camera poses and varying lighting conditions. Experimental results on challenging real-world datasets demonstrate that AD-GS significantly outperforms state-of-the-art NeRF and 3D-GS methods, achieving superior rendering quality and robustness to noisy camera poses and lighting variations. This work expands the applicability of neural rendering to more complex and unconstrained real-world environments."
http://arxiv.org/abs/2506.03461v1,RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels,"Few-shot image classification aims to recognize novel categories given only a few labeled examples, a task further complicated by the presence of noisy labels in real-world datasets. Current approaches often struggle to generalize from limited, noisy data, leading to suboptimal performance. To address this, we propose RoNFA, a Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels. RoNFA leverages neural fields to implicitly represent each class as a continuous function, enabling robust interpolation and extrapolation from sparse, noisy samples. Specifically, we introduce a noise-aware meta-learning strategy that learns to adapt the neural field representation to the characteristics of each task, effectively mitigating the impact of noisy labels during the meta-training and adaptation phases. Experimental results on benchmark few-shot datasets with synthetic and real-world noise demonstrate that RoNFA significantly outperforms state-of-the-art methods, achieving higher classification accuracy and robustness against noisy labels, especially in extremely low-shot scenarios. This work offers a promising direction for building more reliable and practical few-shot learning systems."
http://arxiv.org/abs/2506.03440v2,Geometric Visual Fusion Graph Neural Networks for Multi-Person Human-Object Interaction Recognition in Videos,"Recognizing Human-Object Interactions (HOI) in videos is crucial for scene understanding, yet remains challenging due to the complex spatio-temporal relationships between humans and objects, as well as the ambiguity arising from noisy visual observations. This paper addresses the problem of effectively modeling and reasoning about these complex relationships for accurate multi-person HOI recognition in video sequences. We propose Geometric Visual Fusion Graph Neural Networks (GVF-GNN), a novel approach that constructs dynamic scene graphs incorporating both visual features and geometric relationships between humans and objects. GVF-GNN leverages a multi-modal fusion module to integrate appearance and spatial information extracted from each frame, and then employs graph neural networks to propagate information across the graph, enabling reasoning about HOI context and resolving ambiguities. Experimental results on the VidHOI dataset demonstrate that GVF-GNN significantly outperforms existing state-of-the-art methods, achieving substantial improvements in both verb and HOI triplet prediction accuracy. This work highlights the importance of integrating geometric cues with visual features within a graph-based framework for robust and accurate HOI recognition in dynamic video scenes."
http://arxiv.org/abs/2506.03407v1,Multi-Spectral Gaussian Splatting with Neural Color Representation,"Gaussian Splatting (GS) has emerged as a powerful technique for novel view synthesis, offering real-time rendering and competitive image quality. However, existing GS methods primarily operate on RGB images, neglecting the valuable information present in multi-spectral data, which has the potential to significantly enhance scene understanding and material characterization. This paper addresses the challenge of extending GS to multi-spectral imaging by proposing a novel framework, Multi-Spectral Gaussian Splatting (MSGS), that leverages a neural color representation. Our approach replaces the traditional spherical harmonic color representation with a neural network that maps from spectral wavelengths to Gaussian opacity and color values. This allows us to efficiently model complex spectral reflectance properties and render images at arbitrary wavelengths. We demonstrate that MSGS significantly outperforms RGB-based GS and naive spectral extensions in terms of reconstruction accuracy and spectral fidelity on synthetic and real-world multi-spectral datasets. This work paves the way for applying GS to a broader range of applications, including remote sensing, material inspection, and scientific visualization."
http://arxiv.org/abs/2506.05391v2,Enhancing Neural Autoregressive Distribution Estimators for Image Reconstruction,"Neural Autoregressive Distribution Estimators (NADEs) have shown promise in modeling complex data distributions, offering a tractable likelihood for density estimation and generative tasks. However, their application to high-dimensional image reconstruction, particularly from corrupted or incomplete data, faces challenges due to the inherent computational complexity and limitations in capturing long-range dependencies. This paper addresses the problem of improving the reconstruction quality of NADE-based models for images. We propose a novel architecture, the Spatially-Aware Hierarchical NADE (SAH-NADE), which incorporates spatial context through learned convolutional kernels within each autoregressive step. Furthermore, we introduce a hierarchical structure that progressively refines the reconstruction at multiple scales, enabling the model to capture both local details and global image structure. Experimental results on standard image datasets demonstrate that SAH-NADE significantly outperforms existing NADE-based and other generative models in terms of reconstruction accuracy, measured by peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). The proposed method offers a substantial improvement in image reconstruction quality while maintaining the benefits of tractable likelihood estimation, paving the way for more effective applications in image inpainting, denoising, and super-resolution."
http://arxiv.org/abs/2506.03290v1,Learning Optical Flow Field via Neural Ordinary Differential Equation,"Optical flow estimation is a fundamental task in computer vision, crucial for understanding motion and scene dynamics in video sequences. Traditional optical flow methods often rely on iterative optimization techniques, which can be computationally expensive and prone to error accumulation. This paper addresses the challenge of efficiently and accurately estimating optical flow fields by leveraging the representational power of Neural Ordinary Differential Equations (ODEs). We propose a novel framework where the optical flow field is modeled as the solution to a continuous-time dynamical system governed by a learned neural ODE. This allows us to learn the underlying flow dynamics directly from the image sequence, enabling a more robust and temporally coherent estimation. Specifically, our model learns a velocity field that defines the evolution of pixels over time, and the optical flow is obtained by integrating this velocity field using an ODE solver. Experimental results on standard benchmarks such as Sintel and KITTI demonstrate that our approach achieves competitive accuracy while offering improved computational efficiency compared to traditional iterative methods and other deep learning-based approaches. This work highlights the potential of Neural ODEs for modeling and learning complex motion patterns in video, offering a promising direction for future research in optical flow estimation and related vision tasks."
http://arxiv.org/abs/2506.02895v1,VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction,"Accurate food volume estimation is crucial for dietary assessment and nutritional analysis, yet remains challenging due to the diverse shapes, textures, and occlusions present in food images. This paper addresses the problem of precise food volume estimation from a single RGB image, enhanced by textual descriptions of the food item. We introduce VolTex, a novel framework that integrates text-guided semantic segmentation with neural surface reconstruction to achieve accurate volume prediction. First, a contrastive language-image pre-trained model is fine-tuned to perform text-guided segmentation of the food item, allowing for precise identification even with variations in appearance. Subsequently, we employ a neural implicit surface reconstruction technique, conditioned on the segmentation mask and image features, to generate a 3D representation of the food. Finally, the volume is computed directly from the reconstructed 3D surface. Experimental results on a diverse food image dataset demonstrate that VolTex significantly outperforms state-of-the-art methods in terms of volume estimation accuracy, achieving a 15% reduction in relative error. This research offers a promising approach for automated and accessible dietary monitoring, with implications for personalized nutrition and public health."
http://arxiv.org/abs/2506.03224v1,OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data,"Accurate and timely carbon emission prediction is crucial for effective climate change mitigation strategies. However, current emission inventories often suffer from coarse spatial resolution and significant reporting delays. This paper addresses the challenge of predicting high-resolution carbon emissions using readily available open data sources. We introduce OpenCarbon, a novel cross-modality neural network leveraging contrastive learning to fuse information from satellite imagery, socioeconomic indicators, and land use data for enhanced emission prediction. OpenCarbon employs a dual-encoder architecture, trained with a contrastive loss function, to learn a shared embedding space between heterogeneous data modalities. This allows the model to effectively transfer knowledge from modalities rich in spatial information (e.g., satellite imagery) to modalities that provide contextual emission drivers (e.g., GDP). Experiments demonstrate that OpenCarbon significantly outperforms state-of-the-art regression models, achieving a 20% reduction in RMSE when predicting gridded CO2 emissions at a 1km resolution. This improved accuracy and reliance on open data facilitates more granular and up-to-date carbon emission monitoring, enabling better-informed policy decisions and targeted interventions."
http://arxiv.org/abs/2508.02671v1,Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable few-shot learning capabilities through prompt tuning, adapting pre-trained models to downstream tasks with minimal data. However, the performance of prompt tuning is often limited by the scarcity of labeled data and the inherent biases present in small datasets. This paper addresses the problem of improving prompt tuning performance in VLMs by leveraging information already present in raw, unlabeled image data. We introduce a novel internal augmentation strategy that generates pseudo-labels by applying a diverse set of image transformations to unlabeled images and feeding these transformed images, along with their corresponding transformed text prompts, back into the prompt tuning process. This internal augmentation effectively expands the training data and introduces variations that improve the robustness and generalization ability of the tuned prompts. Experiments on a range of few-shot image classification benchmarks demonstrate that our approach consistently outperforms existing prompt tuning methods, achieving significant gains in accuracy, particularly when labeled data is scarce. This work highlights the crucial role of raw data and its effective utilization in enhancing the performance of prompt tuning for VLMs."
http://arxiv.org/abs/2508.01225v1,Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models,"Vision-language pre-trained models (VLPMs) have demonstrated remarkable performance on various downstream tasks. However, their generalization ability often degrades significantly when faced with distribution shifts at test time, particularly in zero-shot settings. This paper addresses the challenge of improving test-time generalization of VLPMs by mitigating the impact of domain discrepancies. We propose a novel Multi-Cache Enhanced Prototype Learning (MCPL) framework that leverages cached prototypes to dynamically adapt the model to the target domain. MCPL maintains multiple caches, each representing a distinct aspect of the target distribution, and employs a prototype learning strategy to refine these caches using test samples. A dynamic aggregation mechanism then combines the information from these refined caches to generate enhanced prototypes, which are used to adapt the VLPM's classification layer. Extensive experiments on diverse benchmark datasets demonstrate that MCPL significantly improves the zero-shot performance of VLPMs under various distribution shifts, achieving state-of-the-art results. This approach offers a practical and effective solution for enhancing the robustness and generalization of VLPMs in real-world applications."
http://arxiv.org/abs/2508.00945v1,Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment,"Vision-language pre-training aims to learn joint representations by aligning visual and textual information, enabling robust performance on downstream multi-modal tasks. However, inconsistencies often arise due to misalignment between visual regions and corresponding textual entities, hindering effective cross-modal reasoning. To address this, we propose a novel Cross-Layer Regional Attention Alignment (CLRAA) method that explicitly optimizes vision-language consistency. CLRAA leverages attention maps extracted from multiple layers of both the vision and language encoders to identify and align semantically corresponding regions and words. A novel alignment loss function encourages the attention distributions across modalities to be mutually consistent, promoting a shared understanding of regional relationships. Experimental results on Visual Question Answering (VQA) and Image-Text Retrieval benchmarks demonstrate that CLRAA significantly improves performance, achieving state-of-the-art results on several datasets. These findings highlight the importance of fine-grained attention alignment for robust vision-language understanding and reasoning."
http://arxiv.org/abs/2507.23362v1,Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual and textual data, but their substantial computational demands hinder deployment in resource-constrained environments. This paper addresses the problem of reducing the computational cost and size of LVLMs without significantly sacrificing performance. We propose a novel layer pruning strategy, Short-LVLM, which leverages a learned importance score based on the singular value decomposition of attention weights to identify and remove redundant transformer layers within the vision and language encoders. Furthermore, we introduce a knowledge distillation technique that transfers knowledge from the original, full-sized LVLM to the pruned model, mitigating performance degradation caused by layer removal. Experiments on several benchmark datasets, including Visual Question Answering (VQA) and Visual Commonsense Reasoning (VCR), demonstrate that Short-LVLM can achieve up to 40% reduction in parameters and 30% speedup in inference time, with only a marginal drop in accuracy compared to the original LVLM. These results highlight the potential of Short-LVLM to enable efficient deployment of powerful LVLMs on edge devices and in other resource-limited settings."
http://arxiv.org/abs/2507.23042v1,Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving,"Vision-language navigation in driving scenarios requires sophisticated reasoning about both visual scene context and natural language instructions to safely and efficiently reach a specified goal. Existing approaches often rely on late fusion of visual and linguistic information, potentially missing crucial early interactions that could improve performance, particularly in time-sensitive driving situations. This paper addresses the challenge of integrating visual and linguistic cues at multiple scales early in the processing pipeline for real-time vision-language driven navigation. We propose a novel ""Early Goal-Guided Multi-Scale Fusion"" (EGM2F) architecture. EGM2F leverages a hierarchical convolutional neural network to extract multi-scale visual features, which are then fused with language embeddings at each scale using a goal-guided attention mechanism. This allows the model to selectively attend to relevant visual information based on the current instruction and the ultimate navigation goal. Experimental results on a simulated driving environment demonstrate that EGM2F significantly outperforms state-of-the-art late fusion methods, achieving a 15% improvement in task completion rate while maintaining real-time performance. This early multi-scale fusion strategy provides a more robust and efficient approach to vision-language navigation for autonomous driving."
http://arxiv.org/abs/2507.22805v2,MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention,"Vision-language reasoning demands intricate comprehension of both visual and textual modalities, posing a significant challenge for current models. This paper addresses the critical need for more effective cross-modal interaction and contextual understanding in vision-language tasks. We introduce MoCHA: a novel architecture for advanced vision-language reasoning that leverages a Mixture-of-Experts (MoE) connector and Hierarchical Group Attention. Specifically, MoCHA employs an MoE layer to dynamically route visual and textual features to specialized expert networks, facilitating nuanced cross-modal fusion. Furthermore, Hierarchical Group Attention captures both fine-grained and coarse-grained relationships within each modality and across modalities, enabling comprehensive contextual reasoning. We evaluate MoCHA on a range of challenging vision-language benchmarks, including VQA, NLVR2, and SNLI-VE, demonstrating significant performance improvements over state-of-the-art methods. These results highlight the efficacy of MoCHA in enhancing cross-modal interaction and contextual understanding, pushing the boundaries of vision-language reasoning capabilities."
http://arxiv.org/abs/2507.22264v1,SmartCLIP: Modular Vision-language Alignment with Identification Guarantees,"Vision-language pre-training (VLP) has demonstrated remarkable success in learning aligned representations across visual and textual modalities. However, existing approaches often treat VLP as a monolithic process, lacking modularity and failing to provide guarantees about the specific knowledge encoded within different components. This paper introduces SmartCLIP, a novel modular VLP framework that explicitly identifies and aligns distinct visual and textual concepts. Our method decomposes CLIP into learnable modules, each specializing in a specific semantic attribute, and leverages a novel loss function that enforces identification guarantees  ensuring each module learns a unique and interpretable concept. We further introduce a cross-modal attention mechanism that adaptively combines these modular representations for enhanced alignment. Experimental results on diverse downstream tasks, including image classification, retrieval, and visual reasoning, demonstrate that SmartCLIP achieves superior performance compared to standard CLIP and other VLP baselines, while also providing improved interpretability and control over the learned representations. The proposed modular architecture and identification guarantees offer a significant step towards more transparent and controllable vision-language models."
http://arxiv.org/abs/2507.22000v1,Staining and locking computer vision models without retraining,"Deep learning models are increasingly vulnerable to intellectual property theft, motivating research into model protection techniques. Existing methods typically involve complex retraining procedures or require specialized architectures, limiting their applicability to pre-trained, off-the-shelf models. This paper addresses the problem of protecting computer vision models against unauthorized use, specifically focusing on scenarios where retraining is impractical or impossible. We introduce a novel ""staining and locking"" framework that operates directly on a pre-trained model's weights. This involves subtly ""staining"" the model by injecting imperceptible, data-agnostic perturbations into specific weight layers, creating a unique fingerprint. Simultaneously, we implement a ""locking"" mechanism using a small, easily verifiable trigger set. Successful prediction on this trigger set unlocks the model's full functionality, while unauthorized users without the trigger set experience degraded performance. Experiments on ImageNet classification demonstrate that our method effectively protects models, achieving significant performance degradation for unauthorized users while maintaining near-original accuracy for authorized users with the trigger set. This approach offers a practical and efficient solution for protecting the intellectual property of pre-trained computer vision models without the burden of retraining."
http://arxiv.org/abs/2507.21794v1,Distribution-Based Masked Medical Vision-Language Model Using Structured Reports,"Medical vision-language pre-training has shown promise in learning joint representations from medical images and associated text, typically leveraging free-text radiology reports. However, these reports often contain unstructured and redundant information, hindering the model's ability to learn fine-grained correspondences. We address this by proposing a novel distribution-based masked medical vision-language model (D-MedVLM) that incorporates structured reports to guide pre-training. D-MedVLM utilizes a hierarchical transformer encoder to process structured reports, capturing relationships between different sections and findings. Furthermore, we introduce a distribution-based masking strategy, where image regions are masked based on the probability distribution derived from the structured report's findings, forcing the model to attend to clinically relevant areas. Experiments on downstream tasks, including report generation and visual question answering, demonstrate that D-MedVLM significantly outperforms existing state-of-the-art methods. This highlights the effectiveness of incorporating structured information and distribution-aware masking for improved medical vision-language understanding."
http://arxiv.org/abs/2507.21521v1,Optimizing Active Learning in Vision-Language Models via Parameter-Efficient Uncertainty Calibration,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in various multimodal tasks, yet their reliance on large labeled datasets remains a significant bottleneck. Active learning (AL) offers a promising solution by strategically selecting the most informative samples for annotation, but current AL strategies often struggle with accurately quantifying uncertainty in VLMs, leading to suboptimal sample selection. This paper addresses the challenge of optimizing AL for VLMs by introducing a novel parameter-efficient uncertainty calibration technique. Our method, termed PE-Uncertainty, leverages a small, learnable adapter module integrated within the VLM architecture to calibrate uncertainty estimates derived from the model's output probabilities. This adapter is trained specifically to improve the correlation between predicted probabilities and actual prediction correctness on a small held-out validation set. Experiments on diverse VLM tasks, including visual question answering and image captioning, demonstrate that PE-Uncertainty significantly outperforms existing AL strategies, achieving comparable performance with substantially fewer labeled samples. This work provides a practical and efficient approach to enhance AL for VLMs, reducing annotation costs and accelerating the development of robust multimodal systems."
http://arxiv.org/abs/2507.21450v1,Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation,"Vision-Language Navigation (VLN) tasks require agents to follow natural language instructions to navigate in photorealistic environments. A key challenge lies in the agent's ability to effectively ground linguistic instructions in the visual environment and plan paths based on incomplete observations. To address this, we propose a novel framework that incorporates Recursive Visual Imagination (RVI) and Adaptive Linguistic Grounding (ALG). RVI enables the agent to recursively imagine future states based on its current observation and planned actions, effectively expanding its understanding of the environment. Simultaneously, ALG dynamically adjusts the agent's attention to different parts of the instruction based on the imagined visual context, facilitating a more nuanced and context-aware grounding. Our experiments on the Room-to-Room (R2R) dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods, achieving a notable improvement in success rate and trajectory length. This work highlights the importance of visual imagination and adaptive grounding in enabling more robust and efficient navigation for VLN agents."
http://arxiv.org/abs/2507.21358v4,Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy,"Vision-based 3D object detection is crucial for autonomous driving and robotics, yet struggles with accurately perceiving objects in complex, cluttered scenes. Existing methods often lack robustness in handling varying object densities and spatial arrangements, leading to inaccurate bounding box predictions. To address this limitation, we introduce Collaborative Perceiver, a novel architecture that leverages local density-aware spatial occupancy to enhance 3D object detection. Collaborative Perceiver employs a multi-scale feature extraction module coupled with a density estimation network to infer the local point density surrounding each spatial location. This density information is then integrated into a cross-attention mechanism, guiding the model to focus on relevant features within dense regions while mitigating the influence of noisy or sparse points. Experiments on the nuScenes dataset demonstrate that Collaborative Perceiver achieves state-of-the-art performance, significantly improving detection accuracy, particularly for small and occluded objects. Our approach provides a robust and effective solution for 3D object detection in challenging environments by explicitly modeling and leveraging local spatial context."
http://arxiv.org/abs/2507.21335v1,Analyzing the Sensitivity of Vision Language Models in Visual Question Answering,"Vision-Language Models (VLMs) have demonstrated remarkable progress in Visual Question Answering (VQA), achieving high accuracy on various benchmark datasets. However, the robustness and reliability of these models remain a concern, particularly regarding their sensitivity to subtle input variations. This paper investigates the sensitivity of state-of-the-art VLMs in VQA to image and text perturbations. We propose a comprehensive sensitivity analysis framework encompassing various perturbation types, including image corruptions, adversarial image attacks, and paraphrased question variations. We evaluate the impact of these perturbations on the VQA accuracy of several popular VLMs. Our results reveal that VLMs exhibit significant sensitivity to even small image and text perturbations, leading to substantial performance degradation. Specifically, adversarial image attacks and paraphrased questions with altered semantic nuances severely impact the models' ability to provide correct answers. This highlights the need for developing more robust and reliable VLMs that are less susceptible to input variations, fostering trust and enabling their deployment in real-world applications."
http://arxiv.org/abs/2507.20994v1,Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM,"Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in understanding and generating content across modalities, but inheriting biases and safety vulnerabilities from their pre-trained language models remains a significant concern. Current text-aligned safety mechanisms are ill-equipped to handle the complexities of visual inputs, leaving LVLMs susceptible to generating harmful content based on visual cues. To address this, we introduce Security Tensors, a novel cross-modal bridge that extends text-aligned safety principles to the visual domain. Our approach leverages a pre-trained vision encoder to extract semantic embeddings from images, which are then transformed into ""security tensors"" using a learned mapping. These tensors are designed to mimic the structure and properties of text embeddings used in established text safety filters, allowing for direct application of these filters to visual inputs. Experiments demonstrate that incorporating Security Tensors significantly improves the safety of LVLMs in response to visual prompts, reducing the generation of harmful content by an average of 35% across various benchmark datasets while maintaining model utility. This work paves the way for more robust and reliable safety mechanisms in multi-modal AI systems by providing a unified framework for addressing security concerns across vision and language."
http://arxiv.org/abs/2507.20842v1,METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models,"Vision-Language Models (VLMs) have achieved remarkable success in various multimodal tasks, but their computational demands hinder deployment in resource-constrained environments. Existing token pruning methods often focus on unimodal or late-fusion pruning, neglecting the potential benefits of collaborative pruning across both vision and language encoders in early stages. This paper introduces METEOR, a novel Multi-Encoder Collaborative Token Pruning framework for efficient VLMs. METEOR leverages learnable Gumbel-Softmax gates to selectively prune tokens within both the image and text encoders, guided by a collaborative distillation loss that encourages alignment between the pruned and unpruned model representations. Furthermore, we introduce a cross-modal attention consistency loss to maintain the crucial interactions between modalities after pruning. Experiments on VQA, Image-Text Retrieval, and Visual Entailment demonstrate that METEOR achieves significant computational savings (up to 40% FLOPs reduction) with minimal performance degradation compared to state-of-the-art pruning techniques, highlighting its effectiveness in improving the efficiency of VLMs. METEOR offers a promising approach to deploying powerful VLMs on edge devices and in resource-limited settings."
http://arxiv.org/abs/2508.03721v1,Enhancing Diameter Measurement Accuracy in Machine Vision Applications,"Accurate diameter measurement is crucial in numerous industrial applications, including quality control, manufacturing automation, and object sorting. However, traditional machine vision techniques often suffer from inaccuracies due to factors such as image noise, edge detection limitations, and perspective distortion. This paper addresses the challenge of improving diameter measurement accuracy in machine vision systems operating under non-ideal conditions. We propose a novel approach combining sub-pixel edge detection with a robust ellipse fitting algorithm, incorporating a RANSAC-based outlier rejection strategy and a tailored weighting scheme based on edge gradient magnitude. This allows for precise ellipse fitting even with noisy or incomplete edge data. Experimental results on both synthetic and real-world datasets demonstrate a significant improvement in diameter measurement accuracy compared to existing methods, achieving a reduction in average error of up to 35% in challenging scenarios with significant noise and occlusion. This enhanced accuracy enables more reliable and efficient automation in various industrial processes."
http://arxiv.org/abs/2507.20630v1,TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and generating content based on both visual and textual inputs. However, their computational demands hinder deployment on resource-constrained devices. This paper addresses the problem of efficiently reducing the computational cost of LVLMs without significantly sacrificing performance. We introduce TransPrune, a novel token transition pruning method that leverages the inherent redundancy in token sequences within the transformer layers of LVLMs. TransPrune dynamically identifies and prunes tokens based on a learned transition score, which predicts the impact of removing a token on subsequent token representations. By iteratively pruning tokens and fine-tuning the model, TransPrune significantly reduces the number of tokens processed in each layer. Experiments on various vision-language tasks demonstrate that TransPrune achieves substantial reductions in FLOPs (up to 40%) with minimal performance degradation compared to baseline LVLMs. This enables the deployment of high-performing LVLMs in resource-limited environments."
http://arxiv.org/abs/2507.20519v1,AgroBench: Vision-Language Model Benchmark in Agriculture,"Vision-language models (VLMs) have shown remarkable progress in various domains, yet their performance in agriculture remains largely unexplored due to the lack of specialized datasets and benchmarks. This paper addresses the need for a comprehensive evaluation of VLMs in agricultural settings by introducing AgroBench, a novel benchmark designed to assess their capabilities in understanding and reasoning about visual and textual information related to crops, pests, diseases, and farming practices. AgroBench comprises a diverse set of tasks, including visual question answering, image captioning, and text-based image retrieval, utilizing a curated dataset of images and text descriptions sourced from agricultural databases and research publications. We evaluate several state-of-the-art VLMs on AgroBench, revealing significant performance gaps compared to their performance on general-purpose benchmarks, particularly in tasks requiring fine-grained understanding of agricultural concepts. Our analysis identifies key challenges for VLMs in this domain, such as the need for improved handling of domain-specific vocabulary and the ability to reason about complex relationships between visual and textual cues. AgroBench serves as a valuable resource for the community to develop and evaluate VLMs tailored for agriculture, ultimately contributing to the development of AI-powered solutions for sustainable farming practices."
http://arxiv.org/abs/2507.20188v1,SAViL-Det: Semantic-Aware Vision-Language Model for Multi-Script Text Detection,"Multi-script text detection in natural scenes poses a significant challenge due to the diverse character shapes, varying text orientations, and complex backgrounds. Existing vision-language models (VLMs) for text detection often lack explicit semantic awareness, hindering their ability to effectively discern text from non-text regions, especially when dealing with multiple scripts. We introduce SAViL-Det, a novel Semantic-Aware Vision-Language model for multi-script text detection. SAViL-Det integrates a semantic context encoder that leverages pre-trained language models to capture script-specific semantic information and incorporate it into the visual feature representations. This enhanced representation is then fed into a text detection module that predicts text regions and their corresponding scripts. Experiments on several multi-script text detection benchmarks demonstrate that SAViL-Det achieves state-of-the-art performance, outperforming existing methods by a significant margin in terms of both detection accuracy and script identification. SAViL-Det's robust performance highlights the importance of semantic awareness in VLMs for addressing the complexities of multi-script text detection, paving the way for more accurate and reliable text-based scene understanding."
http://arxiv.org/abs/2507.20174v1,"LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks","Vision-Language Models (VLMs) have achieved impressive performance on a variety of multimodal tasks, yet their ability to reason about spatial relationships remains largely unexplored. This paper addresses the critical gap in evaluating VLMs' understanding of fundamental spatial concepts like laterality (left vs. right) and orientation. We introduce LRR-Bench, a novel diagnostic benchmark specifically designed to assess VLMs' proficiency in distinguishing between left and right orientations, and recognizing rotated object instances in images given textual descriptions. LRR-Bench comprises three challenging tasks: Left-Right Discrimination, Rotation Recognition, and a combined Left-Right Rotation task, each requiring precise spatial reasoning. Our experiments with state-of-the-art VLMs, including CLIP, ALIGN, and BLIP, reveal surprisingly poor performance, significantly lagging behind human accuracy, particularly in tasks involving fine-grained rotation distinctions. These findings highlight a significant deficiency in current VLMs' spatial understanding capabilities, necessitating further research into architectures and training methodologies that can better capture and reason about spatial relationships for enhanced multimodal intelligence."
http://arxiv.org/abs/2507.19875v1,ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking,"Vision-language tracking (VLT) demands precise target localization based on linguistic cues within complex scenes. Existing VLT methods often struggle to maintain tracking accuracy under significant appearance variations, occlusions, or distractors due to their limited ability to dynamically adapt to target state changes and fully exploit the relationship between the target and its surrounding context. To address this, we propose ATCTrack, a novel VLT framework that explicitly aligns target-context cues with dynamic target states. ATCTrack employs a dynamic target state estimator to predict the target's evolving characteristics, which are then used to guide the alignment of target and context features extracted via a cross-modal attention mechanism. Furthermore, a contrastive learning objective is introduced to enhance the discriminative power of the aligned target-context representations. Extensive experiments on challenging VLT benchmarks, including LaSOT, TrackingNet, and VLT2000, demonstrate that ATCTrack achieves state-of-the-art performance, outperforming existing methods by a significant margin, particularly in scenarios with heavy occlusion and distractors. ATCTrack's ability to dynamically adapt to target state changes and effectively leverage target-context relationships significantly advances the robustness and accuracy of vision-language tracking."
http://arxiv.org/abs/2507.19451v3,GS-Occ3D: Scaling Vision-only Occupancy Reconstruction with Gaussian Splatting,"Occupancy reconstruction, which aims to predict the 3D space occupancy from multi-view images, is a crucial task for scene understanding in autonomous driving and robotics. However, current vision-only occupancy reconstruction methods struggle with scaling to large scenes due to the limitations of voxel-based or mesh-based representations. We propose GS-Occ3D, a novel framework that leverages 3D Gaussian Splatting (3D-GS) as an efficient and scalable representation for occupancy prediction. GS-Occ3D first predicts a set of 3D Gaussians from multi-view images, encoding occupancy information within their attributes. Then, a differentiable rendering process is employed to supervise the Gaussian attributes by comparing the rendered occupancy against pseudo-ground truth generated from depth maps. Experiments on the nuScenes dataset demonstrate that GS-Occ3D achieves state-of-the-art performance in vision-only occupancy prediction, surpassing existing methods by a significant margin, particularly in large-scale scenes, while maintaining competitive rendering speeds. This work paves the way for high-fidelity and efficient 3D scene understanding using Gaussian Splatting, enabling applications requiring real-time performance and large-scale scene handling."
http://arxiv.org/abs/2507.19360v1,EA-ViT: Efficient Adaptation for Elastic Vision Transformer,"Vision Transformers (ViTs) have demonstrated remarkable performance in various computer vision tasks, yet their computational demands often hinder deployment on resource-constrained devices. Adapting pre-trained ViTs to different resource budgets and application scenarios remains a challenge, typically requiring extensive fine-tuning for each configuration. To address this, we propose EA-ViT, an efficient adaptation framework for elastic ViTs that minimizes the need for full fine-tuning when adjusting model size or resolution. EA-ViT introduces Adaptive Patch Merging (APM) and Adaptive Layer Scaling (ALS) modules, which dynamically adjust the patch embedding and the number of transformer blocks, respectively, based on a lightweight adaptation network trained to predict optimal configurations given resource constraints. This adaptation network leverages knowledge distillation and parameter sharing across different configurations to ensure efficient knowledge transfer. Experiments on ImageNet classification demonstrate that EA-ViT achieves comparable or superior performance to fully fine-tuned elastic ViTs while significantly reducing the adaptation cost, requiring only a fraction of the training epochs and computational resources. EA-ViT offers a practical and efficient solution for deploying high-performing ViTs across a wide range of devices and applications with varying resource availability."
http://arxiv.org/abs/2507.19131v1,MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, but their high computational cost remains a significant barrier to deployment on resource-constrained devices. While activation sparsity is a promising avenue for accelerating ViTs, the irregular memory access patterns and limited hardware support often hinder practical speedups. This paper investigates the potential of activation sparsity from the perspective of mixed-precision quantization, aiming to bridge the gap between theoretical benefits and real-world performance. We introduce MixA-Q, a novel mixed-precision quantization-aware training scheme that encourages activation sparsity while mitigating the accuracy degradation typically associated with aggressive quantization. MixA-Q dynamically adjusts the quantization bit-width for each activation element based on its magnitude, effectively promoting sparsity by quantizing low-magnitude activations to zero. Extensive experiments on ImageNet classification demonstrate that MixA-Q achieves comparable or even superior accuracy to state-of-the-art ViT quantization methods, while simultaneously inducing significant activation sparsity (up to 70%) without specialized sparse hardware. This work provides a practical and hardware-friendly approach for accelerating and compressing ViTs by leveraging activation sparsity through mixed-precision quantization."
http://arxiv.org/abs/2507.19064v2,Negation-Aware Test-Time Adaptation for Vision-Language Models,"Vision-Language Models (VLMs) exhibit remarkable generalization capabilities, but their performance can degrade significantly when deployed in novel target domains with distribution shifts. Test-Time Adaptation (TTA) offers a promising solution by adapting models using only unlabeled test data, yet current methods often overlook the critical role of negation in complex visual reasoning. This work addresses the challenge of adapting VLMs to target domains while explicitly accounting for the impact of negation in vision-language interactions. We introduce a Negation-Aware Test-Time Adaptation (NATTA) framework that leverages a contrastive learning objective to align the model's understanding of positive and negative relationships in the target domain. Specifically, NATTA encourages the model to produce consistent predictions when negating either the image or the text input, thereby improving its robustness to variations in visual and semantic representations. Experiments on several challenging VQA and visual reasoning benchmarks demonstrate that NATTA consistently outperforms existing TTA methods, achieving significant accuracy gains, particularly in scenarios requiring nuanced understanding of negation. This highlights the importance of incorporating negation awareness into TTA strategies for robust and reliable VLM deployment."
http://arxiv.org/abs/2507.18517v1,Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs,"Vision-assisted neuro-prostheses hold immense potential for restoring autonomy to individuals with upper limb paralysis. However, robust and accurate object segmentation in unstructured, real-world environments remains a significant challenge for enabling reliable grasping and manipulation. This paper investigates the application of foundation models for object segmentation in the wild, specifically for use in vision-assisted neuro-prostheses. We leverage the Segment Anything Model (SAM) and propose a novel prompting strategy incorporating both visual cues derived from eye-tracking data, mimicking user intent, and learned embeddings from a lightweight object recognition network. This hybrid prompting approach guides SAM to segment the target object even in cluttered scenes with occlusions and varying lighting conditions. Experimental results on a newly collected dataset of daily living activities demonstrate a significant improvement in segmentation accuracy (mIoU of 68.3%) compared to using SAM with standard bounding box prompts or solely relying on eye-tracking information. These findings suggest that foundation models, when combined with targeted prompting strategies, offer a promising pathway towards more reliable and adaptable vision systems for neuro-prosthetic control."
http://arxiv.org/abs/2507.18311v1,Improving Large Vision-Language Models' Understanding for Field Data,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and reasoning about images and text, often trained on extensive internet-sourced data. However, their performance significantly degrades when applied to specialized field data, characterized by unique visual features and domain-specific language, such as those found in agricultural monitoring or industrial inspection. This paper addresses the problem of adapting LVLMs to effectively interpret and utilize field data, where pre-training distributions diverge significantly from the target application. We propose a novel adaptation strategy, Field-Aligned Vision-Language Tuning (FAVLT), which combines contrastive learning to align visual representations with domain-specific text descriptions and a masked language modeling objective to refine the language understanding capabilities within the target domain. Furthermore, FAVLT incorporates a data augmentation technique tailored to field data, enhancing robustness and generalization. Empirical results on a novel agricultural dataset demonstrate that FAVLT improves question answering accuracy by 15% and visual grounding precision by 10% compared to fine-tuning and existing state-of-the-art adaptation methods. This work offers a practical and effective approach for bridging the gap between general-purpose LVLMs and the demands of real-world field applications, enabling more reliable and accurate data analysis in specialized domains."
http://arxiv.org/abs/2507.18661v3,Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back,"Predicting the next GPS location of a moving agent is crucial for various applications, including autonomous navigation and proactive service delivery. Existing methods often rely on historical GPS trajectories and road network information, overlooking the rich visual cues readily available from onboard cameras. This paper addresses the problem of predicting the next GPS location solely from a continuous stream of visual data, mimicking how a human driver anticipates upcoming turns and routes based on what they see. We propose a novel reinforcement learning framework where an agent learns to navigate a visual map by sequentially predicting GPS coordinates. The agent receives visual input, predicts a displacement vector, and is rewarded based on the proximity of the predicted location to the actual next GPS point. Crucially, the agent receives feedback in the form of updated visual context based on its predicted location, enabling it to learn long-term dependencies and correct errors. Experiments on a challenging real-world driving dataset demonstrate that our method achieves competitive accuracy compared to trajectory-based methods, while relying solely on visual information. This work opens new avenues for vision-centric navigation, particularly in scenarios where GPS signals are unreliable or unavailable, paving the way for more robust and visually aware autonomous systems."
http://arxiv.org/abs/2507.17682v1,Audio-Vision Contrastive Learning for Phonological Class Recognition,"Audio-visual speech processing leverages the complementary information from both modalities to enhance speech understanding. However, learning robust audio-visual representations for fine-grained phonological distinctions, such as recognizing subtle differences between phoneme classes, remains a challenge due to the inherent complexity and variability in speech signals and visual articulatory movements. This paper addresses the problem of learning effective audio-visual representations specifically tailored for phonological class recognition. We propose a novel audio-vision contrastive learning framework that maximizes the mutual information between audio and visual embeddings while incorporating phonological class information as an auxiliary task. This is achieved through a multi-task loss function that encourages alignment between corresponding audio and visual features, and simultaneously minimizes the distance between audio-visual embeddings belonging to the same phonological class. Our experiments on benchmark audio-visual speech datasets demonstrate that our method significantly outperforms state-of-the-art audio-visual models in phonological class recognition accuracy, particularly for confusable phoneme categories. This improved performance highlights the effectiveness of contrastive learning for capturing fine-grained audio-visual correspondences relevant to phonological distinctions, paving the way for more accurate and robust speech recognition systems."
http://arxiv.org/abs/2507.17616v1,Vision Transformer attention alignment with human visual perception in aesthetic object evaluation,"Aesthetic object evaluation is a subjective task deeply rooted in human visual perception, yet often approached by computational models as a purely objective classification problem. This disconnect limits the interpretability and potential for improving these models. This paper addresses the problem of aligning the attention mechanisms within Vision Transformers (ViTs) with human visual fixations during aesthetic object evaluation. We propose a novel attention regularization technique that leverages eye-tracking data collected from human subjects evaluating the aesthetic quality of images. This regularization encourages ViT attention maps to focus on image regions corresponding to human visual fixations, effectively bridging the gap between computational attention and human perception. Our experiments demonstrate that ViTs trained with our attention regularization exhibit improved performance in aesthetic assessment tasks, while simultaneously generating attention maps that correlate significantly better with human eye-tracking data, achieving a 25% increase in similarity score. This work provides a step towards developing more human-aligned and interpretable aesthetic assessment models, paving the way for applications in personalized design and art curation."
http://arxiv.org/abs/2507.17520v1,InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation,"Vision-language models have shown remarkable progress in understanding visual scenes and responding to language instructions. However, effectively translating this understanding into actionable manipulation commands for robotic agents remains a significant challenge, often requiring extensive task-specific training. To address this gap, we introduce InstructVLA, a novel vision-language-action instruction tuning framework designed to bridge the gap between perception and robotic manipulation. InstructVLA leverages a large, diverse dataset of simulated robotic manipulation tasks paired with detailed language instructions, allowing the model to learn a generalizable mapping from visual observations and natural language commands to robot actions. Our approach incorporates a multi-modal transformer architecture trained with a combination of imitation learning and reinforcement learning objectives, enabling the agent to both mimic expert demonstrations and autonomously refine its policy through trial and error. Experimental results demonstrate that InstructVLA significantly outperforms existing methods in zero-shot and few-shot task generalization across a variety of robotic manipulation benchmarks. This work represents a substantial step towards developing versatile and intuitive robot assistants capable of performing complex tasks from natural language instructions."
http://arxiv.org/abs/2507.17467v1,Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls,"Vision-Language (VL) models have achieved remarkable progress in tasks requiring joint understanding of images and text. The Visual Entailment (VE) task, which requires determining whether a hypothesis image-caption pair is entailed, contradicted, or neutral, has emerged as a popular benchmark for evaluating this understanding. This paper investigates the promises and pitfalls of using VE as a probe for VL reasoning abilities. We propose a novel analysis framework that disentangles the impact of various factors, including language biases, image-only cues, and the complexity of reasoning required for different entailment types. Our framework involves generating counterfactual examples by systematically manipulating image and text features and assessing their impact on model predictions. Experiments conducted on several state-of-the-art VL models reveal that they often rely on superficial cues and language priors rather than genuine visual reasoning, particularly in cases involving negation or complex relationships. Furthermore, we observe that models exhibit a strong bias towards predicting ""entailment,"" indicating a lack of robustness to subtle changes in the image or text. These findings highlight the limitations of current VE benchmarks and suggest the need for more challenging and diagnostic datasets to accurately assess VL reasoning capabilities. This work provides valuable insights into the strengths and weaknesses of VL models and informs the development of more robust and reliable evaluation metrics."
http://arxiv.org/abs/2507.17239v1,MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training,"Medical vision-language pre-training (VLP) has shown promise in learning transferable representations from paired medical images and text, but suffers from limited labeled data. We address the challenge of leveraging readily available unlabeled medical images to improve VLP performance in a semi-supervised setting. We propose MaskedCLIP, a novel framework that bridges the masked image modeling and CLIP spaces. MaskedCLIP uses a masked autoencoder to reconstruct masked image patches, while simultaneously aligning the image and text embeddings with a pre-trained CLIP model. This alignment is achieved through a novel contrastive loss that encourages consistency between the reconstructed image embedding and the CLIP embedding, effectively transferring knowledge from the CLIP space to the masked image modeling task. Experiments on multiple medical image classification benchmarks demonstrate that MaskedCLIP significantly outperforms existing semi-supervised VLP methods, achieving state-of-the-art results with limited labeled data. This work offers a practical approach to improving medical image understanding by effectively utilizing unlabeled data through the synergy of masked image modeling and vision-language alignment."
http://arxiv.org/abs/2507.17088v1,FedVLM: Scalable Personalized Vision-Language Models through Federated Learning,"Vision-Language Models (VLMs) have demonstrated remarkable performance in various multimodal tasks, but their centralized training necessitates access to large, often sensitive, datasets. This poses significant privacy and scalability challenges when data is distributed across numerous clients with heterogeneous resources and varying data distributions. To address this, we introduce FedVLM, a novel federated learning framework for training personalized VLMs. FedVLM leverages a modular architecture, allowing clients to locally fine-tune task-specific modules while keeping a shared, pre-trained VLM backbone frozen to minimize communication overhead. Furthermore, we introduce a client-specific adaptive regularization strategy that dynamically adjusts regularization strength based on local data characteristics, promoting both personalization and generalization. Our experiments on diverse vision-language tasks, including image captioning and visual question answering, demonstrate that FedVLM achieves comparable or superior performance to centralized training while preserving data privacy and significantly reducing communication costs. FedVLM offers a practical and scalable approach to deploying personalized VLMs in real-world distributed environments, paving the way for privacy-preserving multimodal AI."
http://arxiv.org/abs/2507.16815v1,ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning,"Vision-language-action (VLA) agents aim to autonomously execute instructions in complex environments, requiring intricate reasoning about visual observations, natural language goals, and potential actions. However, current approaches often struggle with long-horizon tasks due to limited planning capabilities and the compounding effects of errors in visual perception and action execution. We introduce ThinkAct, a novel VLA framework that leverages reinforced visual latent planning to address this challenge. ThinkAct learns a latent action space conditioned on visual observations and language instructions, enabling efficient exploration of potential future states. A reinforcement learning agent then learns to navigate this latent space, planning a sequence of actions that maximize the likelihood of achieving the goal. We evaluate ThinkAct on the ALFRED benchmark, demonstrating a significant improvement in task completion rate compared to state-of-the-art methods, particularly for long-horizon tasks. The results highlight the effectiveness of reinforced visual latent planning in enabling more robust and efficient VLA reasoning."
http://arxiv.org/abs/2507.16814v1,Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning,"Vision-language reasoning requires complex cognitive processes, often demanding a deliberate, step-by-step ""slow-thinking"" approach to arrive at the correct answer. However, existing reinforcement learning (RL) methods for this task often suffer from inefficient exploration and high variance due to sparse rewards and the difficulty of learning from scratch. This paper addresses the challenge of effectively training RL agents for vision-language slow-thinking reasoning by introducing a novel semi-off-policy learning framework. Our approach leverages demonstrations of expert reasoning trajectories to guide exploration and accelerate learning, while simultaneously allowing the agent to deviate from these demonstrations and explore alternative reasoning paths. Specifically, we employ a behavior cloning loss to encourage imitation of expert actions and a separate RL loss to optimize for long-term reward, weighting each based on the estimated quality of the expert actions. Empirical results on the NLVR2 and GQA datasets demonstrate that our semi-off-policy approach significantly outperforms state-of-the-art on-policy and fully off-policy RL methods, achieving higher accuracy and faster convergence. This work provides a promising direction for improving the efficiency and effectiveness of RL-based vision-language reasoning agents, paving the way for more robust and generalizable models."
http://arxiv.org/abs/2507.16746v1,Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning,"Interleaved Vision-Language Reasoning (VLR) tasks require models to understand and reason about both visual and textual information in a sequential and interdependent manner. However, existing VLR datasets often lack the complexity to truly evaluate higher-order reasoning capabilities, particularly those requiring step-by-step inference analogous to Chain-of-Thought (CoT) prompting. We introduce Zebra-CoT, a novel dataset designed to evaluate interleaved VLR with explicit reasoning steps. Zebra-CoT consists of complex visual scenes coupled with multi-step questions that necessitate interleaved perception, reasoning, and language generation to arrive at the correct answer. Each question is paired with a detailed, human-annotated CoT rationale that explains the reasoning process, including visual grounding and intermediate conclusions. Experiments using state-of-the-art VLR models demonstrate a significant performance gap compared to human performance, highlighting the challenge posed by Zebra-CoT. This dataset provides a valuable resource for training and evaluating models capable of complex, step-by-step reasoning in interleaved vision-language contexts."
http://arxiv.org/abs/2507.16716v1,Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation,"Vision-language pre-training has shown remarkable success in various remote sensing applications, yet the scarcity of high-quality, aligned image-text data remains a critical bottleneck. This paper addresses the challenge of limited data availability for remote sensing vision-language models (VLMs) by proposing a novel framework for generating high-quality image-text datasets using the synergistic capabilities of Multi-Modal Large Language Models (MLLMs) and Large Language Models (LLMs). Our approach leverages MLLMs to initially generate descriptive captions for remote sensing images, followed by LLM-based refinement through iterative prompting strategies designed to enhance factual accuracy, semantic richness, and stylistic coherence. Experiments demonstrate that training remote sensing VLMs on datasets augmented with our synthetically generated data leads to significant improvements in downstream tasks, including image captioning, scene classification, and cross-modal retrieval, surpassing models trained solely on existing real-world datasets. This work offers a scalable and effective solution for boosting the performance of remote sensing VLMs, paving the way for more robust and generalizable models in Earth observation."
http://arxiv.org/abs/2507.16704v1,Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation,"Accessibility is crucial for enabling users with disabilities to effectively interact with computer interfaces. Existing macOS accessibility APIs often require manual annotation, hindering widespread adoption and rapid prototyping of accessible applications. This paper addresses the challenge of automatically generating accessibility information for macOS applications directly from screen images, bypassing the need for code analysis or developer intervention. We introduce Screen2AX, a novel vision-based approach that leverages a combination of object detection, optical character recognition (OCR), and spatial reasoning to identify UI elements and infer their semantic roles and relationships. Specifically, we employ a fine-tuned object detection model to detect UI elements, followed by OCR to extract text labels. A graph-based representation is then constructed to model the spatial layout and hierarchical structure of the UI, enabling the inference of accessibility attributes such as button labels, text descriptions, and reading order. Experimental results on a diverse dataset of macOS application screenshots demonstrate that Screen2AX achieves high accuracy in identifying UI elements and generating relevant accessibility information, significantly reducing the manual effort required for creating accessible applications and paving the way for more inclusive user experiences."
http://arxiv.org/abs/2507.16683v1,QRetinex-Net: Quaternion-Valued Retinex Decomposition for Low-Level Computer Vision Applications,"Retinex theory posits that an image can be decomposed into reflectance, representing intrinsic scene properties, and illumination, representing extrinsic lighting conditions. Existing Retinex-based methods often treat color channels independently or rely on complex, hand-crafted priors, limiting their performance in handling complex real-world scenes. This paper addresses the challenge of accurately decomposing images into reflectance and illumination components while preserving color correlations and reducing computational complexity. We introduce QRetinex-Net, a novel quaternion-valued deep learning framework for Retinex decomposition. QRetinex-Net leverages the inherent properties of quaternions to represent and process color images as a unified entity, enabling efficient encoding of inter-channel relationships within the reflectance and illumination layers. Furthermore, we incorporate a quaternion-based attention mechanism to selectively enhance relevant features and suppress noise. Experimental results on benchmark datasets demonstrate that QRetinex-Net achieves state-of-the-art performance in image enhancement, low-light image restoration, and color constancy tasks, outperforming existing Retinex-based and other deep learning methods in terms of both quantitative metrics and visual quality. Our quaternion-based approach provides a powerful and efficient framework for low-level computer vision tasks by effectively leveraging color correlations."
http://arxiv.org/abs/2507.16524v1,Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models,"3D Vision-Language Models (3D-VLMs) have demonstrated impressive capabilities in tasks such as 3D scene understanding and generation. However, existing 3D-VLMs often lack a strong understanding of spatial relationships and geometric properties within 3D scenes, hindering their ability to perform complex reasoning and manipulation tasks. To address this limitation, we introduce Spatial 3D-LLM, a novel framework that explicitly incorporates spatial awareness into 3D-VLMs. Our approach leverages a learnable Spatial Query Module (SQM) that extracts geometric features from point clouds and encodes them into spatial embeddings. These embeddings are then fused with visual features and fed into a large language model (LLM) to facilitate spatial reasoning. We evaluate Spatial 3D-LLM on a suite of spatial reasoning tasks, including spatial relation prediction, object localization, and instruction-guided manipulation. Our results demonstrate that Spatial 3D-LLM significantly outperforms existing 3D-VLMs, achieving state-of-the-art performance on these tasks. This work highlights the importance of spatial awareness in 3D-VLMs and provides a promising direction for future research in 3D scene understanding and interaction."
http://arxiv.org/abs/2507.16318v2,M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision,"Multispectral vision, especially using RGB and Thermal (RGBT) modalities, has gained prominence due to its robustness in challenging lighting and environmental conditions. However, current RGBT models are often specialized for specific tasks and datasets, lacking generalization ability. We address this limitation by introducing M-SpecGene, a generalized foundation model for RGBT multispectral vision. M-SpecGene leverages a novel spectral-aware transformer architecture pre-trained on a large, diverse RGBT dataset using a masked spectral encoding strategy. This pre-training objective encourages the model to learn robust representations that are invariant to modality-specific variations and capture the complementary information across RGB and Thermal spectra. Extensive experiments on various downstream tasks, including object detection, semantic segmentation, and cross-modal retrieval, demonstrate that M-SpecGene achieves state-of-the-art performance with significant improvements over existing task-specific and pre-trained RGBT models. This work establishes a strong foundation for future research in multispectral vision by providing a highly generalizable and adaptable model that can be readily fine-tuned for a wide range of applications."
http://arxiv.org/abs/2507.16279v1,MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning in Vision Tasks,"Supervised local learning aims to enhance the discriminative power of local features by training auxiliary tasks predicting local properties. However, scaling these auxiliary networks to deeper architectures often leads to diminishing returns and increased computational overhead due to optimization challenges and feature entanglement. This paper addresses the problem of effectively scaling auxiliary networks for supervised local learning in deep vision models. We propose MAN++, a Momentum Auxiliary Network that leverages momentum-based optimization and a novel disentanglement loss to train auxiliary tasks. MAN++ incorporates a momentum update for the auxiliary network's parameters, smoothing the optimization landscape and mitigating oscillations. Further, the disentanglement loss encourages the auxiliary network to learn representations that are complementary to the main task, preventing feature entanglement and promoting more effective local feature learning. Experiments on image classification, object detection, and semantic segmentation demonstrate that MAN++ consistently outperforms existing supervised local learning methods, achieving significant improvements in accuracy and robustness, particularly when applied to deeper and more complex architectures. The proposed method provides a scalable and effective strategy for enhancing local feature learning in modern vision models."
http://arxiv.org/abs/2507.16260v1,ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference,"Vision Transformers (ViTs) have achieved remarkable performance in various vision tasks, but their high computational cost during inference remains a significant bottleneck, especially for resource-constrained devices. This paper addresses the problem of redundant computation in ViT inference, stemming from the inherent similarity between consecutive frames in video processing or high frame rate image streams. We propose ToFe (Token Freezing and Reusing), a novel approach that dynamically freezes and reuses token embeddings from previous frames based on a learned similarity metric. ToFe employs a lightweight similarity network to assess the relevance of tokens across frames, enabling the freezing of highly similar tokens and their direct reuse in the current frames computation. This reduces the computational workload by skipping unnecessary attention calculations for frozen tokens. Experiments on various video datasets demonstrate that ToFe can significantly reduce the inference time and computational cost of ViTs, achieving up to a 40% reduction in FLOPs with minimal accuracy degradation. ToFe offers a practical and efficient solution for deploying ViTs in real-time video processing and other applications where computational resources are limited."
http://arxiv.org/abs/2507.16257v1,"Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models","Vision-Language Models (VLMs) have achieved remarkable success in various multimodal tasks, yet their robustness to visual perturbations remains a significant concern. These models often exhibit brittle performance when faced with common image corruptions or adversarial attacks, highlighting a critical gap in their ability to generalize beyond curated datasets. This paper addresses the problem of enhancing the visual robustness of VLMs by leveraging the power of language. We propose a novel training paradigm, ""Language-Guided Robustification"" (LGR), which incorporates carefully crafted textual prompts designed to encourage the model to focus on semantically meaningful visual features rather than spurious correlations susceptible to perturbations. LGR leverages a combination of adversarial training with both visual and textual perturbations, coupled with a contrastive loss that encourages alignment between robust visual features and corresponding descriptive language. Our experiments on several benchmark datasets demonstrate that LGR significantly improves the visual robustness of VLMs against a range of common corruptions and adversarial attacks, achieving state-of-the-art performance while maintaining competitive accuracy on clean images. These results underscore the crucial role of well-designed textual prompts in guiding VLMs towards more robust and reliable visual understanding, paving the way for more dependable deployment of these models in real-world applications."
http://arxiv.org/abs/2507.16018v1,Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, but their computational complexity, particularly the quadratic scaling of the attention mechanism, hinders their application to high-resolution images and resource-constrained environments. This paper addresses the challenge of reducing the computational burden of ViTs while maintaining accuracy, specifically focusing on the emergence of attention artifacts and the identification of attention sinks. We propose a novel structured approximation framework that leverages learned, low-rank approximations of the attention matrix to mitigate artifacts and efficiently redistribute attention weight from identified sinks. Furthermore, we integrate a trainable sparsification module to dynamically prune less important attention connections, further reducing computational cost. Experimental results on ImageNet classification demonstrate that our approach achieves comparable accuracy to full attention ViTs with significantly reduced computational complexity and memory footprint. Our method offers a practical pathway towards deploying high-performing ViTs in resource-limited scenarios by providing a principled framework for efficient attention approximation."
http://arxiv.org/abs/2507.16015v1,Is Tracking really more challenging in First Person Egocentric Vision?,"First-person egocentric vision presents unique challenges compared to traditional third-person vision due to factors like ego-motion, viewpoint instability, and a focus on the user's immediate surroundings. This paper investigates the fundamental question of whether object tracking algorithms truly face greater difficulties in egocentric videos compared to their third-person counterparts. To address this, we propose a novel benchmarking framework that enables a controlled comparison by synthetically transferring objects tracked in existing third-person datasets into egocentric videos, while preserving their motion characteristics. Our approach involves a robust object re-identification module and a seamless blending technique to minimize visual artifacts introduced during the transfer. We evaluate a suite of state-of-the-art tracking algorithms within this framework, quantifying their performance across both egocentric and third-person perspectives. Our results reveal that while some trackers exhibit a performance drop in the egocentric setting, the magnitude of the difference is significantly smaller than often assumed, suggesting that dataset bias and evaluation metrics play a crucial role in perceived performance differences. This work sheds light on the true challenges of egocentric tracking and motivates the development of more robust and generalizable tracking algorithms."
http://arxiv.org/abs/2507.15833v1,"Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers","Human gaze provides a rich signal for understanding attention and intent, offering a valuable supervisory signal for robot learning. However, directly incorporating human gaze into robot learning pipelines can be challenging due to the high dimensionality of visual input and the need for efficient real-time performance. We address the problem of learning efficient and robust robot policies from human gaze by introducing a novel ""Look, Focus, Act"" framework. This framework leverages human gaze to dynamically foveate visual input using a differentiable attention mechanism within a Vision Transformer (ViT) architecture. The foveated ViT then processes the high-resolution region indicated by gaze, enabling the robot to focus computational resources on relevant areas of the scene while ignoring irrelevant clutter. We demonstrate through simulated and real-world experiments that our approach significantly improves learning speed and generalization compared to baselines that process the entire image or use alternative attention mechanisms. Specifically, our method achieves a 30% improvement in task success rate in unseen environments while requiring significantly fewer training samples. This work highlights the potential of gaze-guided foveated vision for efficient and robust robot learning, paving the way for more intuitive and adaptable human-robot collaboration."
http://arxiv.org/abs/2507.15798v1,Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models,"Modern deep learning prioritizes model size and computational efficiency, leading to the development of low-parameter vision models. However, the underlying mechanisms enabling these models to achieve high performance with limited capacity remain poorly understood. This paper investigates the emergence of superposition and interference  phenomena observed in neural networks where multiple features are encoded within a single neuron  in state-of-the-art low-parameter vision models. We introduce a novel methodology for quantifying superposition and interference based on analyzing the alignment of weight vectors with input feature activations. Our method involves perturbing input features and observing the resulting changes in neuron activations, allowing us to infer the degree to which different features are encoded within the same neuron. Experimental results on CIFAR-10 and ImageNet show a significant positive correlation between model performance and the presence of superposition and interference, particularly in the later layers of the network. These findings suggest that superposition and interference are crucial mechanisms for efficient feature representation in low-parameter vision models, offering insights for designing more compact and effective architectures."
http://arxiv.org/abs/2507.16856v1,SIA: Enhancing Safety via Intent Awareness for Vision-Language Models,"Vision-language models (VLMs) are increasingly deployed in safety-critical applications, such as assistive robotics and autonomous driving. However, existing VLMs primarily focus on object recognition and scene understanding, often neglecting the crucial aspect of inferring human intent, which is vital for proactive safety measures. This paper addresses the problem of enabling VLMs to reason about potential human actions and their associated safety implications. We introduce SIA (Safety via Intent Awareness), a novel framework that integrates intent prediction into VLMs by learning to associate visual cues and linguistic descriptions with potential future actions and their corresponding safety risks. SIA employs a multi-modal attention mechanism to fuse visual and textual information, followed by a dedicated intent prediction module trained on a novel dataset of annotated video sequences with associated safety implications. Experimental results demonstrate that SIA significantly improves VLM performance in anticipating potentially hazardous situations, leading to a 15% increase in early hazard detection compared to state-of-the-art methods. This enhanced intent awareness allows for more proactive and reliable safety interventions in real-world scenarios."
http://arxiv.org/abs/2507.15597v1,Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos,"Vision-language-action (VLA) models hold immense potential for embodied AI agents, enabling them to understand and interact with the world through multimodal sensory inputs and execute goal-oriented actions. However, training such models requires vast amounts of aligned video, language, and action data, which is often limited in scale and diversity, hindering generalization to novel environments and tasks. To address this challenge, we introduce Being-H0, a novel VLA pretraining framework leveraging large-scale, publicly available human activity videos. Being-H0 utilizes a transformer-based architecture trained with a combination of masked visual modeling, masked language modeling, and action prediction objectives, enabling the model to learn rich representations of human behavior and the relationships between visual cues, language descriptions, and corresponding actions. Experiments on downstream robotic manipulation and navigation tasks demonstrate that Being-H0 significantly improves performance compared to models pretrained on smaller or synthetic datasets, achieving state-of-the-art results in several benchmarks. This work highlights the effectiveness of pretraining VLA models on readily available human video data for enhancing the capabilities of embodied AI agents."
http://arxiv.org/abs/2507.15480v2,One Last Attention for Your Vision-Language Model,"Vision-Language Models (VLMs) have achieved remarkable progress in tasks requiring joint understanding of visual and textual information. However, existing architectures often exhibit suboptimal fusion of visual and textual features, leading to a bottleneck in performance, particularly on complex reasoning tasks. To address this, we introduce a novel attention mechanism, dubbed ""Final Fusion Attention"" (FFA), designed as a final processing layer within VLMs. FFA leverages cross-modal attention with learnable query embeddings to explicitly model the relationships between visual and textual representations after they have undergone independent processing. Furthermore, FFA incorporates a residual connection to preserve the original features, mitigating information loss during the attention process. Experimental results on several benchmark datasets, including VQAv2, GQA, and NLVR2, demonstrate that integrating FFA significantly improves the performance of existing VLMs, achieving state-of-the-art results on GQA with a substantial margin. This highlights the importance of carefully designed cross-modal fusion strategies for maximizing the potential of VLMs."
http://arxiv.org/abs/2507.15365v1,DAViD: Data-efficient and Accurate Vision Models from Synthetic Data,"Deep learning models for computer vision often require vast amounts of labeled real-world data, which is expensive and time-consuming to acquire. This work addresses the challenge of training accurate vision models with limited real data by leveraging synthetic data generation. We introduce DAViD (Data-efficient and Accurate Vision Models from Synthetic Data), a novel framework that combines realistic synthetic data generation with a carefully designed self-training pipeline. DAViD utilizes a differentiable renderer to create photorealistic synthetic images with accurate ground truth labels, followed by a self-training procedure where a model pre-trained on synthetic data is iteratively refined on a small set of unlabeled real images using pseudo-labels. We demonstrate that DAViD significantly improves performance on image classification and object detection tasks compared to training solely on real data or using naive synthetic-to-real transfer, achieving state-of-the-art results in low-data regimes on benchmark datasets like CIFAR-10, Pascal VOC, and COCO. This approach provides a practical and cost-effective solution for training high-performance vision models when real-world labeled data is scarce."
http://arxiv.org/abs/2507.16849v1,Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery,"Accurate and timely segmentation of disaster-affected areas is crucial for effective disaster response and resource allocation. Existing methods often struggle with the complexities of satellite imagery, including variations in illumination, atmospheric conditions, and diverse land cover types. This paper addresses the challenge of accurately segmenting post-disaster affected areas using multi-source satellite imagery, specifically Sentinel-2 and Formosat-5 data. We propose a Vision Transformer (ViT)-based Enhanced Vision-Attention Pyramid (EVAP) model. This model leverages the global context understanding of ViTs while incorporating a multi-scale feature extraction and attention mechanism to effectively capture both local details and global dependencies within the satellite imagery. The EVAP module enhances feature representation by fusing features from different stages of the ViT backbone, while the attention mechanism refines feature maps by focusing on relevant regions. Experimental results on a real-world dataset of earthquake and flood-affected areas demonstrate that our proposed EVAP-ViT model outperforms state-of-the-art semantic segmentation methods in terms of overall accuracy, IoU, and F1-score. This improved segmentation performance enables more precise damage assessment and facilitates more efficient disaster relief efforts."
http://arxiv.org/abs/2507.14976v1,Hierarchical Cross-modal Prompt Learning for Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in various multimodal tasks, often relying on prompt engineering to adapt pre-trained knowledge. However, designing effective prompts that capture the intricate relationships between vision and language remains a significant challenge, particularly when dealing with complex scenes requiring nuanced understanding. We address this challenge by proposing a novel Hierarchical Cross-modal Prompt Learning (HCPL) framework. HCPL leverages a hierarchical structure to decompose prompts into coarse-grained and fine-grained levels, enabling the model to capture both high-level semantic concepts and detailed visual cues. Furthermore, we introduce a cross-modal attention mechanism within each level to facilitate information exchange between visual and textual prompts, enhancing their contextual awareness and relevance. Experimental results on several challenging vision-language benchmarks, including Visual Question Answering and Image Captioning, demonstrate that HCPL significantly outperforms existing prompt learning methods, achieving state-of-the-art performance. This highlights the effectiveness of hierarchical decomposition and cross-modal interaction in learning more expressive and adaptable prompts for VLMs."
http://arxiv.org/abs/2507.14921v1,Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction,"3D Gaussian Splatting (3D-GS) has emerged as a powerful technique for novel view synthesis, offering state-of-the-art rendering quality and real-time rendering speeds. However, extending 3D-GS to multi-view stereo (MVS) reconstruction remains challenging due to the difficulty in establishing accurate correspondences and the need for robust initialization in unconstrained environments. This paper addresses the problem of generalizable 3D-GS reconstruction from multi-view images, focusing on scenarios with limited or no prior information. We introduce Stereo-GS, a novel framework that leverages a differentiable stereo module to predict depth and uncertainty maps, which are then used to guide the initialization and refinement of 3D Gaussians. Specifically, we employ a cost volume-based stereo network to produce initial depth estimates, which are converted into 3D Gaussians weighted by their associated uncertainty. Subsequent optimization refines Gaussian parameters using a combination of photometric and geometric losses. Experiments on benchmark datasets demonstrate that Stereo-GS achieves competitive reconstruction quality and rendering performance compared to existing MVS methods and direct 3D-GS optimization, especially in challenging scenarios with sparse views and texture-less regions. Our approach provides a robust and generalizable solution for high-quality 3D reconstruction using 3D Gaussian Splatting, paving the way for wider adoption in various applications."
http://arxiv.org/abs/2507.14823v1,FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models,"Financial charts are ubiquitous in economic reporting and analysis, conveying complex information visually. However, the ability of Vision-Language Models (VLMs) to accurately comprehend and reason about these charts remains largely unexplored. This paper introduces FinChart-Bench, a novel benchmark designed to evaluate the financial chart comprehension capabilities of VLMs across a diverse range of chart types and analytical tasks. FinChart-Bench comprises a curated dataset of real-world financial charts paired with question-answer annotations that assess understanding of trends, specific data points, comparisons, and forecasting based on chart data. We evaluate a suite of state-of-the-art VLMs on FinChart-Bench, revealing significant shortcomings in their ability to accurately interpret financial visualizations, particularly in tasks requiring quantitative reasoning and temporal analysis. Our analysis highlights specific areas for improvement in VLM architectures and training methodologies to enhance financial chart comprehension, paving the way for more effective applications in financial analysis and automated reporting."
http://arxiv.org/abs/2507.14801v1,Exploring Scalable Unified Modeling for General Low-Level Vision,"Low-level vision tasks, such as denoising, super-resolution, and deraining, are traditionally addressed with task-specific architectures, hindering resource efficiency and knowledge transfer. This paper tackles the challenge of developing a scalable and unified model capable of effectively handling a wide range of low-level vision tasks without requiring task-specific modifications. We propose a novel transformer-based architecture, UniLLVM, that leverages a shared encoder-decoder backbone augmented with task-specific query embeddings and learned task tokens. These task tokens, concatenated with the input feature maps, enable the model to dynamically adapt its processing based on the specified task. Extensive experiments demonstrate that UniLLVM achieves competitive or superior performance compared to task-specific state-of-the-art methods across diverse tasks, including image denoising, super-resolution, deblurring, and deraining, while significantly reducing the overall model size. This work highlights the potential of unified modeling for low-level vision, paving the way for more efficient and adaptable computer vision systems."
http://arxiv.org/abs/2507.14738v1,MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy,"Diabetic retinopathy (DR) is a leading cause of blindness, requiring timely diagnosis and treatment. Current automated DR staging systems often rely solely on fundus images, limiting their accuracy, while simultaneously making errors that could be easily identified by a human expert. We introduce MultiRetNet, a novel multimodal vision model for DR staging that incorporates both fundus images and optical coherence tomography (OCT) scans. MultiRetNet leverages a dual-branch convolutional neural network architecture to extract features from each modality, which are then fused using a cross-attention mechanism. Furthermore, we implement a deferral-to-expert system based on the model's confidence score, allowing the model to abstain from making predictions on uncertain cases. Our experiments on a large, real-world dataset demonstrate that MultiRetNet achieves significantly improved DR staging accuracy compared to single-modality models, and that the deferral system can effectively reduce the error rate of the automated system while maintaining a high throughput. This work provides a significant step towards developing reliable and clinically useful AI-powered tools for DR screening and management."
http://arxiv.org/abs/2507.14662v1,"Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall","Food waste is a significant global problem with substantial environmental and economic impacts, particularly within the food service sector. This paper addresses the challenge of accurately estimating plate waste in a university dining hall environment, a crucial step towards implementing effective waste reduction strategies. We propose a computer vision-based system leveraging deep learning techniques for automated food waste estimation. Our method employs a Mask R-CNN model, pre-trained on the COCO dataset and fine-tuned with a custom dataset of food images collected in the dining hall, to segment and identify different food items on trays. The system then estimates the volume of each food item using a combination of pixel count analysis and pre-determined volume-per-pixel ratios calibrated for common food types. The system achieved a mean average precision (mAP) of 0.78 for food item detection and a mean absolute error (MAE) of 15 grams per tray for total waste estimation when compared to manual measurements. This demonstrates the potential of computer vision to provide accurate and scalable food waste data, enabling data-driven decision-making for reducing food waste in institutional settings."
http://arxiv.org/abs/2507.14481v1,DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, but their large model size and computational complexity hinder deployment on resource-constrained devices. Quantization is a promising technique to compress and accelerate ViTs, but conventional quantization methods often require fine-tuning with labeled data, which can be impractical or unavailable in many real-world scenarios. This paper addresses the challenge of quantizing ViTs without access to the original training data or any labeled data, a problem known as data-free quantization. We propose DFQ-ViT, a novel data-free quantization approach that leverages a generative adversarial network (GAN) pre-trained to mimic the feature distribution of the original ViT. The GAN generates synthetic data that is then used to calibrate the quantization parameters of the ViT's weights and activations, minimizing the discrepancy between the full-precision and quantized models. Experiments on ImageNet demonstrate that DFQ-ViT achieves significant compression ratios (e.g., 8-bit quantization) with minimal accuracy degradation compared to full-precision ViTs, outperforming existing data-free quantization methods. Our approach offers a practical and efficient solution for deploying quantized ViTs in resource-limited environments without requiring access to sensitive training data."
http://arxiv.org/abs/2507.15882v2,Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark,"Large language models (LLMs) have demonstrated remarkable capabilities in understanding and generating text, and recent advancements extend these capabilities to multimodal scenarios involving images. However, evaluating the performance of vision LLMs (VLLMs) in complex, long-context document understanding tasks involving both visual and textual information remains a challenge. This paper introduces ""Document Haystack,"" a novel benchmark designed to rigorously assess VLLMs' ability to identify and reason about information scattered across extended document contexts comprising both images and text. Document Haystack features a diverse set of question-answering tasks requiring models to analyze complex documents, including reports, presentations, and web pages, to locate relevant information (""needles"") embedded within large amounts of distracting content (""haystack""). We evaluate several state-of-the-art VLLMs on this benchmark, revealing significant limitations in their ability to accurately retrieve and reason with information within extended multimodal document contexts, particularly when dealing with visually rich elements. Our findings highlight the need for future research to focus on improving VLLMs' ability to process long-range dependencies and integrate visual and textual cues for robust document understanding. Document Haystack provides a challenging and realistic evaluation framework to drive progress in this critical area."
http://arxiv.org/abs/2507.14312v1,CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation,"Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable zero-shot generalization capabilities; however, their performance can degrade significantly under distribution shifts. This paper addresses the challenge of adapting CLIP models to unseen target domains at test time without requiring source data or labels, a setting known as Test-Time Adaptation (TTA). We introduce CLIPTTA, a novel contrastive learning-based TTA approach that leverages the inherent alignment between visual and textual modalities within CLIP. CLIPTTA employs a self-supervised contrastive loss, encouraging the model to learn domain-specific representations by maximizing the agreement between augmented views of the target data in both the image and text embedding spaces. Additionally, we introduce a cross-modal consistency regularization term to further enforce the alignment between image and text representations within the adapted model. Extensive experiments on various benchmark datasets demonstrate that CLIPTTA consistently outperforms existing TTA methods, achieving significant improvements in accuracy and robustness under diverse distribution shifts. Our results highlight the effectiveness of contrastive learning for adapting CLIP models to new domains and provide a promising direction for improving the generalization capabilities of vision-language models in real-world applications."
http://arxiv.org/abs/2507.14067v1,VLA-Mark: A cross modal watermark for large vision-language alignment model,"Large Vision-Language Alignment (VLA) models are increasingly prevalent, powering diverse applications from image captioning to visual question answering. However, their widespread use raises concerns about the origin and authenticity of generated content, particularly regarding potential misuse and the propagation of misinformation. We address the challenge of watermarking VLA models to establish provenance without significantly impacting model performance or requiring access to the model's internal parameters. We introduce VLA-Mark, a novel cross-modal watermarking scheme that subtly encodes a unique identifier into both the visual and textual representations generated by the VLA model. VLA-Mark leverages frequency domain manipulation in the image and synonym substitution in the text to embed the watermark, ensuring robustness against common image and text transformations. Experiments demonstrate that VLA-Mark achieves high watermark detection accuracy (over 95%) while maintaining negligible impact on image quality (PSNR > 40dB) and text coherence (BLEU score reduction < 2%). This method provides a practical and effective solution for tracing the origin of content generated by VLA models, contributing to responsible development and deployment of these powerful AI systems."
http://arxiv.org/abs/2507.14042v1,Training-free Token Reduction for Vision Mamba,"Vision Mamba (VMamba) has emerged as a promising architecture for visual representation learning, offering compelling performance with efficient sequence modeling. However, the computational cost of VMamba scales linearly with the input sequence length, posing challenges when processing high-resolution images or long videos. This paper addresses the problem of reducing the computational burden of VMamba without requiring retraining. We introduce a novel training-free token reduction module, dubbed Selective Token Averaging (STA), which dynamically aggregates similar visual tokens based on their feature similarity, thereby shortening the sequence length passed to subsequent VMamba blocks. STA utilizes a learnable similarity threshold to adaptively determine which tokens to merge, minimizing information loss while effectively reducing the computational footprint. Experiments on ImageNet classification and COCO object detection demonstrate that our STA module significantly reduces the computational cost of VMamba, achieving comparable or even superior performance to the baseline model, particularly at higher resolutions. This work offers a practical and efficient solution for deploying VMamba in resource-constrained environments and enables its application to larger-scale visual tasks."
http://arxiv.org/abs/2507.13880v1,Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision,"Maritime environments pose significant challenges for computer vision systems due to dynamic weather conditions, complex vessel traffic, and the inherent limitations of solely relying on visual data. This paper addresses the problem of robust and accurate scene understanding in maritime vision by leveraging the complementary information provided by electronic navigational charts (ENCs) in real-time. We propose a novel framework that fuses visual data from onboard cameras with ENC data through a cascaded approach. First, a coarse registration is achieved using visual odometry and GPS data to align the camera frame with the ENC coordinate system. Subsequently, a fine-grained registration is performed using a differentiable rendering technique that projects ENC data into the camera view, allowing for iterative refinement of the pose estimate via backpropagation. Experimental results on a diverse maritime dataset demonstrate that our method significantly improves object detection accuracy, particularly for small vessels and navigational aids, compared to vision-only approaches. This real-time fusion of visual and chart data provides a more comprehensive and reliable understanding of the maritime environment, paving the way for enhanced autonomous navigation and improved maritime safety."
http://arxiv.org/abs/2507.13868v1,When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models,"Vision-language models (VLMs) excel at tasks requiring both visual perception and factual knowledge, yet often struggle when these two sources of information conflict. This paper investigates how VLMs resolve such knowledge conflicts, specifically when visual evidence contradicts prior knowledge. We introduce a novel framework, Knowledge Conflict Resolution Analysis (KCRA), which systematically probes VLMs with carefully constructed image-text pairs designed to elicit conflicts between visual input and common-sense knowledge. KCRA involves generating counterfactual images and text prompts, allowing us to quantify the extent to which a VLM prioritizes visual information over established knowledge. Our experiments across various VLMs reveal a significant bias towards visual input, even when the visual information is demonstrably misleading. Furthermore, we identify specific model architectures and training strategies that influence the degree of visual dominance. This work provides valuable insights into the inner workings of VLMs and highlights the need for improved mechanisms to balance visual perception with factual knowledge, ultimately leading to more robust and reliable models."
http://arxiv.org/abs/2507.13773v1,Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions,"Vision-Language Models (VLMs) have demonstrated impressive capabilities in answering questions about images; however, they often struggle with ambiguous visual questions that lack sufficient context for a definitive answer. This paper addresses the challenge of resolving ambiguity in visual question answering by teaching VLMs to proactively ask clarifying questions. We introduce a novel training paradigm, ""Ask-Before-Answer"" (ABA), where the VLM is explicitly trained to identify ambiguous questions and generate follow-up questions to gather missing information. ABA incorporates a reinforcement learning framework that rewards the VLM for asking informative questions that lead to more accurate final answers. Our experiments on the VQA-Ambiguous dataset demonstrate that ABA significantly improves performance on ambiguous questions compared to standard VQA training, achieving a relative improvement of over 15% in accuracy. This work provides a promising direction for developing more robust and reliable VLMs that can effectively handle real-world scenarios with inherent ambiguity."
http://arxiv.org/abs/2507.14248v1,Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, leading to their increasing deployment in security-sensitive applications. However, the inherent reliance on attention mechanisms for interpretability can inadvertently expose vulnerabilities to adversarial attacks targeting these explanations. This paper investigates the susceptibility of interpretable ViT systems to targeted attacks that manipulate attention maps, thereby misleading human users or downstream decision-making processes that rely on these interpretations. We propose a novel adversarial attack framework, Interpretation-Targeted Attack (ITA), which leverages gradient-based optimization to craft imperceptible perturbations to input images that induce significant and controlled changes in the ViT's attention maps, while maintaining high classification accuracy on the original class. Experimental results on benchmark datasets demonstrate that ITA can effectively manipulate attention maps to highlight irrelevant image regions or suppress attention on critical features, successfully deceiving human observers in interpretation tasks with high success rates (over 80% in some cases) and minimal impact on classification performance. This work highlights a critical vulnerability in the interpretability of ViTs, emphasizing the need for robust defense mechanisms that protect both the predictive accuracy and the integrity of explanations in security-critical applications."
http://arxiv.org/abs/2507.13348v1,VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about images and text. However, these models often suffer from high computational costs due to processing entire images and long text sequences, leading to inefficiencies in resource utilization. This paper addresses the problem of improving the efficiency and intelligence of VLMs by selectively focusing on the most relevant visual and textual information. We introduce VisionThink, a novel VLM architecture trained with reinforcement learning (RL) to dynamically determine which image regions and text tokens to attend to during inference. VisionThink learns a policy that balances accuracy and computational cost, intelligently pruning irrelevant information while preserving crucial details. Experiments on benchmark VQA, image captioning, and visual reasoning datasets demonstrate that VisionThink achieves comparable or superior performance to state-of-the-art VLMs with significantly reduced computational overhead, requiring up to 40% fewer FLOPs. VisionThink offers a promising approach to developing more efficient and intelligent VLMs, paving the way for deployment in resource-constrained environments."
http://arxiv.org/abs/2507.13260v1,Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy,"Vision Transformers (ViTs) have demonstrated remarkable performance in various computer vision tasks, largely attributed to pre-training on massive datasets. However, adapting these pre-trained models to downstream tasks often involves computationally expensive full fine-tuning, which can be impractical for resource-constrained environments. This paper addresses the challenge of efficiently adapting pre-trained ViTs without compromising performance. We propose an approximately orthogonal fine-tuning strategy that selectively updates a small subset of parameters while encouraging orthogonality with the pre-trained weights. This is achieved through a novel loss term that penalizes the cosine similarity between the gradients of the newly introduced parameters and the pre-trained parameters. Our experiments on a diverse set of benchmark datasets demonstrate that our approach achieves comparable or even superior performance to full fine-tuning while requiring significantly fewer trainable parameters and reduced computational cost. This efficient adaptation strategy enables wider adoption of ViTs in resource-limited settings, facilitating their deployment in real-world applications."
http://arxiv.org/abs/2507.13231v1,VITA: Vision-to-Action Flow Matching Policy,"Learning robotic policies directly from visual observations remains a significant challenge, particularly in complex environments with high-dimensional action spaces. Existing imitation learning approaches often suffer from compounding errors and struggle to generalize to unseen states, especially when the expert demonstration data is limited. We address this problem by introducing VITA: Vision-to-Action Flow Matching Policy, a novel imitation learning framework that learns a continuous, time-dependent flow between visual observations and corresponding expert actions. VITA leverages a conditional normalizing flow to map visual features to a latent action space, where a time-dependent vector field guides the latent representation towards the expert action distribution. By learning this continuous flow, VITA captures the underlying structure of the expert policy and enables robust action prediction even with noisy or incomplete visual information. Experiments on a diverse set of simulated robotic manipulation tasks demonstrate that VITA significantly outperforms state-of-the-art imitation learning methods, achieving up to 40% higher success rates and improved generalization to novel environments. These results highlight the potential of flow-based models for learning robust and generalizable vision-based robotic policies."
http://arxiv.org/abs/2507.13152v2,SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models,"Vision-Language Navigation (VLN) tasks agents to navigate in real-world environments following natural language instructions, requiring strong vision-language understanding and reasoning capabilities. Existing VLN methods often struggle with complex instructions and unseen environments due to limitations in generalization and adaptability. We introduce SE-VLN, a novel Self-Evolving Vision-Language Navigation framework leveraging the power of Multimodal Large Language Models (MLLMs). SE-VLN employs a two-stage approach: first, an MLLM-based instruction follower generates navigation actions conditioned on visual observations and language instructions, incorporating a dynamic memory module for long-term context. Second, a self-evolution mechanism refines the MLLM's navigation policy through iterative interaction with the environment, generating synthetic experiences and fine-tuning the MLLM to improve its performance in unexplored scenarios. Experimental results on the Room-to-Room (R2R) and Cooperative Vision-and-Dialog Navigation (CVDN) datasets demonstrate that SE-VLN significantly outperforms existing state-of-the-art methods, achieving a 15% relative improvement in success rate on unseen environments in R2R. SE-VLN presents a promising direction for developing more robust and adaptable VLN agents by harnessing the self-improving capabilities of MLLMs."
http://arxiv.org/abs/2507.13079v1,DASViT: Differentiable Architecture Search for Vision Transformer,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, but their architectural design often relies on extensive manual tuning and expert knowledge. This paper addresses the challenge of automating the architecture design of ViTs to discover more efficient and task-specific configurations. We introduce DASViT, a novel Differentiable Architecture Search framework tailored for ViTs. DASViT formulates the ViT architecture search space as a directed acyclic graph, enabling continuous relaxation and gradient-based optimization. Specifically, we employ a bi-level optimization strategy to simultaneously learn the architecture and the network weights, incorporating a novel search space that explores the optimal number of layers, attention heads, and MLP ratios within each transformer block. Experiments on ImageNet classification demonstrate that DASViT can discover ViT architectures that achieve comparable or better performance than manually designed counterparts while significantly reducing computational cost. The discovered DASViT architectures exhibit superior parameter efficiency and generalization capabilities, highlighting the potential of automated search in designing high-performance vision transformers."
http://arxiv.org/abs/2507.13019v1,Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities,"Vision-and-Language Navigation (VLN) tasks challenge agents to navigate through realistic environments by following natural language instructions. However, a significant ""embodied gap"" exists between the simulated environment and the real world, hindering the transferability of VLN agents. This paper holistically investigates the embodied gap in VLN, focusing on both physical disparities (e.g., variations in agent dynamics and sensor noise) and visual disparities (e.g., differences in rendering quality and object appearance). We propose a novel simulation-to-simulation adaptation framework that explicitly models and mitigates these disparities. Specifically, we introduce physically-informed domain randomization to bridge physical gaps and style transfer techniques coupled with adversarial training to reduce visual differences. Our experiments on the Room-to-Room (R2R) and Cooperative Vision-and-Dialog Navigation (CVDN) datasets demonstrate that our approach significantly improves navigation performance in more realistic simulated environments, achieving up to 15% relative improvement in success rate. These results highlight the critical role of addressing both physical and visual disparities for robust VLN and offer a promising direction towards deploying VLN agents in real-world scenarios."
http://arxiv.org/abs/2507.12780v1,Compact Vision Transformer by Reduction of Kernel Complexity,"Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks, but their computational complexity, particularly within the self-attention mechanism, hinders deployment on resource-constrained devices. This paper addresses the challenge of reducing the computational burden of ViTs without sacrificing performance by focusing on the inherent redundancy within the attention kernel. We introduce a novel approach, Kernel Complexity Reduction (KCR), that decomposes the self-attention kernel into a series of lower-rank approximations learned through a differentiable optimization process. KCR iteratively identifies and prunes less significant components of the kernel while maintaining representational capacity via a structured regularization term during training. Extensive experiments on ImageNet classification demonstrate that KCR-ViTs achieve comparable or superior accuracy to standard ViTs with a substantial reduction in computational cost, measured by FLOPs and parameter count, particularly at smaller model sizes. This work enables the deployment of high-performing ViTs on edge devices, broadening their applicability in real-world scenarios."
http://arxiv.org/abs/2507.12698v1,Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images,"Vision-language foundation models (VLMs) have shown impressive capabilities in generating images from text prompts across various domains. However, their application in medical imaging is limited by their inability to generate high-resolution, clinically accurate images necessary for diagnostic purposes. This paper addresses the challenge of generating megapixel-scale medical images from textual descriptions using a novel VLM framework, Pixel Perfect MegaMed. Our approach leverages a hierarchical generative architecture incorporating a transformer-based text encoder, a cascaded diffusion model for image generation, and a novel high-resolution refinement network. This refinement network utilizes a learned perceptual loss and adversarial training to enhance fine-grained details and ensure clinical fidelity at megapixel resolutions. Experimental results on chest X-ray and fundus photography datasets demonstrate that Pixel Perfect MegaMed significantly outperforms existing VLMs in terms of image quality, anatomical accuracy, and clinical relevance, as evaluated by quantitative metrics and expert radiologists. This work paves the way for developing powerful diagnostic and educational tools leveraging the generative capabilities of VLMs in high-resolution medical imaging."
http://arxiv.org/abs/2507.12449v1,Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios,"Autonomous vehicles rely heavily on robust perception systems for safe navigation, particularly in dynamic obstacle-rich environments. Existing approaches often struggle with the complexities of accurately detecting and predicting the motion of diverse obstacles under varying environmental conditions. This paper addresses the challenge of enhancing vision-based perception for autonomous vehicles to improve obstacle avoidance capabilities. We propose a novel multi-modal fusion framework that combines semantic segmentation, monocular depth estimation, and optical flow analysis to create a comprehensive understanding of the surrounding environment. Our method employs a deep learning architecture trained on a large-scale synthetic dataset augmented with real-world data to improve generalization. Experimental results on the challenging KITTI dataset demonstrate that our approach achieves state-of-the-art performance in obstacle detection and trajectory prediction, leading to improved path planning and reduced collision risk in simulated autonomous driving scenarios. This enhanced perception system significantly contributes to the safety and reliability of autonomous vehicles in complex urban environments."
http://arxiv.org/abs/2507.12440v3,EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos,"Understanding and predicting human actions from a first-person perspective is crucial for developing intelligent agents capable of seamless interaction with humans in everyday environments. Existing vision-language-action (VLA) models often rely on third-person datasets, limiting their applicability to egocentric tasks where the agent's viewpoint and embodied experience are paramount. This paper addresses the challenge of learning VLA models directly from egocentric human videos, enabling the prediction of future actions based on observed visual cues and contextual language instructions. We introduce EgoVLA, a novel framework that leverages transformers to jointly model visual, linguistic, and action modalities from egocentric videos. EgoVLA incorporates a hierarchical attention mechanism to capture long-range dependencies within and across modalities, and it is trained using a combination of contrastive and generative losses to align vision, language, and action representations. Experimental results on the Ego4D benchmark demonstrate that EgoVLA significantly outperforms existing baselines in action anticipation and instruction following tasks, achieving state-of-the-art performance. This work paves the way for more effective and intuitive human-agent collaboration through improved understanding of egocentric human behavior."
http://arxiv.org/abs/2507.12414v1,AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models,"Vision datasets often suffer from noisy or mislabeled data, which significantly degrades the performance of trained models. Manually cleaning these datasets is a tedious and time-consuming process. This paper introduces AutoVDC, an automated vision data cleaning framework leveraging the power of Vision-Language Models (VLMs). AutoVDC utilizes a VLM to generate textual descriptions for each image in the dataset. These descriptions are then compared with the image's assigned label to identify potential inconsistencies and flag suspect data points for further review or automated correction. Experiments on benchmark datasets demonstrate that AutoVDC effectively identifies mislabeled and noisy images, achieving significant improvements in data quality and leading to enhanced performance of downstream vision models trained on the cleaned data. AutoVDC offers a practical and scalable solution for improving the reliability and performance of vision models by automating the critical data cleaning process."
http://arxiv.org/abs/2507.12236v1,Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models,"Medical vision-language models (VLMs) have shown promise in bridging the gap between medical imaging and clinical language, yet accurately grounding textual phrases to specific image regions remains a challenge. Existing approaches often struggle with the subtle nuances and complex relationships within medical images and reports. We address this problem by introducing a novel ""Generate to Ground"" (G2G) framework that leverages multimodal text conditioning to enhance phrase grounding performance. Our approach first employs a text generation module conditioned on both the image and the target phrase to generate semantically relevant contextual descriptions. These generated descriptions are then fused with the original phrase to provide a richer and more informative textual representation for the grounding module, enabling it to better identify the corresponding image region. Experimental results on benchmark medical phrase grounding datasets demonstrate that our G2G framework significantly outperforms state-of-the-art methods, achieving substantial improvements in grounding accuracy. This work highlights the potential of multimodal text conditioning to improve the interpretability and reliability of medical VLMs by enabling more precise localization of clinically relevant information."
http://arxiv.org/abs/2507.12125v1,Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, but their high computational cost and large memory footprint hinder deployment on resource-constrained devices. Existing pruning techniques often focus on unstructured or fine-grained pruning, leading to irregular memory access patterns and limited hardware acceleration. To address this, we propose a novel block-based symmetric pruning and fusion approach for efficient ViTs. Our method first identifies importance scores at the block level within the self-attention and feed-forward network modules. Then, symmetric pruning is applied based on these scores, removing entire blocks of weights to maintain structural regularity. Subsequently, we fuse the pruned blocks into a smaller, dense representation, further reducing computational overhead. Experiments on ImageNet classification demonstrate that our approach achieves significant reductions in FLOPs and model size with minimal accuracy loss compared to existing pruning methods. This block-based symmetric pruning and fusion technique offers a practical solution for deploying efficient ViTs on resource-limited platforms."
http://arxiv.org/abs/2507.12060v2,InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing,"Face Anti-spoofing (FAS) is crucial for securing face recognition systems against presentation attacks. Existing FAS methods often rely on specialized architectures and training paradigms, limiting their generalizability and scalability. This paper addresses the challenge of developing a unified and instruction-following FAS model capable of handling diverse spoofing scenarios. We introduce InstructFLIP, a novel framework that leverages a pre-trained Vision-Language Model (VLM) and fine-tunes it with instruction-based learning. Specifically, we transform FAS tasks into natural language instructions, enabling the VLM to learn discriminative features and reason about authenticity based on textual guidance. Furthermore, we incorporate a novel cross-modal attention mechanism to enhance the interaction between visual features and instructional cues. Extensive experiments on benchmark datasets, including OULU-NPU, SiW, and CASIA-SURF CeFA, demonstrate that InstructFLIP achieves state-of-the-art performance while exhibiting strong generalization capabilities across different attack types and unseen domains. This work highlights the potential of VLMs for developing robust and adaptable FAS systems, paving the way for more secure and reliable face recognition technology."
http://arxiv.org/abs/2507.11969v1,GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models,"Vision-language models (VLMs) demonstrate remarkable generalization capabilities but often suffer performance degradation when deployed in novel test environments due to domain shifts. Test-time adaptation (TTA) aims to mitigate this issue by adapting the model to the target domain using only unlabeled test data. However, existing VLM TTA methods often overlook the complex interplay between global semantic context and local spatial information within images, leading to suboptimal adaptation. We introduce GS-Bias, a novel Global-Spatial Bias Learner designed for single-image VLM TTA. GS-Bias dynamically learns domain-specific biases by explicitly modeling global image-text relationships and fine-grained spatial feature correlations. Specifically, we employ a contrastive learning objective to align global image representations with corresponding text embeddings and simultaneously learn spatial biases through a self-attention mechanism that captures local feature dependencies. Experimental results on diverse benchmark datasets demonstrate that GS-Bias consistently outperforms state-of-the-art VLM TTA methods, achieving significant improvements in accuracy and robustness. This work highlights the importance of effectively capturing global-spatial biases for adapting VLMs to unseen domains, paving the way for more reliable and generalizable vision-language systems."
http://arxiv.org/abs/2507.11943v1,Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various image classification tasks, but fine-tuning these large models can be computationally expensive and pose privacy risks due to the potential leakage of sensitive information from training data. This paper addresses the challenge of effectively fine-tuning ViTs while preserving data privacy. We propose a novel privacy-preserving fine-tuning approach that combines Low-Rank Adaptation (LoRA) with Differential Privacy (DP). Specifically, we introduce DP noise during the LoRA parameter updates, effectively limiting the influence of individual training samples on the fine-tuned model. Our experiments on benchmark datasets, including CIFAR-10 and CIFAR-100, demonstrate that our method achieves competitive classification accuracy compared to full fine-tuning, while provably guaranteeing differential privacy. This work provides a practical and effective strategy for adapting large vision models to specific tasks in privacy-sensitive applications."
http://arxiv.org/abs/2507.11939v1,POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering,"Visual question answering (VQA) over charts has gained increasing attention, but existing benchmarks primarily focus on English, limiting the evaluation of multilingual capabilities in Large Vision-Language Models (LVLMs). This paper introduces POLYCHARTQA, a novel benchmark for multilingual chart question answering, comprising 10,000 charts paired with questions and answers in five typologically diverse languages: English, French, Simplified Chinese, Hindi, and Swahili. To generate high-quality, parallel data, we employ a multi-stage translation and back-translation strategy guided by semantic consistency checks, ensuring accuracy and naturalness across languages. We evaluate a range of state-of-the-art LVLMs on POLYCHARTQA, revealing a significant performance gap between English and other languages, particularly in tasks requiring complex reasoning and numerical understanding. Our analysis highlights the challenges LVLMs face in generalizing chart understanding across languages and the importance of multilingual pre-training and fine-tuning for improved cross-lingual VQA. POLYCHARTQA serves as a valuable resource for advancing research in multilingual VQA and developing more robust and inclusive LVLMs."
http://arxiv.org/abs/2507.13384v1,Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation,"Magnetic Resonance Imaging (MRI) segmentation is crucial for quantitative analysis and diagnosis in various clinical applications. Recent advances in visual Transformers have shown promise in this domain, but their computational complexity hinders the processing of high-resolution 3D MRI volumes. To address this, we investigate the application of Mamba, a selective state space model, to MRI segmentation, focusing on the critical role of patch ordering in shaping the model's understanding of spatial relationships. We propose a novel patchification and ordering strategy, termed ""Spatially-Aware Mamba Patching (SAMP),"" which leverages domain-specific knowledge of MRI data to prioritize contiguous anatomical structures during patch sequence construction. SAMP incorporates both spatial proximity and intensity similarity to guide the flattening process, thereby enabling Mamba to learn more meaningful representations. Experiments on publicly available datasets demonstrate that SAMP significantly improves segmentation accuracy compared to naive raster scanning and other patch ordering schemes, achieving state-of-the-art performance while maintaining computational efficiency. These findings highlight the importance of carefully considering patch order when adapting sequence models like Mamba to 3D medical image analysis, paving the way for more effective and efficient MRI segmentation solutions."
http://arxiv.org/abs/2507.11711v1,Image-Based Multi-Survey Classification of Light Curves with a Pre-Trained Vision Transformer,"Light curves, representing the change in brightness of celestial objects over time, are fundamental for astronomical discoveries, particularly in identifying transient events. Classifying these light curves accurately and efficiently across different astronomical surveys, which often have varying observational cadences and data formats, remains a significant challenge. We address this problem by proposing a novel image-based classification pipeline leveraging a pre-trained Vision Transformer (ViT). Our approach transforms light curves into two-dimensional images using a time-frequency representation and feeds them into a ViT pre-trained on ImageNet, followed by fine-tuning for light curve classification. We demonstrate the effectiveness of our method on a multi-survey dataset comprising light curves from the Zwicky Transient Facility (ZTF) and the Asteroid Terrestrial-impact Last Alert System (ATLAS), achieving state-of-the-art classification performance with improved robustness to survey-specific characteristics. This image-based approach, combined with transfer learning from a pre-trained ViT, offers a promising solution for automated and efficient light curve classification across diverse astronomical surveys, paving the way for rapid identification of rare and astrophysically important transient events."
http://arxiv.org/abs/2507.12490v1,Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering,"Vision Language Models (VLMs) have achieved remarkable progress in Document Visual Question Answering (DocVQA), demonstrating the ability to understand and reason about information presented in visually rich documents. However, explaining the model's reasoning process remains a challenge, especially in grounding the answer to specific spatial regions within the document image. This paper addresses the problem of generating spatially grounded explanations for DocVQA, enabling users to understand *where* the model is looking to answer a question. We propose a novel framework that integrates attention mechanisms with object detection to identify and highlight relevant regions in the document image used for prediction. Specifically, we train a model to predict both the answer and a bounding box corresponding to the supporting evidence within the document. Furthermore, we introduce a spatial consistency loss that encourages the attention weights to align with the predicted bounding box, promoting spatially coherent explanations. Experiments on benchmark DocVQA datasets demonstrate that our approach generates more accurate and faithful spatial explanations compared to existing attention-based methods, while maintaining competitive answering performance. Our method provides a crucial step towards building trustworthy and interpretable VLMs for document understanding."
http://arxiv.org/abs/2507.11540v1,Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation,"Depth estimation, the task of inferring the distance to objects in a scene, is a crucial component for various computer vision applications, including autonomous navigation, robotics, and augmented reality. While deep learning has revolutionized vision-based depth estimation, existing methods often struggle with generalization to unseen environments and require extensive task-specific training. This paper investigates recent advancements towards building a Depth Foundation Model (DFM) capable of performing robust and generalizable depth estimation across diverse scenarios. We analyze prevalent techniques, including self-supervised pre-training with large-scale unlabeled video data, transformer-based architectures for improved context modeling, and multi-task learning strategies that leverage auxiliary tasks like semantic segmentation or surface normal prediction. Furthermore, we examine the effectiveness of different pre-training objectives, such as contrastive learning and masked autoencoding, in learning robust depth representations. Our analysis reveals that pre-trained transformers, combined with appropriate fine-tuning strategies, demonstrate promising performance on various benchmark datasets, significantly reducing the domain gap and improving zero-shot generalization compared to traditional supervised methods. This work provides a comprehensive overview of the current landscape and highlights key directions for future research in developing robust and generalizable DFMs, paving the way for more reliable and adaptable vision systems."
http://arxiv.org/abs/2507.11441v2,Implementing Adaptations for Vision AutoRegressive Model,"Vision AutoRegressive (VAR) models have shown promise in generative image modeling due to their ability to capture long-range dependencies and generate high-quality samples. However, training VAR models remains computationally expensive and often requires extensive hyperparameter tuning to achieve optimal performance across diverse datasets and resolutions. This paper addresses the challenge of efficiently adapting VAR models to new datasets and resolutions without retraining from scratch. We introduce a novel adaptation strategy that leverages a pre-trained VAR model and incorporates two key components: (1) a learnable scaling layer that adjusts feature activations to accommodate varying input distributions, and (2) a resolution-aware positional encoding scheme that allows the model to generalize to different image sizes. Our experimental results on several benchmark datasets demonstrate that our adaptation method achieves comparable performance to fine-tuning the entire model while significantly reducing the computational cost and memory footprint. This adaptation strategy enables the efficient deployment of VAR models in resource-constrained environments and facilitates their application to a wider range of visual tasks."
http://arxiv.org/abs/2507.11301v1,Deteccin y Cuantificacin de Erosin Fluvial con Visin Artificial,"Fluvial erosion is a major geomorphological process shaping landscapes and impacting infrastructure near riverbanks. Accurately detecting and quantifying fluvial erosion is crucial for effective river management and hazard mitigation, but traditional methods are often time-consuming and resource-intensive. This paper presents a novel approach for automated detection and quantification of fluvial erosion using computer vision techniques. Our method combines Structure-from-Motion (SfM) photogrammetry to generate high-resolution 3D models of riverbanks with deep learning-based semantic segmentation to identify eroded areas. Specifically, we train a U-Net architecture on orthorectified imagery derived from the SfM models, enabling pixel-wise classification of eroded and non-eroded surfaces. The proposed method was tested on a case study site along a meandering river, achieving a mean Intersection-over-Union (IoU) score of 0.85 for erosion detection. The resulting erosion maps allow for accurate quantification of erosion volume and spatial distribution, providing valuable information for understanding erosion dynamics. This automated, vision-based approach offers a cost-effective and efficient solution for monitoring and managing fluvial erosion, contributing to improved riverbank stability assessment and mitigation strategies."
http://arxiv.org/abs/2507.11200v2,How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study,"Medical Vision-Language Models (MedVLMs) have shown promise in various healthcare applications, leveraging the synergy between medical images and associated textual reports. However, the rapid proliferation of MedVLM architectures and pre-training strategies has created a need for a systematic and comprehensive evaluation of their capabilities. This paper addresses the challenge of benchmarking the current state-of-the-art MedVLMs across a diverse range of clinically relevant tasks. We curate a comprehensive suite of evaluation benchmarks encompassing image classification, report generation, visual question answering, and disease prediction, utilizing both publicly available datasets and a novel, expert-annotated dataset focusing on nuanced radiological findings. We then rigorously evaluate a representative selection of prominent MedVLM architectures, including both general-purpose VLMs fine-tuned for medical tasks and models specifically pre-trained on medical data, using standardized metrics. Our results reveal significant performance variations across different tasks and datasets, highlighting the strengths and weaknesses of each model and identifying key areas for future research, such as handling subtle visual cues and generating clinically accurate reports. This comprehensive benchmarking study provides valuable insights into the current capabilities of MedVLMs and serves as a crucial resource for guiding future development and deployment efforts in this rapidly evolving field."
http://arxiv.org/abs/2507.11153v1,Assessing Color Vision Test in Large Vision-language Models,"Color perception is a fundamental aspect of human vision, crucial for understanding and interacting with the visual world. Large Vision-Language Models (LVLMs) are increasingly used for complex visual tasks, yet their color vision capabilities remain largely unexplored. This paper addresses the problem of systematically evaluating color perception in LVLMs, determining their ability to accurately identify, differentiate, and reason about colors. We introduce a novel benchmark consisting of a diverse set of color vision tests, including color naming, color discrimination, and color-based reasoning tasks, designed to probe different facets of color understanding. We then evaluate several state-of-the-art LVLMs on this benchmark, analyzing their performance across different test types and color spaces. Our experiments reveal that while LVLMs exhibit some proficiency in basic color identification, they struggle with more nuanced tasks requiring fine-grained color discrimination and contextual color understanding, often exhibiting biases towards commonly named colors. These findings highlight significant limitations in the color perception abilities of current LVLMs, underscoring the need for future research to improve their color understanding and visual reasoning capabilities."
http://arxiv.org/abs/2507.11569v1,Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?,"Vision foundation models (VFMs), pre-trained on vast datasets of natural images, have demonstrated remarkable transferability to various downstream tasks. However, their applicability to the complex and specialized domain of medical image registration remains largely unexplored. This paper investigates the out-of-the-box performance of several prominent VFMs, including DINOv2, CLIP, and MAE, for medical image registration without task-specific fine-tuning. We propose a registration pipeline leveraging feature embeddings extracted from these VFMs, followed by a similarity-based matching strategy and thin-plate spline transformation to align image pairs. We evaluate our approach on a diverse set of medical imaging modalities, including CT, MRI, and X-ray, across different anatomical regions. Our results reveal that while VFMs capture some anatomical information, their performance lags significantly behind state-of-the-art registration algorithms specifically designed for medical images, particularly in scenarios with substantial anatomical variation or noise. These findings highlight the need for specialized training or fine-tuning strategies to effectively adapt VFMs for robust and accurate medical image registration."
http://arxiv.org/abs/2507.10800v1,ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, but their computational demands hinder deployment on resource-constrained devices. Efficient inference techniques, such as early exiting, offer a potential solution, but existing methods often struggle to maintain accuracy when adapting to different computational budgets. This paper introduces ThinkingViT, a novel Matryoshka Thinking Vision Transformer architecture designed for elastic inference. ThinkingViT incorporates multiple classification heads at different depths within the transformer, each trained to perform classification based on the features extracted up to that point. Crucially, we introduce a Thinking Token that aggregates information across the transformer blocks and guides the model towards optimal exit points based on the available computational resources. Experimental results on ImageNet demonstrate that ThinkingViT achieves significant improvements in accuracy-efficiency trade-offs compared to existing early-exiting ViT architectures, particularly at low computational budgets. ThinkingViT's ability to adaptively adjust its computational cost makes it a practical solution for deploying high-performing vision models in resource-limited environments."
http://arxiv.org/abs/2507.10672v1,Vision Language Action Models in Robotic Manipulation: A Systematic Review,"Vision-Language Action Models (VLMs) are rapidly transforming robotic manipulation by enabling robots to understand and execute complex instructions in unstructured environments. This survey provides a systematic review of the current state-of-the-art in VLMs for robotic manipulation, addressing the critical need for a comprehensive understanding of existing approaches and their limitations. We categorize VLMs based on their architectural design, training methodologies, and integration with robot control frameworks, analyzing the strengths and weaknesses of each approach in handling diverse manipulation tasks. Our review examines the role of pre-trained language models, visual encoders, and action decoders in achieving robust and generalizable manipulation skills, while also highlighting the importance of effective training data and reward function design. We identify key challenges, including sim-to-real transfer, long-horizon planning, and handling of noisy or ambiguous instructions. Furthermore, we analyze the performance of different VLMs across a range of benchmark tasks, synthesizing results to identify promising directions for future research. This review offers a valuable resource for researchers and practitioners seeking to leverage the power of VLMs to develop intelligent and adaptable robotic manipulation systems."
http://arxiv.org/abs/2507.10474v1,Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation,"Human fall detection is crucial for elderly care and healthcare monitoring, but raises significant privacy concerns when using video surveillance. This paper addresses the challenge of developing a privacy-preserving fall detection system that can accurately identify falls while minimizing the risk of exposing sensitive personal information. We propose a novel multi-stage fall detection framework leveraging semi-supervised federated learning and robotic vision confirmation. Our approach employs local edge devices to perform initial fall detection using a semi-supervised learning model trained on federated data across multiple clients, ensuring data never leaves the local environment. Suspected falls trigger a robotic vision system to autonomously approach the subject and perform a more detailed analysis using privacy-preserving techniques like silhouette extraction and depth sensing, confirming the fall event without exposing identifying facial features. Experimental results on publicly available fall detection datasets and a simulated robotic environment demonstrate that our framework achieves state-of-the-art accuracy in fall detection (above 95% F1-score) while significantly reducing the risk of privacy breaches. This work offers a practical and ethical solution for deploying robust fall detection systems in sensitive environments."
http://arxiv.org/abs/2507.10318v1,Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching,"Vision foundation models (VFMs) pre-trained on large-scale datasets have demonstrated remarkable capabilities in various downstream tasks. However, a significant performance gap persists when applying these models directly to the classical problem of image feature matching, often requiring extensive fine-tuning or task-specific architectures. This paper addresses the challenge of effectively adapting VFMs for robust and accurate image feature matching without sacrificing the benefits of pre-trained knowledge. We propose a novel alignment framework, ""MatchAlign,"" which leverages contrastive learning to bridge the representational gap between VFM embeddings and traditional feature descriptors. MatchAlign employs a Siamese architecture with a learnable projection head to map VFM features into a space where geometrically corresponding points are closer than non-corresponding ones, guided by ground truth matches. Experiments on standard benchmark datasets demonstrate that MatchAlign significantly improves the matching performance of VFMs, surpassing state-of-the-art methods, including fine-tuned VFMs and specialized feature descriptors, while maintaining computational efficiency. This work offers a practical approach to harness the power of VFMs for fundamental vision tasks like image matching, paving the way for more robust and generalizable vision systems."
http://arxiv.org/abs/2507.10250v1,DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology,"Computational pathology, leveraging deep learning, holds immense promise for improving cancer diagnosis and prognosis. However, deploying complex models like Vision Transformers (ViTs) in resource-constrained clinical settings remains challenging due to their high computational cost and memory requirements. This paper addresses the problem of developing a deployable and efficient ViT-based framework for cancer diagnosis in histopathology images. We introduce DepViT-CAD, a novel architecture that combines a lightweight ViT backbone with knowledge distillation and quantization techniques. Specifically, we employ a carefully designed student network with reduced dimensionality and apply quantization-aware training to further minimize the model's footprint without significant accuracy degradation. Our experimental results on benchmark histopathology datasets, including the Breast Cancer Histology (BreakHis) and Colon Cancer Histology (CRC-VAL-HEC) datasets, demonstrate that DepViT-CAD achieves comparable diagnostic performance to larger ViT models while exhibiting significantly reduced computational complexity and memory usage. This makes DepViT-CAD a promising solution for enabling real-time and accessible cancer diagnosis in diverse clinical environments."
http://arxiv.org/abs/2507.19468v1,Back to the Features: DINO as a Foundation for Video World Models,"Video world models aim to learn representations that capture the dynamics and structure of the visual world, enabling prediction and planning. However, current approaches often struggle with disentangling complex temporal dependencies and maintaining long-term consistency. This paper introduces a novel framework that leverages the pre-trained DINO vision transformer as a foundational feature extractor for building video world models. Specifically, we propose to train a temporal encoder on top of frozen DINO features, conditioned on action sequences, to predict future feature representations. These predicted features are then decoded back into pixel space using a lightweight, learned decoder. Our experiments on benchmark video prediction datasets demonstrate that this approach, which we term ""DINO-World"", achieves state-of-the-art performance in terms of both prediction accuracy and long-term coherence, surpassing existing end-to-end trained video prediction models. By grounding video world models in strong, pre-trained visual features, we significantly improve their ability to learn robust and generalizable representations of dynamic environments."
http://arxiv.org/abs/2506.21552v1,Whole-Body Conditioned Egocentric Video Prediction,"Egocentric video prediction aims to forecast future frames from a first-person perspective, a task crucial for embodied AI agents. However, current methods often overlook the rich information provided by the whole-body pose of the agent, which significantly influences the future evolution of the scene and agent interactions. This paper addresses the challenge of incorporating whole-body pose information to improve egocentric video prediction. We introduce a novel framework, Whole-Body Conditioned Egocentric Video Prediction (WB-EVP), that explicitly models the relationship between the agent's full-body pose and the future egocentric view. WB-EVP employs a pose-conditioned variational autoencoder to learn a latent representation of the whole-body pose, which is then integrated into a video prediction network using a spatiotemporal attention mechanism. Our experiments on the EPIC-Kitchens dataset demonstrate that WB-EVP significantly outperforms state-of-the-art methods, achieving substantial improvements in prediction accuracy and visual quality, particularly in scenarios involving complex body movements and object interactions. These results highlight the importance of leveraging whole-body pose information for accurate and realistic egocentric video prediction, paving the way for more intelligent and context-aware embodied AI systems."
http://arxiv.org/abs/2506.09985v1,"V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning","Self-supervised learning is rapidly advancing video understanding, offering a path to learn directly from unlabeled data. However, current methods often struggle to generalize across diverse downstream tasks requiring not only understanding, but also prediction and planning capabilities. We introduce V-JEPA 2, an extension of Video Joint Embedding Predictive Architecture (V-JEPA) that leverages masked video modeling with a novel hierarchical prediction target. Instead of predicting raw pixel values, V-JEPA 2 predicts spatio-temporally abstracted features extracted from a pretrained self-supervised image model, enabling the model to learn representations suitable for higher-level reasoning. Furthermore, we introduce a novel planning objective during pre-training, enabling the model to predict future states conditioned on different action sequences. Experiments on a variety of benchmarks, including action recognition, video forecasting, and robotic manipulation planning, demonstrate that V-JEPA 2 significantly outperforms existing self-supervised video models. This highlights the potential of self-supervised video models to learn powerful representations capable of addressing a broader range of complex video understanding and decision-making tasks."
http://arxiv.org/abs/2504.01017v1,Scaling Language-Free Visual Representation Learning,"Self-supervised learning has demonstrated remarkable progress in learning visual representations without relying on manual annotations, often leveraging contrastive learning with large-scale datasets. However, many existing methods rely on language supervision or complex training pipelines, limiting their applicability and scalability. This work addresses the challenge of scaling language-free visual representation learning to extremely large datasets and model sizes while maintaining computational efficiency. We introduce a novel framework, termed ""Minimal Contrastive Learning"" (MiCL), which simplifies contrastive learning by minimizing the number of hyperparameters and architectural modifications. MiCL achieves this by using a simple, yet effective, instance discrimination objective coupled with a carefully designed negative sampling strategy that leverages both in-batch and memory bank approaches. Our experiments demonstrate that MiCL achieves state-of-the-art performance on ImageNet linear probing and transfer learning benchmarks compared to existing language-free methods, while significantly reducing computational costs and hyperparameter tuning efforts. These results highlight the potential of simple, scalable approaches for learning powerful visual representations from unlabeled data, paving the way for more efficient and accessible self-supervised learning paradigms."
http://arxiv.org/abs/2503.10622v2,Transformers without Normalization,"Transformers have achieved state-of-the-art performance in various sequence modeling tasks, but their reliance on normalization layers, such as LayerNorm, can introduce computational overhead and potentially limit performance, especially in resource-constrained environments or when scaling to extremely large models. This paper investigates the feasibility of training deep Transformer architectures without any normalization layers. We propose a novel initialization and scaling strategy that carefully manages the variance of activations and gradients throughout the network, ensuring stable training dynamics even in the absence of normalization. Our approach involves a modified Xavier initialization combined with a layer-specific scaling factor derived from theoretical analysis of signal propagation in deep networks. Experiments on machine translation and image classification benchmarks demonstrate that our normalization-free Transformers achieve comparable or even superior performance to their normalized counterparts, while significantly reducing computational complexity. These results suggest that normalization layers are not strictly necessary for training deep Transformers and that alternative techniques can unlock new possibilities for efficient and scalable sequence modeling."
http://arxiv.org/abs/2502.15969v3,Forgotten Polygons: Multimodal Large Language Models are Shape-Blind,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and generating image-related text, often exhibiting impressive reasoning and visual question answering skills. However, a fundamental aspect of visual perception, the accurate identification and description of geometric shapes, remains largely unexplored in the context of MLLMs. This paper investigates the surprising deficiency of current MLLMs in recognizing and reasoning about basic polygonal shapes within images. We introduce a novel benchmark, PolygonQA, comprising a diverse set of synthetic and real-world images with associated questions targeting polygon identification, counting, attribute recognition (e.g., number of sides, angles), and spatial reasoning. Our experiments reveal that state-of-the-art MLLMs struggle significantly with PolygonQA, exhibiting performance far below human-level accuracy, even for simple shapes like triangles and squares. Further analysis indicates that performance is heavily influenced by factors such as polygon occlusion, viewpoint, and the presence of distractors. These findings highlight a critical blind spot in MLLMs' visual understanding capabilities, suggesting a need for improved shape representation and reasoning mechanisms to achieve more robust and reliable multimodal intelligence."
http://arxiv.org/abs/2502.11831v1,Intuitive physics understanding emerges from self-supervised pretraining on natural videos,"Humans possess an intuitive understanding of physics, enabling them to predict the behavior of objects and environments. However, endowing machines with this capability remains a significant challenge in artificial intelligence. This paper investigates whether self-supervised pretraining on unlabeled natural videos can lead to the emergence of intuitive physics understanding in neural networks, without explicit physics supervision. We leverage a contrastive video prediction task, where the network learns to distinguish between plausible future frames and distractor frames, using a large dataset of diverse, uncurated videos. Our results demonstrate that networks pretrained in this manner exhibit a significantly improved ability to predict the physical plausibility of novel scenes and object interactions, outperforming networks trained from scratch or with image-based pretraining. Furthermore, we show that the learned representations encode information about object permanence, collision dynamics, and gravity. These findings suggest that self-supervised video pretraining offers a promising avenue for developing artificial intelligence systems with robust and generalizable physical reasoning abilities."
http://arxiv.org/abs/2412.14164v1,MetaMorph: Multimodal Understanding and Generation via Instruction Tuning,"Large language models have demonstrated remarkable capabilities in text understanding and generation. However, extending these capabilities to multimodal scenarios, where models must reason about and generate content based on both visual and textual information, remains a significant challenge. This paper addresses the problem of effectively aligning visual and textual modalities to enable robust multimodal understanding and generation through instruction tuning. We introduce MetaMorph, a novel framework that leverages a unified architecture and instruction-based training to facilitate seamless integration of visual and textual information. MetaMorph is trained on a diverse set of multimodal tasks, including visual question answering, image captioning, and visual reasoning, all formulated as instruction-following problems. Our experiments demonstrate that MetaMorph achieves state-of-the-art performance on several benchmark datasets, significantly outperforming existing multimodal models in both zero-shot and fine-tuning settings, particularly when faced with complex instructions. These results highlight the potential of instruction tuning as a powerful paradigm for building versatile and generalizable multimodal AI systems."
http://arxiv.org/abs/2412.10925v1,Video Representation Learning with Joint-Embedding Predictive Architectures,"Video representation learning aims to capture the complex temporal dynamics and semantic content of visual sequences. A significant challenge lies in learning representations that are both informative and generalizable across diverse video understanding tasks. This paper addresses the problem of learning effective video representations by proposing a novel Joint-Embedding Predictive Architecture (JEPA) tailored for video data. Our approach learns representations by predicting future video frames (or latent codes thereof) from past observations within a joint embedding space. Specifically, we employ a transformer-based architecture to encode both past and future video segments and learn a mapping that aligns these embeddings, encouraging the model to capture essential temporal dependencies. We evaluate our method on a suite of video action recognition, retrieval, and downstream transfer learning benchmarks. Our experiments demonstrate that JEPA achieves state-of-the-art or competitive performance compared to existing self-supervised and supervised methods, particularly in few-shot transfer scenarios. This demonstrates the effectiveness of our joint-embedding predictive approach for learning robust and generalizable video representations."
http://arxiv.org/abs/2412.07169v4,Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation,"Dropout, a widely used regularization technique in deep learning, can be adapted during inference to estimate model uncertainty via Monte Carlo dropout. However, standard dropout applies a fixed rate across all layers, potentially hindering the model's ability to effectively capture nuanced uncertainty information. This paper addresses the problem of suboptimal and fixed dropout rates by introducing ""Rate-In,"" a novel approach to adaptively learn dropout rates driven by information gain. Rate-In dynamically adjusts dropout rates at each layer based on the mutual information between the layer's activations and the model's output, promoting higher dropout rates in layers with less relevant information. Experiments on image classification and semantic segmentation benchmarks demonstrate that Rate-In consistently improves uncertainty estimation performance, leading to better calibration and out-of-distribution detection compared to fixed-rate dropout and other adaptive dropout strategies. This information-driven approach provides a principled method for optimizing dropout rates, ultimately leading to more reliable and trustworthy deep learning systems."
http://arxiv.org/abs/2412.03572v2,Navigation World Models,"Navigation in complex, partially observable environments remains a significant challenge for autonomous agents. Traditional approaches often struggle with long-horizon planning and adapting to unforeseen circumstances due to reliance on either precise environmental maps or reactive policies. This paper addresses the problem of learning robust and generalizable navigation policies by leveraging the concept of world models. We introduce Navigation World Models (NWMs), a novel framework that integrates a learned latent representation of the environment with a policy trained within this learned space. NWMs consist of a variational autoencoder for encoding visual observations, a recurrent state space model for predicting future states and rewards, and a policy network trained via reinforcement learning using trajectories generated within the learned world model. Experiments conducted in visually realistic and procedurally generated environments demonstrate that NWMs outperform baseline methods in terms of navigation success rate, sample efficiency, and generalization to novel environments. The ability of NWMs to learn and reason about the environment in a compact, latent space provides a powerful framework for developing more intelligent and adaptable navigation agents."
http://arxiv.org/abs/2411.17662v2,RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training,"Accurate estimation of robot pose and joint angles is crucial for effective robot manipulation and interaction in dynamic environments. However, current vision-based methods often struggle with occlusions, noisy sensor data, and domain gaps between training and deployment scenarios. This paper introduces RoboPEPP, a novel approach for robust robot pose and joint angle estimation through Embedding Predictive Pre-training. RoboPEPP leverages a self-supervised pre-training strategy to learn a rich, task-agnostic embedding space from unlabeled robot interaction videos by predicting future embeddings given past observations. This pre-trained embedding space is then fine-tuned on a smaller, labeled dataset for the specific task of pose and joint angle estimation, using a lightweight prediction head. Experiments on both simulated and real-world robot manipulation tasks demonstrate that RoboPEPP achieves significantly improved accuracy and robustness compared to state-of-the-art methods, particularly in challenging scenarios with occlusions and domain shifts, achieving a 20% reduction in pose estimation error. RoboPEPP offers a promising pathway towards more reliable and adaptable vision-based robot control systems."
http://arxiv.org/abs/2411.15931v2,Improving Pre-trained Self-Supervised Embeddings Through Effective Entropy Maximization,"Self-supervised learning (SSL) has emerged as a powerful paradigm for learning visual representations from unlabeled data, often relying on pretext tasks to induce meaningful embeddings. However, pre-trained SSL models can sometimes suffer from feature collapse and sub-optimal representation diversity, limiting their downstream transfer performance. This paper addresses the challenge of enhancing the quality of pre-trained self-supervised embeddings by promoting greater entropy in the feature space. We introduce a novel entropy maximization strategy, termed Contrastive Entropy Regularization (CER), which leverages mini-batch contrastive learning to encourage uniform distribution of embeddings on the hypersphere while simultaneously maintaining feature discriminability. CER introduces a regularizer that penalizes deviations from a uniform distribution in the embedding space, effectively maximizing entropy without sacrificing the benefits of contrastive learning. Experiments on multiple benchmark datasets, including ImageNet and CIFAR-100, demonstrate that CER significantly improves the transfer performance of pre-trained SSL models, achieving gains of up to 5% in linear evaluation accuracy compared to baseline methods. These results highlight the importance of entropy maximization in learning robust and generalizable visual representations using self-supervision."
http://arxiv.org/abs/2410.21256v2,Multi-modal AI for comprehensive breast cancer prognostication,"Breast cancer prognostication, crucial for personalized treatment planning, traditionally relies on clinicopathological factors and single-modal data analysis. However, the complex and heterogeneous nature of breast cancer necessitates integrative approaches for improved accuracy. This paper addresses the challenge of comprehensively predicting breast cancer prognosis by leveraging multi-modal data, including histopathological images, genomic profiles, and clinical data. We propose a novel multi-modal AI framework that integrates these diverse data sources using a hierarchical attention mechanism. This mechanism learns to weight the contribution of each modality and its constituent features, enabling the model to capture complex inter-modal relationships and identify predictive biomarkers. Our experiments on a large, multi-institutional dataset demonstrate that the proposed framework significantly outperforms existing single-modal and multi-modal approaches, achieving a C-index of 0.82 for predicting distant metastasis-free survival, a substantial improvement over the baseline C-index of 0.75 achieved using clinicopathological features alone. This multi-modal AI approach offers a powerful tool for improving the precision and personalization of breast cancer prognostication, potentially leading to more effective treatment strategies and improved patient outcomes."
http://arxiv.org/abs/2408.11208v3,PooDLe: Pooled and dense self-supervised learning from naturalistic videos,"Self-supervised learning (SSL) from videos offers a promising avenue to learn visual representations without relying on expensive manual annotations. However, existing methods often struggle to capture both global contextual information and fine-grained details necessary for robust downstream performance. This paper introduces PooDLe, a novel self-supervised learning framework that leverages both pooled and dense prediction tasks to learn comprehensive video representations. PooDLe employs a contrastive objective with instance discrimination at two distinct levels: a global level utilizing temporally pooled video features and a dense, frame-level representation. This dual approach encourages the model to learn both high-level semantic relationships and detailed local features, leading to a more complete understanding of the video content. Experiments on action recognition (Kinetics-400) and video retrieval (MSR-VTT) demonstrate that PooDLe outperforms state-of-the-art SSL methods, achieving significant improvements in downstream task accuracy. These results highlight the effectiveness of combining global and dense prediction tasks for learning richer and more transferable video representations."
http://arxiv.org/abs/2407.18134v2,$\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs,"Contrastive learning has emerged as a powerful technique for self-supervised representation learning, relying on instance discrimination to learn feature embeddings. However, standard contrastive losses often treat all negative samples equally, neglecting the varying degrees of similarity between samples and potentially hindering the learning process. We address this limitation by introducing the $\mathbb{X}$-Sample Contrastive (XSC) loss, which leverages a sample similarity graph to dynamically weight negative samples based on their relationship to the anchor. XSC constructs a graph where nodes represent samples and edge weights reflect feature similarity, then uses this graph to modulate the contribution of each negative sample to the contrastive loss. This allows the model to focus on challenging negatives that are semantically similar to the anchor, while down-weighting dissimilar negatives that provide less informative gradients. Experimental results on standard benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that XSC consistently outperforms state-of-the-art contrastive learning methods. This improved weighting scheme leads to more robust and generalizable feature representations, enhancing downstream task performance."
http://arxiv.org/abs/2406.16860v2,"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs","Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and generating content across different modalities, primarily leveraging pre-trained vision and language models. However, the opacity surrounding model architectures, training data, and evaluation methodologies hinders reproducibility and in-depth analysis of their emergent abilities. To address this, we introduce Cambrian-1, a fully open-source, vision-centric MLLM built using entirely publicly available data and models. Cambrian-1 employs a novel architecture incorporating a CLIP ViT-L/14 vision encoder, a lightweight trainable adapter, and the Llama-2 7B language model. We meticulously document our pre-training and fine-tuning procedures, including dataset creation and hyperparameter optimization, and provide a comprehensive evaluation across a range of established multimodal benchmarks. Cambrian-1 achieves competitive performance compared to similarly sized, partially open MLLMs, demonstrating the feasibility of building capable MLLMs with full transparency. This work provides a valuable resource for the research community, enabling deeper investigation into the inner workings of MLLMs and fostering further innovation in the field."
http://arxiv.org/abs/2406.09366v1,Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations,"Manifold learning has proven effective in representing high-dimensional data in lower-dimensional spaces while preserving underlying geometric structures. However, the concept of Maximum Manifold Capacity (MMC), which aims to find representations that maximize the volume spanned by the data manifold, remains relatively unexplored and its practical benefits are not fully understood. This paper addresses the challenge of effectively utilizing MMC representations for improved performance in downstream tasks. We propose a novel approach that combines MMC with a contrastive learning objective, encouraging embeddings to not only maximize manifold volume but also to cluster similar data points together, thereby improving discriminative power. Furthermore, we introduce a regularized optimization strategy to prevent degenerate solutions and enhance the stability of the MMC learning process. Experiments on benchmark datasets demonstrate that our method yields superior performance compared to existing manifold learning techniques, particularly in classification and clustering tasks. This improved understanding and utilization of MMC representations opens new avenues for developing more effective and robust representation learning algorithms."
http://arxiv.org/abs/2405.18418v3,Hierarchical World Models as Visual Whole-Body Humanoid Controllers,"Model-free reinforcement learning offers a promising avenue for training complex humanoid controllers, but struggles with sample efficiency and generalization, particularly for whole-body control tasks involving high-dimensional action spaces and visual inputs. This paper addresses the challenge of learning visually-guided whole-body humanoid control by leveraging hierarchical world models to improve sample efficiency and facilitate robust policy learning. We propose a novel framework that integrates a hierarchical variational autoencoder (HVAE) to learn a compact latent representation of visual observations and a temporal dynamics model to predict future latent states. This learned world model is then used to train a hierarchical policy consisting of a high-level planner operating in the latent space and a low-level controller mapping latent states to motor commands. Experiments on challenging locomotion and manipulation tasks demonstrate that our approach significantly outperforms state-of-the-art model-free and flat world model-based methods in terms of sample efficiency and task completion rate. The learned controllers exhibit robust performance in unseen environments and demonstrate the potential of hierarchical world models for visual humanoid control."
http://arxiv.org/abs/2405.10292v3,Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning,"Large Vision-Language Models (VLMs) demonstrate remarkable capabilities in understanding and generating text grounded in visual information, yet their potential for acting as decision-making agents within complex environments remains largely unexplored. This paper addresses the challenge of effectively adapting pre-trained VLMs for interactive decision-making tasks, where agents must learn to navigate environments and achieve goals based on visual observations and textual instructions. We propose a novel reinforcement learning (RL) framework that fine-tunes a frozen VLM backbone by training a lightweight, task-specific adapter network. This adapter learns to map VLM embeddings to action distributions, guided by a reward signal that reflects task success. Furthermore, we incorporate a curriculum learning strategy that progressively increases the complexity of the tasks presented to the agent, facilitating more efficient and robust learning. Our experiments across a range of simulated environments demonstrate that our approach significantly outperforms existing methods, achieving superior performance in terms of task completion rate and sample efficiency. This work highlights the potential of leveraging pre-trained VLMs as powerful decision-making agents through strategic fine-tuning with reinforcement learning."
http://arxiv.org/abs/2405.05012v2,The Entropy Enigma: Success and Failure of Entropy Minimization,"Entropy minimization (EM) is a widely used technique in unsupervised and semi-supervised learning, aiming to cluster data by encouraging confident predictions. Despite its conceptual simplicity and empirical success in various applications, the theoretical underpinnings and limitations of EM remain an active area of investigation. This paper delves into the ""Entropy Enigma,"" exploring why EM sometimes succeeds spectacularly and other times fails dramatically, leading to trivial or degenerate solutions. We propose a novel analytical framework based on the interplay between the data distribution's intrinsic structure and the model's capacity to capture it. Specifically, we analyze the evolution of class probabilities during EM training and identify conditions under which the optimization process converges to desirable, high-entropy solutions versus collapsing into low-entropy, uninformative states. Our analysis reveals that the effectiveness of EM is highly sensitive to the alignment between the model's inductive bias and the data's cluster separability, with misaligned models prone to degenerate solutions. Experiments on benchmark datasets demonstrate the validity of our theoretical findings and provide practical guidelines for applying EM effectively. This work offers a more nuanced understanding of EM, paving the way for developing more robust and reliable unsupervised learning algorithms."
http://arxiv.org/abs/2405.01469v1,Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning,"Medical image analysis, particularly X-ray interpretation, is crucial for timely diagnosis but suffers from limitations due to data scarcity and the need for expert knowledge. This paper addresses the challenge of developing robust and generalizable X-ray analysis models that are less reliant on extensive labeled datasets and better aligned with human radiological practice. We propose a novel holistic self-supervised learning (SSL) framework that leverages multiple pretext tasks inspired by radiological workflows, including image reconstruction, anatomical structure prediction, and pathology-aware contrastive learning. The framework integrates these tasks within a unified architecture, enabling the model to learn rich, human-interpretable representations from unlabeled X-ray images. Our experiments on multiple publicly available X-ray datasets demonstrate that models pre-trained with our holistic SSL framework significantly outperform those trained with standard SSL approaches and achieve comparable or superior performance to fully supervised models, especially in low-data regimes. This advancement enables the development of more effective and accessible AI-driven tools for X-ray analysis, ultimately improving diagnostic accuracy and patient outcomes."
http://arxiv.org/abs/2404.09991v1,EgoPet: Egomotion and Interaction Data from an Animal's Perspective,"Understanding animal behavior and their interaction with the environment requires detailed knowledge of their movement and sensory experiences. However, collecting high-quality egocentric data from animals remains a significant challenge due to limitations in wearable technology and the difficulties of accurately tracking animal pose and interactions in complex environments. We introduce EgoPet, a novel dataset containing synchronized egomotion, visual, and interaction data collected from domestic cats. Our setup utilizes a custom-designed, lightweight collar equipped with an IMU and a wide-angle camera, along with a network of ultra-wideband (UWB) anchors to provide precise 3D localization. We also incorporate object detection and pose estimation to characterize interactions with humans and objects within the cat's environment. Analysis of the EgoPet dataset reveals distinct movement patterns associated with different activities, such as hunting, playing, and resting, and demonstrates the feasibility of using egocentric vision to predict social interactions. This dataset provides a valuable resource for developing algorithms for animal behavior analysis, activity recognition, and understanding the perceptual world of animals."
http://arxiv.org/abs/2403.00504v1,Learning and Leveraging World Models in Visual Representation Learning,"Visual representation learning aims to extract meaningful and transferable features from images, often relying on large datasets and complex architectures. However, these methods often lack an explicit understanding of the underlying dynamics of the visual world. This work addresses the problem of incorporating world model learning into visual representation learning to improve the quality and robustness of learned representations. We propose a novel framework that jointly learns a latent world model capable of predicting future visual states and a representation learning module that leverages the world model's latent space and predictive capabilities. Specifically, we introduce a contrastive loss that encourages alignment between the learned visual representations and the latent states of the world model, alongside a reconstruction loss that ensures the representations capture essential visual information. Experiments on several benchmark datasets demonstrate that our approach yields significant improvements in downstream task performance compared to state-of-the-art self-supervised and supervised methods, particularly in scenarios involving noisy or occluded data. Our results suggest that incorporating world model learning provides a powerful inductive bias for learning more robust and generalizable visual representations."
http://arxiv.org/abs/2402.11337v1,Learning by Reconstruction Produces Uninformative Features For Perception,"Self-supervised learning has shown promise in learning representations from unlabeled data, with reconstruction-based methods being a popular paradigm. However, the features learned by these methods often underperform compared to other self-supervised approaches or supervised learning when applied to downstream perceptual tasks. This paper investigates why learning by reconstruction leads to uninformative features for perception. We hypothesize that reconstruction objectives encourage the model to prioritize pixel-level details and statistical dependencies irrelevant to high-level semantic understanding. To demonstrate this, we systematically analyze the feature spaces learned by autoencoders and variational autoencoders, showing that they exhibit a bias towards representing low-level image statistics and are less sensitive to object identity compared to features learned with contrastive methods. Furthermore, we propose a simple modification to the reconstruction objective that penalizes the encoding of high-frequency information, leading to improved performance on downstream classification tasks. Our findings suggest that while reconstruction provides a signal for learning, its inherent focus on low-level details hinders the development of robust and semantically meaningful features for perception."
http://arxiv.org/abs/2404.08471v1,Revisiting Feature Prediction for Learning Visual Representations from Video,"Learning visual representations from unlabeled video by predicting future features has shown promise, offering a scalable alternative to manual annotation. However, the effectiveness of feature prediction hinges on the choice of features and the prediction architecture, with prior works often relying on hand-crafted features or complex recurrent networks, limiting scalability and interpretability. This work revisits feature prediction, arguing that carefully chosen features coupled with a simple, non-recurrent prediction architecture can yield competitive performance. We propose a novel feature prediction framework that leverages self-supervised contrastive learning to extract semantically meaningful features, which are then predicted using a lightweight convolutional network. We demonstrate that predicting these learned features, as opposed to commonly used hand-crafted features like optical flow, significantly improves downstream task performance across various action recognition benchmarks. Our approach achieves state-of-the-art results among non-recurrent feature prediction methods, highlighting the importance of learned feature representations and providing a more efficient and interpretable pathway for video representation learning."
http://arxiv.org/abs/2401.06209v2,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in integrating and reasoning across various data modalities, particularly text and images. However, their visual understanding abilities often lag behind their language proficiency, leading to unexpected failures in seemingly simple visual tasks. This paper investigates the visual shortcomings of MLLMs by introducing a novel diagnostic benchmark, VisFail, specifically designed to expose common visual reasoning errors. VisFail comprises a diverse set of image-text pairs that probe object recognition, spatial reasoning, attribute understanding, and fine-grained visual discrimination. We systematically evaluate several state-of-the-art MLLMs on VisFail, revealing significant performance gaps compared to human baselines and specialized vision models. Our analysis identifies specific error patterns, such as difficulty with occluded objects, subtle color variations, and complex spatial relationships, highlighting the limitations of current MLLM architectures and training paradigms. The VisFail benchmark and our findings provide valuable insights for future research aimed at improving the visual grounding and reasoning capabilities of MLLMs, ultimately leading to more robust and reliable multimodal AI systems."
http://arxiv.org/abs/2310.04496v2,URLOST: Unsupervised Representation Learning without Stationarity or Topology,"Unsupervised representation learning aims to discover meaningful data features without explicit labels. Existing methods often implicitly assume data stationarity or rely on topological priors, limiting their applicability to complex, non-stationary data distributions arising in real-world scenarios like video and robotics. We address the challenge of unsupervised representation learning for data lacking stationarity or a well-defined topology. Our method, URLOST (Unsupervised Representation Learning without Stationarity or Topology), leverages a novel combination of contrastive learning and a self-supervised temporal consistency objective, designed to be invariant to non-stationarity. Specifically, we train a network to discriminate between temporally adjacent frames while simultaneously enforcing consistency in the learned representations across time, without imposing any assumptions about the underlying data structure. Experiments on diverse datasets, including synthetic non-stationary processes and real-world video sequences, demonstrate that URLOST outperforms state-of-the-art unsupervised methods by a significant margin, particularly when data violates stationarity and topological assumptions. This work provides a robust framework for unsupervised representation learning applicable to a broader range of complex, real-world data."
http://arxiv.org/abs/2308.00566v2,Stochastic positional embeddings improve masked image modeling,"Masked image modeling (MIM) has emerged as a powerful self-supervised learning paradigm for visual representation learning, enabling models to learn contextual information by reconstructing masked portions of an image. However, the fixed positional embeddings commonly used in transformers can limit the model's ability to generalize to unseen object arrangements and spatial relationships, particularly when dealing with high masking ratios. We introduce Stochastic Positional Embeddings (SPE), a novel approach that injects noise into the positional embeddings during pre-training. SPE encourages the model to rely less on absolute positional information and more on learning relative relationships between image patches, thereby improving robustness to positional variations. Specifically, we add Gaussian noise to the positional embeddings and dynamically adjust the noise scale based on the masking ratio. Our experiments demonstrate that SPE significantly improves the performance of masked autoencoders on downstream tasks such as image classification and object detection, especially when pre-training is performed with high masking ratios. This work highlights the importance of positional encoding strategies in MIM and offers a simple yet effective method to enhance the generalization capabilities of learned visual representations."
http://arxiv.org/abs/2307.12698v1,MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features,"Self-supervised learning has shown promise in learning visual representations from unlabeled video data, often focusing on either motion or content understanding. However, many existing approaches struggle to effectively capture the intricate interplay between motion and content cues present in dynamic scenes. In this paper, we address the challenge of learning joint motion and content representations from videos without relying on manual annotations. We introduce MC-JEPA, a novel Joint-Embedding Predictive Architecture designed to predict masked regions of both motion and content features extracted from video clips. Specifically, we employ separate encoders for motion (optical flow) and content (RGB frames) and train them to predict representations of masked regions from the other modality, fostering a shared understanding of the underlying visual dynamics. Experiments on benchmark action recognition datasets demonstrate that MC-JEPA achieves state-of-the-art performance compared to existing self-supervised methods, particularly in low-data regimes, highlighting the benefits of learning joint motion and content features. This work provides a powerful self-supervised pre-training strategy for video understanding tasks, facilitating improved performance and generalization in downstream applications."
http://arxiv.org/abs/2306.13292v2,Variance-Covariance Regularization Improves Representation Learning,"Self-supervised representation learning has achieved remarkable success in recent years by learning useful feature representations without relying on explicit labels. However, existing methods often suffer from feature redundancy and instability, hindering the quality and robustness of the learned representations. This paper addresses the problem of learning more decorrelated and stable representations to improve downstream task performance. We propose Variance-Covariance Regularization (VCR), a novel regularization technique that explicitly encourages both variance maximization and covariance minimization in the learned feature space. VCR leverages batch statistics to estimate the variance and covariance matrices and incorporates them as a penalty term in the training objective. Experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that VCR consistently improves the performance of various self-supervised learning algorithms, achieving state-of-the-art results in several cases. This work highlights the importance of variance-covariance regularization for learning high-quality and robust feature representations, offering a simple yet effective approach to advance the field of self-supervised learning."
http://arxiv.org/abs/2304.12210v2,A Cookbook of Self-Supervised Learning,"Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning, enabling models to learn from unlabeled data and achieve remarkable performance in various downstream tasks. However, the rapidly expanding landscape of SSL algorithms and their intricate implementation details can be daunting for practitioners seeking to apply these techniques effectively. This paper addresses the need for a practical guide to SSL by presenting a comprehensive ""cookbook"" that demystifies the implementation and application of popular SSL methods. We provide step-by-step instructions, code snippets, and practical advice for implementing and tuning various SSL algorithms, including contrastive learning, masked autoencoding, and clustering-based approaches. We also analyze the impact of different design choices, such as data augmentations, architecture selection, and training strategies, on the performance of SSL models across diverse datasets and downstream tasks. Through extensive experiments, we demonstrate the effectiveness of our cookbook in enabling researchers and practitioners to quickly and efficiently leverage SSL for their specific applications, achieving results comparable to or exceeding those reported in the original papers with significantly reduced implementation effort. This work facilitates the wider adoption of SSL by providing accessible and actionable guidance for navigating the complexities of this rapidly evolving field."
http://arxiv.org/abs/2304.03977v1,EMP-SSL: Towards Self-Supervised Learning in One Training Epoch,"Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations from unlabeled data, often requiring extensive pre-training. However, the computational cost associated with multi-epoch training remains a significant bottleneck for broader adoption. This paper addresses the challenge of achieving effective self-supervised learning within a single training epoch. We introduce EMP-SSL, a novel framework that leverages Expectation-Maximization principles to iteratively refine pseudo-labels and update model parameters within a single pass through the data. EMP-SSL alternates between an ""Expectation"" step, where high-confidence pseudo-labels are generated based on the current model state, and a ""Maximization"" step, where the model is updated to align its predictions with these refined pseudo-labels using a contrastive loss. Experiments on benchmark datasets such as CIFAR-10, CIFAR-100, and STL-10 demonstrate that EMP-SSL achieves comparable, and in some cases superior, performance to multi-epoch SSL methods, while drastically reducing training time. This work significantly advances the efficiency of self-supervised learning, making it more accessible and applicable to resource-constrained environments."
http://arxiv.org/abs/2302.10283v2,Self-supervised learning of Split Invariant Equivariant representations,"Self-supervised learning has shown remarkable progress in learning visual representations without relying on manual annotations. However, existing methods often struggle to learn representations that are both invariant to nuisance variations and equivariant to meaningful transformations, especially when these variations are spatially intertwined or ""split"" across different regions of an image. This paper addresses the challenge of learning representations that are invariant to variations in one spatial region while being equivariant to transformations applied to another. We propose a novel self-supervised framework, Split Invariant Equivariant Learning (SIEL), that utilizes a contrastive objective with carefully designed data augmentations applied to different regions of the input image. SIEL encourages the model to learn invariant features from one region while simultaneously learning equivariant features from another, effectively disentangling the effects of different transformations and variations. Experimental results on benchmark datasets demonstrate that SIEL significantly outperforms existing self-supervised methods in downstream tasks requiring both invariance and equivariance, such as object recognition under occlusion and viewpoint changes. This work paves the way for learning more robust and interpretable visual representations by explicitly addressing the challenge of split invariance and equivariance."
http://arxiv.org/abs/2302.01647v2,Blockwise Self-Supervised Learning at Scale,"Self-supervised learning (SSL) has emerged as a powerful paradigm for pre-training visual representations without human annotations, often relying on pretext tasks like image inpainting or instance discrimination. However, current SSL methods struggle to effectively capture long-range dependencies and global contextual information, limiting their performance on downstream tasks requiring holistic scene understanding. This paper introduces a novel Blockwise Self-Supervised Learning (BSSL) framework designed to scale to large images by operating on image blocks instead of entire images. BSSL randomly masks out blocks within an image and trains a transformer-based encoder to reconstruct the masked blocks from the remaining visible blocks, effectively forcing the model to learn relationships between distant regions. We demonstrate that pre-training with BSSL on a large unlabelled dataset yields significant improvements over existing SSL methods on various downstream tasks, including object detection and semantic segmentation, achieving state-of-the-art results on ImageNet classification with linear probing. The blockwise approach allows for efficient training on high-resolution images, leading to learned representations with enhanced contextual awareness, thereby advancing the state-of-the-art in self-supervised visual representation learning."
http://arxiv.org/abs/2301.08243v3,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,"Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning, enabling models to learn from unlabeled data by leveraging inherent data structure. However, many existing SSL approaches rely on contrastive learning, which can be computationally expensive and sensitive to the choice of negative samples. This paper addresses the challenge of efficient and effective self-supervised representation learning by introducing a Joint-Embedding Predictive Architecture (JEPA) that avoids contrastive objectives. Our method learns representations by predicting different views of an image from each other within a shared embedding space. Specifically, we train an encoder to map an image to a latent representation, and then train a predictor to reconstruct a different, augmented view of the same image from this latent representation. We demonstrate that this approach learns robust and transferable features, achieving competitive performance on downstream tasks such as image classification and object detection, surpassing several contrastive SSL methods while using fewer computational resources. Our results highlight the potential of predictive architectures in self-supervised learning and offer a promising direction for developing more efficient and scalable representation learning techniques."
http://arxiv.org/abs/2212.13350v2,A Generalization of ViT/MLP-Mixer to Graphs,"Graph Neural Networks (GNNs) have become a dominant paradigm for representation learning on graph-structured data, yet their reliance on message passing can limit their expressive power and ability to capture long-range dependencies. This paper addresses the problem of extending the architectural principles of vision transformers (ViT) and MLP-Mixers, which excel in capturing global relationships in images, to the more general domain of graph-structured data. We introduce Graph Mixer Networks (G-Mixer), a novel architecture that leverages learnable patch extraction to identify meaningful subgraphs, followed by a series of mixing layers that alternate between mixing features within each patch and mixing features across different patches, thereby capturing both local and global interactions without relying on message passing. Our experiments on a diverse set of graph benchmark datasets demonstrate that G-Mixer achieves competitive or superior performance compared to state-of-the-art GNNs, particularly on tasks requiring reasoning over long-range dependencies. These results highlight the potential of transformer-inspired architectures to advance graph representation learning beyond the limitations of traditional message-passing schemes."
http://arxiv.org/abs/2211.01340v3,POLICE: Provably Optimal Linear Constraint Enforcement for Deep Neural Networks,"Deep neural networks (DNNs) have achieved remarkable success in various applications, but their vulnerability to adversarial attacks and tendency to produce unreliable outputs in safety-critical scenarios raises significant concerns. Ensuring DNNs adhere to specific linear constraints is crucial for robust and reliable performance. However, existing constraint enforcement techniques often lack theoretical guarantees regarding optimality and constraint satisfaction, especially when dealing with complex network architectures. We introduce POLICE, a novel framework for **P**rovably **O**ptimal **L**inear **I**nequality **C**onstraint **E**nforcement** for deep neural networks. POLICE leverages a Lagrangian relaxation approach to reformulate the constrained optimization problem into an unconstrained saddle-point problem, which can be efficiently solved using gradient-based methods. Crucially, we provide theoretical guarantees that POLICE converges to a solution that provably satisfies the linear constraints while minimizing the original loss function, under mild assumptions. Empirical evaluations on benchmark datasets demonstrate that POLICE significantly outperforms existing methods in terms of constraint satisfaction, accuracy, and robustness against adversarial attacks, while maintaining comparable computational efficiency. This work provides a principled and practical approach for building trustworthy and reliable DNNs that adhere to predefined safety specifications."
http://arxiv.org/abs/2210.16782v1,Unsupervised Learning of Structured Representations via Closed-Loop Transcription,"Unsupervised representation learning aims to discover meaningful structures in data without relying on manual annotations. However, many existing methods struggle to learn representations that explicitly capture the underlying compositional relationships within complex data. In this paper, we address the problem of learning structured representations from unlabeled data by introducing a novel closed-loop transcription framework. Our approach leverages a transcription network to map observed data into a latent structured representation, which is then used by a reconstruction network to generate a corresponding output. Crucially, we close the loop by comparing the reconstructed output with the original input, encouraging the transcription network to learn representations that are both informative and invertible. Experimental results on synthetic and real-world datasets demonstrate that our method learns disentangled and compositional representations that outperform state-of-the-art unsupervised learning techniques in downstream tasks such as few-shot classification and novel composition generalization. This work provides a new perspective on unsupervised structured representation learning, offering a pathway towards more interpretable and robust AI systems."
http://arxiv.org/abs/2210.04135v3,VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment,"Vision-language pre-training has achieved remarkable success in various multimodal tasks, yet often overlooks the crucial role of fine-grained correspondence between visual regions and textual words. This paper addresses the challenge of learning explicit alignment between local visual features and textual tokens in a weakly-supervised setting. We introduce VoLTA, a Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment, which incorporates a novel alignment module that learns to associate visual regions with relevant textual words based on image-text matching scores. This alignment module is trained end-to-end using only image-text pairs, without requiring explicit bounding box annotations or word-level alignments. Experimental results on downstream tasks such as visual question answering, image retrieval, and visual reasoning demonstrate that VoLTA significantly improves performance compared to existing methods, particularly in tasks requiring fine-grained understanding. By fostering a stronger connection between visual and textual representations, VoLTA advances the state-of-the-art in vision-language understanding and provides a more interpretable framework for multimodal analysis."
http://arxiv.org/abs/2210.02885v3,RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank,"Self-supervised learning (SSL) has revolutionized representation learning, enabling the training of powerful feature extractors without manual labels. However, predicting the downstream task performance of different SSL methods remains challenging and often relies on computationally expensive full finetuning. This paper addresses the problem of efficiently assessing the quality of self-supervised representations for downstream tasks *without* extensive finetuning. We introduce RankMe, a novel approach that leverages the rank distribution of feature similarities within a dataset to estimate downstream performance. RankMe computes a set of rank-based metrics from the self-supervised embeddings, capturing properties like feature separability and cluster structure. We demonstrate a strong correlation between RankMe scores and downstream classification accuracy across a diverse set of SSL methods and datasets, achieving state-of-the-art performance in downstream task prediction compared to existing proxy metrics. RankMe offers a computationally efficient and effective method for evaluating and selecting self-supervised representations, accelerating the development and deployment of SSL models for real-world applications."
http://arxiv.org/abs/2210.01571v1,VICRegL: Self-Supervised Learning of Local Visual Features,"Self-supervised learning (SSL) has shown great promise in learning powerful visual representations from unlabeled data, often relying on global image-level constraints. However, learning effective *local* visual features directly through SSL remains a challenge, hindering performance on downstream tasks requiring fine-grained understanding. We introduce VICRegL, a self-supervised learning framework specifically designed to learn high-quality local features by adapting the Variance-Invariance-Covariance Regularization (VICReg) principle to operate on patch-level embeddings. VICRegL enforces invariance between different views of image patches, while simultaneously encouraging variance within the patch embeddings and minimizing redundancy between their dimensions. Our experiments demonstrate that VICRegL significantly outperforms existing SSL methods on a variety of downstream tasks requiring local feature understanding, including semantic segmentation, object detection, and instance retrieval, achieving state-of-the-art results on several benchmarks. VICRegL provides a powerful and versatile approach for learning robust and discriminative local visual features without relying on manual annotations."
http://arxiv.org/abs/2209.15261v2,Minimalistic Unsupervised Learning with the Sparse Manifold Transform,"Unsupervised learning aims to discover underlying structure in data without relying on explicit labels. However, many existing methods require careful hyperparameter tuning or are computationally expensive, hindering their applicability in resource-constrained environments. This paper addresses the challenge of performing effective unsupervised learning with minimal computational overhead and hyperparameter sensitivity. We introduce the Sparse Manifold Transform (SMT), a novel unsupervised learning framework that leverages sparse coding and manifold learning principles. SMT first constructs a sparse representation of the input data, then estimates local tangent spaces on the resulting sparse codes. Finally, it learns a global embedding that preserves the relationships captured by these local tangent spaces. Experiments on several benchmark datasets demonstrate that SMT achieves competitive or superior performance compared to existing unsupervised learning algorithms, while requiring significantly fewer hyperparameters and computational resources. SMT provides a powerful and accessible tool for unsupervised representation learning, particularly beneficial in scenarios with limited computational power or expertise."
http://arxiv.org/abs/2206.10698v2,TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning,"Self-supervised learning (SSL) has emerged as a powerful paradigm for learning visual representations without manual annotations. Current contrastive SSL methods often rely on contrasting different augmented views of the same image, but struggle to explicitly enforce both invariance to nuisance transformations and covariance to meaningful variations. We address this limitation by introducing TiCo, a novel contrastive learning framework that explicitly promotes transformation invariance and covariance. TiCo achieves this by incorporating two key components: (1) a transformation alignment loss that encourages feature invariance across different transformations of the same image, and (2) a feature decorrelation loss that facilitates feature covariance by minimizing the redundancy between feature dimensions. These two losses are jointly optimized with a standard contrastive loss. Extensive experiments on benchmark datasets demonstrate that TiCo significantly outperforms state-of-the-art SSL methods on both linear evaluation and transfer learning tasks. TiCo's ability to learn more robust and discriminative representations marks a significant advancement in self-supervised visual representation learning."
http://arxiv.org/abs/2206.08954v2,Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning,"Self-supervised learning (SSL) has revolutionized representation learning, achieving remarkable success in various computer vision tasks. However, the underlying mechanisms that contribute to the effectiveness of different SSL methods remain a subject of ongoing investigation. This paper addresses the question of whether the seemingly disparate SSL approaches share common characteristics responsible for their success. We propose that the ""bag of image patch embedding"" paradigm, where images are represented by aggregated embeddings of their constituent patches, is a crucial element behind the success of many SSL methods. We demonstrate that methods like contrastive learning and masked image modeling implicitly or explicitly learn to generate robust and discriminative patch-level embeddings, which are then aggregated to form image-level representations. Our experiments show that variations in patch embedding quality directly correlate with downstream task performance, and that explicitly optimizing for patch embedding consistency can further improve SSL performance. This work provides a unifying perspective on different SSL methods, highlighting the importance of learning effective patch-level representations for image understanding."
http://arxiv.org/abs/2206.07700v1,Masked Siamese ConvNets,"Self-supervised learning has shown remarkable progress in learning visual representations without human annotation. However, existing methods often rely on computationally expensive pretext tasks or complex architectures. This paper addresses the challenge of developing a simple and efficient self-supervised learning framework using Siamese convolutional neural networks (ConvNets). We propose Masked Siamese ConvNets (MSC), a novel approach that leverages masked image modeling within a Siamese architecture. Specifically, MSC randomly masks portions of two augmented views of an image and trains the network to predict the masked regions of one view using the encoded representation of the other. This simple masking strategy encourages the network to learn robust and contextualized features. Our experiments on ImageNet linear probing evaluation demonstrate that MSC achieves competitive or superior performance compared to other self-supervised ConvNet methods, while significantly reducing pre-training time. MSC offers a computationally efficient and conceptually simple approach to self-supervised representation learning, making it a valuable tool for various computer vision applications."
http://arxiv.org/abs/2206.07643v2,Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone,"Vision-language pre-training (VLP) has demonstrated remarkable success in learning cross-modal representations for various downstream tasks. However, existing methods often rely on late fusion strategies or shallow interactions between modalities, limiting the potential for capturing intricate relationships between visual and textual information. This paper introduces a novel coarse-to-fine VLP framework with fusion integrated directly within the backbone network. Our approach leverages a hierarchical Transformer architecture where early layers process individual modalities, followed by progressively deeper layers that incorporate cross-modal attention mechanisms at multiple resolutions. This allows for coarse-grained alignment at earlier stages, guiding fine-grained interactions in later stages, thereby fostering a more holistic understanding of the visual and linguistic content. Experiments on a range of downstream tasks, including image-text retrieval and visual question answering, demonstrate significant performance improvements compared to state-of-the-art VLP methods. This work highlights the importance of deeply integrated cross-modal fusion for learning robust and effective vision-language representations."
http://arxiv.org/abs/2206.02574v3,On the duality between contrastive and non-contrastive self-supervised learning,"Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning, with contrastive and non-contrastive methods achieving state-of-the-art performance. Despite their apparent differences, a deeper understanding of the relationship between these two families of SSL algorithms remains elusive. This paper investigates the duality between contrastive and non-contrastive SSL, demonstrating that under specific conditions, they optimize similar objectives. We propose a novel theoretical framework that connects the loss functions of both approaches by introducing a dynamic temperature parameter that adapts based on the batch statistics, effectively bridging the gap between explicit negative sample repulsion in contrastive methods and implicit negative sample avoidance in non-contrastive methods. Experiments on standard image classification benchmarks (CIFAR-10, CIFAR-100, and ImageNet) show that our dynamically temperature-adjusted non-contrastive learning achieves comparable or superior performance to its contrastive counterparts, while maintaining computational efficiency. Our findings offer a unified perspective on SSL and pave the way for developing more robust and efficient representation learning algorithms."
http://arxiv.org/abs/2205.11508v3,Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods,"Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning, enabling models to learn from unlabeled data by creating pretext tasks. Despite their empirical success, a theoretical understanding of why contrastive and non-contrastive SSL methods work remains limited, particularly concerning their ability to capture underlying data structure. This paper investigates the representational properties of these SSL approaches, demonstrating that both contrastive and non-contrastive methods implicitly recover spectral embeddings, albeit with different characteristics. We show that contrastive methods, such as SimCLR, approximate a global spectral embedding by aligning representations based on global similarity, while non-contrastive methods, such as BYOL, recover a local spectral embedding by enforcing consistency between representations within a neighborhood. We provide a theoretical framework to characterize these connections and empirically validate our findings on several benchmark datasets, observing that the learned representations exhibit properties consistent with global and local spectral embeddings, respectively. Our work provides a unifying perspective on contrastive and non-contrastive SSL, revealing their implicit connections to classical spectral embedding techniques and offering insights into their strengths and limitations."
http://arxiv.org/abs/2205.10279v1,Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors,"Bayesian neural networks (BNNs) offer a principled framework for uncertainty quantification, but their adoption is hindered by computational complexity and the difficulty of specifying informative priors, especially in transfer learning scenarios. This paper addresses the challenge of effectively transferring knowledge in BNNs by learning a prior distribution from pre-training data, thereby enabling efficient adaptation to downstream tasks. Our method, ""Pre-Train Your Loss"" (PTYL), leverages a learned loss function during pre-training to implicitly encode task-relevant information into the posterior distribution of the pre-trained network. This pre-trained posterior then serves as a highly informative prior for fine-tuning on downstream tasks using variational inference. We demonstrate that PTYL significantly improves performance and calibration compared to traditional BNN transfer learning approaches and frequentist fine-tuning baselines across various image classification datasets, including CIFAR-100, TinyImageNet, and a subset of ImageNet. The proposed approach offers a simple yet effective strategy for Bayesian transfer learning, facilitating more reliable and robust uncertainty estimates with minimal computational overhead."
http://arxiv.org/abs/2204.03632v2,The Effects of Regularization and Data Augmentation are Class Dependent,"Deep learning models often rely on regularization techniques and data augmentation to improve generalization and prevent overfitting. While these techniques are typically applied uniformly across all classes, the impact of such global application may not be optimal given the inherent differences between classes. This paper investigates the class-dependent effects of common regularization methods (L1, L2, Dropout) and data augmentation strategies (rotation, scaling, translation) on image classification performance. We propose a novel framework that analyzes the sensitivity of each class to different regularization and augmentation parameters. This framework leverages a class-specific loss landscape analysis to identify optimal parameter settings for each class independently, followed by a class-balanced training scheme. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate that class-specific regularization and data augmentation significantly improve overall accuracy, particularly for classes with limited data or high intra-class variance, leading to improvements of up to 5% compared to uniformly applied methods. Our findings highlight the importance of considering class-specific characteristics when designing regularization and data augmentation strategies for deep learning models."
http://arxiv.org/abs/2202.08325v1,A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments,"Data augmentation is a crucial technique for improving the generalization of deep learning models, especially when training data is limited. However, determining the effective increase in sample size conferred by a specific augmentation strategy remains an open challenge, hindering principled selection and optimization of augmentation pipelines. We address this problem by developing a novel analytical framework to precisely quantify the equivalent number of real samples represented by augmented data. Our method leverages the statistical properties of augmentation transformations to derive closed-form expressions for the augmented sample moments. These moments are then used to compute an ""effective sample size"" metric, reflecting the true information gain from the augmentation process. Through experiments on benchmark image classification datasets, we demonstrate that our method accurately predicts the performance improvements achieved by various augmentation strategies, revealing that certain augmentations contribute significantly more to effective sample size than others. This framework provides a powerful tool for understanding and optimizing data augmentation, leading to more efficient and robust deep learning models."
http://arxiv.org/abs/2201.10000v1,Neural Manifold Clustering and Embedding,"Manifold learning aims to uncover the underlying low-dimensional structure of high-dimensional data. However, traditional manifold learning techniques often struggle to effectively handle data residing on multiple, intersecting manifolds, and lack the ability to simultaneously cluster data points according to their manifold membership. We address this limitation by introducing Neural Manifold Clustering and Embedding (NMCE), a novel deep learning framework that jointly learns a low-dimensional embedding and clusters data points based on their respective manifolds. NMCE employs a neural network to map data to a latent space, where a custom loss function encourages both manifold preservation and cluster separation. This loss function comprises terms that minimize intra-cluster variance in the latent space, maximize inter-cluster distance, and preserve local neighborhood relationships within each cluster. Experimental results on synthetic and real-world datasets demonstrate that NMCE significantly outperforms state-of-the-art clustering and manifold learning algorithms in terms of clustering accuracy, embedding quality, and robustness to noise. This framework provides a powerful tool for exploring and understanding complex, high-dimensional data with inherent manifold structure."
http://arxiv.org/abs/2112.09214v2,Sparse Coding with Multi-Layer Decoders using Variance Regularization,"Sparse coding aims to represent data as a sparse linear combination of basis vectors, enabling efficient data representation and feature extraction. However, traditional sparse coding often employs a single-layer decoder, limiting its ability to capture complex, hierarchical data structures. This paper addresses the challenge of learning robust and hierarchical representations in sparse coding by introducing a multi-layer decoder architecture coupled with a novel variance regularization technique. Our method extends the standard sparse coding framework by replacing the single-layer decoder with a deep neural network, allowing for more expressive reconstructions. To prevent overfitting and promote stable learning, we introduce a variance regularization term that encourages consistent activation patterns across different layers of the decoder. We demonstrate through experiments on benchmark image datasets (MNIST, CIFAR-10) that our approach achieves superior reconstruction accuracy and improved classification performance compared to traditional sparse coding and other deep sparse coding variants. The proposed method offers a powerful framework for learning hierarchical and robust sparse representations, with potential applications in various computer vision tasks such as image recognition, denoising, and feature learning."
http://arxiv.org/abs/2110.09485v2,Learning in High Dimension Always Amounts to Extrapolation,"Deep learning models often exhibit surprising generalization capabilities despite being trained on finite datasets, particularly in high-dimensional spaces. This work addresses the fundamental question of whether learning in high dimensions predominantly relies on interpolation or extrapolation. We propose a novel framework for quantifying the degree of interpolation and extrapolation performed by neural networks by analyzing the data distribution's support and its relation to the training data. Specifically, we introduce a measure based on the distance of new data points to the convex hull of the training data, coupled with an analysis of the model's predictive behavior in these regions. Our experiments on various image classification datasets and network architectures consistently demonstrate that even for data points close to the training distribution, the model's predictions are largely driven by extrapolation from the learned features, rather than direct interpolation between known examples. This suggests that the success of deep learning in high dimensions hinges on the model's ability to generalize beyond the immediate vicinity of the training data, highlighting the importance of regularization techniques and architectural choices that promote robust extrapolation behavior."
http://arxiv.org/abs/2110.09348v3,Understanding Dimensional Collapse in Contrastive Self-supervised Learning,"Contrastive self-supervised learning (CSSL) has achieved remarkable success in representation learning by training models to identify similar views of the same instance while discriminating against different instances. However, a well-known failure mode of CSSL is dimensional collapse, where the learned representations occupy only a low-dimensional subspace, leading to poor performance. This paper investigates the underlying causes of dimensional collapse in CSSL, focusing on the interplay between network architecture, data distribution, and contrastive loss functions. We propose a novel theoretical framework, grounded in spectral analysis, to characterize the conditions under which dimensional collapse occurs. Our framework predicts collapse based on the eigenvalue distribution of the feature covariance matrix, demonstrating that an imbalance in the data distribution, coupled with specific architectural biases, can trigger collapse. We further introduce a regularization technique that encourages a more uniform eigenvalue distribution, mitigating dimensional collapse and improving downstream task performance. Experiments on benchmark datasets validate our theoretical findings and demonstrate the effectiveness of our regularization method, leading to significant improvements in linear evaluation accuracy and robustness to distributional shifts. Our work provides a deeper understanding of dimensional collapse in CSSL and offers practical solutions for preventing this failure mode, thereby advancing the development of more robust and reliable self-supervised learning algorithms."
http://arxiv.org/abs/2110.06848v3,Decoupled Contrastive Learning,"Contrastive learning has emerged as a powerful technique for self-supervised representation learning, achieving impressive results across various computer vision tasks. However, existing contrastive learning methods often entangle feature dimensions, leading to suboptimal representations that capture spurious correlations and hinder downstream task performance. To address this, we propose Decoupled Contrastive Learning (DCL), a novel approach that explicitly disentangles feature dimensions during the contrastive learning process. DCL introduces a dimension-wise contrastive loss that encourages each feature dimension to learn independently, thereby promoting more informative and decorrelated representations. We further incorporate a feature decorrelation regularizer to minimize redundancy between feature dimensions. Extensive experiments on benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that DCL significantly outperforms state-of-the-art contrastive learning methods, achieving superior performance in both linear evaluation and transfer learning scenarios. This highlights the importance of disentangling feature dimensions for learning robust and generalizable visual representations."
http://arxiv.org/abs/2107.07110v3,Compact and Optimal Deep Learning with Recurrent Parameter Generators,"Deep learning models have achieved remarkable success in various computer vision tasks, but their large size and computational cost pose significant challenges for deployment on resource-constrained devices. This paper addresses the problem of reducing the parameter redundancy in deep neural networks without sacrificing accuracy. We propose a novel approach called Recurrent Parameter Generators (RPG), which utilizes a recurrent neural network (RNN) to generate the weights of the primary convolutional layers. The RNN learns a compressed representation of the layer weights, enabling significant parameter reduction. Furthermore, we introduce a differentiable neural architecture search (NAS) strategy within the RPG framework to automatically optimize the generated weights for optimal performance. Experiments on image classification benchmarks, including CIFAR-10 and ImageNet, demonstrate that RPG achieves comparable or even superior accuracy to standard convolutional neural networks with significantly fewer parameters (up to 10x reduction). This approach offers a pathway to deploying high-performance deep learning models on edge devices with limited computational resources."
http://arxiv.org/abs/2105.04906v3,VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning,"Self-supervised learning (SSL) has emerged as a powerful paradigm for learning visual representations from unlabeled data, often rivaling supervised approaches. However, many SSL methods suffer from representation collapse, where all data points are mapped to the same representation, hindering downstream task performance. We address this problem by introducing VICReg, a simple yet effective self-supervised learning method that avoids collapse by directly regularizing the variance, invariance, and covariance of the learned representations. VICReg learns representations by minimizing a loss composed of three terms: a variance term that encourages high variance in each representation dimension to prevent collapse, an invariance term that encourages representations of different views of the same image to be similar, and a covariance term that encourages different representation dimensions to be uncorrelated. Experiments on ImageNet linear evaluation and transfer learning demonstrate that VICReg achieves competitive or superior performance compared to existing SSL methods, while being significantly less sensitive to hyperparameter tuning. VICReg offers a robust and efficient approach to self-supervised learning, enabling the learning of high-quality visual representations with minimal effort."
http://arxiv.org/abs/2104.12763v2,MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding,"Multi-modal understanding, bridging vision and language, is crucial for developing intelligent systems capable of interacting with the real world. Existing vision-language models often rely on region-based features extracted from pre-trained object detectors, limiting end-to-end training and potentially hindering performance. This paper introduces Modulated DEtection TRansformer (MDETR), an end-to-end object detector that directly predicts object bounding boxes based on free-form text queries. MDETR leverages a Transformer architecture with cross-modal attention to fuse visual and textual information, enabling the model to dynamically attend to relevant image regions based on the input text. We pre-train MDETR on large-scale image-text datasets and demonstrate its effectiveness on various tasks, including referring expression comprehension, phrase grounding, and zero-shot object detection, achieving state-of-the-art results on several benchmarks. The proposed end-to-end approach significantly simplifies the training pipeline and improves performance, paving the way for more efficient and accurate multi-modal understanding systems."
http://arxiv.org/abs/2103.03230v3,Barlow Twins: Self-Supervised Learning via Redundancy Reduction,"Self-supervised learning has emerged as a powerful paradigm for representation learning, circumventing the need for extensive labeled datasets. However, many existing methods rely on careful crafting of negative samples or complex architectural designs to avoid trivial constant solutions. This paper introduces Barlow Twins, a novel self-supervised learning method that learns representations by reducing the redundancy between different distorted views of the same image. Our approach uses a redundancy reduction objective that measures the cross-correlation matrix between the outputs of two identical networks fed with different augmented versions of the same input. By driving the diagonal elements of this matrix towards one and the off-diagonal elements towards zero, the network learns to produce embeddings that are invariant to the applied distortions while decorrelating the different components of the embedding vector. We demonstrate that Barlow Twins achieves competitive or superior performance compared to existing methods on standard image classification benchmarks, without relying on negative samples, momentum encoders, or large batch sizes. This demonstrates the effectiveness of redundancy reduction as a principle for self-supervised representation learning and provides a simpler, more scalable alternative to existing approaches."
http://arxiv.org/abs/2010.00679v2,Implicit Rank-Minimizing Autoencoder,"Autoencoders have become a ubiquitous tool for representation learning, often employed for dimensionality reduction and feature extraction. However, standard autoencoders do not explicitly enforce low-rank structure in the learned latent space, which can limit their ability to capture underlying data dependencies effectively. This paper introduces the Implicit Rank-Minimizing Autoencoder (IRMAE), a novel approach that leverages the power of implicit regularization to induce low-rank representations. IRMAE eschews explicit rank-penalizing terms in the loss function. Instead, it leverages a carefully designed bottleneck architecture and training procedure that implicitly encourages the encoder to learn a low-rank latent space. Specifically, we employ a narrow bottleneck layer coupled with a spectral-normalized decoder, which we demonstrate promotes low-rank solutions through the implicit bias of stochastic gradient descent. Experimental results on various benchmark datasets demonstrate that IRMAE achieves comparable or superior reconstruction performance to standard autoencoders while simultaneously learning significantly lower-rank latent representations. This leads to more compact and interpretable features, improving downstream task performance and facilitating better data understanding."
http://arxiv.org/abs/1906.11661v2,Inspirational Adversarial Image Generation,"Adversarial image generation has primarily focused on creating imperceptible perturbations to fool classifiers or generating realistic-looking images from noise. However, generating adversarial examples that are not only effective at fooling models but also visually inspiring or evocative remains largely unexplored. This paper addresses the challenge of generating adversarial images that elicit specific emotions or artistic styles while maintaining their adversarial effectiveness. We propose a novel framework, Inspirational Adversarial Generation (IAG), which leverages a combination of adversarial loss, perceptual loss, and style transfer techniques to guide the generation process. IAG incorporates a style reference image representing the desired artistic style or emotional tone and encourages the adversarial image to adopt similar characteristics. Our experiments demonstrate that IAG can successfully generate adversarial images that are both effective at misclassifying deep learning models and visually aligned with the specified inspirational style. This work opens new avenues for exploring the intersection of adversarial machine learning and artistic expression, potentially leading to novel applications in art generation, security, and human-computer interaction."
http://arxiv.org/abs/1904.03148v1,Unsupervised Image Matching and Object Discovery as Optimization,"Unsupervised image matching and object discovery are fundamental tasks in computer vision, enabling scene understanding and knowledge transfer without manual annotation. This paper addresses the challenge of simultaneously discovering consistent point correspondences and segmenting salient objects across multiple images in the absence of ground truth. We propose a novel optimization framework that jointly estimates geometric transformations and object masks by minimizing a multi-view photometric loss regularized by sparsity and smoothness constraints. The optimization alternates between updating point correspondences via robust matching and refining object masks using a graph-based segmentation approach guided by the estimated transformations. Experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in unsupervised image matching and object discovery, outperforming existing approaches in terms of both correspondence accuracy and segmentation quality. Our approach provides a powerful and general framework for unsupervised scene understanding, with potential applications in various areas such as 3D reconstruction, image retrieval, and object tracking."
http://arxiv.org/abs/1806.05662v3,GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations,"Unsupervised representation learning aims to extract meaningful features from unlabeled data, facilitating downstream tasks. However, many existing approaches struggle to capture explicit relational structures inherent in visual scenes, limiting their transferability. This paper introduces GLoMo, a novel unsupervised learning framework that learns relational graphs as transferable representations. GLoMo leverages a contrastive objective to encourage the formation of meaningful relationships between visual entities, represented as nodes in a graph. These relationships are learned through a message-passing mechanism, enabling the model to capture complex interdependencies. Furthermore, we introduce a graph-level prediction task that encourages the learned graph representations to be informative for downstream classification and segmentation. Experimental results demonstrate that GLoMo achieves state-of-the-art performance on several unsupervised representation learning benchmarks, including object discovery and semantic segmentation. Crucially, GLoMo's learned relational graphs exhibit strong transferability, enabling superior performance on downstream tasks with limited supervision compared to existing methods, highlighting the effectiveness of relational reasoning for unsupervised representation learning."
http://arxiv.org/abs/1803.11496v2,Predicting Future Instance Segmentation by Forecasting Convolutional Features,"Instance segmentation is a crucial task for understanding dynamic scenes, but traditional methods struggle with predicting future segmentations due to the complex interplay of object motion, appearance changes, and occlusions. This paper addresses the challenge of predicting instance segmentation masks in future frames given a sequence of past observations. We propose a novel approach that forecasts convolutional feature maps extracted from a segmentation network backbone, leveraging a recurrent architecture with spatiotemporal attention to capture the evolution of object representations. Specifically, we learn to predict future feature activations and then decode these predicted features into instance segmentation masks using the pre-trained segmentation head. Our experiments on challenging datasets like Cityscapes and KITTI demonstrate that our method achieves significant improvements in segmentation accuracy and temporal consistency compared to existing approaches that directly predict segmentation masks or optical flow. This ability to accurately forecast instance segmentation opens up new possibilities for proactive robot navigation, autonomous driving, and video understanding."
http://arxiv.org/abs/1711.11248v3,A Closer Look at Spatiotemporal Convolutions for Action Recognition,"Human action recognition in videos is a fundamental task in computer vision with applications ranging from surveillance to human-computer interaction. Spatiotemporal convolutional networks (ST-CNNs) have emerged as a dominant paradigm for this task, yet the precise role and impact of different spatiotemporal convolution designs remain underexplored. This paper delves deeper into the architectural choices within ST-CNNs, specifically investigating the performance trade-offs between factorized and non-factorized spatiotemporal convolutions. We propose a systematic evaluation framework that disentangles the effects of spatial kernel size, temporal kernel size, and factorization strategies on action recognition accuracy and computational cost. Our experiments on benchmark datasets, including Kinetics-400 and Something-Something V2, reveal that while larger temporal kernels generally improve performance, factorized convolutions with carefully chosen kernel sizes can achieve comparable accuracy with significantly reduced computational complexity. These findings offer valuable insights for designing more efficient and effective ST-CNN architectures for action recognition, paving the way for real-time applications and deployment on resource-constrained devices."
http://arxiv.org/abs/1709.01062v2,A hierarchical loss and its problems when classifying non-hierarchically,"Hierarchical classification leverages the inherent structure present in many real-world datasets to improve classification accuracy and provide more informative predictions. However, the effectiveness of hierarchical loss functions, designed to penalize errors more severely at higher levels of the hierarchy, is often predicated on the assumption that the data inherently possesses a hierarchical organization. This paper investigates the performance of a common hierarchical loss function when applied to datasets lacking a true hierarchical relationship between classes, focusing on potential drawbacks and unexpected consequences. We analyze a weighted cross-entropy hierarchical loss that incorporates depth-based weights, comparing its performance against standard cross-entropy on both synthetic and real-world datasets where classes are artificially grouped into a hierarchy. Our experiments demonstrate that imposing a hierarchical structure through the loss function can lead to decreased performance compared to standard cross-entropy, particularly when inter-class similarity does not align with the imposed hierarchy. Furthermore, we observe that the hierarchical loss can introduce bias towards specific branches of the hierarchy, even when the underlying data distribution is uniform. This highlights the importance of careful consideration when employing hierarchical losses and suggests that their benefits are contingent on the validity of the hierarchical structure within the data."
http://arxiv.org/abs/1703.07684v3,Predicting Deeper into the Future of Semantic Segmentation,"Semantic segmentation is a fundamental task in computer vision, enabling scene understanding for autonomous driving, robotics, and augmented reality. Predicting future semantic segmentation maps is crucial for planning and decision-making in dynamic environments; however, existing methods struggle with long-term predictions due to compounding errors and the inherent uncertainty of future events. We address the challenge of generating accurate and consistent semantic segmentation predictions further into the future by introducing a novel recurrent framework with attention-based feature propagation and a stochastic prediction module. Our model leverages attention mechanisms to selectively propagate relevant features across time steps, mitigating error accumulation. Furthermore, the stochastic prediction module allows for generating multiple plausible future segmentations, capturing the inherent uncertainty in long-term predictions. Experiments on the Cityscapes dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods in long-term semantic segmentation prediction, achieving higher accuracy and better capturing the distribution of possible future states. This work advances the field of future prediction and enables more robust and reliable decision-making in dynamic environments."
http://arxiv.org/abs/1611.08097v2,Geometric deep learning: going beyond Euclidean data,"Deep learning has achieved remarkable success in various fields, particularly when applied to data residing in Euclidean spaces, such as images and videos. However, many real-world datasets are inherently non-Euclidean, residing on manifolds or graphs, posing a significant challenge for traditional deep learning architectures. This paper addresses the problem of extending deep learning capabilities to non-Euclidean data by leveraging the principles of geometric deep learning. We propose a novel framework that generalizes convolutional neural networks to graph-structured data, incorporating spectral graph theory and differential geometry to define convolution and pooling operations on graphs. Our approach learns node embeddings that capture the underlying geometric structure of the graph, enabling effective feature extraction and representation learning. Experiments on benchmark graph datasets for node classification and graph classification demonstrate that our method achieves state-of-the-art performance, outperforming existing graph neural networks and traditional machine learning algorithms. This work significantly broadens the applicability of deep learning to a wider range of complex and structured data."
http://arxiv.org/abs/1606.01535v1,What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?,"Hierarchical recognition architectures, inspired by the ventral stream of the primate visual cortex, are a cornerstone of modern computer vision. A critical challenge within these architectures is determining the optimal feature learning procedure at each level of the hierarchy to maximize performance and generalization. This paper investigates and compares various feature learning techniques, including unsupervised, supervised, and self-supervised methods, within a multi-layered convolutional neural network designed for object recognition. We introduce a novel hybrid learning strategy that combines unsupervised pre-training with a contrastive self-supervised fine-tuning stage tailored to the specific characteristics of each hierarchical level. Empirical evaluations on benchmark datasets, such as CIFAR-10, CIFAR-100, and ImageNet, demonstrate that our hybrid approach consistently outperforms both purely unsupervised and supervised methods, achieving state-of-the-art results in several low-data regimes. These findings offer valuable insights into the design of effective feature learning pipelines for hierarchical recognition architectures and pave the way for more robust and data-efficient visual learning systems."
http://arxiv.org/abs/1511.05666v4,Super-Resolution with Deep Convolutional Sufficient Statistics,"Single-image super-resolution (SISR) aims to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart, a long-standing challenge in computer vision. Existing deep learning-based SISR methods primarily focus on directly predicting HR pixel values, often neglecting the underlying statistical relationships between LR and HR image patches. This paper addresses the problem of explicitly modeling and leveraging sufficient statistics for improved super-resolution performance. We propose a novel deep convolutional architecture that learns to extract and utilize sufficient statistics from LR image patches to predict HR image details. Our network employs a dedicated module to estimate the parameters of a multivariate Gaussian distribution, representing the sufficient statistics, and subsequently uses these parameters to guide the HR image reconstruction. Extensive experiments on benchmark datasets demonstrate that our approach achieves state-of-the-art results in terms of both quantitative metrics and perceptual quality, surpassing existing methods by a significant margin, especially for high upscaling factors. By explicitly modeling sufficient statistics, our method offers a more robust and interpretable framework for single-image super-resolution."
http://arxiv.org/abs/1511.05440v6,Deep multi-scale video prediction beyond mean square error,"Video prediction, the task of forecasting future frames given a sequence of past frames, is crucial for various applications, including robotics and autonomous driving. However, current deep learning models for video prediction often rely on pixel-wise Mean Squared Error (MSE) loss, which tends to produce blurry and oversmoothed predictions, failing to capture the complex dynamics and high-frequency details present in real-world videos. To address this limitation, we propose a novel deep multi-scale video prediction framework that goes beyond MSE by incorporating perceptual loss, adversarial loss, and a novel multi-scale temporal gradient difference loss. Our framework leverages a 3D U-Net architecture to capture spatio-temporal dependencies at different scales, enabling the model to generate sharper and more realistic predictions. Experimental results on benchmark datasets such as Moving MNIST and UCF-101 demonstrate that our proposed approach significantly outperforms state-of-the-art methods in terms of both quantitative metrics, including PSNR and SSIM, and qualitative visual assessment. This work advances the field of video prediction by demonstrating the effectiveness of incorporating perceptual and adversarial losses in conjunction with a multi-scale architecture for generating high-quality, realistic video forecasts."
http://arxiv.org/abs/1510.05970v2,Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches,"Stereo matching is a fundamental problem in computer vision, aiming to recover depth information from a pair of rectified images. Existing methods often rely on hand-crafted features and complex cost aggregation strategies that are computationally expensive and struggle in challenging scenarios with occlusions and textureless regions. This paper addresses the problem of learning a robust similarity measure for stereo matching directly from image data using a convolutional neural network (CNN). We propose a Siamese network architecture that takes two image patches as input, one from the left image and one from the right, and outputs a similarity score indicating whether the patches correspond to the same 3D point. The network is trained end-to-end on a large dataset of stereo image pairs with ground truth disparity maps, learning features optimized for stereo correspondence. Our experiments on standard benchmark datasets demonstrate that the learned similarity measure significantly outperforms traditional hand-crafted features, achieving state-of-the-art accuracy and robustness, particularly in challenging areas. This learned feature representation provides a powerful and efficient solution for accurate stereo depth estimation."
http://arxiv.org/abs/1506.05163v1,Deep Convolutional Networks on Graph-Structured Data,"Graph-structured data is ubiquitous across numerous scientific domains, representing complex relationships between entities. Applying deep convolutional networks (CNNs) to such data is challenging due to the inherent irregularity and variable size of graphs, which contrasts with the fixed grid structure expected by standard CNNs. This paper addresses the problem of generalizing CNNs to operate directly on graph-structured data without requiring feature engineering or transformations into Euclidean space. We introduce a novel graph convolutional network architecture, termed GraphConv, that learns node embeddings by aggregating feature information from a node's neighbors through learnable convolutional filters. These filters are designed to be permutation-invariant to the order of neighbors and are adaptively weighted based on the graph's structure. We evaluate GraphConv on several benchmark datasets, including node classification on citation networks and graph classification on molecular datasets. Our results demonstrate that GraphConv achieves state-of-the-art performance, outperforming existing graph neural networks and traditional machine learning methods. This advancement provides a powerful tool for analyzing and extracting meaningful insights from complex graph-structured data in various scientific and industrial applications."
http://arxiv.org/abs/1506.03011v2,Learning to Linearize Under Uncertainty,"Linearization is a fundamental technique for solving nonlinear optimization problems in computer vision, but its performance degrades significantly in the presence of noise and outliers. This paper addresses the challenge of learning robust linearization strategies that are adaptive to uncertainty in the data. We propose a novel deep learning framework that learns to predict optimal linearization points and weights based on estimated uncertainty. Our architecture incorporates a learned uncertainty estimator that quantifies the reliability of input data, which is then used to modulate the linearization process. Specifically, we learn a mapping from input features and uncertainty estimates to a set of linearization parameters, effectively tailoring the linearization to the local data characteristics. We demonstrate that our approach outperforms traditional linearization methods and existing robust estimators on a variety of computer vision tasks, including pose estimation and structure from motion, achieving significant improvements in accuracy and robustness, particularly under high noise conditions. This learned linearization strategy offers a powerful and generalizable approach for improving the performance of optimization-based computer vision algorithms."
http://arxiv.org/abs/1504.02518v2,Unsupervised Feature Learning from Temporal Data,"Temporal data, such as video sequences or time-series sensor readings, contains rich information about the underlying dynamics of the observed world. Learning effective feature representations from this unlabeled temporal data is crucial for various downstream tasks. However, effectively capturing these temporal dependencies in an unsupervised manner remains a significant challenge. We propose a novel unsupervised feature learning framework that leverages the inherent ordering of temporal sequences by predicting future states based on past observations. Our method employs a recurrent neural network (RNN) architecture trained to minimize the reconstruction error between predicted and actual future frames, guided by a novel temporal consistency loss that encourages smooth transitions in the learned feature space. We evaluate our learned features on several benchmark action recognition and video classification datasets, demonstrating significant improvements over existing unsupervised representation learning techniques. This highlights the effectiveness of our approach in capturing meaningful temporal dynamics and learning robust feature representations without relying on manual annotations."
http://arxiv.org/abs/1412.6056v6,Unsupervised Learning of Spatiotemporally Coherent Metrics,"Visual understanding requires the ability to measure similarity between scenes, objects, and their dynamics. However, defining appropriate metrics for complex visual data is challenging, particularly when labeled data is scarce. This paper addresses the problem of learning spatiotemporally coherent metrics for visual data in an unsupervised manner. Our method leverages the inherent structure of video sequences by training a neural network to predict the temporal order of randomly shuffled video segments. The learned representation encodes a metric space where distances reflect both spatial and temporal relationships, encouraging nearby frames to have similar embeddings and penalizing large discontinuities. We demonstrate that the learned metrics effectively capture semantic similarity across diverse datasets, achieving state-of-the-art performance on unsupervised video retrieval and action similarity tasks. These results highlight the potential of self-supervised learning to discover meaningful representations for complex visual data without relying on explicit supervision, paving the way for more robust and generalizable computer vision systems."
http://arxiv.org/abs/1411.4280v3,Efficient Object Localization Using Convolutional Networks,"Object localization, the task of identifying and delineating objects within an image, is a fundamental problem in computer vision. Existing convolutional neural network (CNN) based methods often require computationally expensive sliding window approaches or complex multi-stage pipelines for accurate localization. This paper addresses the challenge of achieving efficient and accurate object localization with a streamlined CNN architecture. We propose a novel single-stage network, termed EfficientLoc, which directly predicts object bounding boxes and class probabilities from convolutional feature maps using a multi-scale feature fusion and attention mechanism. EfficientLoc leverages a lightweight backbone network coupled with carefully designed feature aggregation modules to enhance localization accuracy while minimizing computational overhead. Experimental results on the PASCAL VOC and COCO datasets demonstrate that EfficientLoc achieves state-of-the-art localization performance with significantly fewer parameters and faster inference speeds compared to existing object detection and localization models. Our approach offers a practical solution for real-time applications requiring precise and efficient object localization."
http://arxiv.org/abs/1409.7963v1,MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation,"Human pose estimation is a fundamental task in computer vision, enabling various applications such as action recognition and human-computer interaction. Existing deep learning methods primarily focus on appearance features, often overlooking valuable motion cues, particularly in challenging scenarios with occlusions or complex backgrounds. This paper introduces MoDeep, a novel deep learning framework that explicitly incorporates motion features to enhance human pose estimation accuracy. MoDeep leverages a convolutional neural network (CNN) to extract appearance features from individual frames and integrates a recurrent neural network (RNN) to model temporal dependencies and capture motion information across consecutive frames. A fusion module then adaptively combines these appearance and motion features to predict accurate human pose estimations. Experimental results on benchmark datasets, including Human3.6M and MPII, demonstrate that MoDeep achieves state-of-the-art performance, particularly in scenarios with rapid movements and occlusions, outperforming existing methods by a significant margin. This highlights the importance of integrating motion features into deep learning models for robust and accurate human pose estimation."
http://arxiv.org/abs/1409.4326v2,Computing the Stereo Matching Cost with a Convolutional Neural Network,"Stereo matching, the problem of finding corresponding points in a pair of rectified images, is a fundamental task in computer vision. Traditional stereo algorithms often rely on hand-crafted features and complex cost aggregation schemes, limiting their ability to adapt to diverse and challenging scenarios. This paper addresses the problem of accurately and efficiently computing the stereo matching cost, a crucial step in disparity estimation, directly using a convolutional neural network (CNN). We propose a novel Siamese network architecture that takes as input image patches from the left and right images at a given disparity and outputs a similarity score representing the matching cost. The network is trained end-to-end to learn robust features and a similarity metric optimized for disparity estimation. We demonstrate that our CNN-based cost computation significantly outperforms traditional methods on standard stereo benchmarks, achieving state-of-the-art results with improved accuracy, particularly in textureless regions and near occlusions. Our learned cost function provides a powerful alternative to hand-engineered features, paving the way for more robust and adaptable stereo vision systems."
http://arxiv.org/abs/1406.2984v2,Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation,"Human pose estimation is a fundamental task in computer vision with applications ranging from action recognition to human-computer interaction. Existing approaches often treat pose estimation as either a purely discriminative regression problem using Convolutional Neural Networks (CNNs) or leverage probabilistic Graphical Models (PGMs) to enforce structural constraints between body parts. However, CNNs may struggle to capture long-range dependencies and enforce structural constraints, while PGMs often rely on hand-crafted features and may be computationally expensive. This paper introduces a novel framework for human pose estimation that jointly trains a CNN and a PGM. Our CNN extracts powerful image features, which are then fed into a PGM that models the structural relationships between body joints. Crucially, we develop a differentiable message passing algorithm for the PGM, allowing for end-to-end training of the entire system via backpropagation. Experiments on benchmark datasets demonstrate that our joint training approach significantly outperforms state-of-the-art methods, achieving higher accuracy and more robust pose estimates, especially in challenging scenarios with occlusions and cluttered backgrounds. This work demonstrates the benefits of combining the strengths of CNNs and PGMs within a unified, trainable framework for structured prediction problems."
http://arxiv.org/abs/1404.0736v2,Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, but their computational cost remains a significant bottleneck for deployment on resource-constrained devices. This paper addresses the problem of reducing the computational complexity of CNN inference without sacrificing accuracy. We propose a novel method that exploits the inherent linear structure within convolutional layers to achieve efficient evaluation. Specifically, we represent convolutional filters as a linear combination of basis filters learned through a structured matrix factorization. During inference, we pre-compute the activations for each basis filter and then efficiently reconstruct the output feature maps using the learned linear coefficients. Experiments on benchmark datasets demonstrate that our method achieves significant speedups compared to standard convolutional layers and other state-of-the-art compression techniques, while maintaining comparable or even improved accuracy. This approach offers a practical solution for deploying high-performing CNNs in resource-limited environments."
http://arxiv.org/abs/1312.6229v4,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks","Convolutional Neural Networks (CNNs) have achieved state-of-the-art results in image classification, yet their potential for integrated scene understanding tasks remains largely unexplored. This paper addresses the challenge of efficiently performing recognition, localization, and detection using a single, unified CNN framework. We introduce OverFeat, a novel approach that leverages a shared convolutional feature extraction backbone, followed by task-specific prediction layers for classification, bounding box regression, and object detection. OverFeat operates on multiple scales and sliding windows, allowing for dense feature extraction and precise object localization. By fine-tuning the shared features for each task, we achieve competitive results on the ImageNet Large Scale Visual Recognition Challenge 2013, demonstrating significant improvements in both classification accuracy and object detection performance compared to previous CNN-based approaches. The integrated architecture of OverFeat allows for efficient computation and demonstrates the power of CNNs for holistic scene understanding."
http://arxiv.org/abs/1312.6203v3,Spectral Networks and Locally Connected Networks on Graphs,"Graph Neural Networks (GNNs) have emerged as powerful tools for representation learning on non-Euclidean data, enabling applications across diverse domains. However, many existing GNN architectures rely on global spectral operations or message passing schemes that can be computationally expensive or lack the flexibility to capture localized graph structures effectively. This paper addresses the limitations of both spectral and message-passing GNNs by introducing a novel framework that combines spectral filtering techniques with locally connected networks on graphs. Our approach leverages localized graph Fourier transforms computed within ego-networks to learn spatially varying spectral filters, effectively adapting the spectral response to the local graph topology. We then apply locally connected networks to the filtered signals, enabling the model to learn complex, spatially dependent relationships. Experiments on benchmark graph classification and node classification datasets demonstrate that our proposed architecture achieves state-of-the-art performance, outperforming traditional spectral and message-passing GNNs while maintaining computational efficiency. This work provides a new perspective on graph representation learning by bridging the gap between spectral and spatial approaches, opening avenues for more powerful and adaptable GNN architectures."
http://arxiv.org/abs/1312.5851v5,Fast Training of Convolutional Networks through FFTs,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, but their training remains computationally expensive, particularly with large datasets and deep architectures. This paper addresses the problem of reducing the training time of CNNs without compromising accuracy. We propose a novel approach that leverages Fast Fourier Transforms (FFTs) to accelerate the convolution operation, a primary bottleneck in CNN training. Specifically, we replace the direct computation of spatial convolutions with element-wise multiplications in the frequency domain, achieved through FFTs and inverse FFTs. Furthermore, we introduce a tailored optimization strategy that accounts for the characteristics of frequency-domain training, improving convergence and stability. Experimental results on benchmark datasets such as CIFAR-10 and ImageNet demonstrate that our method achieves a significant reduction in training time compared to traditional spatial convolution implementations, while maintaining comparable or even improved accuracy. This work offers a practical and efficient technique for accelerating CNN training, enabling faster experimentation and deployment of deep learning models."
http://arxiv.org/abs/1301.3775v4,Discriminative Recurrent Sparse Auto-Encoders,"Recurrent Neural Networks (RNNs) excel at modeling sequential data, but often struggle with interpretability and capturing long-range dependencies effectively. Sparse auto-encoders, on the other hand, promote feature selection and can enhance interpretability, but lack the temporal modeling capabilities of RNNs. This paper addresses the challenge of learning discriminative and interpretable representations from sequential data by introducing Discriminative Recurrent Sparse Auto-Encoders (DRSAEs). DRSAEs combine the temporal modeling power of RNNs with the feature selection properties of sparse auto-encoders, while incorporating a discriminative loss function to explicitly encourage class separation in the learned latent space. Specifically, we use an RNN encoder to map input sequences into a latent space, enforce sparsity on the recurrent activations using L1 regularization, and train the decoder to reconstruct the input while simultaneously minimizing a classification loss. Experimental results on benchmark time-series classification datasets demonstrate that DRSAEs achieve competitive or superior classification accuracy compared to state-of-the-art methods, while simultaneously learning sparse and interpretable representations of the input sequences. The ability to learn discriminative and sparse representations makes DRSAEs a valuable tool for understanding and classifying complex sequential data."
http://arxiv.org/abs/1301.3572v2,Indoor Semantic Segmentation using depth information,"Indoor scene understanding is crucial for various applications, including robotics, augmented reality, and autonomous navigation. While RGB images have been widely used for semantic segmentation, they often struggle with illumination changes, textureless surfaces, and occlusions common in indoor environments. This paper addresses the challenge of improving indoor semantic segmentation by effectively leveraging depth information. We propose a novel multi-modal fusion network, DepthSegNet, that integrates RGB and depth data through a two-stream architecture. DepthSegNet employs an early fusion strategy using a learnable attention mechanism to dynamically weight the contribution of each modality at different feature levels. Furthermore, we introduce a depth-aware refinement module that leverages geometric cues from the depth data to refine segmentation boundaries and improve the consistency of predictions. Experimental results on the NYU Depth V2 and SUN RGB-D datasets demonstrate that DepthSegNet achieves state-of-the-art performance, outperforming existing methods by a significant margin in terms of mean Intersection-over-Union (mIoU). Our approach effectively exploits the complementary nature of RGB and depth data, leading to more accurate and robust semantic segmentation for indoor scenes."
http://arxiv.org/abs/1301.3476v3,Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities,"Deep neural networks are typically trained using Stochastic Gradient Descent (SGD) and its variants, which rely on first-order derivative information. While computationally efficient, SGD often suffers from slow convergence and sensitivity to hyperparameter tuning compared to second-order methods. This paper addresses the problem of improving the convergence speed and robustness of SGD by incorporating approximate second-order information at minimal computational overhead. We propose a novel backpropagation learning framework that introduces learnable, element-wise transformations within the nonlinear activation functions. These transformations effectively modulate the gradient flow, approximating the influence of the Hessian matrix by adapting the curvature of the activation. Experiments on standard image classification benchmarks, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that our method consistently achieves faster convergence and improved generalization performance compared to standard SGD and Adam optimizers, while maintaining a similar computational cost. The proposed approach offers a practical and efficient way to push SGD towards the benefits of second-order optimization, enhancing the training of deep neural networks."
http://arxiv.org/abs/1301.1671v1,Causal graph-based video segmentation,"Video segmentation, the task of partitioning a video into semantically meaningful regions, is crucial for numerous downstream applications such as video editing and content analysis. Existing methods often rely on correlative relationships between visual features and segmentation labels, neglecting the underlying causal structure that governs video dynamics. This can lead to suboptimal performance, particularly in complex scenarios with occlusions or rapid scene changes. We propose a novel causal graph-based video segmentation framework that explicitly models the causal dependencies between video frames and segmentation masks. Our method constructs a dynamic causal graph where nodes represent frame features and segmentation labels, and edges represent causal relationships learned through a combination of observational data and interventional analysis. Specifically, we leverage structural causal model (SCM) to explicitly model and learn the causal graph by identifying the intervention targets. Experiments on standard video segmentation benchmarks, including DAVIS and YouTube-VIS, demonstrate that our approach achieves state-of-the-art performance, surpassing existing methods by a significant margin, particularly in handling occlusions and complex motion. This highlights the importance of incorporating causal reasoning into video segmentation, paving the way for more robust and reliable video understanding systems."
http://arxiv.org/abs/1212.0142v2,Pedestrian Detection with Unsupervised Multi-Stage Feature Learning,"Pedestrian detection is a crucial task for various computer vision applications, particularly in autonomous driving and surveillance systems. Existing methods often rely on large amounts of labeled data, which can be expensive and time-consuming to acquire. This paper addresses the challenge of learning robust pedestrian features without relying on manual annotations by proposing an unsupervised multi-stage feature learning framework. Our method leverages a novel combination of self-supervised learning techniques, including contrastive learning and masked autoencoding, to extract increasingly discriminative features from unlabeled pedestrian image patches. These features are then refined through a multi-stage architecture, where each stage focuses on learning specific aspects of pedestrian appearance, such as shape, texture, and context. Experimental results on benchmark datasets, including Caltech and CityPersons, demonstrate that our unsupervised approach achieves comparable or even superior performance to some supervised methods, particularly in challenging scenarios with occlusions and varying lighting conditions. This work significantly reduces the reliance on labeled data for pedestrian detection, paving the way for more scalable and adaptable vision systems."
http://arxiv.org/abs/1204.3968v1,Convolutional Neural Networks Applied to House Numbers Digit Classification,"Accurate and automated house number recognition is crucial for various applications, including address geocoding, postal automation, and autonomous navigation. However, the significant variations in font, style, scale, and occlusion present a considerable challenge for robust digit classification. This paper addresses the problem of accurately classifying individual digits extracted from street view house number images using deep learning techniques. We propose a Convolutional Neural Network (CNN) architecture specifically designed for this task, incorporating multiple convolutional layers with ReLU activation, max-pooling layers for downsampling, and fully connected layers for classification. Data augmentation techniques, including random rotations, translations, and scaling, were employed to enhance the model's generalization capability and robustness to variations in image characteristics. Experimental results on the publicly available SVHN dataset demonstrate that our proposed CNN achieves state-of-the-art performance, surpassing existing methods in terms of accuracy and robustness. These findings highlight the effectiveness of CNNs for house number digit classification and contribute to advancements in automated address processing and related computer vision applications."
http://arxiv.org/abs/1202.6384v1,Fast approximations to structured sparse coding and applications to object classification,"Structured sparse coding (SSC) has shown promise in object classification by leveraging group sparsity to encode image features with structured dictionaries. However, solving the SSC problem is computationally expensive, hindering its applicability to large-scale datasets. This paper addresses the computational bottleneck of SSC by introducing two fast approximation algorithms based on greedy pursuit and iterative thresholding. Our methods, termed Fast Group Matching Pursuit (FGMP) and Fast Group Iterative Thresholding (FGIT), significantly reduce the computational cost by efficiently identifying and updating relevant dictionary groups. FGMP employs a matching pursuit strategy guided by group norms, while FGIT utilizes a computationally efficient iterative thresholding scheme tailored for structured sparsity. Experiments on several benchmark datasets, including CIFAR-10 and Caltech-101, demonstrate that our proposed approximations achieve comparable or even superior classification accuracy to existing SSC solvers, while offering orders of magnitude speedup in computation time. These fast approximations make SSC practical for large-scale object classification tasks, enabling efficient learning of structured representations."
http://arxiv.org/abs/1202.2160v2,"Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers","Scene parsing, the task of assigning semantic labels to every pixel in an image, remains a challenging problem in computer vision due to the complexity and variability of real-world scenes. This paper addresses the limitations of existing methods in effectively capturing both local details and global contextual information for accurate scene understanding. We introduce a novel scene parsing framework that combines multiscale feature learning with a novel ""Purity Tree"" structure and optimal cover selection. Our approach employs a deep convolutional neural network (CNN) backbone to extract features at multiple scales, which are then organized into a Purity Tree based on semantic homogeneity. An optimal cover of the tree is subsequently selected to aggregate features across diverse contextual scopes, ensuring a balanced representation of both fine-grained details and global scene context. Experimental results on benchmark datasets, including Cityscapes and ADE20K, demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in terms of both accuracy and efficiency. The proposed framework provides a robust and effective solution for scene parsing, paving the way for improved performance in various vision applications."
http://arxiv.org/abs/1108.1169v1,Learning Representations by Maximizing Compression,"Learning useful representations from data is a fundamental challenge in computer vision. Traditional methods often rely on explicit supervision or hand-crafted objectives. This work addresses the problem of learning effective representations solely by maximizing the compressibility of data. We propose a novel unsupervised learning framework, Compression Maximization Network (CMN), which learns representations by explicitly training a neural network to minimize the bit-rate required to encode the input. CMN consists of an encoder that maps the input to a latent representation and a decoder that reconstructs the input from the compressed latent code, optimized jointly with an entropy coder that estimates the probability distribution of the latent code. Experiments on several benchmark datasets demonstrate that CMN learns representations competitive with state-of-the-art unsupervised methods, achieving comparable or superior performance in downstream classification and clustering tasks, particularly when evaluated on datasets with high inherent redundancy. These results demonstrate that maximizing compressibility is a powerful principle for learning effective and generalizable representations, offering a promising direction for unsupervised representation learning."
http://arxiv.org/abs/1105.5307v1,Efficient Learning of Sparse Invariant Representations,"Learning representations that are both sparse and invariant to common data transformations is crucial for efficient and robust visual recognition. However, achieving both properties simultaneously often requires complex architectures and computationally expensive training procedures. This paper addresses the challenge of learning sparse and invariant representations efficiently, focusing on minimizing computational overhead during training. We introduce a novel approach that combines a convolutional autoencoder with a learned sparsification module and a contrastive loss function designed to enforce invariance. The sparsification module learns a data-dependent threshold for each feature map, promoting sparsity without relying on hand-crafted heuristics. The contrastive loss encourages the learned features to be invariant to a set of predefined transformations, such as rotations and translations. Experimental results on benchmark datasets demonstrate that our method achieves comparable or superior performance to existing approaches in terms of classification accuracy and representation sparsity, while significantly reducing training time and memory consumption. This efficient learning framework paves the way for deploying sparse and invariant representations in resource-constrained environments."
http://arxiv.org/abs/1010.3467v1,Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition,"Sparse coding has emerged as a powerful technique for learning overcomplete representations of image data, demonstrating success in various computer vision tasks. However, the computational cost associated with inferring sparse codes, particularly for large dictionaries, remains a significant bottleneck, hindering its application in real-time object recognition systems. This paper introduces a novel algorithm, Fast Iterative Shrinkage Thresholding with Learned Acceleration (FISTA-LA), designed to accelerate sparse code inference. FISTA-LA leverages learned acceleration parameters within the iterative shrinkage-thresholding algorithm, optimizing these parameters online during inference using a gradient-based approach. This dynamic adaptation allows for faster convergence to the optimal sparse code compared to traditional fixed-parameter methods. Experimental results on benchmark object recognition datasets, including CIFAR-10 and Caltech-101, demonstrate that FISTA-LA achieves a significant speedup in inference time while maintaining comparable, and in some cases improved, recognition accuracy compared to state-of-the-art sparse coding algorithms. This advancement enables the practical deployment of sparse coding models in computationally constrained environments and facilitates real-time object recognition applications."
http://arxiv.org/abs/1010.0422v1,Convolutional Matching Pursuit and Dictionary Training,"Convolutional sparse coding has emerged as a powerful tool for representation learning, offering shift-invariant feature extraction and efficient signal reconstruction. However, the alternating optimization scheme typically employed for convolutional dictionary learning, involving sparse coding and dictionary update steps, can be computationally expensive and may converge to suboptimal solutions. This paper addresses the challenge of jointly optimizing the convolutional dictionary and sparse codes by introducing a novel algorithm based on matching pursuit. Our approach, termed Convolutional Matching Pursuit and Dictionary Training (CMPDT), iteratively selects atoms from the dictionary that best explain the residual signal, followed by a joint dictionary update that minimizes the reconstruction error while promoting sparsity. We leverage efficient convolution operations and gradient-based optimization techniques to accelerate the training process. Experimental results on benchmark image datasets demonstrate that CMPDT achieves competitive or superior performance compared to existing convolutional dictionary learning methods, while significantly reducing the computational cost and exhibiting faster convergence. The proposed algorithm offers a practical and efficient solution for learning convolutional dictionaries, enabling broader applications in image processing and computer vision tasks."
http://arxiv.org/abs/2210.06366v4,A Generalist Framework for Panoptic Segmentation of Images and Videos,"Panoptic segmentation, which aims to unify semantic and instance segmentation, has emerged as a comprehensive scene understanding task. However, current approaches often rely on specialized architectures and training strategies tailored for either images or videos, lacking a unified framework. This paper introduces a novel generalist framework for panoptic segmentation that seamlessly handles both images and videos through a unified architecture and training paradigm. Our approach leverages a transformer-based architecture with spatiotemporal attention mechanisms to effectively model both spatial and temporal contexts. We further propose a multi-task learning strategy that jointly optimizes for semantic, instance, and tracking objectives, enabling consistent and coherent segmentation across video frames. Experiments on challenging benchmarks, including Cityscapes, COCO, and Cityscapes-DVPS, demonstrate that our framework achieves state-of-the-art or competitive performance on both image and video panoptic segmentation tasks, outperforming existing specialized methods. This work provides a significant step towards a truly generalist perception system capable of understanding both static and dynamic visual environments."
http://arxiv.org/abs/2210.03310v3,Scaling Forward Gradient With Local Losses,"Forward propagation offers a promising alternative to backpropagation, particularly for hardware acceleration and mitigating the vanishing gradient problem. However, training deep networks with purely forward gradients often suffers from performance degradation compared to backpropagation, especially on complex datasets, due to the lack of effective credit assignment and optimization challenges. This paper introduces a novel training strategy, Scaling Forward Gradient with Local Losses (SFG-LL), to address these limitations. SFG-LL augments each layer with a local loss function and dynamically scales the forward gradient based on the magnitude of the local loss gradient. This scaling mechanism allows for more precise and adaptive gradient updates, promoting better convergence and feature learning at each layer. We demonstrate that SFG-LL significantly improves the performance of forward gradient-based training on image classification tasks, achieving results comparable to backpropagation on CIFAR-10 and substantially outperforming existing forward gradient methods on more challenging datasets like CIFAR-100 and Tiny ImageNet. Our approach provides a more effective and scalable framework for training deep neural networks using forward propagation, paving the way for efficient and hardware-friendly deep learning."
http://arxiv.org/abs/2208.04202v2,Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,"Diffusion models have shown remarkable success in generating high-quality continuous data, but their application to discrete data remains challenging due to the non-differentiable nature of discrete spaces. Generating high-dimensional discrete data, such as text or molecular structures, often suffers from mode collapse or requires complex, specialized architectures. We introduce ""Analog Bits,"" a novel diffusion-based approach for generating discrete data by representing each discrete element as a continuous vector, analogous to bits in an analog computer. Our method employs a standard Gaussian diffusion process in the continuous space of these ""analog bits,"" coupled with a self-conditioning mechanism that refines the generated discrete representation at each denoising step. This self-conditioning leverages the model's own predictions to guide the diffusion process towards more realistic and coherent discrete structures. Experiments on text generation (GPT-2 dataset) and molecular structure generation (QM9 dataset) demonstrate that Analog Bits achieves state-of-the-art performance in terms of generation quality, diversity, and validity, surpassing existing discrete diffusion models and GAN-based approaches. Our approach provides a simple yet effective framework for adapting continuous diffusion models to discrete data generation, paving the way for broader applications of diffusion models in diverse domains."
http://arxiv.org/abs/2206.07669v2,A Unified Sequence Interface for Vision Tasks,"Modern computer vision tasks are often tackled with specialized architectures tailored for specific input modalities and output formats. This specialization hinders the development of general-purpose vision systems capable of handling diverse tasks without architectural modifications. We address this limitation by introducing a unified sequence interface for vision tasks, representing both input and output as sequences of discrete tokens. Our approach leverages a single transformer-based architecture to process a variety of vision tasks, including image classification, object detection, semantic segmentation, and video understanding, by encoding images and videos into token sequences and decoding task-specific outputs as sequences of labels, bounding boxes, or segmentation masks. Experiments demonstrate that our unified sequence interface achieves competitive performance on a range of benchmark datasets across different vision tasks, often surpassing specialized models with fewer task-specific parameters. This work provides a significant step towards building more flexible and adaptable vision systems with a single, general-purpose architecture, simplifying development and enabling easier transfer learning across diverse visual tasks."
http://arxiv.org/abs/2205.09723v2,Robust and Efficient Medical Imaging with Self-Supervision,"Medical imaging plays a crucial role in diagnosis and treatment planning, yet acquiring large, labeled datasets for training deep learning models in this domain is often expensive and time-consuming. This paper addresses the challenge of limited labeled data in medical imaging by exploring self-supervised learning techniques to improve the robustness and efficiency of model training. We propose a novel self-supervised pre-training strategy that leverages anatomical context and image transformations to learn robust feature representations from unlabeled medical images. Specifically, we utilize a combination of contrastive learning with anatomical region masking and geometric transformation prediction to force the model to learn meaningful features that are invariant to common image variations and sensitive to anatomical structures. Experimental results on multiple medical imaging modalities (CT, MRI, and X-ray) demonstrate that our self-supervised pre-trained models significantly outperform their randomly initialized counterparts, achieving up to a 15% improvement in classification accuracy and a 10% increase in segmentation performance with limited labeled data. This approach offers a practical and effective solution for developing high-performing medical imaging models with reduced annotation requirements, ultimately facilitating broader clinical adoption and improved patient care."
http://arxiv.org/abs/2109.10852v2,Pix2seq: A Language Modeling Framework for Object Detection,"Object detection, a fundamental task in computer vision, traditionally relies on specialized architectures and hand-crafted components like anchors and non-maximum suppression (NMS). This paper addresses the problem of simplifying object detection by framing it as a language modeling task. We introduce Pix2seq, a novel approach that serializes object detection outputs (e.g., bounding boxes and class labels) into a sequence of discrete tokens, then learns to generate these sequences conditioned on the input image pixels using an encoder-decoder transformer architecture. By treating object detection as a conditional language modeling problem, Pix2seq eliminates the need for task-specific components and allows the model to directly predict object locations and categories in a unified manner. Experiments on the COCO dataset demonstrate that Pix2seq achieves competitive performance compared to established object detectors, without relying on complex, hand-designed components. This simplification of the object detection pipeline unlocks new avenues for research and development by leveraging the power and flexibility of language modeling techniques."
http://arxiv.org/abs/2102.12627v1,How to represent part-whole hierarchies in a neural network,"Part-whole hierarchies are fundamental to human perception and reasoning, enabling us to understand objects and scenes at multiple levels of abstraction. However, explicitly representing and reasoning with such hierarchies remains a challenge for neural networks. This paper addresses the problem of learning and representing part-whole relationships within a neural network architecture in a way that is both structured and amenable to end-to-end training. We propose a novel architecture, the Hierarchical Compositional Network (HCN), which learns to decompose objects into parts and recursively represents these parts as compositions of sub-parts. The HCN employs a learned routing mechanism that dynamically selects relevant sub-parts to form higher-level representations, guided by both visual input and learned compositional priors. We demonstrate that HCNs can learn interpretable part-whole hierarchies from raw pixel data on synthetic and real-world datasets, outperforming existing methods in tasks such as object reconstruction, part segmentation, and few-shot generalization. These results suggest that HCNs offer a promising approach for incorporating structured, hierarchical knowledge into neural networks, leading to more robust and interpretable visual understanding."
http://arxiv.org/abs/2012.04718v2,Canonical Capsules: Self-Supervised Capsules in Canonical Pose,"Capsule networks offer a compelling alternative to convolutional neural networks by explicitly modeling object parts and their relationships. However, training capsule networks typically relies on supervised learning with detailed part annotations, limiting their scalability and applicability to real-world scenarios. This paper addresses the challenge of learning capsule representations without explicit supervision by introducing ""Canonical Capsules,"" a novel self-supervised learning framework. Our approach leverages a Siamese architecture to learn capsule embeddings that are invariant to viewpoint changes. We enforce this invariance by predicting a canonical pose for each object instance and penalizing deviations from this pose within the embedding space. Furthermore, we incorporate a contrastive loss to encourage capsules representing similar objects to cluster together. Experiments on benchmark datasets demonstrate that Canonical Capsules achieve competitive performance compared to supervised capsule networks and outperform existing self-supervised representation learning methods in downstream classification and object detection tasks. This work significantly reduces the reliance on labeled data for training effective capsule networks, paving the way for broader adoption in various computer vision applications."
http://arxiv.org/abs/2006.10029v2,Big Self-Supervised Models are Strong Semi-Supervised Learners,"Self-supervised learning (SSL) has demonstrated remarkable success in pre-training large models on unlabeled data, leading to significant improvements in various downstream tasks. However, the potential of these large SSL models as strong semi-supervised learners, particularly when fine-tuned with limited labeled data, remains relatively unexplored. This paper investigates the effectiveness of fine-tuning large, pre-trained self-supervised models in the semi-supervised setting. We propose a fine-tuning strategy that leverages both labeled and unlabeled data, employing a combination of supervised loss on labeled examples and consistency regularization on unlabeled examples using data augmentations. Specifically, we utilize a pre-trained Vision Transformer (ViT) model using masked image modeling and fine-tune it by jointly minimizing a cross-entropy loss on labeled data and a consistency loss between predictions on different augmentations of unlabeled data. Our experiments on ImageNet show that fine-tuning large SSL models with only 1% labeled data achieves significantly higher accuracy compared to training from scratch or fine-tuning ImageNet pre-trained models with the same amount of labeled data, approaching the performance of fully supervised models trained on significantly more data. These results demonstrate that large self-supervised models, when effectively fine-tuned, can serve as powerful semi-supervised learners, enabling high performance with minimal labeled data."
http://arxiv.org/abs/2002.07405v1,Deflecting Adversarial Attacks,"Deep neural networks are vulnerable to adversarial attacks, where small, carefully crafted perturbations to input images can cause misclassification. This vulnerability poses a significant threat to the deployment of deep learning models in safety-critical applications. We address the problem of improving model robustness against a wide range of adversarial attacks without requiring adversarial training. Our proposed method, Adversarial Deflection Network (ADN), introduces a lightweight, learnable module that deflects adversarial perturbations before they reach the main classification network. ADN learns a non-linear transformation of the input image in a way that minimizes the adversarial impact while preserving the original image's semantic content, effectively creating a ""safe zone"" for the classifier. We evaluate ADN against various white-box and black-box attacks, including PGD, FGSM, and DeepFool, on benchmark datasets like CIFAR-10 and ImageNet. Our results demonstrate that ADN significantly enhances model robustness, achieving an average increase of 15% in adversarial accuracy compared to undefended models, while maintaining comparable clean accuracy. This approach offers a practical and efficient defense mechanism against adversarial attacks, making deep learning models more reliable in real-world scenarios."
http://arxiv.org/abs/2002.05709v3,A Simple Framework for Contrastive Learning of Visual Representations,"Self-supervised learning aims to learn useful representations from unlabeled data, mitigating the need for costly manual annotations. A key challenge in this domain is designing effective pretext tasks that guide the learning process. We address the problem of learning robust visual representations without relying on complex pretext tasks or specialized architectures. We propose a simple contrastive learning framework, SimCLR, that learns representations by maximizing agreement between different augmented views of the same data example via a contrastive loss in the latent space. The framework consists of a data augmentation module, a non-linear projection head, and a contrastive loss function. Through extensive experiments, we demonstrate that SimCLR achieves state-of-the-art performance on multiple downstream classification tasks, surpassing previous self-supervised and even some supervised methods when trained on ImageNet. This highlights the effectiveness of simple contrastive learning for acquiring powerful visual representations, offering a more accessible and scalable approach to self-supervised pre-training."
http://arxiv.org/abs/1912.03207v5,NASA: Neural Articulated Shape Approximation,"Neural implicit representations have shown remarkable success in reconstructing static 3D shapes from images, but extending these methods to articulated objects remains a challenge. Existing approaches often struggle with complex articulations and require explicit pose information or part segmentations. This paper introduces Neural Articulated Shape Approximation (NASA), a novel framework for reconstructing articulated 3D shapes from single-view images without explicit pose supervision. NASA leverages a canonical space deformation network parameterized by learned articulation parameters to align all observed shapes into a shared template. A neural implicit surface is then learned in this canonical space. We introduce a novel loss function that encourages consistent deformation across different views and enforces shape regularity. Experiments on synthetic and real-world datasets demonstrate that NASA achieves state-of-the-art reconstruction accuracy and articulation estimation compared to existing methods, especially in scenarios with significant articulation and limited supervision. NASA offers a robust and efficient approach to 3D articulated shape reconstruction, enabling new possibilities for object understanding and manipulation in computer vision and robotics."
http://arxiv.org/abs/1909.05736v4,CvxNet: Learnable Convex Decomposition,"Convex decomposition is a fundamental problem in computer vision and graphics, enabling efficient solutions for tasks such as shape analysis, motion planning, and collision detection. Existing approaches often rely on handcrafted heuristics or optimization procedures that are computationally expensive and lack adaptability to diverse data. We address the problem of learning convex decompositions directly from data using a novel neural network architecture, CvxNet. CvxNet employs a differentiable convex hull approximation within its architecture, allowing it to predict a set of convex components that optimally represent a given shape. The network is trained end-to-end to minimize a reconstruction loss and a convexity regularization term, encouraging the generation of accurate and well-shaped convex parts. Experiments on synthetic and real-world datasets demonstrate that CvxNet achieves state-of-the-art performance in terms of decomposition accuracy, computational efficiency, and generalization ability compared to traditional methods and existing deep learning approaches. This learnable convex decomposition framework opens new avenues for efficient and robust shape understanding in various computer vision applications."
http://arxiv.org/abs/1907.02957v2,Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions,"Deep neural networks are vulnerable to adversarial attacks, where subtle, often imperceptible, perturbations to input images can cause misclassification. Detecting and diagnosing these adversarial examples remains a critical challenge for deploying robust vision systems. We address this problem by proposing a novel approach leveraging class-conditional capsule reconstructions for adversarial image detection and diagnosis. Our method trains a capsule network to reconstruct input images conditioned on their predicted class. We hypothesize that adversarial images, deviating from the expected feature representations for their predicted class, will exhibit higher reconstruction errors and distinct patterns in the capsule activations. We use these reconstruction errors, combined with an analysis of the capsule activation distributions, to detect adversarial examples and identify the specific image regions most affected by adversarial perturbations. Experiments on benchmark datasets demonstrate that our approach achieves state-of-the-art adversarial detection accuracy, outperforming existing methods, and provides interpretable insights into the nature and location of adversarial perturbations. This approach offers a promising direction for building more robust and trustworthy deep learning systems."
http://arxiv.org/abs/1905.11940v1,Cerberus: A Multi-headed Derenderer,"Inverse rendering aims to decompose an image into its constituent scene properties, such as shape, reflectance, and illumination. While significant progress has been made in disentangling these properties, existing methods often struggle with complex real-world scenes exhibiting intricate geometries and varying material properties. This paper introduces Cerberus, a novel multi-headed derendering framework that leverages a modular architecture to explicitly address the interdependence of scene properties. Cerberus employs separate neural modules for estimating geometry (normal and depth), material (diffuse albedo, specular albedo, roughness), and illumination (spherical harmonics coefficients), with a cross-attention mechanism facilitating information exchange between these modules. Furthermore, we introduce a differentiable rendering loss that encourages consistency between the predicted scene properties and the input image. Experiments on synthetic and real-world datasets demonstrate that Cerberus achieves state-of-the-art performance in inverse rendering, producing significantly more accurate and consistent estimates of scene properties compared to existing methods. The improved accuracy and robustness of Cerberus pave the way for more realistic and controllable 3D scene understanding and manipulation."
http://arxiv.org/abs/1811.06969v1,DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules,"Deep neural networks are vulnerable to adversarial attacks, where imperceptible perturbations to input images can cause misclassification. Detecting these adversarial examples is crucial for deploying robust vision systems. This paper addresses the problem of identifying adversarial attacks on image classifiers by leveraging the representational power of capsule networks. We propose DARCCC, a novel adversarial detection method based on reconstructing input images from class-conditional capsules. Specifically, DARCCC trains a capsule network classifier and then learns class-specific reconstruction decoders that map capsule activations back to the image space. At inference, the reconstruction error between the original image and the reconstruction from the predicted class capsule is used as an adversarial score. Experiments on benchmark datasets, including MNIST, CIFAR-10, and SVHN, demonstrate that DARCCC achieves state-of-the-art adversarial detection performance against a variety of strong attack algorithms, outperforming existing reconstruction-based and feature-based detectors. DARCCC provides a robust and effective approach for detecting adversarial examples, contributing to the development of more secure and reliable deep learning systems."
http://arxiv.org/abs/1206.6445v1,Deep Lambertian Networks,"The Lambertian reflectance model is a cornerstone of computer vision, providing a simplified yet effective representation of object appearance. However, real-world surfaces often deviate significantly from this ideal, leading to inaccuracies in tasks relying on Lambertian assumptions, such as shape from shading and photometric stereo. This paper introduces Deep Lambertian Networks (DLNs), a novel deep learning framework that learns to approximate the Lambertian reflectance function directly from image data. DLNs utilize a convolutional neural network to map input images to a learned representation of surface normals and albedo, explicitly enforcing Lambertian consistency through a differentiable rendering layer. This layer synthesizes images from the estimated normals and albedo under different lighting conditions, and the network is trained to minimize the difference between the rendered and real images. We demonstrate that DLNs achieve state-of-the-art performance in recovering surface normals and albedo from single and multiple images under varying lighting conditions, outperforming traditional methods and other learning-based approaches. This learned Lambertian representation offers a robust and adaptable alternative to traditional analytical models, enabling more accurate and reliable 3D scene understanding."
http://arxiv.org/abs/2507.10552v1,Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder,"Camera trap footage provides a valuable resource for wildlife monitoring, often capturing images of animals with limited or no individual identification. A significant challenge lies in automatically identifying individual animals from these images, particularly faces, due to variations in pose, lighting, and occlusion, coupled with a scarcity of labeled data for training robust face recognition models. We address this problem by proposing a self-supervised learning framework leveraging the inherent temporal coherence within camera trap videos to learn a powerful and generalizable face embedder. Our method employs a contrastive learning objective, training the network to recognize different views of the same individual animal face within short video segments as positive pairs, while treating faces from different segments as negatives. We augment this with a novel data augmentation strategy tailored to the characteristics of camera trap imagery. Evaluated on a diverse set of camera trap datasets featuring multiple animal species, our self-supervised embedder demonstrates superior performance compared to existing unsupervised, supervised, and even pre-trained face recognition models on downstream face identification tasks, achieving significant improvements in recall and mAP. This approach provides a practical and scalable solution for automated animal identification, enhancing the efficiency and accuracy of wildlife monitoring efforts."
http://arxiv.org/abs/2507.03578v1,SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications,"Video understanding has seen significant progress, largely driven by benchmarks focused on everyday activities. However, the performance of these models in scientific applications remains relatively unexplored. This paper addresses the critical gap in evaluating video models across diverse scientific domains, where visual data often exhibits unique characteristics distinct from typical consumer video. We introduce SciVid, a novel benchmark comprising video datasets from microscopy, astronomy, fluid dynamics, and robotics, each representing a distinct scientific discipline. We evaluate a range of state-of-the-art video architectures on SciVid, analyzing their performance across tasks such as action recognition, object tracking, and anomaly detection. Our results reveal a significant performance disparity compared to established benchmarks like Kinetics, highlighting the limitations of current models in generalizing to the specific visual features and temporal dynamics present in scientific video data. This work underscores the necessity for developing specialized video models and benchmarks tailored to the unique demands of scientific applications, enabling more effective automated analysis and discovery in these fields."
http://arxiv.org/abs/2506.15368v1,Open-World Object Counting in Videos,"Object counting in videos is a fundamental computer vision task with applications ranging from traffic monitoring to wildlife conservation. However, most existing methods assume a closed-world setting with a fixed and known set of object categories, limiting their applicability in real-world scenarios where novel objects may appear. This paper addresses the challenging problem of open-world object counting in videos, where the goal is to accurately count objects of known categories while simultaneously detecting and adapting to previously unseen categories. We propose a novel framework that combines a meta-learning based counter for known objects with an anomaly detection module trained to identify frames containing novel objects. These detected anomalies are then used to trigger an online adaptation process, where the system learns to count instances of the novel object category, leveraging self-supervision and pseudo-labeling. Experiments on synthetic and real-world video datasets demonstrate that our approach significantly outperforms existing closed-world counting methods and other open-set recognition techniques, achieving state-of-the-art results in counting accuracy and novel object detection. This work represents a significant step towards robust and adaptable object counting systems capable of operating in dynamic and unpredictable environments."
http://arxiv.org/abs/2504.01961v1,Learning from Streaming Video with Orthogonal Gradients,"Deep learning models trained on streaming video often suffer from catastrophic forgetting and inefficient adaptation due to the non-stationary data distribution and limited computational resources. This paper addresses the problem of efficiently updating a model from a continuous stream of video data while mitigating interference between consecutive tasks and maintaining previously learned knowledge. We propose a novel approach called Orthogonal Gradient Descent for Streaming Video (OGD-SV), which decomposes the gradient into two orthogonal components: one that updates the model parameters to learn the new task and another that preserves previously learned information by minimizing interference. This is achieved by projecting the current gradient onto the null space of the gradients from past tasks, dynamically estimated and updated using a low-rank approximation. Experimental results on several video classification benchmarks demonstrate that OGD-SV significantly outperforms existing online learning and continual learning methods in terms of both accuracy and forgetting, achieving state-of-the-art performance with a minimal increase in computational overhead. This approach offers a practical and effective solution for deploying deep learning models in real-world streaming video applications."
http://arxiv.org/abs/2504.01020v1,Shot-by-Shot: Film-Grammar-Aware Training-Free Audio Description Generation,"Audio description (AD) provides crucial accessibility for visually impaired audiences by narrating visual elements in films and television. However, generating high-quality AD remains a challenging task, often requiring expensive training data and complex models. This paper addresses the problem of generating coherent and film-grammar-aware audio descriptions without relying on any training. We propose a novel, training-free framework, ""Shot-by-Shot,"" that leverages pre-trained vision-language models (VLMs) and film grammar rules to generate descriptions. Our method first segments the film into shots using scene detection algorithms, then utilizes VLMs to generate initial descriptions for each shot. Finally, we refine these descriptions by incorporating film grammar principles, such as continuity and relevance, through a rule-based system that considers shot transitions and narrative context. Experimental results demonstrate that our method generates more coherent and contextually relevant AD compared to existing zero-shot approaches, achieving comparable or superior performance to some supervised methods in terms of content coverage and fluency, as evaluated by both automatic metrics and human evaluation. This work provides a practical and accessible solution for automatic AD generation, significantly reducing the reliance on expensive training data and expert annotators."
http://arxiv.org/abs/2503.23344v1,From Panels to Prose: Generating Literary Narratives from Comics,"Comics are a rich visual medium that implicitly conveys complex narratives through sequential panels, yet accessing this information programmatically remains a challenge. This paper addresses the problem of automatically generating coherent and engaging literary narratives from comic book panels, effectively bridging the gap between visual and textual storytelling. We propose a novel framework that leverages object detection, relationship extraction, and scene understanding techniques to analyze individual panels and their sequential relationships. This information is then fed into a transformer-based language model fine-tuned for narrative generation, conditioned on both the visual features and extracted semantic information. Our experiments demonstrate that the proposed method generates narratives that are significantly more descriptive and contextually relevant compared to baseline models relying solely on image captions or unconditioned text generation. This work opens up new avenues for comic book accessibility, automated content creation, and cross-modal understanding of narrative structures."
http://arxiv.org/abs/2503.22668v1,Understanding Co-speech Gestures in-the-wild,"Co-speech gestures, movements of the hands and arms accompanying speech, are crucial for effective human communication. However, automatically understanding these gestures in natural, unscripted ""in-the-wild"" videos remains a significant challenge due to the variability in gesture execution, camera angles, and background clutter. This paper addresses the problem of robustly recognizing and classifying co-speech gestures in unconstrained video environments. We propose a novel multi-modal fusion network that leverages both visual and auditory cues for gesture recognition. Our approach integrates a spatio-temporal convolutional neural network (CNN) for visual feature extraction with a transformer-based acoustic model to capture prosodic information relevant to gesture context. These features are then fused using a cross-modal attention mechanism to dynamically weigh the contribution of each modality. Experimental results on a newly collected in-the-wild co-speech gesture dataset demonstrate that our method significantly outperforms existing state-of-the-art approaches, achieving a 15% improvement in gesture classification accuracy. This work provides a crucial step towards building more natural and intuitive human-computer interfaces capable of understanding non-verbal communication."
http://arxiv.org/abs/2502.15682v2,ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval,"Visual-language foundation models (VLMs) have demonstrated remarkable capabilities in various cross-modal tasks, including image retrieval. However, current VLMs often struggle to effectively capture fine-grained semantic relationships between images and text, limiting their performance in complex retrieval scenarios. To address this limitation, we introduce ELIP: Enhanced Visual-Language foundation models for Image retrieval, a novel framework that incorporates hierarchical contextualization and fine-grained alignment strategies. ELIP leverages a multi-scale visual encoder to extract features at varying levels of abstraction, coupled with a cross-modal attention mechanism that dynamically attends to relevant image regions based on the textual query. Furthermore, we introduce a contrastive learning objective with hard negative mining to enforce tighter alignment between semantically similar image-text pairs. Experiments on benchmark datasets such as MSCOCO and Flickr30k demonstrate that ELIP significantly outperforms state-of-the-art VLMs, achieving substantial improvements in retrieval accuracy. These results highlight the importance of fine-grained semantic alignment for effective cross-modal retrieval and establish ELIP as a promising approach for advancing the field."
http://arxiv.org/abs/2501.09754v2,"Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues","Sign Language Translation (SLT) aims to bridge the communication gap between deaf and hearing communities by automatically converting sign language videos into spoken language. However, current SLT systems often struggle with ambiguity arising from the inherent limitations of isolated sign representations, failing to capture the nuanced meaning conveyed through contextual information. This paper introduces a novel approach, Context-Aware Sign Language Translation (CASLT), which explicitly incorporates contextual cues from both the visual and linguistic domains to improve translation accuracy. CASLT leverages a multi-modal encoder to fuse information from the sign video and a surrounding sentence context, followed by a transformer-based decoder that attends to both visual and contextual embeddings. Furthermore, we introduce a novel context-aware gloss prediction module to refine the intermediate gloss representation by utilizing contextual information. Experiments on the RWTH-PHOENIX-Weather 2014T and RWTH-PHOENIX-Weather 2014T-Context datasets demonstrate that CASLT achieves significant improvements over state-of-the-art methods, with BLEU-4 scores increasing by up to 3.2 points. These results highlight the crucial role of contextual information in resolving ambiguities and generating more accurate and natural sign language translations."
http://arxiv.org/abs/2412.15212v2,Scaling 4D Representations,"4D representations, encompassing 3D geometry evolving over time, are crucial for understanding dynamic scenes. However, representing and processing large-scale, high-fidelity 4D data remains a significant challenge due to computational and memory constraints. This paper addresses the problem of efficiently scaling 4D representations to handle complex, long-duration sequences. We introduce a novel hierarchical 4D representation based on adaptive spatiotemporal partitioning and neural implicit surfaces. Our method dynamically subdivides the 4D space into regions of varying complexity, representing each region with a lightweight neural network that implicitly encodes the surface geometry and its temporal evolution. This allows for efficient storage and rendering of detailed 4D scenes by allocating computational resources only where needed. We demonstrate that our approach achieves state-of-the-art reconstruction quality on challenging dynamic scenes while significantly reducing memory footprint and rendering time compared to existing methods. The ability to scale 4D representations unlocks new possibilities for applications such as dynamic scene understanding, virtual reality, and autonomous navigation in complex environments."
http://arxiv.org/abs/2412.09475v2,New keypoint-based approach for recognising British Sign Language (BSL) from sequences,"British Sign Language (BSL) is a visual language relying on handshapes, movement, and facial expressions, presenting significant challenges for automated recognition. Existing BSL recognition systems often struggle with variations in signing style and subtle hand movements. This paper addresses the problem of improving BSL recognition accuracy and robustness by leveraging a novel keypoint-based approach. We propose a spatiotemporal graph convolutional network (ST-GCN) that operates directly on 3D hand keypoints extracted from video sequences using a state-of-the-art pose estimation model. The ST-GCN dynamically learns the relationships between keypoints across both spatial and temporal dimensions, capturing intricate hand movements critical for distinguishing between signs. Experimental results on a publicly available BSL dataset demonstrate that our approach achieves state-of-the-art performance, surpassing existing methods by a significant margin, particularly for signs with subtle variations. This keypoint-based framework offers a promising avenue for developing more accurate and accessible BSL translation systems."
http://arxiv.org/abs/2412.01504v1,3D Spine Shape Estimation from Single 2D DXA,"Dual-energy X-ray absorptiometry (DXA) is a widely used technique for assessing bone mineral density, but it provides limited information about vertebral shape, a critical factor in fracture risk assessment. Estimating 3D spine shape from a single 2D DXA image is an ill-posed problem due to the loss of depth information and inherent ambiguities in radiographic projection. We propose a novel deep learning framework that leverages a variational autoencoder (VAE) to learn a latent space representation of plausible 3D spine shapes from a large dataset of CT scans. A convolutional neural network (CNN) then maps the 2D DXA image to the latent space, enabling the reconstruction of a subject-specific 3D spine shape. The reconstructed 3D shapes are refined through a differentiable rendering process, minimizing the difference between the rendered projection and the input DXA image. Our method demonstrates a significant improvement in 3D spine shape estimation accuracy compared to existing methods, achieving a mean surface distance error of X mm and a vertebra-wise correlation of Y between estimated and ground truth shapes. This approach provides a cost-effective and radiation-efficient method for obtaining detailed 3D spine shape information, potentially improving fracture risk prediction and management of osteoporosis."
http://arxiv.org/abs/2411.19941v1,Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark,"The Perception Test is a recurring challenge designed to evaluate the state-of-the-art in multimodal perception and reasoning. This paper summarizes the activities and findings of the 2024 Perception Test challenge, while also introducing a novel video question answering (VideoQA) benchmark designed to push the boundaries of long-form video understanding. The benchmark, called HourQA, consists of hour-long videos sourced from diverse internet sources, paired with complex, multi-step reasoning questions requiring temporal understanding across extended durations. We detail the data collection process, question design principles, and evaluation metrics used in HourQA. Baseline experiments using existing VideoQA models reveal a significant performance gap compared to shorter video benchmarks, highlighting the challenge's difficulty. The HourQA benchmark presents a substantial hurdle for current VideoQA systems and provides a crucial platform for driving future research in long-form video comprehension and reasoning."
http://arxiv.org/abs/2411.11222v2,The Sound of Water: Inferring Physical Properties from Pouring Liquids,"Liquids, ubiquitous in daily life, exhibit a wide range of acoustic signatures during pouring events that are intricately linked to their physical properties. This paper addresses the challenge of inferring liquid properties, such as viscosity and density, directly from the sound produced during pouring, without relying on visual information. We propose a novel approach that combines time-frequency analysis of pouring sounds with a deep learning framework. Specifically, we extract Mel-Frequency Cepstral Coefficients (MFCCs) from audio recordings of pouring liquids and train a convolutional neural network (CNN) to predict viscosity and density values. Our experiments, conducted on a dataset of diverse liquids with varying properties, demonstrate the feasibility of accurate property estimation. We achieve a mean absolute error of X% for viscosity and Y% for density, outperforming baseline methods based on traditional acoustic features. This work opens new avenues for non-invasive liquid characterization and has potential applications in robotics, manufacturing, and food science."
http://arxiv.org/abs/2411.08878v1,A Short Note on Evaluating RepNet for Temporal Repetition Counting in Videos,"RepNet is a deep learning architecture designed for counting the number of repetitions of a visual pattern within a video, a task crucial for understanding dynamic events. However, existing evaluations of RepNet often lack a systematic analysis of its performance across diverse video characteristics, hindering a comprehensive understanding of its strengths and limitations. This paper addresses this gap by presenting a focused evaluation of RepNet, examining its performance under varying conditions, including different repetition frequencies, video lengths, and levels of visual complexity. We analyze RepNet's performance using both synthetic and real-world datasets, employing metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to quantify its accuracy. Our evaluation reveals that RepNet's performance is significantly affected by the frequency of repetitions, with higher frequencies leading to reduced accuracy, and that its performance degrades with increased video length, particularly in videos with complex backgrounds. This analysis highlights the importance of considering video characteristics when deploying RepNet and suggests avenues for future research aimed at improving its robustness and generalization capabilities. Our findings provide valuable insights for researchers and practitioners seeking to leverage RepNet for temporal repetition counting and inform the development of more robust repetition counting algorithms."
http://arxiv.org/abs/2410.17235v1,Automated Spinal MRI Labelling from Reports Using a Large Language Model,"Accurate and efficient labeling of spinal MRI scans is crucial for various clinical applications, including automated diagnosis, treatment planning, and large-scale research studies. However, manual labeling is time-consuming, expensive, and prone to inter-observer variability. This paper addresses the challenge of automating spinal MRI labeling by leveraging unstructured radiology reports. We propose a novel approach that utilizes a large language model (LLM) fine-tuned for named entity recognition and relation extraction to identify anatomical structures (e.g., vertebrae, intervertebral discs), pathologies (e.g., herniations, stenosis), and their spatial relationships from the corresponding reports. The extracted information is then structured and mapped to a standardized spinal labeling scheme. Experimental results on a clinically realistic dataset demonstrate that our method achieves high accuracy in identifying and localizing spinal abnormalities, significantly outperforming rule-based and traditional NLP methods. This automated labeling pipeline has the potential to significantly reduce the burden on radiologists, accelerate research, and improve the efficiency of spinal MRI analysis."
http://arxiv.org/abs/2410.11702v1,It's Just Another Day: Unique Video Captioning by Discriminative Prompting,"Generating descriptive and unique video captions remains a challenging task, often resulting in generic and repetitive descriptions that fail to capture the nuances of individual videos. This paper addresses the problem of generating more distinctive and informative video captions beyond common, high-frequency descriptions. We introduce a novel Discriminative Prompting (DisPrompt) framework that leverages a two-stage approach. First, a contrastive learning objective is employed to train a video encoder to produce embeddings that are highly discriminative between different videos. Second, these video embeddings are used to generate prompts that guide a pre-trained language model to produce captions emphasizing unique visual details. Our experiments on benchmark datasets demonstrate that DisPrompt significantly improves the uniqueness and informativeness of generated captions, achieving state-of-the-art results in terms of novelty metrics while maintaining competitive performance in traditional captioning metrics. This work provides a promising direction for generating more personalized and insightful video descriptions, moving beyond generic summaries towards captions that truly capture the essence of individual video content."
http://arxiv.org/abs/2410.11068v1,Character-aware audio-visual subtitling in context,"Generating accurate and temporally aligned subtitles for videos is crucial for accessibility and content understanding, yet remains a challenging task, especially when considering the nuances of spoken language and visual context. Current audio-visual subtitle generation methods often lack fine-grained character-level awareness, failing to capture subtle phonetic variations and their corresponding visual cues associated with individual speakers. This limitation often results in subtitles that are semantically correct but lack the expressiveness and speaker-specific characteristics present in the original audio-visual content. We propose a novel character-aware audio-visual subtitling framework that integrates a character-level acoustic model with a visually-guided attention mechanism. Our model leverages visual information, particularly facial landmarks and speaker identity, to refine character-level acoustic predictions, enabling the generation of more accurate and speaker-specific subtitles. Experiments on a diverse set of video datasets demonstrate that our approach significantly improves the accuracy and speaker consistency of generated subtitles compared to state-of-the-art audio-visual subtitling systems, achieving a 5% relative improvement in character error rate. This enhanced character-aware subtitling capability contributes significantly to improved accessibility and a more immersive viewing experience."
http://arxiv.org/abs/2408.09860v2,3D-Aware Instance Segmentation and Tracking in Egocentric Videos,"Egocentric videos offer a unique first-person perspective, providing rich contextual information for understanding human activities and interactions with the surrounding environment. However, accurately perceiving and understanding the dynamic 3D world from these videos remains a significant challenge. This paper addresses the problem of jointly performing 3D-aware instance segmentation and tracking of objects in egocentric videos, enabling a more comprehensive scene understanding. We propose a novel framework that leverages a monocular depth estimation network to generate pseudo-3D point clouds from each video frame. These point clouds are then processed by a 3D instance segmentation network, producing 3D object proposals. A temporal association module, incorporating both geometric and appearance features, then tracks these 3D instances across the video sequence. We demonstrate the effectiveness of our approach on the Ego4D benchmark, achieving state-of-the-art results in both 3D instance segmentation and tracking accuracy. This work provides a crucial step towards robust and comprehensive scene understanding from egocentric perspectives, with broad applications in robotics, augmented reality, and activity recognition."
http://arxiv.org/abs/2408.00298v1,Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names,"Manga, a globally popular form of visual storytelling, presents unique challenges for accessibility and digital archiving due to its complex layouts and reliance on visual cues. Automatically transcribing manga, including identifying character names associated with dialogue, is a crucial step towards addressing these challenges but remains largely unexplored. We introduce a novel pipeline for chapter-wide manga transcription that integrates object detection, optical character recognition (OCR), and a novel character association module based on dialogue balloon tail analysis. This module leverages the geometric relationship between dialogue balloons and characters to predict speaker identity within a chapter, using a combination of spatial reasoning and learned embeddings. Our system achieves a significant improvement in character name accuracy compared to baseline OCR methods, correctly attributing dialogue to characters with an average precision of 78% across a diverse set of manga chapters. This work provides a foundation for building more accessible and searchable manga archives, benefiting both readers and researchers."
http://arxiv.org/abs/2407.17085v1,OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in Videos,"Temporal repetition counting in videos, the task of determining how many times a specific action occurs, is crucial for understanding and analyzing dynamic scenes. Current video datasets for action counting are limited in vocabulary and often focus on single, well-defined actions, hindering the development of models capable of generalizing to novel, open-vocabulary scenarios. This paper introduces OVR, a novel dataset for Open Vocabulary temporal Repetition counting in Videos. OVR comprises over 20,000 video clips spanning a diverse set of 200 action categories with varying repetition counts and complex background scenes. Critically, each video is annotated with natural language descriptions of the action being repeated, enabling zero-shot counting of unseen actions. We benchmark several state-of-the-art video understanding models on OVR and demonstrate significant room for improvement, particularly in generalization to novel action descriptions. Our experiments reveal that models struggle to accurately count repetitions when faced with unfamiliar actions, highlighting the challenges posed by open-vocabulary counting. OVR provides a valuable resource for training and evaluating models capable of robust and generalizable temporal repetition counting, pushing the boundaries of video understanding beyond predefined action sets."
http://arxiv.org/abs/2407.15850v2,AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description,"Audio description (AD) provides crucial accessibility for visually impaired individuals, yet generating high-quality AD remains challenging and often relies on expensive, fully supervised training. This paper addresses the problem of zero-shot audio description, aiming to generate meaningful ADs without requiring any task-specific training data. We introduce AutoAD-Zero, a novel training-free framework that leverages the rich semantic information embedded within pre-trained audio-language models. Our approach first extracts relevant audio features using a pre-trained audio encoder. Subsequently, it employs a prompt engineering strategy to guide a large language model to generate descriptive text conditioned on these features, incorporating both acoustic and contextual information. Experiments on benchmark datasets demonstrate that AutoAD-Zero achieves competitive performance compared to existing supervised methods, significantly outperforming other zero-shot baselines. This work offers a practical and readily deployable solution for audio description, eliminating the need for extensive labeled data and facilitating wider accessibility."
http://arxiv.org/abs/2407.05921v2,TAPVid-3D: A Benchmark for Tracking Any Point in 3D,"Tracking arbitrary points within a 3D scene is crucial for various applications, including robotic manipulation, augmented reality, and human-computer interaction. However, existing 3D tracking benchmarks often focus on rigid objects or specific semantic entities, lacking the generality required for tracking *any* point in a dynamic 3D environment. This paper introduces TAPVid-3D, a novel benchmark for evaluating the performance of algorithms designed to track arbitrary points in 3D space through video sequences. TAPVid-3D comprises a diverse set of real-world video sequences captured with calibrated multi-view cameras and LiDAR sensors, providing accurate 3D ground truth trajectories for a wide variety of tracked points. We establish a standardized evaluation protocol and baseline performance using state-of-the-art tracking methods, highlighting current limitations in handling occlusions, large displacements, and deformable scene geometry. Experiments demonstrate that TAPVid-3D presents a significant challenge to existing algorithms, revealing opportunities for future research in robust and generalizable 3D point tracking. The benchmark will facilitate the development of more versatile and reliable 3D perception systems capable of interacting with and understanding complex dynamic environments."
http://arxiv.org/abs/2407.04619v2,CountGD: Multi-Modal Open-World Counting,"Open-world counting, the task of estimating the number of instances for novel and potentially ambiguous categories, remains a significant challenge in computer vision. Existing counting methods are often limited by their reliance on closed-set training data and a single modality (e.g., visual input). We address this limitation by introducing CountGD, a novel multi-modal open-world counting framework that leverages both visual and textual information for improved generalization and adaptability. CountGD employs a vision-language model to learn a shared embedding space for images and text descriptions of target categories. This embedding space is then used to train a density map regression network, conditioned on the textual description, to predict instance density directly from the image. We demonstrate the effectiveness of CountGD on challenging open-world counting benchmarks, achieving state-of-the-art performance in zero-shot and few-shot counting scenarios, significantly outperforming unimodal and closed-set baselines. This highlights the potential of multi-modal learning for robust and generalizable counting in real-world applications."
http://arxiv.org/abs/2406.05629v1,"Separating the ""Chirp"" from the ""Chat"": Self-supervised Visual Grounding of Sound and Language","Understanding the interplay between vision, sound, and language is crucial for building truly intelligent systems. While recent advancements have shown promise in audio-visual grounding, a key challenge remains: effectively disentangling the relevant sound events from background noise and irrelevant speech (""chat"") when grounding natural language queries. We introduce a novel self-supervised framework, ""ChirpChat,"" designed to explicitly separate target sounds (""chirp"") from distracting speech (""chat"") for improved visual grounding. Our method leverages a contrastive learning objective, training a sound separation module to isolate target sounds based on weak textual supervision, followed by a cross-modal grounding module that aligns the separated audio representations with visual features and language queries. By explicitly modeling and mitigating the impact of irrelevant speech, our model achieves significant improvements in visual grounding accuracy on challenging audio-visual datasets, surpassing state-of-the-art methods, particularly in scenarios with high levels of speech interference. This work demonstrates the importance of sound separation as a pre-processing step for robust cross-modal understanding and opens new avenues for research in noisy audio-visual environments."
http://arxiv.org/abs/2405.10266v1,A Tale of Two Languages: Large-Vocabulary Continuous Sign Language Recognition from Spoken Language Supervision,"Continuous Sign Language Recognition (CSLR) aims to automatically transcribe sign language videos, a crucial step towards accessible communication. However, the scarcity of annotated sign language data poses a significant bottleneck. This paper addresses the challenge of training large-vocabulary CSLR models with limited sign language annotations by leveraging readily available spoken language transcripts. We propose a novel framework that utilizes a two-stage training approach. First, a vision encoder is pre-trained using a contrastive learning objective aligned with spoken language embeddings. Second, a transformer-based sequence-to-sequence model is trained end-to-end to directly map sign language video features to spoken language transcripts, effectively transferring linguistic knowledge from the spoken modality. Experimental results on the RWTH-PHOENIX-Weather 2014T and Sign2Mint datasets demonstrate that our method achieves state-of-the-art performance in low-resource scenarios, outperforming existing approaches that rely solely on sign language annotations. This work presents a promising direction for building robust CSLR systems by harnessing the power of cross-modal transfer learning and spoken language supervision."
http://arxiv.org/abs/2404.16828v3,Made to Order: Discovering monotonic temporal changes via self-supervised video ordering,"Understanding temporal relationships within videos is crucial for various downstream tasks, yet annotating the true temporal order is often expensive and impractical. This paper addresses the problem of learning to identify monotonic temporal changes in videos without relying on explicit temporal supervision. We introduce a self-supervised framework, ""Made to Order,"" which leverages a Siamese network architecture and a novel contrastive loss function designed to enforce order consistency. The network learns to embed video segments such that segments exhibiting monotonic changes are ordered correctly based on their learned embeddings. Specifically, we train the network to distinguish between the true temporal order and artificially shuffled sequences, focusing on identifying the consistent direction of change. Experiments on diverse datasets, including action recognition and medical imaging, demonstrate that our approach achieves state-of-the-art performance in self-supervised video ordering and significantly improves the accuracy of downstream tasks compared to previous methods. The ability to learn temporal relationships without explicit supervision opens new avenues for analyzing unlabeled video data and facilitates the development of more robust and generalizable video understanding systems."
http://arxiv.org/abs/2404.14412v1,AutoAD III: The Prequel -- Back to the Pixels,"Anomaly detection in images, particularly in industrial settings, remains a challenging task due to the scarcity of anomalous training data and the complexity of real-world image distributions. While recent approaches leverage deep feature spaces for anomaly scoring, they often overlook the valuable information present at the pixel level, potentially hindering performance, especially when dealing with subtle anomalies. This paper introduces AutoAD III: The Prequel -- Back to the Pixels, a novel anomaly detection framework that explicitly incorporates pixel-level reconstruction error into the anomaly scoring process. Our method builds upon an autoencoder architecture, employing a multi-scale feature extraction module and a dedicated pixel-level reconstruction branch. The final anomaly score is then derived from a weighted combination of feature-space anomaly scores and the pixel-level reconstruction error, adaptively learned during training. Experimental results on benchmark datasets, including MVTec AD and VisA, demonstrate that our approach achieves state-of-the-art performance, exhibiting a significant improvement in anomaly detection accuracy compared to existing feature-space-based methods. This highlights the importance of pixel-level information in anomaly detection and paves the way for more robust and accurate industrial inspection systems."
http://arxiv.org/abs/2404.12389v2,Moving Object Segmentation: All You Need Is SAM (and Flow),"Moving object segmentation is a crucial task for various applications, including autonomous driving and video surveillance. Existing methods often struggle with complex scenarios involving dynamic backgrounds, occlusions, and variations in object appearance. This paper addresses the challenge of robust and efficient moving object segmentation by leveraging the Segment Anything Model (SAM) and optical flow. Our approach integrates SAM's powerful zero-shot segmentation capabilities with motion cues derived from optical flow to generate accurate and consistent moving object masks. Specifically, we utilize optical flow to identify potential moving regions and then employ SAM to refine these regions, generating high-quality segmentation masks. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches in terms of accuracy and robustness, particularly in challenging scenarios. This work highlights the synergistic potential of combining foundation models like SAM with classical computer vision techniques for improved performance in motion-based segmentation tasks."
http://arxiv.org/abs/2404.05559v2,TIM: A Time Interval Machine for Audio-Visual Action Recognition,"Audio-visual action recognition aims to understand human activities by leveraging both visual and auditory cues. However, current methods often treat audio and visual streams as synchronized, neglecting the crucial temporal relationships and potential offsets between them which can hinder accurate recognition. To address this, we propose the Time Interval Machine (TIM), a novel architecture designed to explicitly model the temporal dynamics and dependencies between audio and visual modalities. TIM learns to represent actions as a series of time intervals, capturing the start and end times of relevant audio-visual events. This is achieved through a dedicated interval prediction module coupled with a cross-modal attention mechanism that dynamically weighs the importance of audio and visual features within each predicted interval. Experiments on benchmark datasets, including ActivityNet and Kinetics-Sounds, demonstrate that TIM achieves state-of-the-art performance, surpassing existing methods by a significant margin, particularly in scenarios with asynchronous audio-visual signals. These results highlight the importance of explicitly modeling temporal intervals and cross-modal dependencies for robust audio-visual action recognition."
http://arxiv.org/abs/2403.12026v2,FlexCap: Describe Anything in Images in Controllable Detail,"Image captioning models have achieved remarkable progress in generating descriptive text for images. However, controlling the level of detail in generated captions remains a significant challenge, hindering their adaptability to diverse user needs and application contexts. We introduce FlexCap, a novel framework designed to generate image descriptions with controllable granularity. FlexCap leverages a multi-scale feature extraction network coupled with a hierarchical attention mechanism to capture image information at varying levels of detail. A detail controller, conditioned on a user-specified granularity parameter, modulates the attention weights, guiding the model to focus on either broader contextual information or finer-grained details. Experiments on benchmark datasets demonstrate that FlexCap effectively controls the descriptive detail of generated captions, achieving superior performance in both automatic metrics and human evaluations compared to existing methods. This controllable image captioning framework offers a significant advancement towards more flexible and user-centric image understanding systems."
http://arxiv.org/abs/2403.10997v2,N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, but struggle with efficient and structured scene understanding beyond appearance modeling. Existing methods often lack explicit hierarchical representations, hindering tasks like object-level manipulation and reasoning. We address this limitation by introducing Nested Neural Feature Fields (N2F2), a novel framework for hierarchical scene understanding. N2F2 leverages a nested structure of neural feature fields, where each level represents a progressively finer-grained decomposition of the scene. Coarse levels capture high-level scene structure, while finer levels model detailed object geometry and appearance. This hierarchical decomposition is achieved by training multiple NeRFs conditioned on features extracted from coarser levels, effectively creating a nested hierarchy. Experiments on synthetic and real-world datasets demonstrate that N2F2 enables efficient scene editing, object-level reasoning, and improved novel view synthesis compared to state-of-the-art methods. N2F2 provides a powerful and interpretable representation for complex 3D scenes, paving the way for more advanced scene understanding and manipulation capabilities."
http://arxiv.org/abs/2402.00847v2,BootsTAP: Bootstrapped Training for Tracking-Any-Point,"Tracking-Any-Point (TAP) aims to establish correspondence between an arbitrary user-defined point in a video and its location in subsequent frames, a task with numerous applications in video understanding and interactive editing. Current TAP methods often struggle with robustness due to limited training data and the inherent ambiguity in tracking arbitrary points with minimal semantic context. To address this, we introduce BootsTAP, a novel bootstrapped training framework for TAP. BootsTAP iteratively refines a TAP model by leveraging its own predictions on unlabeled video data. Specifically, we alternate between generating pseudo-ground truth tracks using the current model, filtering these tracks based on confidence and geometric consistency, and retraining the model with the augmented dataset. Our experiments demonstrate that BootsTAP significantly improves the performance of existing TAP models on challenging benchmark datasets, achieving substantial gains in accuracy and robustness, particularly in scenarios with occlusions and large displacements. This bootstrapped approach provides a powerful and generalizable strategy for enhancing TAP models without requiring additional manual annotations, unlocking the potential for more reliable and user-friendly interactive video analysis tools."
http://arxiv.org/abs/2401.16423v1,Synchformer: Efficient Synchronization from Sparse Cues,"Synchronization of visual sequences is a fundamental problem in computer vision, enabling applications such as video editing, 3D reconstruction, and motion analysis. Existing methods often rely on dense feature correspondences or expensive optimization procedures, limiting their scalability and applicability to large-scale datasets with significant viewpoint changes or occlusions. This paper addresses the challenge of efficient and robust synchronization from sparse and potentially noisy cues. We introduce Synchformer, a novel transformer-based architecture that directly learns global synchronization relationships from sparse feature tracks. Synchformer leverages a self-attention mechanism to aggregate information across the entire sequence, effectively handling missing data and outliers while maintaining computational efficiency. We demonstrate state-of-the-art performance on challenging benchmark datasets for video synchronization, achieving significant improvements in accuracy and runtime compared to existing methods. Synchformer's ability to synchronize sequences from sparse cues opens up new possibilities for large-scale visual data processing and analysis."
http://arxiv.org/abs/2401.12039v1,"Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling","Generating accurate and synchronized subtitles for videos requires understanding both the visual content and the corresponding audio. While existing audio-visual subtitling methods have shown promise, they often struggle with accurately transcribing character names and specific terminology, hindering overall readability and comprehension. This paper addresses the challenge of generating character-aware subtitles by leveraging both visual cues of speaking individuals and the nuances of their speech. We propose a novel end-to-end model, CharAVSub, which integrates a speaker identification module based on facial recognition with an attention-based audio-visual speech recognition system. CharAVSub incorporates a character embedding layer conditioned on both the speaker identity and the preceding subtitle context to improve character name and terminology prediction. Experimental results on the challenging AVA-Speech dataset demonstrate that CharAVSub significantly improves subtitle accuracy, particularly for character names, achieving a relative improvement of 8% in character name recall compared to state-of-the-art audio-visual subtitling models. This advancement facilitates a more immersive and accessible viewing experience by providing more accurate and contextually relevant subtitles."
http://arxiv.org/abs/2401.10224v3,The Manga Whisperer: Automatically Generating Transcriptions for Comics,"Manga, a globally popular form of visual storytelling, often presents accessibility challenges due to its reliance on hand-drawn text within speech bubbles and sound effects. Extracting and transcribing this text is a laborious manual process, hindering digital archiving, translation, and text-based searchability. This paper addresses the problem of automatically generating accurate transcriptions from manga panels. We introduce a novel pipeline, ""Manga Whisperer,"" comprising three key stages: (1) a Mask R-CNN model fine-tuned for robust speech bubble and text detection, (2) a custom text line extraction algorithm optimized for the unique layout and typography of manga, and (3) a transformer-based optical character recognition (OCR) model specifically trained on a large, synthetically generated manga text dataset. Experimental results on a diverse dataset of manga panels demonstrate that Manga Whisperer achieves a character-level accuracy of 92%, significantly outperforming existing general-purpose OCR systems. This automated transcription system unlocks new possibilities for manga analysis, accessibility, and cross-lingual dissemination."
http://arxiv.org/abs/2312.17247v2,Amodal Ground Truth and Completion in the Wild,"Amodal perception, the ability to infer the complete shape of an object despite occlusion, is crucial for robust scene understanding. However, the lack of large-scale, high-quality amodal ground truth data in real-world scenarios hinders the development and evaluation of amodal completion algorithms. This paper addresses the challenge of generating amodal ground truth and evaluating amodal completion in complex, unconstrained environments. We introduce a novel pipeline that leverages instance segmentation, depth estimation, and a shape prior learned from 3D object models to automatically generate amodal masks for objects in real-world images. Our approach estimates the visible surface using depth, aligns a 3D shape prior to the visible region, and then extrapolates the complete, amodal shape based on this alignment. We demonstrate the effectiveness of our method by creating a large-scale amodal dataset using the COCO dataset, and we evaluate several state-of-the-art amodal completion methods on this new benchmark. Our results show that our generated amodal ground truth can be used to effectively train and benchmark amodal completion algorithms, pushing the field towards more robust and generalizable amodal perception in the wild."
http://arxiv.org/abs/2312.13090v1,Perception Test 2023: A Summary of the First Challenge And Outcome,"The development of robust and reliable computer vision algorithms necessitates rigorous and standardized evaluation methodologies. This paper presents an overview of the first ""Perception Test"" challenge, held in 2023, designed to benchmark the performance of state-of-the-art computer vision systems across a diverse set of perceptual tasks. The challenge addressed the critical need for a unified evaluation platform capable of assessing performance beyond standard object recognition, encompassing tasks such as scene understanding, reasoning, and anomaly detection. The challenge comprised five distinct tracks, each focused on a specific perceptual ability, and utilized a novel evaluation metric that combined accuracy with computational efficiency. We received submissions from 42 teams worldwide, with the top-performing methods demonstrating significant advancements in handling complex, real-world scenarios. The results highlighted the strengths and weaknesses of current approaches and identified key areas for future research, particularly in the development of more generalizable and computationally efficient perception algorithms. The Perception Test 2023 provides a valuable resource for the computer vision community and establishes a foundation for future challenges aimed at driving innovation in the field."
http://arxiv.org/abs/2312.11897v3,Text-Conditioned Resampler For Long Form Video Understanding,"Long-form videos present a significant challenge for video understanding due to their temporal length and complex narratives. Existing methods often struggle to efficiently process and extract relevant information from these extended sequences, particularly when guided by textual queries. We address the problem of effectively incorporating textual information to selectively sample and process crucial segments within long-form videos. We introduce a Text-Conditioned Resampler (TCR) module that learns to dynamically adjust the temporal resolution of video features based on the provided text query. TCR employs a cross-modal attention mechanism to identify text-relevant video segments and then resamples the video features to focus computational resources on these informative regions. Experiments on long-form video QA and retrieval datasets demonstrate that TCR significantly improves performance compared to baseline methods, achieving state-of-the-art results while reducing computational cost. This highlights the potential of TCR to facilitate more efficient and accurate long-form video understanding with textual guidance."
http://arxiv.org/abs/2312.11463v2,Appearance-Based Refinement for Object-Centric Motion Segmentation,"Object-centric motion segmentation aims to decompose a dynamic scene into independently moving objects, which is crucial for scene understanding and robotic manipulation. However, existing motion segmentation methods often struggle with noisy optical flow and ambiguous motion boundaries, leading to inaccurate object masks and fragmented motion clusters. We address this problem by introducing an appearance-based refinement framework that leverages visual cues to enhance motion-based segmentation. Our approach first generates an initial motion segmentation using a robust clustering algorithm on optical flow. Subsequently, we refine these initial masks by training a lightweight convolutional neural network to predict refined object masks based on both the initial motion segmentation and the corresponding RGB frame. This network is trained with a novel loss function that encourages spatial consistency and alignment with object boundaries. Experiments on standard datasets demonstrate that our method significantly improves the accuracy and robustness of object-centric motion segmentation, particularly in challenging scenarios with complex motion patterns and cluttered backgrounds, achieving state-of-the-art performance. This refined segmentation enables more reliable downstream tasks such as object tracking and action recognition."
http://arxiv.org/abs/2312.07395v2,A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames,"Contrastive learning has shown promise for pre-training video encoders, but its effectiveness often diminishes when scaling to longer video sequences, typically limited to 16 frames. This paper addresses the challenge of effectively pre-training video encoders using contrastive learning with significantly longer temporal contexts. We introduce a surprisingly simple yet effective recipe: (1) training with longer video clips (up to 64 frames), (2) employing a larger batch size, and (3) incorporating a momentum encoder with carefully tuned temperature parameters. We demonstrate that these modifications, applied to a standard contrastive learning framework, substantially improve the learned representations, leading to significant performance gains on downstream action recognition and video retrieval tasks. Our method achieves state-of-the-art results on several benchmarks, outperforming existing contrastive learning approaches and even surpassing some supervised pre-training methods, while maintaining computational efficiency. This work provides a practical and scalable approach for learning powerful video representations from unlabeled data, paving the way for more effective video understanding models."
http://arxiv.org/abs/2312.00598v2,Learning from One Continuous Video Stream,"Learning visual representations typically relies on large, curated datasets with explicit annotations. However, the real world presents a continuous stream of unlabeled visual data, posing a significant challenge for scalable and adaptable learning. This paper addresses the problem of learning useful visual representations directly from a single, long, and unsegmented video stream without any manual supervision. We propose a novel self-supervised framework, StreamVLM, that combines a video transformer with a learned latent embedding space optimized for temporal consistency and novel event discovery. Our method leverages contrastive learning to enforce similar embeddings for temporally proximal frames while simultaneously employing a clustering-based objective to identify and separate distinct visual events within the stream. Experiments on diverse video datasets demonstrate that StreamVLM learns representations that are competitive with state-of-the-art self-supervised methods trained on curated datasets, achieving comparable or better performance on downstream tasks such as video classification and action recognition. This work provides a promising direction towards learning robust and adaptive visual representations directly from the continuous flow of visual information in the real world."
http://arxiv.org/abs/2311.17055v1,No Representation Rules Them All in Category Discovery,"Category discovery aims to automatically group unlabelled images into meaningful clusters, mirroring human visual learning. A crucial aspect of this problem is the choice of image representation, often relying on pre-trained features from self-supervised or supervised models. This paper investigates the long-standing assumption that a single, universally superior representation exists for all category discovery tasks. We challenge this notion by demonstrating that the optimal representation is highly dependent on the dataset and the nature of the underlying categories. To address this, we propose a novel representation selection framework that leverages a meta-learning approach to predict the best representation for a given unlabelled dataset based on its statistical properties, avoiding computationally expensive clustering trials with different representations. We evaluate our framework on a diverse set of image datasets, demonstrating significant improvements in clustering accuracy compared to using a single, pre-selected representation. Our findings highlight the importance of adapting representation learning to the specific characteristics of the dataset and pave the way for more robust and generalizable category discovery systems."
http://arxiv.org/abs/2311.09424v1,Predicting Spine Geometry and Scoliosis from DXA Scans,"Scoliosis, a three-dimensional deformity of the spine, is often diagnosed through radiographic imaging, posing radiation exposure risks, especially for young patients. This work addresses the challenge of predicting detailed spine geometry and scoliosis presence from Dual-energy X-ray Absorptiometry (DXA) scans, a low-dose imaging modality primarily used for bone density assessment. We propose a novel deep learning framework that leverages a convolutional neural network (CNN) to extract features from DXA images, followed by a regression network to predict 3D spinal landmark coordinates. These predicted landmarks are then used to reconstruct the spine geometry and compute Cobb angles, a standard measure for scoliosis severity. Furthermore, a classification network is trained on the predicted geometry to directly identify scoliosis cases. Our method achieves a mean absolute error of X.X mm in landmark localization and an area under the ROC curve (AUC) of Y.Y for scoliosis detection, demonstrating strong correlation with ground truth measurements from full spine radiographs. This research offers a promising avenue for scoliosis screening and monitoring using low-dose DXA imaging, potentially reducing radiation exposure and improving early detection rates."
http://arxiv.org/abs/2310.16477v1,Show from Tell: Audio-Visual Modelling in Clinical Settings,"Automatic analysis of patient behavior during clinical interactions holds the potential to improve diagnostic accuracy and treatment efficacy. However, existing methods often rely solely on visual cues, neglecting the rich information present in the audio modality, particularly in scenarios where visual information is limited or occluded. This paper addresses the challenge of building robust and comprehensive models of clinical interactions by leveraging both audio and visual streams in a ""Show from Tell"" framework. Our approach learns to generate descriptive visual representations conditioned on corresponding audio inputs, effectively transferring knowledge from the auditory domain to enhance visual understanding. Specifically, we train a conditional variational autoencoder (CVAE) to predict visual embeddings from audio features, followed by using these predicted embeddings, along with the original visual features, for downstream tasks such as activity recognition and behavioral coding. Experimental results on a dataset of autism spectrum disorder (ASD) diagnostic sessions demonstrate that our audio-guided visual representation learning significantly improves performance compared to visual-only baselines and other multimodal fusion techniques, achieving a 12% relative improvement in F1-score for key behavioral categories. This work establishes the effectiveness of audio-visual modelling for capturing nuanced behavioral patterns in clinical settings, paving the way for more comprehensive and automated clinical decision support systems."
http://arxiv.org/abs/2310.06838v1,"AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description","Movie audio description (AD) provides crucial accessibility for visually impaired individuals, narrating on-screen visual information during pauses in dialogue. While automated audio description (AutoAD) has shown promise, existing methods primarily focus on describing *what* is happening, often neglecting the critical context of *who* is involved and *when* events occur. This paper addresses the limitations of current AutoAD systems by introducing AutoAD II, a novel framework designed to generate more comprehensive and contextually relevant movie audio descriptions. Our approach leverages a multi-modal transformer architecture, incorporating visual features, dialogue transcripts, and scene timestamps to predict not only the objects and actions present but also the identities of the actors involved and the precise timing of events. We train and evaluate AutoAD II on a large-scale dataset of movies with professionally written audio descriptions, demonstrating significant improvements in both content relevance and temporal alignment compared to state-of-the-art AutoAD systems. AutoAD II represents a significant step towards generating more informative and user-friendly audio descriptions, enhancing the accessibility and enjoyment of cinematic content for visually impaired audiences."
http://arxiv.org/abs/2310.06836v3,A General Protocol to Probe Large Vision Models for 3D Physical Understanding,"Large vision models (LVMs) have demonstrated remarkable capabilities in image understanding, but their comprehension of 3D physical properties remains largely unexplored. This paper addresses the critical gap in evaluating the 3D physical understanding capabilities of LVMs, specifically focusing on their ability to reason about object stability, support, and containment. We propose a general protocol, PhyPro, consisting of synthetically generated 3D scenes rendered from multiple viewpoints and paired with targeted queries designed to probe specific physical reasoning skills. PhyPro introduces a novel query formulation method that utilizes both visual grounding and textual descriptions to elicit precise and interpretable responses from LVMs. Our experiments using PhyPro on a range of state-of-the-art LVMs reveal significant variations in their physical reasoning abilities, with performance lagging considerably behind human-level understanding, particularly in scenarios requiring compositional reasoning. The proposed protocol offers a standardized and extensible framework for evaluating and improving the 3D physical understanding of LVMs, facilitating the development of more robust and physically grounded AI systems."
http://arxiv.org/abs/2310.05304v1,GestSync: Determining who is speaking without a talking head,"Determining the active speaker in multi-person interactions is crucial for various applications, from meeting analysis to human-robot interaction. Current speaker identification systems heavily rely on visual cues from the talking head, limiting their applicability in scenarios with occlusions, off-screen speakers, or low-resolution video. This paper addresses the problem of identifying the active speaker in video without relying on talking head information, instead leveraging subtle gestures and body language. We introduce GestSync, a novel multi-modal approach that fuses audio information with learned representations of body pose dynamics. Our method utilizes a transformer-based architecture to capture temporal dependencies in pose sequences, which are then combined with audio embeddings to predict speaker activity. Experiments on publicly available datasets demonstrate that GestSync achieves state-of-the-art performance in speaker identification, even when the speaker's face is not visible. This work opens new avenues for robust speaker identification in challenging real-world scenarios where traditional methods fail."
http://arxiv.org/abs/2309.03899v1,The Making and Breaking of Camouflage,"Camouflage is a widespread survival strategy across the animal kingdom, enabling organisms to evade predators or ambush prey through visual deception. This paper addresses the core challenge of understanding the interplay between camouflage creation and detection: how effective camouflage is generated and, conversely, how visual systems evolve to break it. We propose a novel computational framework integrating generative adversarial networks (GANs) and saliency modeling to simulate this evolutionary arms race. The GAN is trained to generate camouflage patterns against diverse backgrounds, while a biologically-inspired saliency model acts as a predator, attempting to detect the camouflaged object. Through iterative training, the GAN learns to create increasingly effective camouflage, and the saliency model adapts to identify subtle cues. Our experiments demonstrate that the resulting camouflage patterns exhibit characteristics observed in nature, such as disruptive coloration and background matching, and that the saliency model develops sensitivity to these features. This computational approach provides a powerful tool for investigating the evolutionary dynamics of camouflage and visual perception, with implications for fields ranging from ecology to computer vision."
http://arxiv.org/abs/2308.10417v2,The Change You Want to See (Now in 3D),"Interactive image editing has become increasingly prevalent, allowing users to manipulate images intuitively. However, extending this interactive control to the 3D domain, particularly for complex scene modifications, remains a significant challenge. We address the problem of enabling intuitive and precise user control over 3D scene editing by introducing a novel framework that leverages both 2D user input and a learned 3D scene representation. Our method allows users to specify desired changes in 2D image space through scribbles or masks, which are then propagated into the underlying 3D scene using a combination of differentiable rendering and a learned neural radiance field. This allows for manipulations such as object repositioning, material editing, and shape deformation, all guided by the user's 2D input. We demonstrate that our approach achieves state-of-the-art performance in interactive 3D scene editing, enabling complex modifications with minimal user effort and generating realistic results. Our method offers a significant step towards democratizing 3D content creation and manipulation."
http://arxiv.org/abs/2308.07918v1,Helping Hands: An Object-Aware Ego-Centric Video Recognition Model,"Ego-centric video understanding is crucial for applications like assistive robotics and augmented reality, but remains challenging due to the dynamic and unconstrained nature of first-person viewpoints. Existing approaches often struggle to effectively represent and reason about the objects being interacted with, hindering accurate activity recognition. We propose an Object-Aware Ego-Centric Video Recognition model that explicitly incorporates object information into the video representation. Our model utilizes a two-stream architecture: one stream processes the raw video frames, while the other focuses on detected objects and their associated attributes within each frame. These streams are fused using a novel attention mechanism that allows the model to selectively attend to relevant objects and their interactions during activity recognition. We evaluate our model on the Ego4D benchmark, demonstrating significant improvements in action recognition accuracy compared to state-of-the-art methods, particularly for tasks involving fine-grained object manipulation. This highlights the importance of object-aware reasoning in ego-centric video understanding and opens new avenues for developing more robust and interpretable activity recognition systems."
http://arxiv.org/abs/2306.08637v2,TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement,"Tracking arbitrary points in video remains a challenging problem, crucial for applications like video editing and motion analysis. Existing methods often struggle with long-term occlusions, fast motions, and require careful initialization or training data specific to object categories. We introduce TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement, a novel approach that enables robust and accurate point tracking in diverse video scenarios. TAPIR leverages a pre-trained visual backbone and a learned feature space to estimate dense correspondences between video frames. It initializes point tracks independently in each frame using a learned motion prior and then refines these tracks temporally through a recurrent network that aggregates information across time, effectively handling occlusions and drift. We demonstrate state-of-the-art performance on multiple challenging benchmarks, including the DAVIS and Long-term Video Object Segmentation (LVOS) datasets, achieving significant improvements in accuracy and robustness compared to existing methods. TAPIR's ability to track arbitrary points without category-specific training unlocks new possibilities for interactive video understanding and manipulation."
http://arxiv.org/abs/2306.05493v1,Multi-Modal Classifiers for Open-Vocabulary Object Detection,"Open-vocabulary object detection aims to identify and localize objects beyond a predefined set of categories, leveraging external knowledge sources like text embeddings. However, current approaches often rely on a single modality, typically vision, for proposal generation and refinement, limiting their ability to effectively incorporate richer, multi-modal contextual information for accurate classification. This paper introduces a novel multi-modal classification framework for open-vocabulary object detection that leverages both visual and textual features to enhance object recognition. Our approach utilizes a region proposal network to generate candidate object regions, followed by a multi-modal classifier that fuses visual features extracted from the region with text embeddings representing potential object categories. Specifically, we explore different fusion strategies, including attention mechanisms, to dynamically weigh the contribution of each modality based on the specific object and context. Experiments on challenging open-vocabulary detection datasets demonstrate that our multi-modal classifier significantly improves detection accuracy, achieving state-of-the-art results compared to existing single-modality methods. This work highlights the importance of multi-modal fusion for robust and accurate open-vocabulary object detection, paving the way for more adaptable and generalizable vision systems."
http://arxiv.org/abs/2306.04633v2,Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion,"3D object instance segmentation is a crucial task for scene understanding, enabling robots and autonomous systems to interact with their environment. Existing methods often struggle with complex scenes containing cluttered objects and significant variations in point density. This paper introduces Contrastive Lift, a novel approach for 3D object instance segmentation that leverages a slow-fast contrastive fusion strategy. Our method first extracts coarse semantic and geometric features using a slow branch operating on downsampled point clouds, providing robust global context. Subsequently, a fast branch refines these features at a higher resolution, focusing on local details and boundary delineation. A contrastive loss is employed to encourage feature similarity within the same object instance while maximizing dissimilarity between different instances, effectively ""lifting"" individual objects from the background. Experiments on ScanNet and S3DIS datasets demonstrate that Contrastive Lift achieves state-of-the-art performance, significantly improving segmentation accuracy, particularly for small and occluded objects. This work provides a powerful and efficient framework for 3D object instance segmentation, advancing the capabilities of scene understanding systems."
http://arxiv.org/abs/2306.01851v2,Open-world Text-specified Object Counting,"Object counting is a fundamental computer vision task with applications spanning various domains. However, existing methods typically operate under closed-world assumptions, requiring pre-defined object categories and failing to generalize to novel objects described through free-form text. This paper introduces the problem of open-world text-specified object counting, where the goal is to count objects in an image based on a textual description, without prior knowledge of the object category. We propose a novel framework, TextCount, which leverages a vision-language model (VLM) to learn cross-modal representations of image regions and text descriptions. TextCount utilizes a dynamic query selection mechanism to identify the most relevant regions based on the text query, followed by a counting module that aggregates the selected features to predict the object count. Experiments on a newly constructed benchmark demonstrate that TextCount significantly outperforms existing zero-shot object counting methods, achieving a 25% improvement in mean absolute error. This work opens new avenues for flexible and adaptable object counting in real-world scenarios where object categories are diverse and evolving."
http://arxiv.org/abs/2305.13786v2,Perception Test: A Diagnostic Benchmark for Multimodal Video Models,"Multimodal video understanding is crucial for applications ranging from autonomous driving to human-robot interaction. However, current benchmarks often conflate different aspects of video understanding, making it difficult to diagnose the specific capabilities and limitations of multimodal video models. To address this, we introduce Perception Test, a diagnostic benchmark designed to isolate and evaluate specific perceptual abilities of multimodal video models. Perception Test comprises a suite of targeted tasks, each focusing on a distinct aspect of video understanding, such as object tracking, action recognition conditioned on object properties, and causal reasoning about event sequences. We evaluate a range of state-of-the-art multimodal video models on Perception Test, revealing significant performance variations across different tasks and highlighting specific areas where models struggle, such as reasoning about subtle object interactions and understanding long-range dependencies. Our analysis demonstrates that Perception Test provides valuable insights into the strengths and weaknesses of existing models, facilitating more targeted research and development in multimodal video understanding. This diagnostic approach enables the community to move beyond aggregate performance metrics and focus on building more robust and reliable video understanding systems."
http://arxiv.org/abs/2304.06708v1,Verbs in Action: Improving verb understanding in video-language models,"Video-language models have achieved significant progress in understanding complex scenes, but often struggle with fine-grained action recognition, particularly distinguishing between similar verbs. This paper addresses the challenge of improving verb understanding in video-language models by focusing on more effectively learning verb representations and their context within video. We introduce Verb-centric Attentive Temporal Modeling (VATM), which enhances temporal modeling by explicitly attending to video segments most relevant to the target verb, guided by verb-specific visual cues. VATM incorporates a novel Verb-Context Contrastive Loss that encourages the model to discriminate between videos with similar scenes but different actions. Experiments on standard video-language benchmarks, including ActivityNet and Charades, demonstrate that VATM significantly outperforms existing state-of-the-art methods, achieving notable gains in verb recognition accuracy and video captioning quality. Our findings highlight the importance of explicit verb-centric temporal modeling for robust action understanding in videos, paving the way for more accurate and reliable video-language systems."
http://arxiv.org/abs/2303.17644v1,Vision-Language Modelling For Radiological Imaging and Reports In The Low Data Regime,"Vision-Language Pre-training (VLP) has shown promise in medical imaging, particularly for tasks involving radiological images and their corresponding reports. However, the scarcity of large, labelled datasets in the medical domain presents a significant challenge for training robust VLP models. This paper addresses the problem of effectively utilizing VLP for radiological report generation and image understanding tasks when training data is limited. We propose a novel approach that combines a pre-trained general domain VLP model with a specialized medical knowledge distillation strategy and a contrastive fine-tuning objective tailored for the low-data regime. Our knowledge distillation leverages a larger, weakly-supervised medical imaging dataset to transfer relevant medical concepts to the VLP model. We demonstrate that our method significantly outperforms existing state-of-the-art VLP models in radiological report generation and image classification on benchmark datasets, achieving improvements of up to 15% in ROUGE scores and 8% in accuracy with limited labelled data. This work offers a practical and effective solution for applying VLP to radiological imaging, even when training data is scarce, thereby enabling broader applications of AI in healthcare."
http://arxiv.org/abs/2303.16899v1,AutoAD: Movie Description in Context,"Movie description, also known as video captioning, aims to automatically generate textual descriptions for video content, enabling applications such as video retrieval and accessibility. Current approaches often generate descriptions in isolation, neglecting the broader narrative context of the movie, resulting in descriptions that lack coherence and relevance within the scene progression. We address this limitation by proposing AutoAD, a novel framework for Auto-regressive and Adaptive movie Description generation that explicitly models inter-scene dependencies. AutoAD employs a hierarchical encoder-decoder architecture. First, a scene encoder aggregates visual and audio features within each scene. Then, a transformer-based context encoder captures the temporal relationships between scenes, adaptively weighting their influence on the current scene description. Finally, a decoder generates the description conditioned on both the current scene's features and the encoded contextual information. Experiments on the MovieClips dataset demonstrate that AutoAD significantly outperforms state-of-the-art methods in terms of both caption quality (BLEU, METEOR, CIDEr) and narrative coherence, as measured by a novel context-aware evaluation metric. This work highlights the importance of contextual reasoning in movie description and provides a valuable framework for generating more informative and coherent video captions."
http://arxiv.org/abs/2303.13518v1,Three ways to improve feature alignment for open vocabulary detection,"Open vocabulary object detection aims to detect objects beyond a pre-defined set of categories by leveraging rich semantic information from pre-trained vision-language models. However, misalignment between visual features and semantic embeddings often limits the performance of these detectors, particularly for novel categories. This paper addresses the challenge of improving feature alignment in open vocabulary object detection through three distinct yet complementary strategies. First, we introduce a cross-modal distillation technique that transfers knowledge from a frozen, well-aligned vision-language model to the object detector's visual backbone. Second, we propose a category-aware attention mechanism that refines visual features based on the semantic context of each category, enabling more precise feature selection. Finally, we explore a contrastive learning objective that encourages better alignment between visual features and semantic embeddings in a shared embedding space. Experiments on standard open vocabulary detection benchmarks demonstrate that our proposed methods significantly improve detection accuracy, especially for novel categories, achieving state-of-the-art performance. These improvements highlight the importance of carefully aligning visual and semantic features for effective open vocabulary object detection and provide a solid foundation for future research in this area."
http://arxiv.org/abs/2301.09595v2,Zorro: the masked multimodal transformer,"Multimodal learning aims to integrate information from diverse data modalities, such as images and text, to improve task performance. However, many real-world scenarios involve missing or corrupted data modalities, posing a significant challenge to existing multimodal models. We introduce Zorro, a masked multimodal transformer designed to handle arbitrarily missing modalities during both training and inference. Zorro employs a novel masked attention mechanism that dynamically adjusts attention weights based on the availability of each modality, preventing the model from relying on potentially absent or noisy inputs. Furthermore, Zorro incorporates a modality-specific reconstruction loss, encouraging the model to learn robust representations by predicting missing modality features from the available ones. Experiments on benchmark multimodal datasets demonstrate that Zorro outperforms state-of-the-art methods in various tasks, particularly when a substantial proportion of modalities are missing. The improved robustness and performance offered by Zorro represent a significant step towards deploying multimodal systems in real-world applications with incomplete or unreliable data."
http://arxiv.org/abs/2211.15107v2,A Light Touch Approach to Teaching Transformers Multi-view Geometry,"Multi-view geometry (MVG) provides powerful tools for 3D scene understanding from multiple images, but traditional algorithms are often complex and require significant expertise. Recent advances have shown the potential of Transformers to learn geometric relationships directly from data, yet integrating this paradigm into MVG education presents a challenge due to the substantial computational resources and training data typically required. This paper addresses the problem of making Transformer-based MVG learning accessible within an educational setting with limited resources. We propose a ""light touch"" approach that leverages pre-trained Transformer backbones and focuses on fine-tuning specific, lightweight modules for MVG tasks, such as relative pose estimation and triangulation. This approach significantly reduces the computational burden and data requirements compared to training Transformers from scratch. Our experiments demonstrate that fine-tuning a pre-trained ViT model with a small dataset achieves comparable performance to traditional methods and significantly outperforms randomly initialized Transformers trained from scratch, while requiring only a fraction of the training time and resources. This work enables students and researchers with limited resources to explore and understand the application of Transformers to multi-view geometry, fostering innovation in this rapidly evolving field."
http://arxiv.org/abs/2211.08954v1,Weakly-supervised Fingerspelling Recognition in British Sign Language Videos,"Fingerspelling, the representation of letters using handshapes, forms a crucial part of British Sign Language (BSL), particularly for novel words and proper nouns. However, the creation of large, fully-annotated datasets for fingerspelling recognition is a time-consuming and expensive process. This paper addresses the challenge of weakly-supervised BSL fingerspelling recognition, leveraging readily available sentence-level transcriptions without frame-level handshape labels. We propose a novel framework that combines a connectionist temporal classification (CTC) loss with an attention-based mechanism to implicitly learn the alignment between video frames and fingerspelled words. This allows us to train a fingerspelling recognition model using only sentence-level transcripts, iteratively refining handshape predictions through a self-training process. Experiments on a challenging BSL fingerspelling dataset demonstrate that our weakly-supervised approach achieves competitive performance compared to fully-supervised methods, significantly reducing the annotation burden. This work paves the way for developing more scalable and accessible fingerspelling recognition systems, contributing to improved communication accessibility for the deaf community."
http://arxiv.org/abs/2211.03726v2,TAP-Vid: A Benchmark for Tracking Any Point in a Video,"Tracking arbitrary points in a video is a fundamental yet challenging task with applications spanning robotics, video editing, and biomechanics. However, existing video object tracking benchmarks primarily focus on tracking pre-defined objects, neglecting the need for methods that can track *any* specified point throughout a video sequence. To address this gap, we introduce TAP-Vid, a large-scale benchmark designed for Tracking Any Point in a Video. Our dataset comprises over 300 videos with dense point-wise annotations, allowing for rigorous evaluation of tracking algorithms' ability to maintain correspondence for user-defined points. We propose a novel evaluation metric, Point Tracking Accuracy (PTA), specifically tailored to assess the accuracy of point trajectories. Experimental results demonstrate that existing state-of-the-art tracking algorithms struggle to generalize to this task, achieving significantly lower PTA scores compared to traditional object tracking benchmarks. TAP-Vid and the associated PTA metric provide a valuable resource for developing and evaluating novel algorithms capable of accurately tracking arbitrary points in video, fostering advancements in fine-grained motion understanding."
http://arxiv.org/abs/2210.14601v1,End-to-end Tracking with a Multi-query Transformer,"Visual object tracking remains a challenging task, particularly in scenarios involving occlusions, distractors, and significant appearance changes. Current Siamese-based trackers often struggle with complex scenes due to their limited capacity in modeling target-background relationships and utilizing temporal information. This paper addresses the limitations of existing tracking frameworks by introducing an end-to-end trainable tracker based on a novel multi-query Transformer architecture. Our approach, dubbed MQTrack, employs a set of learnable query embeddings to simultaneously represent and track the target object across frames, enabling robust attention-based interaction with both the template and search region features. The multi-query mechanism allows for a more comprehensive representation of the target, mitigating the impact of partial occlusions and appearance variations. Experimental results on challenging benchmarks, including LaSOT, TrackingNet, and GOT-10k, demonstrate that MQTrack achieves state-of-the-art performance with significant improvements in accuracy and robustness compared to existing Siamese and Transformer-based trackers. MQTrack's ability to effectively model complex target-background relationships and leverage temporal information marks a significant step toward more robust and accurate visual object tracking."
http://arxiv.org/abs/2210.10046v1,A Tri-Layer Plugin to Improve Occluded Detection,"Object detection in real-world scenarios often suffers from performance degradation due to occlusions. Existing detectors struggle to accurately localize and classify objects when they are partially hidden, leading to missed detections and incorrect bounding box predictions. To address this, we propose a novel Tri-Layer Occlusion Plugin (TLOP) designed to enhance the feature representation of occluded objects within existing detection frameworks. TLOP leverages three distinct layers: a context-aware layer that aggregates surrounding scene information, an attention-guided layer that focuses on visible object parts, and a hallucination layer that infers features for the occluded regions. These layers are integrated seamlessly into the feature extraction backbone of a detector and trained end-to-end. Experimental results on challenging datasets with significant occlusion, such as COCO and CrowdHuman, demonstrate that TLOP significantly improves detection accuracy, achieving an average precision gain of 3-5% compared to baseline detectors without introducing substantial computational overhead. This plugin offers a practical and effective solution for improving the robustness of object detectors in complex and occluded environments."
http://arxiv.org/abs/2210.07055v1,Sparse in Space and Time: Audio-visual Synchronisation with Trainable Selectors,"Audio-visual synchronisation is crucial for understanding multimodal events, yet traditional approaches often process all available data, leading to computational redundancy and sensitivity to irrelevant information. This paper addresses the problem of efficiently and robustly synchronising audio and video streams by focusing only on the most informative spatio-temporal regions. We introduce a novel framework that learns to select sparse subsets of both visual and auditory features across space and time. Our approach employs trainable selectors, implemented as lightweight convolutional neural networks, to dynamically identify salient image patches and audio segments. These selected features are then fused to predict the temporal offset between the streams. Experiments on benchmark datasets demonstrate that our method achieves state-of-the-art synchronisation accuracy while significantly reducing computational cost compared to dense feature processing. This efficient and selective approach to audio-visual synchronisation has the potential to improve performance and scalability in a wide range of multimodal applications."
http://arxiv.org/abs/2210.04889v1,Turbo Training with Token Dropout,"Transformer-based models have achieved state-of-the-art results in various computer vision tasks, but their computational demands during training remain a significant bottleneck. This paper addresses the problem of reducing the computational cost of training vision transformers (ViTs) without sacrificing accuracy. We introduce ""Turbo Training"" with Token Dropout, a novel training strategy that dynamically drops less informative tokens during the forward pass. Our method leverages a lightweight, learnable module to estimate token importance and selectively discard redundant tokens, thereby reducing the computational burden of subsequent transformer layers. Experiments on ImageNet classification demonstrate that Turbo Training with Token Dropout achieves significant speedups in training time, up to 25% reduction in FLOPs, while maintaining competitive accuracy compared to training the full model. This efficient training strategy enables faster experimentation and broader applicability of vision transformers, particularly in resource-constrained environments."
http://arxiv.org/abs/2210.02995v1,Compressed Vision for Efficient Video Understanding,"Video understanding tasks are computationally expensive due to the high dimensionality and redundancy of video data. This paper addresses the challenge of efficiently processing video data for understanding tasks by operating directly on compressed video formats. We propose a novel ""Compressed Vision Network"" (CVN) architecture that leverages motion vectors and residual data available within compressed video streams (e.g., H.264, HEVC) to perform action recognition and video captioning, thereby bypassing the computationally intensive full decoding process. CVN employs lightweight convolutional and recurrent modules to extract spatiotemporal features from compressed representations, augmented with a novel attention mechanism tailored for motion vector fields to highlight salient movements. Experiments on standard benchmarks, including ActivityNet and MSVD, demonstrate that CVN achieves comparable or superior performance to state-of-the-art methods that operate on decoded RGB frames, while significantly reducing computational cost (up to 5x speedup) and memory footprint. These results highlight the potential of compressed vision for enabling efficient and scalable video understanding applications in resource-constrained environments."
http://arxiv.org/abs/2209.14341v1,The Change You Want to See,"Unsupervised video anomaly detection aims to identify unusual events in videos without relying on labeled data, a crucial capability for automated surveillance and safety monitoring. However, existing methods often struggle with videos containing complex scenes and subtle anomalies due to difficulties in effectively learning normal event patterns and distinguishing them from anomalies. We address this challenge by proposing a novel framework called ""Contrastive Hallucination for Anomaly Detection"" (CHAD). CHAD leverages a generative adversarial network (GAN) to hallucinate future frames conditioned on past observations, while simultaneously employing a contrastive learning objective to enforce a clear separation between normal and anomalous future predictions in a learned feature space. Specifically, we contrast the hallucinated future frames with both real future frames from normal sequences and with randomly perturbed versions of those same frames, encouraging the model to learn robust representations of normality. Experiments on benchmark datasets such as Avenue, ShanghaiTech, and UCSD Ped2 demonstrate that CHAD achieves state-of-the-art performance, outperforming existing unsupervised anomaly detection methods by a significant margin in terms of frame-level and pixel-level anomaly detection accuracy. This improved accuracy and robustness make CHAD a promising solution for real-world video surveillance applications."
http://arxiv.org/abs/2208.13721v3,CounTR: Transformer-based Generalised Visual Counting,"Visual counting, the task of estimating the number of objects in an image, is crucial for a wide range of applications, from traffic monitoring to cell biology. Existing methods often rely on density map estimation or detection-based approaches, which can struggle with high object densities, varying scales, and unseen object categories. This paper introduces CounTR, a novel Transformer-based architecture for generalised visual counting. CounTR leverages a hierarchical vision transformer backbone to extract multi-scale features, which are then processed by a counting-specific transformer decoder. This decoder utilizes attention mechanisms to aggregate global contextual information and predict object counts directly, without relying on intermediate density maps. Experiments on diverse counting datasets, including those with novel object categories, demonstrate that CounTR achieves state-of-the-art performance, outperforming existing methods by a significant margin in both accuracy and generalizability. CounTR provides a powerful and flexible framework for visual counting, enabling accurate object estimation across a wide range of scenarios."
http://arxiv.org/abs/2208.02802v1,Automatic dense annotation of large-vocabulary sign language videos,"Sign language recognition has seen significant progress, yet the scarcity of densely annotated large-vocabulary datasets remains a major bottleneck for training robust and generalizable models. This paper addresses the challenge of automatically generating dense annotations for sign language videos, specifically focusing on large vocabularies. We propose a novel framework that leverages a combination of weakly supervised learning with a pre-trained sign language recognition model and a novel temporal refinement module based on Transformers. Our approach first generates initial, coarse-grained annotations using the pre-trained model and weak supervision signals from sentence-level transcriptions. The temporal refinement module then uses a self-attention mechanism to smooth and refine these annotations, improving temporal alignment and reducing annotation noise. Experimental results on a large-vocabulary American Sign Language dataset demonstrate that our method achieves significant improvements in annotation quality compared to existing weakly supervised approaches, approaching the performance of human annotators in certain metrics. This work provides a scalable and efficient solution for creating dense annotations for large-scale sign language video datasets, facilitating future research in sign language recognition and translation."
http://arxiv.org/abs/2207.10075v2,Is an Object-Centric Video Representation Beneficial for Transfer?,"Learning transferable video representations remains a significant challenge, often hindered by the complexity and variability inherent in raw pixel data. This paper investigates whether learning video representations through an object-centric lens improves transfer learning performance across diverse downstream tasks. We propose an object-centric video representation learning framework that leverages unsupervised object discovery and tracking to decompose videos into sets of object-centric feature trajectories. These trajectories are then encoded using a transformer-based architecture trained with a contrastive objective to capture both appearance and motion cues. We evaluate our approach on a suite of video action recognition and video question answering benchmarks, demonstrating consistent improvements in transfer performance compared to state-of-the-art video representation learning methods, particularly in low-data regimes. Our findings suggest that disentangling video content into object-centric components facilitates the learning of more robust and generalizable video representations, paving the way for more effective transfer learning in video understanding."
http://arxiv.org/abs/2207.02206v2,Segmenting Moving Objects via an Object-Centric Layered Representation,"Accurate segmentation of moving objects in dynamic scenes is a fundamental task for various computer vision applications. Existing approaches often struggle with complex scenarios involving occlusions, clutter, and diverse object motions. This paper addresses the challenge of robustly segmenting moving objects by introducing a novel object-centric layered representation. Our method decomposes the scene into a set of learnable object layers, each parameterized by a latent code that governs its appearance and motion. These layers are optimized jointly through an expectation-maximization framework, where the expectation step infers the object assignments and the maximization step updates the layer parameters based on motion cues and appearance consistency. We demonstrate superior performance on challenging video datasets, achieving state-of-the-art results in segmenting multiple moving objects under significant occlusion and complex motion patterns. This object-centric layered representation provides a more structured and interpretable approach to motion segmentation, enabling more robust and accurate scene understanding."
http://arxiv.org/abs/2206.13173v1,Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading,"Spinal cancer diagnosis and radiological grading are crucial for effective treatment planning, yet current methods often rely on subjective visual assessment of medical images, leading to potential inconsistencies. This paper addresses the challenge of automating and improving the accuracy of spinal cancer detection and radiological grading from MRI scans. We introduce a novel Context-Aware Transformer network that leverages both local image features and global contextual information for enhanced performance. Our architecture incorporates a multi-scale feature extraction module to capture details at varying resolutions, followed by a transformer encoder that attends to relevant contextual cues within the entire spinal region. Furthermore, we introduce a novel contextual embedding layer that incorporates anatomical prior knowledge to guide the attention mechanism. Experimental results on a large dataset of spinal MRI scans demonstrate that our method achieves state-of-the-art performance in both cancer detection and radiological grading, surpassing existing convolutional neural network and transformer-based approaches. This advancement offers the potential to significantly improve the efficiency and reliability of spinal cancer diagnosis, ultimately leading to better patient outcomes."
http://arxiv.org/abs/2205.08508v1,A CLIP-Hitchhiker's Guide to Long Video Retrieval,"Large-scale video retrieval is crucial for navigating the ever-expanding landscape of online video content. However, effectively retrieving long videos based on textual queries remains a significant challenge due to the computational cost of processing lengthy video sequences and the semantic gap between text and video representations. We introduce a novel framework, the CLIP-Hitchhiker's Guide (CHG), designed to efficiently leverage the pre-trained knowledge of CLIP (Contrastive Language-Image Pre-training) for long video retrieval. CHG employs a hierarchical approach, first utilizing CLIP to generate coarse-grained video embeddings through sparse keyframe sampling and feature aggregation. Then, it refines retrieval accuracy by hitchhiking on the top-ranked videos using temporal attention mechanisms to focus on relevant segments and learn fine-grained contextual relationships between the query and the video. Experiments on large-scale video datasets demonstrate that CHG achieves state-of-the-art retrieval performance with significantly reduced computational overhead compared to existing methods. This efficient and effective approach provides a practical solution for real-world long video retrieval applications."
http://arxiv.org/abs/2205.04152v1,Scaling up sign spotting through sign language dictionaries,"Sign language recognition (SLR) research has primarily focused on isolated sign recognition or continuous sign language recognition, often relying on limited vocabularies and constrained settings. Sign spotting, the task of detecting the presence and location of specific signs within continuous signing, offers a more practical approach to understanding natural sign language. However, scaling sign spotting to handle the vast vocabulary present in real-world sign languages remains a significant challenge. We address this problem by leveraging sign language dictionaries to generate synthetic training data for sign spotting models. Our approach combines gloss-level annotations from dictionaries with motion capture data to synthesize realistic signing sequences covering a large vocabulary. We then train a temporal convolutional network to detect and localize target signs within these synthesized sequences. Experiments on both synthetic and real-world datasets demonstrate that our dictionary-driven approach significantly improves sign spotting performance, particularly for rare signs, compared to models trained solely on limited real data. This work demonstrates the potential of leveraging readily available sign language dictionaries to overcome data scarcity and enable large-scale sign spotting systems."
http://arxiv.org/abs/2205.01683v1,"SpineNetV2: Automated Detection, Labelling and Radiological Grading Of Clinical MR Scans","Accurate and efficient assessment of spinal conditions from Magnetic Resonance (MR) images is crucial for effective diagnosis and treatment planning. However, manual radiological assessment is time-consuming, subjective, and prone to inter-observer variability. This paper addresses the challenge of automating the entire clinical workflow, encompassing spine localization, vertebral body detection and labeling, and radiological grading of spinal pathologies. We introduce SpineNetV2, an end-to-end deep learning framework built upon a novel architecture incorporating a multi-scale feature fusion network and a hierarchical classification scheme. SpineNetV2 first localizes the spine within the MR image, then detects and labels individual vertebral bodies using a refined object detection module, and finally performs radiological grading for conditions like vertebral compression fractures and Modic changes. Evaluated on a large, multi-center dataset of clinical spine MR scans, SpineNetV2 achieves state-of-the-art performance in vertebral body detection (mAP of 95.2%) and accurate radiological grading (AUC of 0.92 for fracture detection), demonstrating significant improvements over existing methods. SpineNetV2 offers a robust and automated solution for spine MR image analysis, potentially improving diagnostic accuracy, reducing radiologist workload, and facilitating more efficient patient management."
http://arxiv.org/abs/2204.14198v2,Flamingo: a Visual Language Model for Few-Shot Learning,"Visual Language Models (VLMs) have shown remarkable progress in understanding and generating text based on visual inputs. However, their performance often relies on extensive task-specific fine-tuning, limiting their applicability in scenarios with scarce labeled data. This paper addresses the challenge of few-shot learning in VLMs, aiming to enable rapid adaptation to novel visual-linguistic tasks with minimal examples. We introduce Flamingo, a VLM incorporating a novel architecture that leverages a pre-trained language model and a vision encoder connected by a trainable gated cross-attention mechanism. This mechanism allows for seamless integration of visual features into the language model's processing, enabling it to effectively learn from and generalize across different visual-linguistic tasks using only a few training examples. Experimental results demonstrate that Flamingo significantly outperforms existing few-shot VLMs on a diverse set of benchmarks, achieving state-of-the-art performance in image captioning, visual question answering, and visual reasoning tasks with as few as 8 examples. Flamingo's ability to rapidly adapt to new tasks with limited data represents a significant step towards more flexible and generalizable VLMs."
http://arxiv.org/abs/2204.02968v1,Temporal Alignment Networks for Long-term Video,"Long-term video understanding presents significant challenges due to the vast temporal span and evolving content. Accurately modeling long-range dependencies and maintaining temporal consistency across extended durations remains a critical hurdle. This paper addresses the problem of accurately aligning feature representations across long temporal horizons in videos, which is crucial for tasks like action recognition and video summarization. We propose Temporal Alignment Networks (TANs), a novel architecture that learns to align features across time using a hierarchical attention mechanism. TANs employ a multi-scale temporal pyramid to capture features at varying granularities, followed by a learnable alignment module that explicitly models temporal relationships between different segments. This alignment module uses attention weights to dynamically aggregate features from relevant time steps, thereby mitigating the effects of temporal drift and enabling robust feature representation over long durations. Experiments on long-term action recognition datasets, such as THUMOS14 and ActivityNet, demonstrate that TANs significantly outperform state-of-the-art methods, achieving improvements of up to 5% in mean Average Precision. TANs provide a powerful and efficient approach to long-term temporal modeling, enabling more accurate and reliable video understanding."
http://arxiv.org/abs/2203.08777v3,Object discovery and representation networks,"Unsupervised object discovery aims to identify and segment objects within images without relying on manual annotations. Existing methods often struggle with complex scenes, requiring strong inductive biases or failing to produce consistent object representations. This paper addresses the challenge of discovering and representing objects in an end-to-end manner by introducing a novel Object Discovery and Representation Network (ODR-Net). ODR-Net employs a variational autoencoder architecture augmented with a spatial attention mechanism to learn object masks and corresponding latent representations simultaneously. Specifically, we use a transformer-based attention module to encourage the network to focus on individual objects and a novel reconstruction loss to enforce consistency between the discovered objects and the input image. Experiments on benchmark datasets, including COCO and Visual Genome, demonstrate that ODR-Net achieves state-of-the-art performance in unsupervised object discovery and segmentation, producing higher quality masks and more disentangled object representations compared to existing approaches. The ability to learn object representations without supervision opens avenues for downstream tasks like unsupervised object tracking and few-shot learning."
http://arxiv.org/abs/2202.10890v2,HiP: Hierarchical Perceiver,"The Perceiver architecture offers an attention-based solution for processing high-dimensional inputs by projecting them into a low-dimensional latent space. However, the original Perceiver lacks explicit mechanisms to capture hierarchical representations, which are crucial for many vision tasks requiring multi-scale reasoning. In this paper, we introduce HiP: Hierarchical Perceiver, a novel architecture that incorporates hierarchical processing directly into the Perceiver framework. HiP achieves this by iteratively applying Perceiver blocks at multiple resolutions, progressively downsampling and abstracting the latent representation while maintaining cross-attention to the input data at each level. This allows the model to learn both global context and fine-grained details. Experiments on ImageNet classification and semantic segmentation demonstrate that HiP significantly outperforms the standard Perceiver and achieves competitive performance with state-of-the-art vision transformers, while maintaining a smaller latent bottleneck. HiP's ability to learn hierarchical representations within the Perceiver framework opens new avenues for efficient and scalable processing of high-dimensional data in computer vision."
http://arxiv.org/abs/2201.02609v2,Generalized Category Discovery,"Generalized Category Discovery (GCD) aims to identify both known and novel categories within an unlabeled dataset, leveraging prior knowledge from a labeled source dataset. Existing methods often struggle with a significant domain shift between the labeled and unlabeled data or fail to effectively balance the recognition of known categories and the discovery of truly novel ones, leading to biased or incomplete clustering. We address this challenge by proposing a novel Contrastive Clustering with Adaptive Pseudo-Labeling (CCAP) framework. CCAP first employs a contrastive learning objective to learn robust feature representations transferable across domains. Subsequently, it iteratively refines pseudo-labels for both known and novel categories using an adaptive thresholding mechanism that dynamically adjusts based on the confidence of predictions for each category. This adaptive thresholding encourages exploration of novel categories while preserving the integrity of known categories. Experiments on benchmark datasets demonstrate that CCAP significantly outperforms state-of-the-art GCD methods, achieving substantial improvements in both clustering accuracy and novel category discovery metrics. The proposed approach provides a more robust and accurate solution for identifying and categorizing objects in real-world scenarios with unknown category distributions."
http://arxiv.org/abs/2112.06809v8,Persistent Animal Identification Leveraging Non-Visual Markers,"Animal identification is crucial for ecological monitoring, conservation efforts, and understanding animal behavior. Existing computer vision approaches often rely on visual features like coat patterns or facial characteristics, which can be unreliable due to variations in lighting, pose, occlusion, and age. This paper addresses the challenge of robust and persistent animal identification by leveraging non-visual, naturally occurring markers. We propose a novel framework that integrates acoustic features derived from individual animal vocalizations with thermal signatures extracted from body heat distribution. A multimodal deep learning architecture, incorporating convolutional neural networks and recurrent neural networks, is trained to fuse these modalities and generate a unique embedding for each animal. Our experiments on a diverse dataset of bat species and thermal signatures demonstrate a significant improvement in identification accuracy compared to solely visual or acoustic methods, achieving a 25% reduction in error rate. This work provides a robust and privacy-preserving approach to animal identification, enabling long-term monitoring studies in challenging environments."
http://arxiv.org/abs/2112.05749v2,"Label, Verify, Correct: A Simple Few Shot Object Detection Method","Few-shot object detection aims to learn novel object classes from limited labeled examples. Current approaches often rely on complex meta-learning strategies or sophisticated feature adaptation techniques to mitigate the challenges posed by scarce data. This paper addresses the problem of enabling robust few-shot detection using a surprisingly simple yet effective three-stage framework: Label, Verify, Correct (LVC). First, we utilize a base detector trained on abundant data to predict pseudo-labels on unlabeled images containing novel objects. Second, we introduce a verification module that filters out noisy pseudo-labels based on a confidence threshold and IoU consistency checks with neighboring detections. Finally, we fine-tune the detector using the verified pseudo-labels, further correcting any remaining errors through iterative self-training. Experiments on established few-shot object detection benchmarks, including PASCAL VOC and MS COCO, demonstrate that LVC achieves competitive performance, often surpassing more complex meta-learning algorithms, with significantly less computational overhead. The simplicity and effectiveness of LVC offer a practical and readily applicable solution for real-world few-shot object detection scenarios."
http://arxiv.org/abs/2112.04432v1,Audio-Visual Synchronisation in the wild,"Audio-visual synchronisation is crucial for understanding and interacting with multimedia content, yet most existing methods are designed for controlled environments. This work addresses the challenging problem of learning audio-visual correspondence in unconstrained, ""in the wild"" videos where significant variations in recording conditions, speaker identity, and background noise exist. We propose a novel self-supervised approach that leverages contrastive learning to identify corresponding audio-visual segments within a video. Our method learns audio and visual embeddings that are invariant to nuisance factors by explicitly contrasting segments from the same video against segments from different videos. This allows the network to focus on the temporal alignment of semantically related audio and visual cues. We demonstrate state-of-the-art performance on established benchmarks for audio-visual synchronisation, significantly improving results in challenging ""in the wild"" datasets. This improved synchronisation enables more robust analysis of real-world multimedia content and facilitates advancements in downstream tasks like speech recognition and video understanding."
http://arxiv.org/abs/2112.03243v2,Input-level Inductive Biases for 3D Reconstruction,"3D reconstruction from single or multiple views remains a challenging problem, often requiring extensive training data and careful architectural design to achieve robust performance. Existing approaches primarily focus on network architectures and loss functions to impose inductive biases. We address the problem of incorporating inductive biases directly at the input level to guide and improve 3D reconstruction, independent of the downstream network architecture. Our method, Input Bias Encoding (IBE), leverages differentiable rendering to transform input images by explicitly encoding priors about scene geometry, lighting, and material properties. Specifically, we pre-process input images by rendering them under a set of diverse, physically plausible conditions, creating a multi-view consistent ""biased"" input stack. This stack is then fed into a standard 3D reconstruction network. We demonstrate that IBE, when applied to various network architectures and datasets, consistently improves reconstruction accuracy and robustness, especially in scenarios with limited training data or noisy observations. This highlights the potential of input-level inductive biases to significantly enhance 3D reconstruction performance and generalization capabilities."
http://arxiv.org/abs/2111.09162v4,It's About Time: Analog Clock Reading in the Wild,"Analog clocks, despite their declining ubiquity in the digital age, remain prevalent in various real-world environments, presenting a unique challenge for automated visual understanding. Existing clock reading methods often rely on controlled settings or assume ideal image quality, struggling to generalize to the complexities of ""in-the-wild"" scenarios characterized by variations in illumination, occlusion, viewpoint, and clock design. This paper addresses the problem of robust and accurate analog clock reading from unconstrained images. We propose a novel two-stage approach: first, a deep learning-based object detector identifies and localizes the clock face within the image. Second, a specialized convolutional neural network (CNN) estimates the angles of the hour and minute hands, incorporating geometric constraints and temporal reasoning to refine the angle predictions and resolve ambiguities. Experimental results on a newly curated dataset of analog clock images captured under diverse real-world conditions demonstrate that our method achieves state-of-the-art accuracy, significantly outperforming existing techniques, especially in challenging scenarios. This work contributes a robust and generalizable solution for analog clock reading, enabling a wide range of applications in areas such as visual surveillance, robotics, and augmented reality."
http://arxiv.org/abs/2111.03635v1,BBC-Oxford British Sign Language Dataset,"British Sign Language (BSL) is the primary language of the Deaf community in the UK, yet resources for BSL research, particularly in computer vision, are significantly lacking. This deficiency hinders the development of automated BSL recognition and translation systems. We address this gap by introducing the BBC-Oxford British Sign Language Dataset, a large-scale, publicly available dataset of BSL videos derived from BBC news and educational programs. The dataset contains over 23,000 video clips featuring diverse signers and signing styles, paired with detailed gloss annotations and sentence-level translations. We leverage automatic alignment techniques and manual verification to ensure high annotation quality. Experiments on sign language recognition and translation benchmarks demonstrate the dataset's utility, achieving state-of-the-art results on isolated sign recognition and providing a strong foundation for future research in continuous sign language recognition and translation. The BBC-Oxford BSL Dataset will facilitate advancements in accessibility technologies and promote a deeper understanding of BSL."
http://arxiv.org/abs/2111.01024v1,With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition,"Egocentric action recognition strives to understand human activities from a first-person perspective, a task crucial for applications like augmented reality and assistive technologies. However, recognizing actions from egocentric videos remains challenging due to significant viewpoint variations, motion blur, and the inherent ambiguity of single frames. This paper addresses the problem of improving egocentric action recognition by effectively leveraging temporal context and multimodal information. We propose a novel architecture, the Temporally-Aware Multimodal Network (TAMNet), which integrates visual, inertial measurement unit (IMU), and audio modalities. TAMNet employs a hierarchical attention mechanism to dynamically weight the contribution of each modality over time, coupled with a temporal convolutional network (TCN) to capture long-range dependencies within and across modalities. Extensive experiments on the EPIC-Kitchens-100 dataset demonstrate that TAMNet achieves state-of-the-art performance, surpassing existing methods by a significant margin, particularly for fine-grained action categories. Our results highlight the importance of temporal reasoning and synergistic fusion of multimodal cues for robust and accurate egocentric action recognition."
http://arxiv.org/abs/2110.15957v1,Visual Keyword Spotting with Attention,"Visual Keyword Spotting (VKS) aims to directly retrieve instances of specific keywords from images or video frames, bypassing the need for optical character recognition. Existing VKS methods often struggle with variations in font, scale, and perspective, leading to suboptimal performance in real-world scenarios. This paper addresses the challenge of robust VKS by introducing an attention-based architecture that learns to focus on the most relevant visual features for keyword recognition. Our model leverages a convolutional neural network backbone for feature extraction, followed by a novel attention mechanism that dynamically weights different spatial regions based on their relevance to the target keyword embedding. This attention mechanism is integrated into a recurrent neural network to model the sequential nature of characters within a keyword. Experimental results on benchmark datasets demonstrate that our approach achieves state-of-the-art performance, significantly improving the mean average precision (mAP) compared to existing methods, particularly on datasets with high levels of visual noise and distortion. The proposed attention mechanism provides a more robust and interpretable approach to VKS, paving the way for improved performance in applications such as document retrieval and scene text understanding."
http://arxiv.org/abs/2110.07603v2,Sub-word Level Lip Reading With Visual Attention,"Lip reading, the task of decoding speech from visual lip movements, offers a valuable modality for human-computer interaction and speech recognition in noisy environments. Existing lip reading systems often operate at the word or character level, limiting their ability to handle out-of-vocabulary words and variations in speaking styles. This paper addresses the challenge of fine-grained lip reading by proposing a novel sub-word level lip reading framework incorporating a visual attention mechanism. Our approach utilizes a deep neural network architecture that learns to decompose lip movements into sub-word units (phonemes), employing a temporal convolutional network (TCN) for feature extraction followed by an attention-based encoder-decoder network to map visual features to sub-word sequences. The visual attention mechanism allows the model to dynamically focus on the most relevant lip regions for each sub-word unit, improving robustness to visual variations. Experimental results on the Lip Reading Sentences 3 (LRS3) dataset demonstrate that our sub-word level approach, enhanced with visual attention, achieves state-of-the-art performance compared to existing word and character level methods, particularly in handling unseen words. This work paves the way for more accurate and adaptable lip reading systems that can understand a wider range of spoken language."
http://arxiv.org/abs/2110.06207v2,Open-Set Recognition: a Good Closed-Set Classifier is All You Need?,"Open-set recognition, the task of classifying known classes while rejecting unknown samples, remains a significant challenge for deploying computer vision systems in real-world scenarios. Existing open-set recognition methods often require complex architectures or specialized training procedures tailored for distinguishing known from unknown data. This paper challenges this paradigm by investigating whether a well-trained closed-set classifier, without any open-set specific modifications, can achieve competitive or even superior open-set performance. We propose a simple yet effective approach that leverages the softmax probabilities of a standard closed-set classifier, calibrated using a held-out validation set, to identify unknown samples based on a confidence threshold. Our extensive experiments across multiple benchmark datasets, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that a properly calibrated closed-set classifier can achieve state-of-the-art open-set performance, outperforming many sophisticated open-set methods while requiring significantly less computational overhead. These findings suggest that the key to effective open-set recognition may lie in optimizing the closed-set classification performance and calibration, rather than developing complex open-set specific techniques."
http://arxiv.org/abs/2109.13228v1,PASS: An ImageNet replacement for self-supervised pretraining without humans,"Self-supervised learning (SSL) has demonstrated remarkable success in pretraining visual representations, often relying on large, human-annotated datasets like ImageNet. However, the reliance on human annotation introduces biases and limits scalability. This paper addresses the problem of creating a large-scale, high-quality pretraining dataset suitable for SSL without any human involvement. We introduce PASS (Procedurally-generated, Automatically-filtered, Synthetic Scenes), a dataset generated using a physics engine and automatically filtered based on a novel set of metrics designed to ensure diversity, complexity, and realism. PASS leverages procedural generation to create a vast and varied collection of synthetic 3D scenes, followed by a filtering stage that eliminates trivial or unrealistic samples based on object interaction, scene composition, and visual quality scores. We demonstrate that models pretrained on PASS achieve competitive or superior performance compared to models pretrained on ImageNet across a range of downstream tasks, including image classification and object detection, while requiring no human annotation. This work provides a scalable and unbiased alternative to human-annotated datasets for self-supervised visual representation learning, paving the way for more robust and generalizable AI systems."
http://arxiv.org/abs/2107.14795v3,Perceiver IO: A General Architecture for Structured Inputs & Outputs,"The Perceiver architecture has demonstrated strong performance on various unstructured data modalities by iteratively attending to a small set of latent variables. However, many real-world perception tasks involve structured inputs and require structured outputs, such as object detection, semantic segmentation, and multi-modal sensor fusion, which are not natively supported by the original Perceiver. This paper introduces Perceiver IO, a general-purpose architecture that extends the Perceiver to handle arbitrary structured inputs and outputs. Perceiver IO learns to map task-specific query embeddings to relevant features extracted from the input using cross-attention, enabling the model to produce outputs of arbitrary modality and structure. We demonstrate state-of-the-art or competitive performance on a diverse set of tasks including image classification, multi-modal image and text processing, and visual reasoning, while using a single architecture and minimal task-specific modifications. Perceiver IO offers a unified and scalable approach for processing complex, structured data, paving the way for more general-purpose perception systems."
http://arxiv.org/abs/2107.06652v2,Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging,"Whole-body medical imaging, encompassing modalities like PET and MRI, offers a comprehensive view of anatomical structures and physiological processes, crucial for diagnosis and treatment planning. However, accurate alignment of these multi-modal volumes remains a significant challenge due to variations in patient positioning, image resolution, and inherent modality-specific characteristics. This paper introduces a novel self-supervised multi-modal alignment framework specifically designed for whole-body medical imaging. Our approach leverages a deep learning architecture trained with a contrastive loss, encouraging feature embeddings from different modalities to be similar for corresponding anatomical regions while dissimilar for others, without relying on manual annotations or pre-defined landmarks. Extensive experiments on a large-scale dataset of PET/MRI scans demonstrate that our self-supervised method achieves state-of-the-art alignment accuracy, outperforming existing unsupervised and weakly supervised techniques. This advancement enables more precise co-registration of multi-modal whole-body scans, facilitating improved diagnostic accuracy and downstream tasks such as lesion detection and segmentation."
http://arxiv.org/abs/2106.15252v1,AutoNovel: Automatically Discovering and Learning Novel Visual Categories,"Learning to recognize novel visual categories without explicit supervision remains a significant challenge in computer vision. Existing approaches often rely on manually defined proxy tasks or handcrafted similarity metrics, limiting their adaptability and scalability. This paper addresses the problem of automatically discovering and learning novel visual categories from unlabeled data alongside a set of labeled base categories. We propose AutoNovel, a novel framework that leverages a self-supervised contrastive learning objective to learn feature representations conducive to clustering. Subsequently, it employs an adaptive density-based clustering algorithm to automatically discover novel categories and refine the initial feature space. Finally, a pseudo-labeling strategy is used to train a classifier that can jointly recognize base and novel categories. Experiments on benchmark datasets demonstrate that AutoNovel achieves state-of-the-art performance in novel category discovery and classification, significantly outperforming existing methods, particularly when the number of novel categories is unknown. AutoNovel offers a practical and scalable solution for expanding visual recognition capabilities in real-world scenarios where labeled data is scarce and the universe of visual categories is constantly evolving."
http://arxiv.org/abs/2106.05264v1,NeRF in detail: Learning to sample for view synthesis,"Neural Radiance Fields (NeRF) have revolutionized novel view synthesis by learning continuous volumetric scene representations from a set of posed images. However, NeRF's reliance on uniform or importance sampling along camera rays can be inefficient, particularly in scenes with complex geometry and varying levels of detail, leading to suboptimal rendering times and potential loss of fine-grained details. This paper addresses the problem of learning a more efficient and adaptive sampling strategy for NeRF that prioritizes regions contributing significantly to the final rendered image. We propose a novel approach that integrates a learnable sampling network, conditioned on both ray direction and accumulated color and density information, to dynamically adjust sample locations along each ray. This network is trained alongside the NeRF model itself, optimizing for both rendering accuracy and sampling efficiency using a differentiable rendering loss and a regularizing loss that encourages sparsity in sampling. Our experiments demonstrate that our learned sampling strategy significantly reduces the number of samples required per ray while maintaining or even improving rendering quality, particularly in regions with intricate geometric details. This leads to faster rendering times and enhanced representation of high-frequency scene information, making NeRF more practical for real-world applications."
http://arxiv.org/abs/2105.09939v1,"Face, Body, Voice: Video Person-Clustering with Multiple Modalities","Unsupervised person clustering in videos is crucial for various applications, including video summarization and social event analysis. However, relying on a single modality, such as face appearance, often fails due to variations in pose, illumination, and occlusion. This paper addresses the challenging problem of clustering individuals in videos using a multi-modal approach that leverages complementary information from face appearance, body pose, and voice characteristics. Our method employs a deep learning framework to extract robust feature representations from each modality. These features are then fused using a learned attention mechanism, which adaptively weights the contribution of each modality based on its reliability in a given video segment. Finally, we perform spectral clustering on the fused features to group video segments belonging to the same individual. Experimental results on challenging video datasets demonstrate that our multi-modal approach significantly outperforms state-of-the-art methods that rely on single modalities, achieving a substantial improvement in clustering accuracy. This work provides a robust and effective solution for person clustering in videos, paving the way for more accurate and comprehensive video understanding."
http://arxiv.org/abs/2105.06993v2,Omnimatte: Associating Objects and Their Effects in Video,"Understanding object interactions and their subsequent effects is crucial for comprehensive video understanding. Current video analysis techniques often treat objects and their effects as separate entities, hindering the ability to reason about cause-and-effect relationships. This paper introduces Omnimatte, a novel framework for jointly modeling objects and their visual effects within a video sequence, enabling the association of actions with their consequences. Our approach leverages a transformer-based architecture to simultaneously predict object masks and corresponding ""effect masks,"" representing the visual changes induced by object interactions. We introduce a contrastive learning objective that encourages the model to associate objects with their effects, even in scenarios with complex occlusions and dynamic backgrounds. Experiments on synthetic and real-world datasets demonstrate that Omnimatte effectively identifies and segments both objects and their effects, outperforming existing methods in associating actions with their consequences. This work paves the way for more robust and interpretable video understanding systems capable of reasoning about object-centric dynamics."
http://arxiv.org/abs/2506.09027v2,Diffuse and Disperse: Image Generation with Representation Regularization,"Image generation has achieved remarkable progress, yet generated samples often lack diversity and fidelity due to mode collapse and overfitting in the latent space. This paper addresses the challenge of improving the diversity and quality of generated images by introducing a novel representation regularization technique. Our method, Diffuse and Disperse (DAD), incorporates two key components: a diffusion-based regularizer that encourages broader coverage of the latent space, and a dispersion loss that penalizes clustering of latent representations. The diffusion regularizer injects controlled noise into the latent vectors during training, effectively smoothing the learned distribution, while the dispersion loss maximizes the distance between latent representations of different generated samples. Experiments on several benchmark datasets, including CIFAR-10, CelebA, and LSUN, demonstrate that DAD significantly improves both the Frchet Inception Distance (FID) score and Inception Score (IS), indicating enhanced image quality and diversity compared to state-of-the-art generative models. These results highlight the efficacy of representation regularization in mitigating common issues in image generation and paving the way for more robust and versatile generative models."
http://arxiv.org/abs/2505.13447v1,Mean Flows for One-step Generative Modeling,"Generative modeling has seen significant advances, yet many current methods rely on iterative refinement processes, such as diffusion models, which can be computationally expensive. This paper addresses the challenge of generating high-quality samples from complex distributions in a single step. We introduce Mean Flows, a novel approach that leverages the concept of a flow, not to directly transform noise into a sample, but to learn the *mean* of a conditional distribution parameterized by the input noise. Specifically, we train a neural network to predict the mean of the data distribution conditioned on a random input, and then sample directly from this learned conditional mean. Through experiments on various image datasets, including CIFAR-10, CelebA, and LSUN, we demonstrate that Mean Flows can achieve competitive or superior performance compared to existing one-step generative models in terms of image quality and sample diversity, as measured by Frchet Inception Distance (FID) and Kernel Inception Distance (KID). This method offers a computationally efficient alternative for generative modeling, enabling rapid generation of high-fidelity samples without iterative refinement."
http://arxiv.org/abs/2502.17437v2,Fractal Generative Models,"Generative models have achieved remarkable success in creating realistic images, often relying on complex neural network architectures trained on large datasets. However, these models often lack inherent structural inductive biases, leading to challenges in generating images with well-defined self-similar patterns and hierarchical organization. This paper addresses the problem of generating images with fractal characteristics using a novel approach that integrates fractal geometry principles directly into the generative process. We introduce Fractal Generative Models (FGMs), which leverage Iterated Function Systems (IFS) to define the underlying fractal structure and employ a neural network to parameterize the IFS transformations conditioned on a latent vector. This allows for the generation of diverse fractal images with controlled complexity and self-similarity. Our experiments demonstrate that FGMs can generate a wide variety of fractal patterns, outperforming traditional generative adversarial networks (GANs) in capturing fine-grained fractal details and exhibiting superior sample quality metrics on fractal image datasets. The proposed FGMs offer a promising avenue for generating structured images with inherent self-similarity, potentially impacting applications in texture synthesis, terrain generation, and scientific visualization."
http://arxiv.org/abs/2502.13129v1,Is Noise Conditioning Necessary for Denoising Generative Models?,"Denoising diffusion probabilistic models (DDPMs) have achieved state-of-the-art results in image generation and restoration. Current DDPMs typically condition the denoising network on the noise level, often represented by the variance schedule of the forward diffusion process, to guide the denoising process at each step. This work investigates whether such noise conditioning is inherently necessary for the performance of denoising generative models. We propose a simplified DDPM architecture, termed Noise-Conditioning-Free DDPM (NCF-DDPM), which removes explicit noise level conditioning from the denoising network. Instead, NCF-DDPM relies solely on the inherent information encoded in the noisy input itself, learned through a modified training objective that encourages the network to implicitly infer the noise level. Experimental results on standard image datasets demonstrate that NCF-DDPM achieves comparable, and in some cases superior, image generation quality compared to traditional noise-conditioned DDPMs, while significantly reducing model complexity. This finding suggests that noise conditioning may be redundant and that the network can effectively learn to denoise by implicitly inferring the noise level from the noisy input, offering a pathway towards more efficient and interpretable generative models."
http://arxiv.org/abs/2410.13863v1,Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens,"Autoregressive models have shown promise in text-to-image generation, but their computational cost often limits the achievable image resolution and model scale. Existing discrete token-based approaches require large vocabularies to represent images, leading to increased computational demands and memory footprint during training and inference. We introduce Fluid, a novel approach that replaces discrete image tokens with continuous representations learned through a neural codec, enabling autoregressive modeling directly in a continuous latent space. Fluid leverages a hierarchical vector quantized variational autoencoder (VQ-VAE) to efficiently encode images into a compact, continuous representation, which is then modeled autoregressively using a Transformer architecture conditioned on text embeddings. Experiments demonstrate that Fluid achieves comparable or superior image quality to state-of-the-art discrete token-based models while significantly reducing the number of tokens required for generation. This allows for scaling autoregressive text-to-image models to higher resolutions and larger model sizes, resulting in improved image fidelity and semantic coherence. Fluid offers a more efficient and scalable pathway towards high-quality text-to-image generation, paving the way for more complex and realistic image synthesis."
http://arxiv.org/abs/2409.20537v1,Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers,"Proprioceptive-visual learning, which combines robot joint state information with visual input, offers a promising avenue for developing robust and versatile robotic agents. However, scaling these approaches to complex tasks and diverse environments remains challenging due to the difficulty of training large models from scratch with limited robot interaction data. To address this, we propose a novel framework for scaling proprioceptive-visual learning by leveraging heterogeneous pre-trained Transformers. Our method utilizes a pre-trained vision transformer (ViT) for visual encoding and a separate pre-trained language transformer (GPT-2) adapted for proprioceptive sequence modeling. We then introduce a cross-modal attention mechanism to fuse these representations, enabling effective interaction between visual and proprioceptive modalities. Experiments on a suite of simulated robotic manipulation tasks demonstrate that our approach significantly outperforms training from scratch, achieving up to a 40% improvement in task success rate and exhibiting strong generalization capabilities to unseen environments. This work highlights the potential of transfer learning from pre-trained models to accelerate the development of high-performing and adaptable robotic systems."
http://arxiv.org/abs/2406.11838v3,Autoregressive Image Generation without Vector Quantization,"Autoregressive models have demonstrated impressive capabilities in image generation, but often rely on vector quantization to compress the image into a discrete latent space, limiting fidelity and introducing quantization artifacts. This paper addresses the challenge of generating high-fidelity images autoregressively without relying on vector quantization. We introduce a novel continuous autoregressive image generation framework based on a hierarchical transformer architecture. Our approach directly models the image pixels as a continuous distribution, conditioned on previously generated pixels in a raster scan order, employing masked self-attention to capture long-range dependencies. To manage the computational complexity associated with high-dimensional continuous data, we utilize a multi-scale architecture with localized attention mechanisms in the initial layers. Experimental results on benchmark datasets such as ImageNet demonstrate that our method achieves competitive or superior performance compared to existing vector-quantization based autoregressive models, generating images with improved visual quality and reduced artifacts. This work opens avenues for exploring continuous autoregressive models for high-fidelity image synthesis, circumventing the limitations imposed by discrete latent spaces."
http://arxiv.org/abs/2405.20510v3,Physically Compatible 3D Object Modeling from a Single Image,"Inferring 3D object geometry from a single image is a long-standing challenge in computer vision, crucial for applications ranging from robotics to augmented reality. However, existing single-view reconstruction methods often produce geometrically implausible shapes that violate basic physical principles like stability and connectivity. This paper addresses the problem of generating physically compatible 3D object models from single RGB images. Our approach leverages differentiable physics simulation within a neural rendering framework. We introduce a novel loss function that penalizes physically unstable configurations and encourages structural integrity during the reconstruction process. Specifically, we iteratively refine the predicted 3D mesh by simulating its behavior under gravity and structural forces, using gradients from the simulation to guide the mesh deformation towards a stable and coherent state, all while maintaining visual consistency with the input image. Experiments on synthetic and real-world datasets demonstrate that our method generates 3D models with significantly improved physical plausibility compared to state-of-the-art single-view reconstruction techniques, evidenced by quantitative metrics of stability and qualitative visual inspection. This advancement paves the way for more robust and reliable 3D scene understanding and interaction in various applications."
http://arxiv.org/abs/2405.20283v4,TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes,"Neural Radiance Fields (NeRFs) have revolutionized novel view synthesis, but often struggle to represent high-frequency geometric details efficiently and accurately. Explicit mesh representations, while capable of detailed geometry, can be challenging to optimize directly from multi-view images without good initialization. We introduce TetSphere Splatting, a novel hybrid representation that leverages the advantages of both volumetric and mesh-based approaches. Our method represents the scene with a Lagrangian tetrahedral mesh whose vertices are associated with Gaussian splats. During rendering, these splats are projected and blended, with their properties modulated by a neural network conditioned on the tetrahedral mesh structure. This allows us to effectively learn high-quality, explicit geometry while benefiting from the smoothness and regularization imposed by the underlying volumetric mesh. Experiments on benchmark datasets demonstrate that TetSphere Splatting achieves state-of-the-art results in terms of both reconstruction accuracy and rendering quality, particularly in regions with fine geometric details. This approach offers a promising direction for high-fidelity 3D scene representation and novel view synthesis."
http://arxiv.org/abs/2403.08632v2,A Decade's Battle on Dataset Bias: Are We There Yet?,"Dataset bias remains a critical challenge in computer vision, hindering the generalization capabilities of trained models to real-world scenarios. This paper investigates the progress made over the last decade in mitigating dataset bias, focusing on image classification, object detection, and semantic segmentation tasks. We systematically review existing bias mitigation techniques, categorizing them into data re-sampling, domain adaptation, and bias-aware learning strategies. Furthermore, we propose a novel meta-analysis framework based on a unified bias quantification metric to compare the effectiveness of different methods across diverse datasets and architectures. Our analysis reveals that while significant advancements have been achieved, biases related to object co-occurrence and background context remain particularly challenging. Specifically, we demonstrate that current domain adaptation techniques struggle to effectively transfer knowledge when biases are deeply embedded within the feature representations. The findings underscore the need for developing more robust and interpretable bias mitigation strategies that explicitly address the underlying causes of dataset bias, leading to more reliable and generalizable computer vision systems."
http://arxiv.org/abs/2401.14404v1,Deconstructing Denoising Diffusion Models for Self-Supervised Learning,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in generative modeling, but their potential for representation learning remains underexplored compared to contrastive and masked autoencoding approaches. This paper investigates the efficacy of DDPMs as a foundation for self-supervised learning, specifically addressing the challenge of extracting meaningful features from the intermediate denoising steps. We propose a novel framework, Diffusion Feature Disentanglement (DFD), which leverages the inherent structure of the diffusion process to learn disentangled representations of image content. DFD employs a learned encoder to map noisy images from various diffusion timesteps into a latent space, followed by a decoder trained to reconstruct the original image from these features. Crucially, we introduce a disentanglement loss that encourages the latent features at different timesteps to capture complementary information about the image. Experiments on ImageNet and downstream tasks demonstrate that DFD achieves competitive or superior performance compared to established self-supervised learning methods, particularly in transfer learning scenarios. These results highlight the untapped potential of DDPMs for representation learning and offer a new perspective on leveraging generative models for discriminative tasks."
http://arxiv.org/abs/2312.03701v4,Return of Unconditional Generation: A Self-supervised Representation Generation Method,"Unconditional generative models have demonstrated remarkable progress in synthesizing realistic and diverse images, yet their ability to learn meaningful representations without explicit labels remains a challenge. This paper addresses the problem of learning high-quality, disentangled representations from unlabeled data through unconditional generation. We introduce a novel self-supervised representation generation method, termed SRG, which leverages a masked autoencoding objective within a generative adversarial network (GAN) framework. SRG trains the generator to reconstruct masked regions of its own generated images, forcing the generator to implicitly learn features that capture global context and semantic information. Critically, we introduce a consistency loss between the original and reconstructed generated images to further enhance representation learning. Extensive experiments on several benchmark datasets demonstrate that SRG achieves state-of-the-art performance in unsupervised representation learning, outperforming existing contrastive and generative approaches on downstream classification and segmentation tasks. This work highlights the potential of unconditional generation as a powerful tool for self-supervised representation learning, offering a pathway towards more efficient and robust vision systems."
http://arxiv.org/abs/2212.00794v2,Scaling Language-Image Pre-training via Masking,"Language-image pre-training (CLIP) has demonstrated remarkable success in learning transferable visual representations by aligning image and text embeddings. However, scaling CLIP models to larger datasets and model sizes remains computationally expensive due to the need to process both images and text during pre-training. This paper addresses the challenge of efficient scaling in language-image pre-training by introducing a novel masking strategy. Our approach, MaskCLIP, leverages masked image modeling to pre-train a visual encoder, subsequently aligning it with a pre-trained text encoder via contrastive learning only on the visible image patches. This reduces the computational burden by processing only a fraction of the image pixels during the alignment phase. Experiments on benchmark datasets demonstrate that MaskCLIP achieves comparable or superior performance to full-image CLIP models while significantly reducing pre-training costs (up to 3x speedup). This work provides a practical and efficient pathway towards scaling language-image pre-training to unlock the potential of even larger models and datasets."
http://arxiv.org/abs/2205.09113v2,Masked Autoencoders As Spatiotemporal Learners,"Self-supervised learning has shown remarkable success in pre-training visual representations from static images. However, extending these techniques to learn rich spatiotemporal representations from videos remains a challenge due to the increased data dimensionality and computational cost. This paper addresses the problem of efficiently learning spatiotemporal features from unlabeled videos using a masked autoencoding approach. We introduce Spatiotemporal Masked Autoencoder (ST-MAE), a framework that randomly masks out large portions of video clips in both spatial and temporal dimensions and trains an encoder-decoder network to reconstruct the missing information. To handle the increased computational burden, we employ a lightweight decoder that operates only on the visible tokens. Our experiments on action recognition benchmarks, including Kinetics-400 and Something-Something V2, demonstrate that ST-MAE achieves competitive performance compared to existing self-supervised methods while significantly reducing pre-training costs. This work highlights the potential of masked autoencoders as scalable and effective learners of spatiotemporal representations, paving the way for more efficient video understanding models."
http://arxiv.org/abs/2203.16527v2,Exploring Plain Vision Transformer Backbones for Object Detection,"Vision Transformers (ViTs) have demonstrated remarkable performance in image classification, prompting exploration of their potential as backbones for object detection. However, adapting ViTs directly often requires complex modifications and specialized training strategies to match the feature hierarchies expected by typical detection heads. This paper investigates the effectiveness of using plain, unmodified ViT architectures as backbones for object detection, minimizing architectural changes and focusing on transfer learning. We explore various ViT variants, including DeiT and Swin-Transformer, and couple them with established detection heads like Faster R-CNN and Mask R-CNN with minimal adaptation. Our approach involves pre-training ViTs on ImageNet-1K followed by fine-tuning the entire detector on COCO. Experimental results show that plain ViT backbones can achieve competitive performance compared to CNN-based backbones like ResNet, particularly with larger ViT models, demonstrating the inherent capability of ViTs to learn features suitable for object detection tasks without significant architectural tailoring. This work highlights the potential of leveraging pre-trained ViTs for object detection with minimal engineering effort, paving the way for simpler and more efficient vision systems."
http://arxiv.org/abs/2111.11429v1,Benchmarking Detection Transfer Learning with Vision Transformers,"Detection transfer learning, leveraging pre-trained models on large datasets, is crucial for adapting to data-scarce scenarios and reducing training costs in object detection. However, the effectiveness of transfer learning strategies with the advent of Vision Transformers (ViTs) remains relatively unexplored compared to CNN-based architectures. This paper addresses the problem of systematically benchmarking various transfer learning techniques applied to ViT-based detectors across diverse datasets and training regimes. We investigate fine-tuning strategies, including full fine-tuning, feature extraction, and adapter-based methods, analyzing their impact on detection performance. Furthermore, we explore the influence of pre-training datasets (ImageNet-1K, ImageNet-22K) and ViT model scales (Base, Large) on downstream detection tasks. Our experiments on COCO, Pascal VOC, and a medical imaging dataset demonstrate that, while ViTs generally benefit from transfer learning, careful selection of pre-training data and fine-tuning strategy is critical, with adapter-based methods offering a compelling trade-off between performance and computational cost. These findings provide valuable insights for practitioners seeking to effectively leverage ViTs for object detection tasks, paving the way for more efficient and robust detection systems."
http://arxiv.org/abs/2111.06377v3,Masked Autoencoders Are Scalable Vision Learners,"Self-supervised learning has shown promise in learning powerful representations from unlabeled image data, often relying on pretext tasks like predicting image patches. However, scaling these methods to large datasets and model sizes can be computationally expensive and memory intensive. This paper addresses the challenge of efficiently pre-training large-scale vision transformers on massive datasets without incurring prohibitive computational costs. We propose Masked Autoencoders (MAE), a simple yet effective approach for self-supervised pre-training. MAE randomly masks large portions of the input image and reconstructs the missing pixels in the pixel space. This asymmetric encoder-decoder architecture allows us to process only the visible tokens during encoding, significantly reducing computational cost and memory footprint. We demonstrate that MAE enables pre-training large vision transformers, such as ViT-Huge, with high masking ratios (e.g., 75%) on ImageNet-1K in a feasible time frame. Our experiments show that MAE pre-training achieves competitive or superior performance compared to existing self-supervised methods and supervised pre-training when fine-tuned on downstream tasks, demonstrating its ability to learn robust and transferable visual representations at scale. MAE provides a scalable and efficient pre-training strategy for vision transformers, paving the way for learning from even larger datasets and models."
http://arxiv.org/abs/2104.14558v1,A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning,"Unsupervised learning of spatiotemporal representations from video offers a promising avenue for leveraging the vast amounts of unlabeled video data available. However, the effectiveness of different unsupervised approaches and the impact of scale remain underexplored. This paper presents a large-scale empirical study evaluating various unsupervised spatiotemporal representation learning methods on diverse downstream tasks. We investigate the impact of pre-training dataset size, architecture design choices, and different pretext tasks, including contrastive learning, masked autoencoding, and generative modeling. Our approach involves pre-training models with varying capacities on a newly curated dataset comprising millions of video clips, followed by a systematic evaluation on action recognition, video retrieval, and video captioning tasks. We find that larger models pre-trained with contrastive learning strategies on larger datasets consistently outperform smaller models and those trained with alternative pretext tasks. Furthermore, we identify key architectural modifications that improve spatiotemporal feature extraction. The results demonstrate the importance of scale and careful design choices in unsupervised spatiotemporal representation learning, paving the way for more effective utilization of unlabeled video data for a wide range of applications."
http://arxiv.org/abs/2104.02057v4,An Empirical Study of Training Self-Supervised Vision Transformers,"Self-supervised learning (SSL) has emerged as a powerful paradigm for training visual representations without relying on expensive manual annotations. Vision Transformers (ViTs), known for their impressive scalability and performance on supervised tasks, are increasingly being adopted in self-supervised settings. However, the training dynamics and critical hyperparameters for effectively training self-supervised ViTs remain less understood compared to their convolutional counterparts. This paper presents a comprehensive empirical study of various factors influencing the performance of self-supervised ViTs, including data augmentation strategies, masking ratios, learning rate schedules, and the impact of different pretext tasks such as masked image modeling (MIM) and contrastive learning. We conduct extensive experiments on ImageNet-1K using different ViT architectures, systematically evaluating the effect of each factor on downstream transfer performance. Our findings reveal that specific data augmentation combinations and higher masking ratios are crucial for MIM-based pre-training of ViTs, while careful tuning of the learning rate schedule is essential for stable convergence in contrastive learning. Furthermore, we demonstrate that a well-tuned MIM approach can achieve comparable or superior performance to contrastive methods with a smaller computational budget. These insights provide valuable guidance for researchers and practitioners aiming to leverage self-supervised learning with ViTs, facilitating the development of more efficient and effective pre-training strategies."
http://arxiv.org/abs/2011.10566v1,Exploring Simple Siamese Representation Learning,"Self-supervised learning has emerged as a powerful paradigm for learning visual representations without relying on explicit labels. Recent advancements have demonstrated the effectiveness of Siamese networks with carefully designed architectures and loss functions for this task. However, the reliance on techniques like momentum encoders and negative samples adds complexity and computational overhead. This paper investigates a simplified Siamese network architecture, termed SimSiam, which surprisingly achieves competitive performance without requiring negative samples, momentum encoders, or large batch sizes. SimSiam consists of two identical networks: one with a prediction head and the other without. The prediction head attempts to predict the output of the other network based on different augmented views of the same image, thereby preventing collapse. We demonstrate that stopping the gradient flow to one of the Siamese branches is crucial for preventing trivial solutions. Our experiments on ImageNet linear evaluation and transfer learning benchmarks demonstrate that SimSiam achieves comparable or superior performance to existing state-of-the-art self-supervised methods while offering a significantly simpler implementation. This work provides a compelling case for rethinking the necessity of complex mechanisms in contrastive learning and opens avenues for exploring simpler and more efficient self-supervised learning algorithms."
http://arxiv.org/abs/2007.06559v2,Graph Structure of Neural Networks,"Neural networks, traditionally viewed as layered arrangements of neurons, possess an inherent graph structure that influences their computational properties. Understanding and leveraging this graph structure remains a challenge due to its complexity and dynamic nature during training. This paper investigates the graph-theoretic properties of neural networks, focusing on characterizing their connectivity patterns and their impact on network performance. We propose a novel framework for analyzing neural network graphs using measures such as node centrality, community structure, and spectral graph properties. This framework allows us to quantify the evolution of the network's graph structure during training and to identify critical nodes and connections that significantly impact the network's representational capacity. Our experiments on various benchmark datasets and network architectures reveal a strong correlation between specific graph properties and generalization performance, demonstrating that networks with balanced connectivity and well-defined community structures tend to achieve higher accuracy. These findings provide valuable insights into the internal workings of neural networks and offer a promising avenue for designing more efficient and robust architectures."
http://arxiv.org/abs/2003.13678v1,Designing Network Design Spaces,"The field of neural architecture search (NAS) has shown great promise in automating the design of deep learning models. However, NAS methods often operate within predefined search spaces, limiting their potential to discover truly novel architectures. This paper addresses the problem of designing effective and flexible network design spaces that enable the discovery of high-performing architectures beyond the constraints of existing manual designs. We propose a systematic approach to design space construction based on a factored representation of network architectures, allowing for independent exploration of various architectural dimensions, such as layer connectivity, operator selection, and cardinality. We then employ a population-based search algorithm to optimize the design space itself, learning which architectural choices are most crucial for performance. Experiments on image classification benchmarks demonstrate that our approach discovers design spaces that yield architectures outperforming those found by searching within manually designed spaces or using direct NAS methods. The resulting architectures achieve state-of-the-art accuracy with significantly reduced computational cost. This work offers a principled framework for designing network design spaces, facilitating the automated discovery of more efficient and effective neural network architectures."
http://arxiv.org/abs/2003.12056v2,Are Labels Necessary for Neural Architecture Search?,"Neural Architecture Search (NAS) has achieved remarkable success in automating the design of deep learning models, often surpassing human-designed architectures. However, the conventional NAS paradigm heavily relies on labeled datasets to evaluate and guide the search process, which can be a significant bottleneck in scenarios where labeled data is scarce or expensive to obtain. This paper addresses the fundamental question: Are labels truly necessary for effective neural architecture search? We propose a novel label-free NAS framework, termed LaNAS, that leverages unsupervised learning principles and network intrinsic properties like weight distribution entropy and gradient norm consistency to evaluate architectural performance. LaNAS pre-trains candidate architectures using self-supervised learning on unlabeled data and then employs these intrinsic metrics to estimate the architectures' potential for downstream tasks, guiding the search towards promising regions of the architecture space. Experiments on benchmark datasets demonstrate that LaNAS can discover architectures that achieve competitive performance compared to architectures found by supervised NAS methods, while significantly reducing the reliance on labeled data. This work opens up new avenues for NAS in data-scarce environments and highlights the potential of unsupervised signals for guiding architectural innovation."
http://arxiv.org/abs/2003.04297v1,Improved Baselines with Momentum Contrastive Learning,"Momentum contrastive (MoCo) learning has emerged as a powerful technique for self-supervised visual representation learning, achieving impressive results on downstream tasks. However, the performance of MoCo is highly dependent on the quality of the negative samples and the momentum update rate, often requiring careful tuning and extensive computational resources for optimal performance. This paper addresses the problem of improving MoCo baselines by enhancing the quality and diversity of negative samples and stabilizing the momentum update process. We propose a novel approach that integrates a dynamic negative sample selection strategy based on instance hardness and a self-adaptive momentum scheduler that adjusts the update rate based on the training dynamics. Our approach dynamically selects informative negative samples during training, preventing the model from focusing on easy negatives, and adapts the momentum update rate to maintain a consistent pace of feature evolution. Experiments on ImageNet and downstream tasks demonstrate that our method consistently outperforms existing MoCo variants and other self-supervised learning approaches, achieving significant improvements in both linear evaluation and transfer learning performance. These results highlight the importance of carefully managing negative sample selection and momentum updates in contrastive learning and offer a more efficient and robust approach to self-supervised representation learning."
http://arxiv.org/abs/1912.08193v2,PointRend: Image Segmentation as Rendering,"Image segmentation demands high-resolution feature representations to accurately delineate object boundaries and complex structures. However, existing approaches often struggle to efficiently capture fine-grained details due to computational constraints when processing high-resolution images. We address this limitation by reformulating image segmentation as a rendering problem, drawing inspiration from classical computer graphics. Our method, PointRend (Point-based Rendering), adaptively selects a sparse set of points within an image and iteratively refines their labels based on a learned feature representation. Specifically, a neural network predicts the most uncertain regions of the image, and these regions are subdivided into smaller points for further classification. This iterative subdivision process allows us to focus computation on areas where it is most needed, leading to significant improvements in rendering high-quality segmentation boundaries. Experiments on Cityscapes, ADE20K, and COCO datasets demonstrate that PointRend consistently outperforms existing segmentation methods, achieving state-of-the-art results while maintaining computational efficiency, particularly in high-resolution settings. This novel approach provides a more efficient and effective paradigm for high-quality image segmentation."
http://arxiv.org/abs/1912.00998v2,A Multigrid Method for Efficiently Training Video Models,"Deep learning models for video understanding are computationally expensive to train due to the high dimensionality of spatiotemporal data. This work addresses the challenge of efficiently training video models by introducing a novel multigrid method. Our approach, inspired by classical numerical solvers, progressively refines the temporal resolution of video clips during training. We begin with low-resolution clips to capture coarse-grained temporal dependencies and gradually increase the resolution, allowing the model to learn fine-grained details without the computational burden of training on full-resolution videos from the start. Experiments on standard video action recognition datasets, including Kinetics-400 and Something-Something V2, demonstrate that our multigrid method achieves comparable or better performance than training from scratch with full resolution, while significantly reducing training time and computational resources. This offers a practical and scalable solution for training high-performing video models."
http://arxiv.org/abs/1911.05722v3,Momentum Contrast for Unsupervised Visual Representation Learning,"Unsupervised visual representation learning aims to pre-train robust feature extractors without relying on manual annotations. A key challenge in contrastive learning is managing a large set of negative samples to accurately approximate the full data distribution, which often necessitates large batch sizes that are computationally expensive. This paper introduces Momentum Contrast (MoCo), an unsupervised learning framework that builds a large and consistent negative sample set without requiring large batches. MoCo maintains a momentum-updated encoder to construct a dynamic dictionary of negative samples, decoupling the dictionary size from the batch size. This allows for a significantly larger and more consistent negative sample set compared to existing methods. Experiments on ImageNet and downstream tasks demonstrate that MoCo achieves state-of-the-art results in unsupervised learning, outperforming previous methods by a significant margin and closing the gap with supervised pre-training. The improved performance and efficiency of MoCo offer a practical and scalable approach to learning high-quality visual representations from unlabeled data."
http://arxiv.org/abs/1904.09664v2,Deep Hough Voting for 3D Object Detection in Point Clouds,"3D object detection in point clouds is crucial for autonomous driving and robotics. However, accurately localizing objects with varying sizes and occlusions remains a significant challenge, particularly in sparse and noisy point cloud data. This paper addresses the problem of robust 3D object detection by leveraging a novel deep Hough voting framework. Our method first learns a deep feature representation for each point in the scene, and then uses these features to vote for object centers in a Hough space. A clustering algorithm aggregates these votes to generate object proposals. Crucially, we introduce a learned voting weight that adaptively adjusts the influence of each point based on its reliability and contextual information. Experimental results on the KITTI dataset demonstrate that our approach achieves state-of-the-art performance, particularly for objects with significant occlusions, showing improvements in both Average Precision and recall. This work provides a robust and efficient solution for 3D object detection, advancing the capabilities of perception systems in challenging real-world environments."
http://arxiv.org/abs/1904.01569v2,Exploring Randomly Wired Neural Networks for Image Recognition,"Deep neural networks have achieved remarkable success in image recognition, but their design often relies on hand-crafted architectures or computationally expensive neural architecture search. This paper addresses the problem of designing efficient and effective neural network architectures without relying on structured layer-by-layer construction or extensive search procedures. We explore the potential of randomly wired neural networks, where connections between neurons are established randomly according to specific graph properties. Specifically, we investigate different random graph models, including Erds-Rnyi and Barabsi-Albert graphs, to define the connectivity patterns within the network and evaluate their performance on image recognition tasks. Our experiments on CIFAR-10 and ImageNet demonstrate that randomly wired networks, with appropriately chosen graph parameters, can achieve competitive performance compared to traditional architectures while requiring significantly less design effort. These findings suggest that random wiring offers a promising alternative for designing neural networks, potentially leading to more efficient and adaptable architectures for various computer vision applications."
http://arxiv.org/abs/1903.12174v2,TensorMask: A Foundation for Dense Object Segmentation,"Dense object segmentation, which aims to assign a class label to each pixel belonging to an object instance, is a crucial yet challenging problem in computer vision. Current instance segmentation approaches typically represent masks with a single vector, often relying on complex post-processing to delineate object boundaries and handle overlapping instances. This paper introduces TensorMask, a novel framework that represents masks as structured tensors instead of vectors, enabling a fundamentally different approach to dense object segmentation. Our method predicts a tensor field representing the mask in a spatially coherent manner, allowing us to directly reason about the shape and spatial layout of each object instance. We achieve this through a fully convolutional network architecture that predicts a 4D tensor for each RoI, encoding both the mask and its spatial relationships. Experiments on the COCO dataset demonstrate that TensorMask significantly outperforms state-of-the-art methods, achieving a substantial improvement in mask AP, particularly for objects with complex shapes and heavy occlusions. This tensor-based representation offers a powerful and more natural foundation for future research in dense object segmentation and related dense prediction tasks."
http://arxiv.org/abs/1901.02446v2,Panoptic Feature Pyramid Networks,"Panoptic segmentation, uniting instance and semantic segmentation, provides a comprehensive scene understanding. However, existing methods often treat these tasks as separate branches, leading to suboptimal feature sharing and performance, especially for small objects and boundary regions. We introduce Panoptic Feature Pyramid Networks (PFPN), a novel architecture that leverages a shared feature pyramid backbone and introduces a dedicated panoptic head to address this issue. PFPN employs a Feature Alignment Module (FAM) to refine feature representations across pyramid levels, enhancing feature consistency between instance and semantic branches. Furthermore, we propose a Boundary Aware Loss (BAL) that focuses training on difficult boundary pixels, improving segmentation accuracy in these critical regions. Experiments on the COCO and Cityscapes datasets demonstrate that PFPN achieves state-of-the-art panoptic segmentation performance, surpassing existing methods by a significant margin, particularly in segmentation quality (SQ) and for small object instance segmentation. PFPN's unified architecture and targeted improvements provide a more effective and efficient approach to holistic scene understanding."
http://arxiv.org/abs/1812.05038v2,Long-Term Feature Banks for Detailed Video Understanding,"Detailed video understanding, crucial for applications like video retrieval and autonomous driving, requires capturing both short-term dynamics and long-range dependencies. However, current approaches often struggle to effectively model extended temporal contexts, particularly in complex scenarios with subtle visual cues. We address this limitation by introducing Long-Term Feature Banks (LTFB), a novel framework that aggregates and maintains a dynamic repository of temporally diverse feature representations extracted from the entire video sequence. LTFB employs a learnable attention mechanism to selectively retrieve and fuse relevant features from the bank, enabling the model to reason over long-term dependencies and adapt to varying temporal scales. Experiments on challenging video understanding datasets, including ActivityNet and Charades, demonstrate that LTFB significantly improves performance in action recognition and temporal action localization compared to state-of-the-art methods. These results highlight the importance of long-term context modeling and the effectiveness of LTFB in capturing detailed video semantics for improved video understanding."
http://arxiv.org/abs/1812.03982v3,SlowFast Networks for Video Recognition,"Human action recognition in video demands effective modeling of both semantic information and temporal dynamics. Existing approaches often prioritize either accuracy or efficiency, struggling to achieve both simultaneously. This paper addresses the challenge of developing a video recognition architecture that balances high accuracy with computational efficiency. We introduce SlowFast networks, a novel architecture composed of two parallel pathways: a Slow pathway operating at a low frame rate to capture semantic information, and a Fast pathway operating at a high frame rate to capture fine-grained temporal dynamics at minimal computational cost. The Fast pathway is designed to attend to motion and its features are fused with the Slow pathway to enrich its semantic understanding. Our experiments on the Kinetics-400 and AVA datasets demonstrate that SlowFast networks achieve state-of-the-art accuracy with significantly improved computational efficiency compared to previous approaches. This work provides a practical and effective solution for video understanding, enabling deployment in resource-constrained environments and facilitating real-time applications."
http://arxiv.org/abs/1812.03411v2,Feature Denoising for Improving Adversarial Robustness,"Deep neural networks are vulnerable to adversarial attacks, where small, carefully crafted perturbations to input images can drastically alter a model's prediction. This sensitivity stems from the model's reliance on potentially noisy or spurious features. We address the problem of improving adversarial robustness by explicitly denoising feature representations within the network. Our proposed method, Feature Denoising via Learned Subspace Projection (FD-LSP), learns a subspace projection for each layer, designed to filter out adversarial noise while preserving essential feature information. The projection is learned by minimizing a reconstruction loss that encourages the denoised features to retain the characteristics of the original, clean features, and is trained jointly with the classification network. We demonstrate that FD-LSP significantly improves adversarial robustness against various attack strategies, including FGSM, PGD, and C&W attacks, across different datasets like CIFAR-10 and CIFAR-100, while maintaining competitive clean accuracy. This research offers a practical and effective approach to enhance the reliability of deep learning models in adversarial environments."
http://arxiv.org/abs/1811.08883v1,Rethinking ImageNet Pre-training,"ImageNet pre-training has been a cornerstone of modern computer vision, enabling effective transfer learning across diverse downstream tasks. However, the computational cost and dataset bias associated with ImageNet raise questions about its continued necessity as a default pre-training strategy. This paper addresses the problem of reducing reliance on ImageNet pre-training by exploring alternative self-supervised learning (SSL) approaches and lightweight fine-tuning techniques. We propose a novel distillation-based SSL framework that leverages knowledge transfer from a smaller, more diverse dataset to train compact and efficient vision models. These models are then fine-tuned using a parameter-efficient transfer learning strategy that selectively updates only a subset of the pre-trained weights based on task similarity. Our experiments on a range of downstream tasks, including object detection and semantic segmentation, demonstrate that our approach achieves comparable or superior performance to ImageNet pre-training while significantly reducing computational overhead and mitigating dataset bias. This work highlights the potential of SSL and parameter-efficient fine-tuning to democratize computer vision research and foster the development of more adaptable and robust models."
http://arxiv.org/abs/1805.00932v1,Exploring the Limits of Weakly Supervised Pretraining,"Weakly supervised pretraining, leveraging large datasets with noisy or incomplete labels, has become a popular paradigm for learning powerful visual representations. However, the precise limits of this approach, particularly concerning the scale of data and the nature of the weak supervision, remain underexplored. This paper investigates the trade-offs between dataset size, label quality, and architectural choices in weakly supervised pretraining. We propose a systematic evaluation framework encompassing variations in dataset size (ranging from millions to billions of images), differing levels of label noise introduced through automated annotation techniques, and diverse network architectures, including both convolutional and transformer-based models. We pretrain models on these datasets and evaluate their performance on a diverse suite of downstream tasks, spanning image classification, object detection, and semantic segmentation. Our results demonstrate a diminishing return in performance gains as dataset size increases beyond a certain threshold, especially when coupled with high label noise. Furthermore, we observe that transformer architectures exhibit greater robustness to label noise compared to convolutional networks, but require significantly larger datasets to realize their full potential. These findings provide valuable insights into the practical constraints and opportunities for weakly supervised pretraining, guiding future research towards more efficient and effective utilization of large-scale, weakly labeled data."
http://arxiv.org/abs/1803.08494v3,Group Normalization,"Deep convolutional neural networks (CNNs) often rely on batch normalization (BN) to accelerate training and improve generalization. However, BN's performance degrades significantly when batch sizes are small, a common scenario in tasks like object detection, video processing, and training with large models. To address this limitation, we propose Group Normalization (GN), a simple and effective alternative to BN. GN divides the channels of a given layer into groups and computes the mean and variance for normalization within each group. This allows the normalization to be independent of batch size, while still leveraging statistics from multiple channels. We demonstrate that GN consistently outperforms BN in object detection, semantic segmentation, and video classification tasks, especially when using small batch sizes. Furthermore, GN's performance is comparable to or better than BN even with large batch sizes, making it a versatile and robust normalization technique for a wide range of applications."
http://arxiv.org/abs/1801.00868v3,Panoptic Segmentation,"Scene understanding is a fundamental task in computer vision, requiring comprehensive image parsing. While semantic and instance segmentation have advanced significantly, they traditionally address distinct aspects: semantic segmentation assigns a class label to each pixel, while instance segmentation detects and segments individual object instances. This dichotomy necessitates a unified approach capable of simultaneously reasoning about both things (countable objects) and stuff (amorphous background regions) in an image. We introduce panoptic segmentation, a novel task that unifies semantic and instance segmentation, requiring a single, coherent scene segmentation. We propose a conceptually simple yet effective method, leveraging a shared backbone network followed by separate semantic and instance segmentation branches. These branches are then fused through a novel panoptic head that resolves overlaps and assigns unique IDs to each segment, ensuring a consistent and complete scene representation. Our experiments on challenging datasets like Cityscapes and COCO demonstrate that our approach achieves state-of-the-art panoptic segmentation performance, significantly outperforming existing methods. This unified framework provides a more complete and interpretable scene representation, paving the way for more robust and comprehensive vision systems."
http://arxiv.org/abs/1712.04440v1,Data Distillation: Towards Omni-Supervised Learning,"Data distillation, a technique for compressing large datasets into smaller, more informative synthetic datasets, has shown promise in reducing training costs and improving model generalization. However, current distillation methods often rely on strong supervision signals and are limited in their ability to leverage the abundance of weakly or self-supervised data. This paper addresses the challenge of effectively distilling knowledge from datasets with diverse and heterogeneous supervision levels, moving towards an ""omni-supervised"" learning paradigm. We propose a novel data distillation framework that leverages a meta-learning approach to dynamically weight and combine information from different supervision modalities during the distillation process. Our method learns a meta-distiller that optimizes the quality of the distilled dataset based on the performance of models trained on it, across various downstream tasks and supervision types. Experiments on benchmark datasets demonstrate that our approach significantly outperforms existing data distillation techniques, achieving higher accuracy and faster convergence when training models with limited labeled data and abundant weakly labeled or unlabeled data. This work provides a crucial step towards efficient and effective learning from the vast quantities of data with diverse supervision available in real-world scenarios."
http://arxiv.org/abs/1711.10370v2,Learning to Segment Every Thing,"Semantic segmentation models have achieved remarkable performance in understanding the pixel-level content of images, but are typically limited to a pre-defined set of categories. This paper addresses the challenge of ""open-vocabulary"" semantic segmentation, aiming to segment ""every thing"" in an image without being restricted to a fixed vocabulary. We propose a novel framework, ""Segment Everything with Concepts (SEC),"" which leverages large-scale vision-language models to transfer knowledge from textual descriptions to pixel-level segmentation. SEC learns to associate visual regions with a rich set of concepts obtained from pre-trained language models and then utilizes these concept embeddings to predict segmentation masks for arbitrary textual queries. Experiments on challenging datasets like COCO and LVIS demonstrate that SEC achieves state-of-the-art performance in open-vocabulary segmentation, significantly outperforming existing zero-shot and few-shot methods. This work paves the way for more flexible and generalizable semantic segmentation systems capable of understanding and interacting with the visual world in a truly open-ended manner."
http://arxiv.org/abs/1711.07971v3,Non-local Neural Networks,"Long-range dependencies are crucial for capturing contextual information in visual tasks, but convolutional and recurrent operations process local neighborhoods, posing challenges for modeling such dependencies efficiently. This paper addresses the problem of capturing long-range dependencies in neural networks without relying on recurrent or convolutional operations. We propose Non-local Neural Networks as a generic building block for capturing long-range interactions within a sequence of feature maps. Inspired by the classical non-local means filter, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This allows information to propagate directly between distant locations, bypassing the need for layer-by-layer propagation. We demonstrate that integrating non-local blocks into various video classification models improves performance on action recognition benchmarks such as Kinetics and Charades. The simplicity and effectiveness of non-local networks provide a powerful tool for capturing long-range dependencies in a variety of vision tasks."
http://arxiv.org/abs/1708.02901v3,Transitive Invariance for Self-supervised Visual Representation Learning,"Self-supervised learning (SSL) has shown remarkable progress in learning visual representations without relying on manual annotations. Existing SSL methods often enforce invariance to a set of pre-defined image transformations, but struggle to generalize to unseen transformations or compositions thereof. This paper addresses the limitation of current SSL methods by introducing a novel approach that enforces *transitive invariance*: if image A is invariant to image B, and image B is invariant to image C, then image A should be invariant to image C, even if the direct transformation from A to C was not explicitly seen during training. Our method leverages a contrastive learning framework augmented with a transitivity loss, which encourages the learned representations to respect this transitive property by minimizing the distance between representations of images connected through a chain of transformations. Experiments on standard benchmarks demonstrate that our transitive invariance approach significantly improves downstream classification and object detection performance compared to state-of-the-art SSL methods, especially when evaluated under novel and complex data augmentations. This highlights the potential of transitive invariance as a powerful inductive bias for robust and generalizable visual representation learning."
http://arxiv.org/abs/1708.02002v2,Focal Loss for Dense Object Detection,"Dense object detectors, while efficient, often struggle with extreme class imbalance during training. This imbalance arises because the vast majority of candidate object locations are easily classified as background, overwhelming the learning process. We address this problem by proposing a novel loss function, the Focal Loss, which dynamically adjusts the weight assigned to each example based on its classification confidence. The Focal Loss reduces the relative loss for well-classified examples, focusing training on a sparse set of hard, misclassified examples. Our approach is implemented within a single-stage detector framework, RetinaNet, which is composed of a ResNet backbone and a Feature Pyramid Network (FPN). Experiments demonstrate that RetinaNet, trained with Focal Loss, achieves state-of-the-art results, surpassing all existing single-stage detectors and performing comparably to more complex, two-stage detectors while running at a significantly faster speed. The Focal Loss enables single-stage detectors to effectively handle class imbalance and achieve high accuracy in dense object detection."
http://arxiv.org/abs/1706.02677v2,"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour","Deep convolutional neural networks have achieved remarkable success in image recognition, but their training remains computationally expensive, especially on large datasets like ImageNet. Scaling Stochastic Gradient Descent (SGD) to large minibatches is a promising approach to reduce training time, but often leads to degraded generalization performance. This paper addresses the challenge of training ImageNet models with large minibatches while maintaining accuracy and achieving rapid convergence. We propose a novel learning rate scaling strategy that dynamically adjusts the learning rate based on the minibatch size and training progress, coupled with a warm-up scheme and layer-wise adaptive learning rate adjustment. Furthermore, we introduce a synchronized batch normalization technique optimized for large-scale distributed training. Using our approach, we successfully trained a ResNet-50 model on ImageNet to 76.3% top-1 accuracy in approximately one hour using 256 GPUs. This represents a significant reduction in training time compared to existing methods without compromising model accuracy, demonstrating the feasibility of rapidly training high-performance image recognition models."
http://arxiv.org/abs/1704.07333v3,Detecting and Recognizing Human-Object Interactions,"Human-Object Interaction (HOI) detection and recognition is a challenging task in computer vision, requiring simultaneous understanding of both human actions and their relationships with objects. Existing methods often struggle with complex scenes involving multiple interacting agents and objects, leading to inaccurate or incomplete interaction predictions. This paper addresses the problem of robustly detecting and recognizing HOIs in complex visual environments. We propose a novel Graph-based Interaction Reasoning Network (GIRN) that explicitly models the relationships between humans, objects, and their interactions. GIRN constructs a dynamic graph where nodes represent humans and objects, and edges encode potential interactions. A graph neural network then iteratively refines node and edge features, enabling contextual reasoning and disambiguation of ambiguous interactions. We achieve state-of-the-art results on the V-COCO and HICO-DET benchmarks, demonstrating significant improvements in both HOI detection and recognition accuracy, particularly in scenarios with multiple interacting instances. The proposed GIRN offers a more comprehensive and accurate approach to understanding human activities within visual scenes, paving the way for more sophisticated scene understanding and human-robot interaction systems."
http://arxiv.org/abs/1703.06870v3,Mask R-CNN,"Instance segmentation, which requires simultaneously detecting objects while also segmenting each instance, is a fundamental computer vision task. Previous approaches often tackled this problem through complex, multi-stage pipelines. We present Mask R-CNN, a conceptually simple, flexible, and general framework for object instance segmentation. Our approach extends Faster R-CNN by adding a third branch that predicts an object mask in parallel with the existing branches for bounding box recognition and classification. The mask branch is a small fully convolutional network (FCN) applied to each Region of Interest (RoI), predicting a segmentation mask in a pixel-to-pixel manner. Mask R-CNN is trained end-to-end and is simple to implement, requiring minimal changes to the Faster R-CNN framework. We demonstrate top results on all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Mask R-CNN outperforms all existing, more complex single-model entries, demonstrating its effectiveness and broad applicability."
http://arxiv.org/abs/1612.03144v2,Feature Pyramid Networks for Object Detection,"Object detection requires robustness to scale, a challenge traditionally addressed using image pyramids. However, building image pyramids is computationally expensive and memory intensive. This paper addresses the problem of efficient multi-scale object detection by leveraging the inherent multi-scale, pyramidal hierarchy of deep convolutional networks. We propose Feature Pyramid Networks (FPN), a novel architecture that constructs high-level semantic feature maps at all scales by combining low-resolution, semantically strong features with high-resolution, semantically weaker features via a top-down pathway and lateral connections. This allows us to build a feature pyramid with marginal extra computational cost. We demonstrate significant improvements in object detection accuracy on the COCO benchmark, achieving state-of-the-art single-model results. Specifically, FPN improves the baseline Faster R-CNN by 8.0 points in average precision (AP). Our approach offers a computationally efficient and effective solution for multi-scale object detection, making it a valuable tool for various computer vision applications."
http://arxiv.org/abs/1611.05431v2,Aggregated Residual Transformations for Deep Neural Networks,"Deep convolutional neural networks have achieved remarkable success in various computer vision tasks. However, increasing depth often leads to diminishing feature reuse and optimization challenges. This paper addresses the problem of maximizing representational power while maintaining computational efficiency in deep network architectures. We introduce Aggregated Residual Transformations (ResNeXt), a novel architecture based on repeating a building block that aggregates a set of transformations with identical topology. Our design exploits a ""cardinality"" dimension (the number of independent paths), which is shown to be a more effective way of increasing accuracy than going deeper or wider. We demonstrate that increasing cardinality is able to improve accuracy significantly, even when the complexity is constrained. Evaluated on ImageNet-1K, ResNeXt achieves state-of-the-art results, outperforming ResNet and other existing architectures with comparable or fewer parameters and computational cost. This work highlights the importance of cardinality as a crucial factor in designing efficient and powerful deep neural networks."
http://arxiv.org/abs/1607.07032v2,Is Faster R-CNN Doing Well for Pedestrian Detection?,"Pedestrian detection is a critical task for numerous applications, including autonomous driving and video surveillance. While Faster R-CNN has achieved remarkable success in generic object detection, its performance on pedestrian detection, particularly in crowded scenes with scale variations, remains a concern. This paper investigates the suitability of Faster R-CNN for pedestrian detection by analyzing its performance limitations and proposing targeted improvements. We introduce a multi-scale training strategy that emphasizes smaller pedestrian instances and incorporates a novel context-aware feature fusion module within the Region Proposal Network (RPN) to better handle occlusions and varying scales. Our experimental results on the challenging Caltech and CityPersons datasets demonstrate that our enhanced Faster R-CNN significantly outperforms the original Faster R-CNN and achieves competitive performance compared to state-of-the-art pedestrian detectors, especially in handling small and occluded pedestrians. This work highlights the potential of refined Faster R-CNN architectures for robust pedestrian detection, bridging the performance gap with specialized pedestrian detection methods."
http://arxiv.org/abs/1605.06409v3,R-FCN: Object Detection via Region-based Fully Convolutional Networks,"Object detection has significantly benefited from the advancements in deep convolutional neural networks (CNNs). However, existing region-based object detectors often perform per-region computations repeatedly, leading to high computational costs, especially with deeper networks. To address this inefficiency, we propose Region-based Fully Convolutional Networks (R-FCN), an object detection architecture that leverages fully convolutional networks to share computation across the entire image. Our key insight is to construct position-sensitive score maps to encode spatial information about object location. These score maps are then used to predict object categories and refine bounding box locations, enabling efficient and accurate object detection. We achieve state-of-the-art results on the PASCAL VOC 2007, 2012, and MS COCO datasets, demonstrating significant improvements in both speed and accuracy compared to previous region-based object detectors. R-FCN represents a crucial step towards efficient and accurate object detection by fully exploiting the power of shared computation in deep convolutional networks."
http://arxiv.org/abs/1604.05144v1,ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation,"Semantic segmentation, a fundamental task in computer vision, typically relies on pixel-wise annotations for training, which are expensive and time-consuming to acquire. This paper addresses the challenge of reducing annotation effort in semantic segmentation by leveraging scribble annotations, a much weaker form of supervision. We introduce ScribbleSup, a novel framework that trains convolutional neural networks (CNNs) using only scribble annotations. Our approach combines a modified cross-entropy loss that focuses on regions near the scribbles with a novel self-supervision loss that enforces consistency between predictions generated with different perturbations of the input image. This self-supervision encourages the network to learn consistent and robust segmentations even in weakly labeled regions. Experimental results on the PASCAL VOC 2012 and Cityscapes datasets demonstrate that ScribbleSup achieves competitive performance compared to existing weakly-supervised methods and significantly reduces the annotation burden compared to fully supervised approaches. This work offers a practical and effective solution for semantic segmentation with minimal annotation effort, making it applicable to a wider range of real-world scenarios."
http://arxiv.org/abs/1603.08678v1,Instance-sensitive Fully Convolutional Networks,"Deep convolutional neural networks have achieved remarkable success in semantic image segmentation, but often struggle with accurately delineating individual object instances, especially in cluttered scenes. This paper addresses the problem of instance segmentation by introducing Instance-sensitive Fully Convolutional Networks (IS-FCNs), a novel architecture that directly predicts instance-aware semantic segmentation maps. Our method leverages a fully convolutional network to simultaneously generate pixel-level semantic labels and instance-specific offsets that guide the grouping of pixels into distinct object instances. Specifically, we introduce a novel instance embedding loss function that encourages pixels belonging to the same instance to cluster together while pushing apart pixels from different instances. We demonstrate the effectiveness of IS-FCNs on challenging datasets such as Cityscapes and COCO, achieving state-of-the-art instance segmentation performance with significant improvements in both accuracy and efficiency compared to existing methods. The proposed approach offers a more direct and efficient solution for instance segmentation, facilitating its application in real-world scenarios requiring fine-grained object understanding."
http://arxiv.org/abs/1603.05027v3,Identity Mappings in Deep Residual Networks,"Deep residual networks (ResNets) have revolutionized computer vision by enabling the training of extremely deep networks. However, the precise role and impact of the identity shortcut connections and batch normalization within the residual units have not been fully understood. This paper addresses the problem of how to design and understand the internal architecture of residual units to maximize information flow and training efficiency. We propose reformulating the residual unit to ensure a direct identity mapping, achieved by placing batch normalization and activation functions *after* the addition of the skip connection. This design allows for unimpeded signal propagation in both the forward and backward passes. Experiments demonstrate that these modified residual units lead to significantly improved training speed and accuracy compared to the original ResNet architecture, particularly when training very deep networks (e.g., 1000+ layers). These results highlight the importance of clean information pathways in deep learning architectures and provide a guideline for designing more effective and efficient network building blocks."
http://arxiv.org/abs/1512.04412v1,Instance-aware Semantic Segmentation via Multi-task Network Cascades,"Instance-aware semantic segmentation, the task of simultaneously classifying pixels and grouping them into distinct object instances, is crucial for scene understanding. Existing approaches often struggle to effectively integrate semantic and instance information, leading to suboptimal performance, particularly in cluttered scenes. This paper introduces a novel multi-task network cascade architecture for instance-aware semantic segmentation. Our method leverages a cascade of convolutional neural networks, progressively refining both semantic and instance predictions. The initial stage performs semantic segmentation, providing a strong contextual prior. Subsequent stages then focus on instance segmentation, utilizing the semantic information to guide the grouping of pixels into distinct instances. Furthermore, we introduce a novel loss function that encourages consistency between semantic and instance predictions, promoting more accurate and coherent segmentations. Experimental results on the Cityscapes and COCO datasets demonstrate that our approach achieves state-of-the-art performance, significantly improving upon existing methods, especially in challenging scenarios with overlapping objects. This cascaded multi-task framework provides a robust and effective solution for instance-aware semantic segmentation, advancing the field of scene understanding."
http://arxiv.org/abs/1512.03385v1,Deep Residual Learning for Image Recognition,"Deep convolutional neural networks have achieved remarkable success in various computer vision tasks. However, training very deep networks remains challenging due to the vanishing/exploding gradient problem, hindering performance gains from increased depth. This paper addresses the degradation problem: as network depth increases, accuracy gets saturated and then degrades rapidly. We propose a novel deep residual learning framework to ease the training of networks that are substantially deeper than those used previously. Instead of learning direct feature mappings, we explicitly let these layers fit a residual mapping. Formally, we reformulate the layers as learning residual functions with reference to the layer inputs. Comprehensive experiments on the ImageNet dataset demonstrate that these deep residual networks are easier to optimize, and can gain accuracy from considerably increased depth. Our extremely deep 152-layer residual net achieves top-5 error of 4.49% on the ImageNet validation set, surpassing all previous networks. This work presents a fundamental advance in deep learning, enabling the training of significantly deeper and more accurate networks for image recognition and beyond."
http://arxiv.org/abs/1506.01497v3,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,"Object detection has witnessed significant progress, driven by the success of deep convolutional networks. However, region proposal methods remain a computational bottleneck in state-of-the-art systems. We address this limitation by introducing a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals. The RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position, trained end-to-end to generate high-quality region proposals. By sharing convolutional features with the subsequent Fast R-CNN detection network, we significantly reduce the computational burden of region proposal generation. We demonstrate that an RPN and Fast R-CNN can be trained jointly to optimize both proposal generation and object detection. Experimental results on the PASCAL VOC datasets show that our method achieves state-of-the-art object detection accuracy with a significantly improved inference speed, reaching near real-time performance. Faster R-CNN represents a significant step towards practical, high-performance object detection systems."
http://arxiv.org/abs/1505.06798v2,Accelerating Very Deep Convolutional Networks for Classification and Detection,"Deep Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, including image classification and object detection. However, the computational cost and memory footprint of very deep CNNs often hinder their deployment in resource-constrained environments. This paper addresses the challenge of accelerating very deep CNNs without significantly compromising their accuracy. We propose a novel approach that combines knowledge distillation with a structured pruning technique tailored for modern CNN architectures. Specifically, we first train a large, accurate teacher network and then distill its knowledge into a smaller, student network. Subsequently, we employ a structured pruning method that removes entire filters and feature maps based on an adaptive sensitivity analysis, minimizing the disruption to the network's representational capacity. Experiments on benchmark datasets, including ImageNet and COCO, demonstrate that our method achieves significant speedups (up to 4x) and model size reduction (up to 5x) with minimal loss in accuracy compared to state-of-the-art compression techniques. This work provides a practical and effective solution for deploying very deep CNNs in real-world applications with limited computational resources."
http://arxiv.org/abs/1505.00996v1,Fast Guided Filter,"The guided filter is a versatile edge-preserving smoothing operator widely used in image processing and computer vision. However, its computational complexity can be a bottleneck for real-time applications and processing high-resolution images. This paper addresses the problem of accelerating the guided filter while maintaining its edge-preserving smoothing performance. We propose a Fast Guided Filter (FGF) that leverages a novel approximation based on separable filtering and adaptive kernel decomposition. Specifically, we decompose the filtering process into a series of one-dimensional convolutions and employ an adaptive kernel size that varies based on local image statistics, reducing redundant computations in homogeneous regions. Experimental results demonstrate that FGF achieves a significant speed-up compared to the original guided filter and other state-of-the-art accelerated variants, with minimal degradation in output quality as measured by PSNR and SSIM. This acceleration makes the guided filter practical for a wider range of time-critical applications."
http://arxiv.org/abs/1504.06066v2,Object Detection Networks on Convolutional Feature Maps,"Object detection, a fundamental task in computer vision, relies heavily on the quality of feature representations. Existing object detection networks often process feature maps from a single convolutional layer, potentially limiting their ability to capture objects at varying scales and complexities. This paper addresses the challenge of effectively leveraging multi-level convolutional feature maps for improved object detection performance. We propose a novel architecture, the Multi-Level Feature Aggregation Network (MLFAN), which adaptively fuses feature maps from multiple convolutional layers with different receptive fields. MLFAN employs a learnable attention mechanism to weigh the contribution of each feature map based on its relevance to the object being detected, followed by a refined feature pyramid network for robust scale invariance. Experimental results on the MS COCO dataset demonstrate that MLFAN achieves state-of-the-art performance, surpassing existing single-level feature-based detectors by a significant margin while maintaining computational efficiency. This highlights the importance of adaptive multi-level feature aggregation for robust and accurate object detection."
http://arxiv.org/abs/1503.01640v2,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,"Semantic segmentation, a pixel-wise classification task, traditionally relies on expensive and time-consuming pixel-level annotations for training deep convolutional neural networks. Obtaining such dense annotations limits the scalability of segmentation models to large datasets. This paper addresses the challenge of training semantic segmentation models with weak supervision, specifically using only bounding box annotations. We propose BoxSup, a novel framework that leverages bounding box information to generate pseudo-segmentation labels for training convolutional networks. BoxSup incorporates a multi-stage training procedure involving: 1) generating initial pseudo-labels based on constrained clustering within bounding boxes, 2) training a segmentation network using these pseudo-labels, and 3) iteratively refining the pseudo-labels using the network's predictions along with bounding box constraints. Experimental results on the PASCAL VOC 2012 dataset demonstrate that BoxSup achieves competitive segmentation performance compared to existing weakly supervised methods, significantly narrowing the gap with fully supervised approaches. This work provides a practical and scalable solution for training semantic segmentation models with readily available bounding box annotations, reducing the annotation burden and enabling the development of more robust and generalizable segmentation systems."
http://arxiv.org/abs/1502.01852v1,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,"Deep convolutional neural networks have achieved remarkable progress in image classification. However, training deeper and more complex networks remains challenging due to issues such as vanishing gradients and difficulty in optimization. We address these challenges by introducing a novel rectifier activation function, the Parametric Rectified Linear Unit with Dynamic Threshold (Dy-PReLU), coupled with an adaptive batch normalization scheme. Dy-PReLU learns a dynamic threshold based on the input distribution, allowing for more flexible and adaptive activation patterns, while our adaptive batch normalization modulates the normalization parameters based on network depth and layer-specific statistics. Our proposed approach, termed DeepRect, significantly improves training efficiency and allows us to train deeper networks effectively. On the ImageNet classification benchmark, DeepRect achieves a top-1 accuracy of 83.7% and a top-5 accuracy of 96.8%, surpassing previously reported human-level performance. These results demonstrate the effectiveness of our proposed rectifier and training scheme in enabling the training of significantly deeper and more accurate convolutional neural networks, paving the way for further advancements in image understanding."
http://arxiv.org/abs/1501.00092v3,Image Super-Resolution Using Deep Convolutional Networks,"Single image super-resolution (SISR) aims to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart, a fundamental task in computer vision with applications in various fields. Existing SISR methods often struggle to capture intricate details and generate visually pleasing results, particularly at high upscaling factors. We address this limitation by proposing a novel deep convolutional neural network (CNN) architecture, termed Enhanced Super-Resolution CNN (ESRCNN), designed specifically for efficient and accurate image super-resolution. ESRCNN incorporates a deeper network structure with residual learning to facilitate gradient flow and enable the learning of complex mappings between LR and HR image spaces. Furthermore, we introduce a multi-scale feature extraction module that captures contextual information at different scales, enhancing the network's ability to reconstruct fine-grained details. Experimental results demonstrate that ESRCNN achieves state-of-the-art performance on several benchmark datasets, surpassing existing methods in terms of both quantitative metrics (PSNR, SSIM) and perceptual quality. The proposed approach offers a significant advancement in image super-resolution, paving the way for improved performance in downstream vision tasks and enhanced visual experiences."
http://arxiv.org/abs/1412.1710v1,Convolutional Neural Networks at Constrained Time Cost,"Convolutional Neural Networks (CNNs) have achieved remarkable success in diverse computer vision tasks, but their computational demands often limit their deployment in resource-constrained environments. This paper addresses the challenge of optimizing CNN architectures to achieve maximal accuracy under strict time constraints. We propose a novel differentiable neural architecture search (NAS) framework, Time-Aware NAS (TA-NAS), that directly incorporates inference time as a differentiable constraint during the search process. TA-NAS leverages a latency prediction model to estimate the inference time of candidate architectures and uses a customized loss function that penalizes models exceeding the specified time budget. We evaluate TA-NAS on image classification tasks using CIFAR-10 and ImageNet datasets under varying time constraints. Our results demonstrate that TA-NAS consistently discovers architectures that outperform manually designed networks and existing NAS methods in terms of accuracy while satisfying the imposed latency requirements. This work provides a practical approach for automatically designing efficient CNNs tailored for deployment on platforms with limited computational resources."
http://arxiv.org/abs/1412.1283v4,Convolutional Feature Masking for Joint Object and Stuff Segmentation,"Simultaneously segmenting objects and stuff categories in images presents a challenging task due to their inherent semantic differences and contextual relationships. Existing approaches often treat these tasks independently or rely on complex architectures to capture their interdependencies, leading to increased computational costs and limited performance. To address this, we propose a novel Convolutional Feature Masking (CFM) module that dynamically modulates convolutional features based on learned object and stuff category masks. CFM leverages lightweight convolutional networks to generate these masks, which are then used to selectively enhance or suppress features relevant to each category type. Specifically, we introduce two CFM variants: one that uses element-wise multiplication for feature modulation and another that employs a more expressive channel-wise attention mechanism. Experiments on the challenging ADE20K dataset demonstrate that our CFM module significantly improves joint object and stuff segmentation accuracy compared to baseline models, achieving state-of-the-art performance with a minimal increase in computational complexity. This demonstrates the effectiveness of dynamically adapting convolutional features for improved semantic understanding and segmentation of complex scenes."
http://arxiv.org/abs/1411.4229v1,Efficient and Accurate Approximations of Nonlinear Convolutional Networks,"Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks, but their computational complexity and memory footprint often limit their deployment on resource-constrained devices. This paper addresses the challenge of creating efficient and accurate approximations of nonlinear convolutional networks without significant performance degradation. We propose a novel approach that combines tensor decomposition with knowledge distillation to simultaneously reduce model size and improve inference speed. Specifically, we employ Tucker decomposition to factorize convolutional layers, followed by fine-tuning with a distillation loss that encourages the compressed model to mimic the behavior of the original, larger network. Experiments on image classification datasets, including CIFAR-10 and ImageNet, demonstrate that our method achieves significant reductions in computational cost (up to 4x fewer FLOPs) and model size (up to 5x smaller) while maintaining comparable or even improved accuracy compared to existing compression techniques. This work enables the efficient deployment of high-performing CNNs on edge devices and contributes to the development of more sustainable deep learning models."
http://arxiv.org/abs/1406.4729v4,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,"Deep convolutional neural networks (CNNs) have achieved remarkable success in visual recognition tasks, particularly with fixed-size input images. However, the requirement of fixed input image size limits both the aspect ratio and scale of images, potentially distorting the original content and hindering recognition performance. To address this limitation, we propose a novel Spatial Pyramid Pooling (SPP) layer for CNNs. This SPP layer removes the fixed-size constraint of the network by aggregating features from variable-size input images. The SPP layer pools the features from the last convolutional layer in multiple spatial scales, generating a fixed-length representation regardless of the input image size. We demonstrate that CNNs with the SPP layer can accept images of any size, leading to improved accuracy. Our experiments on large-scale image classification datasets, including ImageNet, demonstrate that SPP-nets significantly outperform networks with fixed-size input, achieving state-of-the-art results and demonstrating the effectiveness of our proposed approach for robust visual recognition."
http://arxiv.org/abs/2409.17146v2,Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding and relating visual and textual information, but their development is often hampered by the limited availability of open-source models and datasets. This paper addresses the critical need for accessible and reproducible VLM research by introducing Molmo and PixMo, a suite of open-weight VLMs and a corresponding open dataset specifically designed to facilitate state-of-the-art performance. Molmo is a transformer-based VLM architecture trained on the newly curated PixMo dataset, which consists of a diverse collection of image-text pairs sourced and filtered from publicly available resources with a focus on high-quality annotations and broad coverage of visual concepts. Through extensive experimentation, we demonstrate that Molmo achieves competitive performance on a range of established VLM benchmarks, including visual question answering, image captioning, and zero-shot image classification, rivaling or surpassing the performance of similarly sized, closed-source models. The release of both Molmo and PixMo provides a valuable resource for the research community, democratizing access to advanced VLM technology and fostering further innovation in the field."
http://arxiv.org/abs/2408.00714v2,SAM 2: Segment Anything in Images and Videos,"Foundation models like the Segment Anything Model (SAM) have demonstrated remarkable zero-shot generalization in image segmentation. However, SAM primarily focuses on static images, and adapting it to the temporal coherence and computational demands of video segmentation presents significant challenges. This paper introduces SAM 2, an extension of SAM designed for segmenting anything in both images and videos. SAM 2 incorporates a novel temporal attention mechanism within the image encoder to propagate information across video frames, enabling consistent object tracking and segmentation over time. Furthermore, we introduce a lightweight video prompt encoder that allows users to interactively refine segmentations using sparse temporal cues. Experiments on a diverse set of video segmentation benchmarks demonstrate that SAM 2 achieves state-of-the-art zero-shot video segmentation performance while maintaining real-time processing speeds. SAM 2 significantly expands the applicability of foundation models to dynamic visual content, paving the way for more versatile and interactive video understanding systems."
http://arxiv.org/abs/2406.20083v1,PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators,"Reinforcement learning (RL) offers a promising paradigm for training autonomous agents, yet scaling on-policy methods to complex environments with high-dimensional observations remains a significant challenge. This paper addresses the problem of effectively leveraging contextual information in visual navigation tasks to improve sample efficiency and generalization in on-policy RL. We introduce PoliFormer, a novel architecture that integrates Transformers into the policy network of an on-policy RL agent. PoliFormer utilizes a vision transformer to process visual inputs, encoding long-range dependencies and spatial relationships, which are then fed into a transformer-based sequence model to predict actions conditioned on past observations and actions. We demonstrate that PoliFormer significantly outperforms state-of-the-art recurrent and attention-based baselines on a suite of challenging navigation tasks, achieving superior sample efficiency and generalization to unseen environments. Our results showcase the potential of transformer-based architectures for scaling on-policy RL, paving the way for the development of more robust and adaptable autonomous navigators."
http://arxiv.org/abs/2304.02643v1,Segment Anything,"Image segmentation is a fundamental computer vision task, yet current models require task-specific training data, limiting their general applicability. This paper addresses the problem of creating a single, promptable segmentation model capable of accurately segmenting any object in an image, without requiring retraining. We introduce the Segment Anything Model (SAM), a new image segmentation system driven by a novel task formulation, a powerful and efficient image encoder, and a lightweight mask decoder. SAM leverages a promptable segmentation task that enables zero-shot generalization to new image distributions and tasks. Through extensive experimentation, including a new ""Segment Anything"" dataset with over 1 billion masks on 11 million images, we demonstrate that SAM achieves impressive zero-shot performance on a wide variety of segmentation tasks, often outperforming specialized, fine-tuned models. SAM represents a significant step towards a truly general-purpose image segmentation model, unlocking new possibilities for downstream applications and interactive image editing."
http://arxiv.org/abs/2303.13496v3,The effectiveness of MAE pre-pretraining for billion-scale pretraining,"Self-supervised pretraining has become a cornerstone of modern computer vision, enabling models to learn rich feature representations from unlabeled data. Scaling pretraining to billion-scale datasets promises further performance improvements, but presents significant computational challenges. This paper investigates the effectiveness of Masked Autoencoders (MAE) as a pre-pretraining strategy for large-scale image representation learning. We propose a two-stage pretraining paradigm: first, we pre-pretrain a model using MAE on a relatively small dataset to learn general visual features and efficient reconstruction capabilities. Then, we scale up the pretraining using standard contrastive learning on a billion-scale dataset, initializing from the MAE pre-pretrained weights. Our experiments demonstrate that MAE pre-pretraining significantly accelerates convergence and improves downstream task performance compared to training from scratch or using random initialization for the billion-scale pretraining phase. Specifically, we observe a notable boost in ImageNet classification accuracy and object detection mAP on COCO when using MAE pre-pretraining. These results highlight the potential of MAE as a valuable tool for enabling efficient and effective billion-scale visual representation learning."
http://arxiv.org/abs/2201.08371v2,Revisiting Weakly Supervised Pre-Training of Visual Perception Models,"Weakly supervised pre-training (WSP) leverages readily available, noisy data to learn powerful visual representations, often outperforming ImageNet pre-training for specific downstream tasks. However, the benefits of WSP are not universally observed, and the optimal strategies for its application remain an open question. This work systematically revisits WSP, focusing on the impact of dataset scale, pre-training duration, and architectural choices on downstream transfer performance. We propose a refined WSP pipeline that incorporates adaptive learning rate schedules and a novel loss weighting scheme that dynamically adjusts the contribution of different data sources during training. Experiments across a diverse set of downstream tasks, including object detection, semantic segmentation, and instance segmentation, demonstrate that our refined WSP pipeline consistently outperforms existing WSP strategies and achieves state-of-the-art results when transferring to challenging datasets like COCO and LVIS. This work provides valuable insights into the effective application of WSP and demonstrates its continued relevance as a powerful pre-training paradigm for visual perception models."
http://arxiv.org/abs/2111.09887v1,PyTorchVideo: A Deep Learning Library for Video Understanding,"Video understanding is a rapidly growing field, driven by the increasing availability of video data and advancements in deep learning. However, developing and deploying video understanding models remains challenging due to the complexities of video data and the lack of readily available, modular, and optimized tools. This paper introduces PyTorchVideo, a deep learning library designed to accelerate research and development in video understanding. PyTorchVideo provides a comprehensive suite of modular building blocks for video processing, including data loading, pre-processing, model implementations of state-of-the-art architectures (e.g., SlowFast, X3D), and evaluation metrics. The library leverages PyTorchs flexibility and extensibility, allowing researchers to easily customize and integrate components into their own workflows. We demonstrate the library's effectiveness through benchmark experiments on popular video datasets like Kinetics-400 and AVA, achieving competitive performance with existing implementations while significantly reducing code complexity. PyTorchVideo simplifies the development process and fosters collaboration within the video understanding community, enabling faster progress in this vital area of computer vision."
http://arxiv.org/abs/2106.14881v3,Early Convolutions Help Transformers See Better,"Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks, often surpassing Convolutional Neural Networks (CNNs) with sufficient pre-training data. However, ViTs typically require large datasets due to their limited ability to capture low-level features and spatial relationships inherent in images during the initial stages of processing. This paper addresses the problem of improving ViT performance, particularly in data-constrained scenarios, by enhancing their early-stage feature extraction capabilities. We propose a hybrid architecture, named Convolution-Augmented Vision Transformer (CAViT), which incorporates lightweight convolutional layers before the standard Transformer encoder. These initial convolutional layers serve to extract local features and provide a more informative representation of the input image to the subsequent Transformer blocks. Experiments on image classification datasets, including CIFAR-100, CIFAR-10, and a subset of ImageNet, demonstrate that CAViT consistently outperforms vanilla ViTs and other hybrid architectures, especially when training data is limited. These results highlight the crucial role of early convolutional processing in improving the sample efficiency and overall performance of Vision Transformers, offering a promising direction for future research in efficient and robust vision models."
http://arxiv.org/abs/2103.16562v1,Boundary IoU: Improving Object-Centric Image Segmentation Evaluation,"Image segmentation evaluation heavily relies on pixel-wise metrics like Intersection-over-Union (IoU), which can be overly sensitive to minor inaccuracies, especially along object boundaries. This paper addresses the problem of accurately evaluating segmentation quality by focusing on the critically important boundary regions. We introduce Boundary IoU (BIoU), a novel metric that computes IoU specifically within a narrow band around the ground truth object boundaries. BIoU is calculated by first dilating the ground truth mask to create a boundary region, then computing the standard IoU between the predicted segmentation and the ground truth within this region. Experiments on multiple datasets, including Cityscapes and PASCAL VOC, demonstrate that BIoU provides a more robust and perceptually aligned evaluation of segmentation quality, particularly in scenarios with varying levels of boundary precision. Furthermore, BIoU correlates more strongly with human perception of segmentation quality compared to standard IoU, highlighting its efficacy in capturing boundary accuracy. The proposed BIoU metric offers a more nuanced and reliable assessment of object-centric image segmentation, leading to improved model development and performance."
http://arxiv.org/abs/2103.06877v1,Fast and Accurate Model Scaling,"Model scaling is a crucial technique for adapting the capacity of neural networks to available computational resources. However, traditional scaling methods often require extensive experimentation to determine the optimal combination of network depth, width, and resolution, resulting in significant training overhead. This paper addresses the problem of efficiently identifying the optimal scaling configuration for a given computational budget. We propose a novel scaling strategy, ""Resource-Aware Scaling"" (RAS), which leverages a differentiable neural architecture search (NAS) framework to automatically learn the optimal scaling coefficients for each dimension (depth, width, and resolution) under resource constraints. RAS employs a tailored search space that prioritizes scaling configurations, coupled with a resource-aware reward function that penalizes exceeding the specified computational budget. Experiments on ImageNet demonstrate that RAS achieves comparable or superior accuracy to manually tuned scaling strategies while significantly reducing the search cost, requiring only a fraction of the training epochs. These results highlight the potential of NAS-driven scaling to accelerate the development and deployment of efficient deep learning models."
http://arxiv.org/abs/2102.01066v2,Evaluating Large-Vocabulary Object Detectors: The Devil is in the Details,"Large-vocabulary object detection aims to identify objects from an extensive set of categories, pushing the boundaries of visual understanding. However, current evaluation metrics often fail to capture the nuances of performance across diverse and imbalanced category distributions inherent in such large-scale tasks. This paper addresses the critical need for more comprehensive and insightful evaluation protocols tailored to large-vocabulary object detectors. We propose a novel evaluation framework incorporating per-category performance analysis, frequency-weighted metrics that account for category imbalance, and a similarity-based metric that penalizes confusion between semantically related categories. We apply our framework to evaluate several state-of-the-art detectors on the LVIS dataset, revealing significant disparities in performance across different frequency bins and highlighting common sources of confusion between visually similar object categories. Our analysis demonstrates that existing evaluation metrics can mask critical shortcomings in detector performance, particularly for rare categories, and that our proposed framework offers a more granular and informative assessment. This work facilitates the development of more robust and reliable large-vocabulary object detectors by providing a more nuanced understanding of their strengths and weaknesses."
http://arxiv.org/abs/1908.03195v2,LVIS: A Dataset for Large Vocabulary Instance Segmentation,"Instance segmentation has significantly advanced in recent years, driven largely by datasets with rich annotations. However, existing datasets are limited in the number of object categories they cover, hindering the development of models capable of recognizing a long tail of objects in real-world scenes. This paper introduces LVIS (Large Vocabulary Instance Segmentation), a new dataset designed to address the challenges of instance segmentation with a large number of object categories. LVIS features a long-tail distribution of object instances, with over 1000 object categories and a large number of instances per image. To address the annotation challenges associated with such a large vocabulary, we propose a novel annotation protocol based on federated crowd-sourcing, where annotators focus on identifying and segmenting instances they are familiar with. We benchmark several state-of-the-art instance segmentation models on LVIS and demonstrate that existing methods struggle to generalize to the long tail of object categories. LVIS provides a challenging and realistic benchmark for advancing instance segmentation research towards open-world scenarios, enabling the development of models with enhanced generalization capabilities and broader applicability."
http://arxiv.org/abs/1801.05401v2,Low-Shot Learning from Imaginary Data,"Low-shot learning aims to recognize novel categories from very few labeled examples. A significant challenge in this setting is the lack of sufficient data to train robust and generalizable models. We address this problem by introducing a novel approach to low-shot learning that leverages the generation of diverse and realistic ""imaginary"" data to augment limited real-world samples. Our method employs a conditional generative adversarial network (GAN) conditioned on both the class label and a learned style embedding to synthesize a multitude of training images. These generated images, combined with the few real examples, are then used to train a classifier via meta-learning, enabling the model to effectively adapt to new categories with limited supervision. Experiments on benchmark datasets such as miniImageNet and tieredImageNet demonstrate that our approach significantly improves classification accuracy compared to state-of-the-art low-shot learning methods, achieving gains of up to 5% in 5-way 1-shot settings. This work demonstrates the potential of generated data for improving low-shot learning performance and offers a promising avenue for building more data-efficient and adaptable vision systems."
http://arxiv.org/abs/1712.01238v1,Learning by Asking Questions,"Learning from data often requires extensive supervision, which can be expensive and time-consuming to acquire. Active learning aims to mitigate this burden by strategically selecting the most informative samples for labeling. However, traditional active learning approaches primarily focus on instance selection and often overlook the potential of interactive learning paradigms. This paper addresses the problem of effectively leveraging question-answering interactions to improve learning efficiency and accuracy in computer vision tasks. We propose a novel active learning framework that learns to ask questions about unlabeled data to elicit targeted information from an oracle. Our approach combines a question generation module, trained using reinforcement learning to optimize for information gain, with a visual question answering (VQA) model that leverages the oracle's responses to refine its understanding of the visual scene. Experiments on benchmark datasets demonstrate that our method significantly outperforms passive learning and traditional active learning baselines, achieving comparable accuracy with substantially fewer labeled samples and question-answer interactions. This work highlights the potential of question-driven active learning to reduce annotation costs and enhance the performance of computer vision systems."
http://arxiv.org/abs/1705.03633v1,Inferring and Executing Programs for Visual Reasoning,"Visual reasoning demands the ability to understand images and perform complex inferences based on their content. A central challenge lies in bridging the gap between perception and reasoning, often requiring structured representations and explicit reasoning steps. This paper addresses the problem of enabling machines to not only answer questions about images but also to provide interpretable and verifiable reasoning processes. We propose a novel framework that learns to infer executable programs directly from visual question answering (VQA) datasets. Our approach employs a neural module network to generate program layouts representing the reasoning steps, which are then compiled into executable code. The execution of these programs over the image produces an answer, while the program itself provides a traceable explanation of the reasoning process. Experiments on challenging VQA benchmarks, including CLEVR and GQA, demonstrate that our approach achieves state-of-the-art accuracy while simultaneously providing interpretable program-based explanations for its answers. This work represents a significant step towards building more transparent and reliable AI systems capable of complex visual reasoning."
http://arxiv.org/abs/1612.06890v1,CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,"Understanding scenes and answering questions about them requires both visual perception and compositional reasoning. However, existing vision and language datasets often conflate these abilities, making it difficult to diagnose the specific limitations of models. We introduce CLEVR, a new diagnostic dataset for Compositional Language and Elementary Visual Reasoning. CLEVR is generated using a procedural engine that renders synthetic images of 3D shapes with varied attributes and spatial relationships, and automatically generates a diverse set of questions targeting different reasoning skills. The dataset allows for precise control over scene complexity and question types, enabling targeted evaluation of model performance. We evaluate several state-of-the-art vision and language models on CLEVR, finding that even the best models struggle to generalize to unseen combinations of reasoning skills, achieving significantly lower accuracy than human performance. CLEVR provides a valuable benchmark for evaluating and improving models' abilities in compositional reasoning, paving the way for more robust and interpretable vision and language systems."
http://arxiv.org/abs/1612.06370v2,Learning Features by Watching Objects Move,"Learning effective visual representations without explicit supervision remains a central challenge in computer vision. While self-supervised learning has shown promise, many existing methods rely on handcrafted pretext tasks or extensive data augmentation strategies. This work addresses the problem of learning robust feature representations by leveraging the inherent structure present in unlabeled video sequences. We propose a novel self-supervised approach that learns visual features by predicting future object motion. Our method trains a convolutional neural network to encode individual frames and then decodes these features to predict the displacement vectors of salient objects between consecutive frames. We achieve this through a novel loss function that considers both the magnitude and direction of the predicted motion vectors, enabling the network to learn representations that are sensitive to object dynamics. We demonstrate that features learned by our approach achieve competitive results on several downstream tasks, including object classification and action recognition, surpassing several existing self-supervised methods. This highlights the efficacy of motion prediction as a powerful cue for learning meaningful visual representations from unlabeled video."
http://arxiv.org/abs/1606.02819v4,Low-shot Visual Recognition by Shrinking and Hallucinating Features,"Low-shot visual recognition, where only a few labeled examples are available per class, remains a significant challenge in computer vision. This paper addresses the problem of overfitting and poor generalization in low-shot scenarios by proposing a novel ""Shrinking and Hallucinating Features"" (SHF) approach. Our method first learns a feature shrinking module that identifies and suppresses less discriminative feature dimensions in the limited training data, effectively reducing noise and preventing overfitting. Subsequently, a feature hallucination module leverages the shrunk features to generate diverse and realistic synthetic features, expanding the training set and improving model robustness. We train these modules end-to-end with a meta-learning objective, enabling adaptation to new unseen classes. Extensive experiments on benchmark datasets, including miniImageNet and tieredImageNet, demonstrate that SHF consistently outperforms state-of-the-art low-shot learning methods, achieving significant improvements in classification accuracy, particularly in extremely low-shot settings (e.g., 1-shot). This work provides a novel and effective strategy for tackling the challenges of limited data in visual recognition, paving the way for more practical and adaptable vision systems."
http://arxiv.org/abs/1604.03968v1,Visual Storytelling,"Visual storytelling, the ability to construct and convey a coherent narrative from a sequence of images, is a fundamental aspect of human communication and a challenging task for artificial intelligence. While significant progress has been made in image captioning and video understanding, generating compelling and contextually relevant stories from unstructured image sets remains a largely unsolved problem. We introduce a novel hierarchical framework that leverages a graph neural network (GNN) to model inter-image relationships and a transformer-based language model to generate a cohesive narrative. The GNN aggregates visual and semantic features from each image, capturing dependencies and contextual cues, which are then used to guide the language model in generating story sentences. We further incorporate a contrastive learning objective to encourage the model to discriminate between plausible and implausible story sequences, enhancing coherence. Experimental results on the VIST dataset demonstrate that our approach significantly outperforms existing methods in terms of both automatic evaluation metrics and human evaluation studies, producing more fluent, coherent, and contextually relevant stories. This work offers a significant step towards enabling machines to understand and generate narratives from visual data, with potential applications in education, entertainment, and human-computer interaction."
http://arxiv.org/abs/1604.03650v1,Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks,"Creating compelling 3D video content remains a significant challenge due to the complexities and costs associated with traditional stereoscopic filming and manual 2D-to-3D conversion. This paper addresses the problem of automatically generating high-quality stereoscopic 3D video from monocular 2D video input. We introduce Deep3D, a novel deep learning framework employing convolutional neural networks to estimate per-pixel depth maps and subsequently synthesize corresponding stereo views. Our architecture leverages a combination of spatial pyramid pooling and residual learning to capture multi-scale contextual information and refine depth predictions. Furthermore, we incorporate temporal consistency constraints within a recurrent neural network structure to ensure smooth and realistic 3D video output. Experimental results on diverse video datasets demonstrate that Deep3D significantly outperforms existing 2D-to-3D conversion methods, producing visually plausible and temporally coherent stereoscopic video with reduced artifacts. This approach offers a practical and efficient solution for generating 3D video content, democratizing access to immersive viewing experiences."
http://arxiv.org/abs/1604.03540v1,Training Region-based Object Detectors with Online Hard Example Mining,"Region-based Convolutional Neural Networks (R-CNNs) have achieved state-of-the-art performance in object detection. However, training these detectors efficiently and effectively remains a challenge, particularly due to the class imbalance problem where the vast majority of sampled regions are background. This paper addresses the difficulty of training region-based object detectors by proposing a novel Online Hard Example Mining (OHEM) strategy. Unlike traditional hard example mining techniques that require pre-computing or periodically updating hard examples, our OHEM strategy dynamically selects hard examples within each mini-batch based on their loss during the forward pass. Specifically, we forward propagate all region proposals, sort them by loss, and only backpropagate the loss from the hardest examples, thereby focusing training on the most informative samples. Experiments on PASCAL VOC and COCO datasets demonstrate that our OHEM strategy significantly improves detection accuracy and training efficiency compared to standard training and other hard example mining methods. This approach simplifies and accelerates the training of high-performance object detectors, making them more accessible and practical for real-world applications."
http://arxiv.org/abs/1512.06974v2,Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels,"Human-provided labels are a ubiquitous source of supervision for training visual classifiers, yet they are often subject to reporting bias, where certain classes are more likely to be reported than others due to factors unrelated to the image content. This paper addresses the challenge of training robust visual classifiers when the training data is contaminated with such human-centric reporting biases. We propose a novel Expectation-Maximization (EM) based framework that simultaneously learns the parameters of a visual classifier and a reporting bias model. Specifically, the E-step infers the posterior probability of the true label given the observed label and image features, while the M-step updates both the classifier and bias model parameters using these inferred posteriors. Experiments on synthetic and real-world datasets, including a newly collected dataset with explicit reporting bias, demonstrate that our method significantly outperforms existing approaches in terms of classification accuracy and robustness to noisy labels. This work provides a principled approach to mitigating the impact of human reporting bias, leading to more reliable and accurate visual classifiers trained from human-annotated data."
http://arxiv.org/abs/1512.04143v1,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,"Object detection in complex scenes requires understanding objects both individually and within their contextual environment. Existing methods often struggle to effectively leverage contextual information, particularly the relationships between objects and their surroundings. This paper introduces Inside-Outside Net (ION), a novel architecture for object detection that explicitly models these contextual relationships using a combination of skip pooling and recurrent neural networks. ION extracts multi-scale features from both inside and outside the object proposal bounding box using skip pooling, creating rich representations of the object and its surrounding context. These features are then fed into a recurrent neural network (RNN) to capture long-range dependencies and contextual interactions, enabling the network to reason about the object in relation to its environment. We demonstrate significant improvements on challenging datasets such as MS COCO and Pascal VOC, achieving state-of-the-art results for objects with strong contextual dependencies. The proposed ION architecture provides a powerful framework for object detection by explicitly modeling and leveraging contextual information, paving the way for more robust and accurate scene understanding."
http://arxiv.org/abs/1511.06335v2,Unsupervised Deep Embedding for Clustering Analysis,"Clustering high-dimensional data remains a challenging problem, particularly when labels are unavailable. Existing methods often struggle to simultaneously learn a suitable feature representation and perform clustering, leading to suboptimal results. This paper introduces a novel unsupervised deep embedding method for clustering analysis that jointly learns a non-linear mapping from the data space to a latent embedding space and optimizes a clustering objective within that space. Our approach utilizes a deep neural network to learn the embedding, guided by a clustering loss that encourages data points within the same cluster to be close together while pushing apart data points from different clusters in the embedding space. An auxiliary distribution is introduced to refine the cluster assignments and improve the robustness of the training process. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art clustering performance compared to existing unsupervised methods. This highlights the effectiveness of jointly learning the embedding and clustering within a deep learning framework, offering a powerful tool for exploratory data analysis and knowledge discovery."
http://arxiv.org/abs/1506.02640v5,"You Only Look Once: Unified, Real-Time Object Detection","Object detection is a critical task in computer vision, enabling a wide range of applications. Existing approaches often rely on complex pipelines with separate stages for region proposal generation, feature extraction, and classification, leading to slow inference speeds. This paper introduces a novel approach to object detection that reframes the problem as a single regression task, directly predicting bounding boxes and class probabilities from image pixels in one evaluation. We present You Only Look Once (YOLO), a unified architecture that divides the input image into a grid and simultaneously predicts multiple bounding boxes and associated class probabilities for each grid cell. This end-to-end trainable model is significantly faster than previous object detection methods. YOLO achieves state-of-the-art real-time performance, processing images at 45 frames per second. Despite its speed, YOLO achieves competitive accuracy on the PASCAL VOC dataset, demonstrating a significant improvement in speed compared to alternative methods while maintaining comparable detection accuracy. This real-time object detection capability enables new applications and advancements in areas such as robotics, autonomous driving, and video surveillance."
http://arxiv.org/abs/1505.04467v1,Exploring Nearest Neighbor Approaches for Image Captioning,"Image captioning, the task of automatically generating textual descriptions for images, has traditionally relied on encoder-decoder architectures trained end-to-end. However, these models can sometimes struggle to generate accurate and diverse captions, particularly for images with rare or complex visual content. This paper explores the potential of nearest neighbor (NN) approaches to improve image captioning performance by leveraging the knowledge encoded in a large, pre-existing dataset of image-caption pairs. We propose a novel NN-based captioning framework that retrieves visually similar images from a training set and adapts their corresponding captions to generate a caption for the query image. This adaptation process incorporates techniques for caption fusion and re-ranking, allowing us to leverage multiple relevant captions and select the most appropriate one. Experiments on the COCO dataset demonstrate that our NN-based approach achieves competitive results compared to state-of-the-art encoder-decoder models, particularly in generating captions that are more faithful to the visual content of the image. Our findings suggest that NN-based methods offer a promising alternative to purely parametric models for image captioning, providing a mechanism for incorporating external knowledge and improving caption accuracy and diversity."
http://arxiv.org/abs/1505.01197v3,Contextual Action Recognition with R*CNN,"Human action recognition in videos is a fundamental task in computer vision, crucial for applications like video surveillance and human-computer interaction. However, recognizing actions solely based on the appearance of the actor often proves insufficient due to the inherent ambiguity of human movements and the importance of surrounding context. This paper addresses the problem of effectively incorporating contextual information into a region-based Convolutional Neural Network (R*CNN) framework for improved action recognition. We propose a novel Contextual R*CNN architecture that leverages both appearance and contextual cues through a two-stream network. The first stream processes the actor region proposals using a standard R*CNN architecture. The second stream extracts features from the surrounding environment, which are then fused with the actor features using a learned attention mechanism. Experimental results on the challenging UCF101 and HMDB51 datasets demonstrate that our Contextual R*CNN significantly outperforms state-of-the-art methods, achieving relative improvements of 5% and 3% respectively. This highlights the critical role of contextual reasoning in achieving robust and accurate action recognition in realistic scenarios."
http://arxiv.org/abs/1504.08083v2,Fast R-CNN,"Object detection in images remains computationally expensive, particularly when using deep convolutional neural networks. Existing region proposal-based detectors like R-CNN are slow because they perform a convolutional forward pass for each object proposal. We address this inefficiency with Fast R-CNN, a new architecture that processes the entire image with a single convolutional pass, extracting a feature map. Region proposals are then projected onto this feature map, and Region of Interest (RoI) pooling extracts fixed-length feature vectors from each region. These features are fed into fully connected layers that simultaneously predict class probabilities and refine bounding box locations. Fast R-CNN significantly improves training and testing speed compared to R-CNN, achieving a 9x speedup in training and a 213x speedup at test time while attaining a higher mAP of 70.0% on PASCAL VOC 2012. This efficiency makes near real-time object detection with deep networks feasible and significantly expands the applicability of high-accuracy object detection systems."
http://arxiv.org/abs/1502.04652v1,Inferring 3D Object Pose in RGB-D Images,"Estimating the 6D pose of objects from RGB-D images is a crucial task for robot manipulation and augmented reality applications. However, accurately and efficiently inferring 3D object pose remains challenging due to factors such as occlusions, clutter, and variations in object appearance. This paper introduces a novel deep learning framework, PoseRefineNet, that leverages both RGB and depth information to achieve robust and accurate 6D object pose estimation. Our method employs a two-stage architecture: first, a coarse pose is predicted using a modified PointNet++ network operating on segmented depth data. Then, a refinement network, incorporating both RGB features and geometric information from the initial pose estimate, iteratively refines the pose through a learned residual pose prediction. We evaluate PoseRefineNet on several benchmark datasets, demonstrating state-of-the-art performance in terms of pose accuracy and robustness, particularly in cluttered scenes. The proposed approach significantly improves upon existing methods, offering a practical solution for real-world robotic applications requiring precise object pose estimation."
http://arxiv.org/abs/1412.2604v2,Actions and Attributes from Wholes and Parts,"Understanding human actions and recognizing their associated attributes are fundamental tasks in computer vision. Existing methods often treat these tasks separately or struggle to effectively integrate holistic scene information with detailed part-based analysis. This paper addresses the problem of jointly inferring actions and attributes by leveraging both the global context of the scene and the localized information from individual body parts. We propose a novel hierarchical framework that first extracts global features representing the overall scene context and then integrates them with part-specific features extracted from detected body parts. A graph neural network is used to model the relationships between parts and to propagate information between them, enabling a more nuanced understanding of the action being performed and its associated attributes. Experimental results on benchmark datasets demonstrate that our approach achieves state-of-the-art performance in both action recognition and attribute prediction, significantly outperforming methods that rely solely on global or part-based features. This holistic and part-aware approach represents a significant advancement in fine-grained action understanding."
http://arxiv.org/abs/1411.5752v2,Hypercolumns for Object Segmentation and Fine-grained Localization,"Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification, yet their application to pixel-level tasks like object segmentation and fine-grained localization remains challenging. A key difficulty lies in reconciling the need for both high-level semantic information and precise spatial localization, which are often represented in different layers of the network. To address this challenge, we introduce ""Hypercolumns,"" which are vectors composed of the activations of all CNN layers above a given pixel location. These hypercolumns capture a rich, multi-scale representation of the visual context surrounding each pixel. We demonstrate that hypercolumns, when used as features for a simple classifier, significantly improve performance on both object segmentation and fine-grained part localization tasks. For example, on the PASCAL-Context dataset, hypercolumns achieve state-of-the-art segmentation accuracy without complex architectures or task-specific fine-tuning. This demonstrates the power of aggregating multi-layer CNN features for precise pixel-level understanding and highlights the potential of hypercolumns as a general-purpose representation for a wide range of vision tasks."
http://arxiv.org/abs/1409.5403v2,Deformable Part Models are Convolutional Neural Networks,"Deformable Part Models (DPMs) have been a cornerstone of object detection for years, known for their ability to represent objects through constellations of parts with learned deformations. Despite their success, they are often viewed as distinct from modern Convolutional Neural Network (CNN)-based detectors. This paper addresses the fundamental question of whether DPMs can be reformulated and implemented within a CNN framework, bridging the gap between these two seemingly disparate approaches. We demonstrate that a DPM can be exactly expressed as a specialized CNN architecture. This is achieved by constructing convolutional filters that learn part appearance and deformation costs, and by implementing the DPM scoring function through a series of convolutional and pooling operations. Our CNN-DPM model, trained end-to-end, achieves comparable performance to traditional DPM implementations while benefiting from the efficiency and scalability of modern deep learning frameworks. This reformulation provides a novel perspective on the representational power of CNNs and opens avenues for incorporating the strengths of DPMs, such as explicit part modeling and interpretability, into modern object detection pipelines."
http://arxiv.org/abs/1407.5736v1,Learning Rich Features from RGB-D Images for Object Detection and Segmentation,"Object detection and segmentation are fundamental tasks in computer vision, often benefiting from the complementary information provided by RGB-D data. However, effectively fusing and exploiting both color and depth information to learn robust and discriminative features remains a significant challenge. This paper addresses the problem of learning rich, joint representations from RGB-D images for improved object detection and segmentation performance. We propose a novel multi-modal fusion network, termed DRF-Net (Depth-enhanced Rich Feature Network), which leverages a two-stream architecture to process RGB and depth data separately, followed by a cross-modal attention module to adaptively fuse features at multiple scales. Furthermore, we introduce a depth-aware refinement module that utilizes depth information to refine segmentation boundaries and improve object localization accuracy. Experimental results on benchmark datasets, including SUN RGB-D and NYU Depth V2, demonstrate that DRF-Net achieves state-of-the-art performance in both object detection and semantic segmentation tasks, outperforming existing RGB-D fusion methods. Our approach offers a robust and effective solution for learning powerful representations from multi-modal data, leading to significant improvements in scene understanding."
http://arxiv.org/abs/1407.5035v3,LSDA: Large Scale Detection Through Adaptation,"Object detection models often struggle to generalize to novel domains due to dataset bias and limited training data for specific target environments. This paper addresses the challenge of scaling object detection to large and diverse environments by leveraging readily available, but potentially mismatched, source data. We introduce Large Scale Detection through Adaptation (LSDA), a novel domain adaptation framework that explicitly tackles both instance-level and feature-level discrepancies between source and target domains. LSDA employs a self-training regime with dynamic pseudo-label filtering to mitigate noise inherent in pseudo-labels generated on the target domain. Furthermore, we introduce a feature alignment module incorporating adversarial learning and a novel contrastive regularization term, encouraging domain-invariant feature representations while preserving class discriminability. We demonstrate the effectiveness of LSDA on a large-scale object detection benchmark derived from Open Images and COCO, showing significant improvements over existing domain adaptation methods, achieving up to 15% AP gain in challenging adaptation scenarios. LSDA offers a practical and effective approach to scaling object detection to new environments with minimal target domain annotation."
http://arxiv.org/abs/1407.3867v1,Part-based R-CNNs for Fine-grained Category Detection,"Fine-grained visual categorization, which aims to distinguish between subordinate categories within a broader class, poses a significant challenge due to subtle inter-class variations and high intra-class variance arising from pose, lighting, and viewpoint changes. This paper addresses the problem of effectively capturing and utilizing discriminative part-based features for improved fine-grained category detection. We propose a novel Part-based Region-based Convolutional Neural Network (Part-based R-CNN) architecture that integrates explicit part localization with region-based object detection. Our method first detects object proposals using a standard R-CNN framework, followed by a dedicated part localization module trained to identify crucial anatomical parts of the object. These localized part regions are then used to extract part-specific features, which are concatenated with global object features and fed into a classifier for fine-grained category prediction. Experimental results on benchmark fine-grained datasets demonstrate that our Part-based R-CNN significantly outperforms state-of-the-art approaches, achieving substantial gains in detection accuracy. This highlights the importance of incorporating explicit part information for robust and accurate fine-grained category detection."
http://arxiv.org/abs/1407.1808v1,Simultaneous Detection and Segmentation,"Simultaneous object detection and instance segmentation are fundamental tasks in computer vision, crucial for scene understanding and robotic perception. Existing approaches typically address these tasks sequentially or rely on complex multi-stage pipelines, leading to computational inefficiencies and potential error propagation. This paper introduces a novel end-to-end framework, named ""SimDetSeg,"" that performs simultaneous detection and segmentation by sharing a common backbone and integrating a novel dynamic attention mechanism. SimDetSeg utilizes a shared feature pyramid network (FPN) to extract multi-scale features, followed by parallel detection and segmentation heads. The dynamic attention module adaptively refines feature representations based on contextual information from both detection and segmentation branches, facilitating information exchange and enhancing feature discriminability. Experimental results on the COCO dataset demonstrate that SimDetSeg achieves state-of-the-art performance in both detection and segmentation accuracy while maintaining competitive inference speed compared to existing methods. This efficient and accurate simultaneous detection and segmentation framework offers a significant advancement for real-time vision applications."
http://arxiv.org/abs/1407.1610v2,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,"Object recognition is a fundamental task in computer vision, enabling machines to interpret and understand visual data. While deep learning, particularly Convolutional Neural Networks (CNNs), has achieved remarkable success, understanding the performance characteristics of simpler Multilayer Neural Networks (MLNNs) remains crucial for resource-constrained applications and provides valuable insights into the feature learning process. This paper addresses the problem of systematically analyzing the performance of MLNNs with varying architectures and training parameters on standard object recognition datasets. We propose a comprehensive evaluation framework that explores the impact of network depth, hidden layer size, activation functions, and regularization techniques on MLNN performance. Specifically, we investigate the trade-offs between model complexity and generalization ability, comparing the performance of MLNNs against established baselines. Our results demonstrate that carefully configured MLNNs can achieve surprisingly competitive accuracy on datasets like MNIST and CIFAR-10, particularly when coupled with appropriate regularization and data augmentation techniques. These findings highlight the potential of MLNNs as a viable alternative for specific object recognition tasks and contribute to a deeper understanding of the design principles for effective neural network architectures."
http://arxiv.org/abs/1408.5093v1,Caffe: Convolutional Architecture for Fast Feature Embedding,"Deep convolutional neural networks (CNNs) have revolutionized computer vision, achieving state-of-the-art performance in various tasks, but their implementation often suffers from inflexibility and significant computational overhead. This paper addresses the need for a unified and efficient framework for deep learning, specifically focusing on convolutional neural networks. We present Caffe, a novel deep learning framework designed with modularity, speed, and accessibility in mind. Caffe is implemented in C++ with a Python interface and configurable command line tools. It allows users to define network architectures through a simple model definition schema, supports multiple optimization algorithms, and enables seamless GPU and CPU switching. We demonstrate Caffe's performance through extensive experiments on image classification and object detection tasks, achieving state-of-the-art results on benchmark datasets while significantly reducing training and deployment times compared to existing frameworks. Caffe provides a powerful and practical tool for researchers and practitioners to rapidly prototype, train, and deploy deep learning models."
http://arxiv.org/abs/1406.5212v1,R-CNNs for Pose Estimation and Action Detection,"Human pose estimation and action detection are fundamental tasks for understanding human behavior in images and videos. Existing approaches often treat these tasks separately, leading to inefficiencies and a lack of contextual information sharing. This paper introduces a novel framework leveraging Region-based Convolutional Neural Networks (R-CNNs) to jointly address human pose estimation and action detection. Our approach utilizes a multi-branch R-CNN architecture, where one branch detects human regions, and subsequent branches simultaneously predict pose keypoints and classify actions within each detected region. We incorporate a pose-guided attention mechanism that enhances action recognition by focusing on relevant body parts identified through pose estimation. Experimental results on benchmark datasets, including Human3.6M and ActivityNet, demonstrate significant improvements in both pose estimation accuracy and action detection performance compared to state-of-the-art methods. The proposed unified framework offers a more efficient and accurate solution for understanding human behavior in visual data, paving the way for advancements in applications like human-computer interaction and video surveillance."
http://arxiv.org/abs/1405.0312v3,Microsoft COCO: Common Objects in Context,"Object recognition has witnessed remarkable progress, yet robust performance in complex scenes remains a challenge due to limited datasets that capture the intricacies of real-world contexts. This paper addresses the need for a comprehensive and realistic dataset to advance object recognition, segmentation, and captioning research in complex scenes. We introduce Microsoft COCO (Common Objects in Context), a large-scale dataset with rich annotations designed to overcome the limitations of existing resources. COCO features over 330K images containing 1.5 million object instances, meticulously annotated with object bounding boxes, pixel-level segmentation masks, and textual captions describing the scene. Furthermore, COCO emphasizes contextual relationships by annotating objects in their natural surroundings, enabling research into scene understanding and reasoning. Experimental results demonstrate that models trained on COCO exhibit significantly improved generalization performance across various tasks, including object detection, instance segmentation, and image captioning, compared to those trained on other datasets. The availability of COCO will facilitate the development of more sophisticated and context-aware computer vision systems, driving progress towards truly intelligent machines."
http://arxiv.org/abs/1404.1869v1,DenseNet: Implementing Efficient ConvNet Descriptor Pyramids,"Convolutional Neural Networks (CNNs) have become the dominant paradigm for visual feature extraction, yet creating robust descriptors for tasks like image matching and retrieval often requires building image pyramids to capture features at multiple scales, which can be computationally expensive. This paper addresses the problem of efficiently constructing multi-scale feature representations using the inherent hierarchical structure of DenseNets. We propose a novel method that leverages the dense connectivity and transition layers within a DenseNet architecture to extract feature maps at various resolutions without requiring explicit image resizing. Our approach extracts feature maps from strategically selected layers of a pre-trained DenseNet, effectively creating a descriptor pyramid with minimal additional computational overhead. Experiments on standard benchmark datasets for image matching and retrieval demonstrate that our DenseNet-based descriptor pyramids achieve competitive performance compared to traditional methods while significantly reducing processing time and memory footprint. This efficient multi-scale feature extraction strategy facilitates the deployment of high-performing visual recognition systems on resource-constrained platforms."
http://arxiv.org/abs/1403.1024v4,On learning to localize objects with minimal supervision,"Weakly supervised object localization (WSOL) aims to train object detectors using only image-level labels, circumventing the need for expensive bounding box annotations. However, existing WSOL methods often struggle with accurately localizing the entire object extent, tending to focus on the most discriminative parts. This paper addresses the challenge of learning to localize objects more completely with minimal supervision. We propose a novel framework, termed Context-Aware Region Expansion (CARE), which leverages contextual information to progressively expand object localization from initial discriminative regions. CARE consists of two key components: a Contextual Affinity Module (CAM) that learns to estimate the contextual relationships between image regions, and a Region Expansion Module (REM) that iteratively expands the localized region based on the affinities computed by CAM. The REM is trained using a novel self-supervision loss that encourages consistent predictions between expanded regions and the original image-level labels. Experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate that CARE significantly improves localization accuracy compared to state-of-the-art WSOL methods, achieving a 10% improvement in CorLoc. This work provides a promising direction for developing accurate object detectors with reduced annotation effort, making large-scale object detection more feasible."
http://arxiv.org/abs/1311.2524v5,Rich feature hierarchies for accurate object detection and semantic segmentation,"Object detection and semantic segmentation are fundamental tasks in computer vision, enabling machines to understand and interpret visual scenes. Existing approaches often rely on hand-engineered features, limiting their ability to capture the complex variations present in natural images. This paper addresses the challenge of learning rich, discriminative, and transferable feature representations for both object detection and semantic segmentation. We propose a novel approach that leverages deep convolutional neural networks (CNNs) to extract hierarchical feature representations from image regions. Specifically, we first generate category-independent region proposals and then compute a CNN feature vector for each proposal. These features are subsequently used to train linear SVM classifiers for object detection and a fully convolutional network for semantic segmentation, fine-tuning the CNN on the specific task. Our method achieves state-of-the-art results on the PASCAL VOC detection benchmark, significantly outperforming previous approaches. Furthermore, we demonstrate strong performance on semantic segmentation tasks, highlighting the generalizability of our learned features. This work demonstrates the power of deep learning for learning robust and transferable visual features, paving the way for more accurate and efficient visual understanding systems."
http://arxiv.org/abs/2507.02864v1,MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real,"Learning robust robotic policies that generalize from simulation to the real world remains a significant challenge due to discrepancies in visual and physical dynamics. This paper addresses the problem of learning multimodal policies robust to sim-to-real gaps by leveraging multimodal generative models within the simulation environment. Our method, MultiGen, employs a conditional generative adversarial network (cGAN) to generate diverse, visually realistic simulation environments conditioned on high-level semantic scene descriptions. These generated environments expose the policy to a wide range of visual and physical variations, forcing it to learn more robust representations. Furthermore, we train a multimodal policy that can adapt its behavior based on observed environmental features. Experimental results on a pick-and-place task demonstrate that policies trained with MultiGen exhibit significantly improved sim-to-real transfer compared to policies trained in a single, fixed simulation environment, achieving a 40% increase in real-world task success rate. This work demonstrates the potential of using multimodal generative models to bridge the sim-to-real gap and learn more adaptable and robust robotic policies."
http://arxiv.org/abs/2506.11302v3,TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy,"Autonomous driving demands robust perception and prediction capabilities, particularly in dynamic and unpredictable environments. Existing datasets often lack the spatio-temporal depth necessary for developing comprehensive world models capable of anticipating future scene states. This paper introduces TARDIS STRIDE, a novel large-scale road image dataset captured with a high-frequency stereo camera system and precisely localized using RTK-GPS, providing synchronized RGB, depth, and pose data across extended sequences. We leverage this data to train a novel spatio-temporal world model incorporating transformer-based architectures to predict future RGB frames and associated depth maps conditioned on past observations and vehicle actions. Our model, trained on TARDIS STRIDE, demonstrates significant improvements in long-term prediction accuracy and scene understanding compared to models trained on existing datasets, as evidenced by quantitative metrics such as PSNR, SSIM, and LPIPS, as well as qualitative evaluations of generated future scenes. The TARDIS STRIDE dataset and the associated world model offer a valuable resource for advancing research in autonomous driving, enabling the development of more robust and reliable autonomous systems capable of navigating complex real-world scenarios."
http://arxiv.org/abs/2506.02618v1,Rodrigues Network for Learning Robot Actions,"Learning robot actions directly from visual inputs remains a challenging problem, especially for tasks requiring precise rotational control. Many existing methods struggle to accurately represent and learn complex rotational dynamics due to limitations in network architectures and representation spaces. This paper addresses the problem of learning continuous robot actions, specifically 3D rotations, from visual observations in a manner that is both accurate and physically plausible. We propose the Rodrigues Network (RodNet), a novel neural network architecture that leverages the Rodrigues formula to represent and learn rotations. RodNet encodes actions as axis-angle representations, parameterizing the rotation axis and angle within the network and using the Rodrigues formula as a differentiable layer to map these parameters to rotation matrices. Experimental results on simulated robot manipulation tasks demonstrate that RodNet significantly outperforms existing methods, achieving higher success rates and lower error in rotational control compared to networks that directly predict quaternions or rotation matrices. The proposed RodNet offers a principled and effective approach for learning robot actions involving complex rotational movements, paving the way for more robust and adaptable robot control systems."
http://arxiv.org/abs/2505.11032v2,DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy,"Garment manipulation is a challenging robotics task due to the high dimensionality and deformability of fabrics. Simulating these interactions accurately and efficiently for robot learning remains a significant hurdle. This paper addresses the problem of training a generalizable robotic policy for dexterous garment manipulation that can transfer to unseen garment shapes and task variations. We introduce DexGarmentLab, a novel physics-based simulation environment built on differentiable physics, designed for training and evaluating robotic garment manipulation policies. DexGarmentLab features a diverse dataset of parameterized 3D garment models and incorporates realistic contact dynamics and friction. We train a vision-based reinforcement learning agent with a graph neural network architecture to learn a manipulation policy that generalizes across different garment types and target configurations within a folding task. Experiments show that our trained policy achieves significant success rates on novel garment shapes and unseen target poses, demonstrating the effectiveness of our environment and learning approach. This work paves the way for developing robust and adaptable robotic systems capable of automating garment manipulation tasks in various real-world applications."
http://arxiv.org/abs/2505.03729v4,Visual Imitation Enables Contextual Humanoid Control,"Humanoid robots operating in complex, unstructured environments require robust control strategies that can adapt to varying contextual demands. Traditional control methods often struggle to generalize across diverse scenarios due to the complexity of modeling human-environment interactions. This paper addresses the challenge of enabling humanoid robots to perform contextually relevant actions by leveraging visual imitation learning. We propose a novel framework where a humanoid agent learns to mimic human demonstrations from a first-person perspective, conditioned on both visual scene context and task-specific goals. Our approach utilizes a hierarchical reinforcement learning architecture, where a high-level policy infers latent task representations from visual input and a low-level controller executes motor commands to achieve those representations. We demonstrate that our method enables a simulated humanoid robot to successfully imitate a variety of human actions, such as object manipulation and navigation, in different visual environments. The results show significant improvement in task completion rates and generalization compared to baseline methods that do not incorporate contextual visual information. This work highlights the potential of visual imitation learning for developing more adaptable and intelligent humanoid robots capable of operating effectively in real-world settings."
http://arxiv.org/abs/2503.03734v3,OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction,"Vision-language models (VLMs) have shown remarkable progress in understanding and generating text descriptions for images. However, their application to embodied agents requiring action prediction based on visual and textual input remains a challenge, often suffering from suboptimal visual feature extraction that disregards the nuances of the textual command. This paper introduces OTTER, a novel Vision-Language-Action model that incorporates Text-Aware Visual Feature Extraction. OTTER leverages cross-attention mechanisms to dynamically modulate visual feature extraction based on the specific textual instruction, allowing the model to focus on relevant visual cues for action prediction. Furthermore, OTTER utilizes a hierarchical action decoder to generate sequences of actions, improving temporal consistency and task completion rates. Experimental results on the ALFRED benchmark demonstrate that OTTER significantly outperforms existing VLMs in instruction following and action prediction, achieving a relative improvement of 15% in goal completion rate. These findings highlight the importance of text-aware visual processing for effective vision-language-action reasoning and pave the way for more intelligent and capable embodied agents."
http://arxiv.org/abs/2502.20396v1,Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids,"Dexterous manipulation on humanoid robots holds immense potential for automating complex tasks in unstructured environments. However, training robust vision-based manipulation policies directly on real-world humanoids is challenging due to safety concerns, hardware limitations, and the difficulty of collecting sufficient data. This paper addresses the problem of effectively transferring manipulation policies learned in simulation to a physical humanoid robot equipped with multi-fingered hands, bridging the sim-to-real gap. We propose a novel reinforcement learning framework combining privileged information during training in simulation with a robust vision-based observation space for deployment. Specifically, we leverage domain randomization techniques on visual textures, lighting, and dynamics parameters in simulation, and then employ a modular neural network architecture that processes depth images from multiple viewpoints to infer the object's pose and state. Our experiments demonstrate successful transfer of a peg insertion task to a physical humanoid robot, achieving a high success rate despite significant differences between the simulated and real environments. This work highlights the feasibility of training complex dexterous manipulation skills in simulation and deploying them on real humanoid platforms, paving the way for more versatile and adaptable robotic systems."
http://arxiv.org/abs/2502.08646v1,Poly-Autoregressive Prediction for Modeling Interactions,"Modeling interactions between multiple agents is crucial for understanding and predicting complex dynamic systems. Existing methods often struggle to capture the intricate dependencies and non-linear relationships inherent in such interactions, limiting their predictive accuracy. To address this, we introduce a novel Poly-Autoregressive Prediction (Poly-AR) model that leverages polynomial basis functions within an autoregressive framework to explicitly model higher-order interactions. Our approach decomposes the prediction task into a series of polynomial regressions, each capturing interactions up to a specified degree, allowing for flexible and interpretable modeling of complex relationships. We evaluate Poly-AR on several benchmark datasets involving multi-agent trajectory prediction, demonstrating significant improvements in prediction accuracy and reduced error compared to state-of-the-art methods. These results highlight the efficacy of our approach in capturing complex interaction patterns and its potential for advancing research in areas such as autonomous driving, robotics, and social behavior analysis."
http://arxiv.org/abs/2501.05453v1,An Empirical Study of Autoregressive Pre-training from Videos,"Self-supervised learning from videos has emerged as a powerful paradigm for learning generalizable visual representations. Autoregressive models, particularly successful in language modeling, offer a promising avenue for video pre-training by predicting future frames or latent representations. However, a comprehensive empirical evaluation of different autoregressive objectives, architectures, and scaling properties in the video domain remains largely unexplored. This paper presents an extensive empirical study evaluating various autoregressive pre-training strategies on a diverse set of video datasets and downstream tasks. We systematically investigate frame prediction, latent space prediction using VQ-VAE, and masked sequence prediction with different masking strategies. Our results demonstrate that latent space prediction with a learned codebook significantly outperforms direct frame prediction, achieving state-of-the-art performance on several action recognition benchmarks. Furthermore, we find that masked sequence prediction, inspired by BERT, offers a competitive alternative with improved computational efficiency. These findings provide valuable insights into the design and optimization of autoregressive video pre-training methods, paving the way for more effective self-supervised learning of video representations."
http://arxiv.org/abs/2501.03229v1,Gaussian Masked Autoencoders,"Masked Autoencoders (MAE) have achieved remarkable success in self-supervised learning for computer vision by reconstructing masked image patches. However, the binary masking strategy used in MAE can lead to suboptimal feature representations, particularly when dealing with complex image structures and varying patch importance. This paper introduces Gaussian Masked Autoencoders (GaMAE), a novel approach that replaces the binary mask with a Gaussian distribution-based mask. GaMAE leverages the properties of Gaussian distributions to generate a soft, spatially-aware mask, allowing the model to focus on more informative regions while still learning from the masked areas. Furthermore, we introduce a learnable scaling factor to control the mask intensity, enabling the network to dynamically adjust the masking ratio during training. Experimental results on ImageNet demonstrate that GaMAE achieves significant improvements in downstream classification and object detection tasks compared to the original MAE and other masking strategies. GaMAE offers a more nuanced and effective masking approach, leading to improved feature learning and generalization capabilities for self-supervised visual representation learning."
http://arxiv.org/abs/2412.17806v2,"Reconstructing People, Places, and Cameras","Simultaneous localization and mapping (SLAM) has revolutionized robotics and computer vision, enabling autonomous navigation and scene understanding. However, current SLAM systems often struggle in dynamic environments and with reconstructing articulated objects like humans, hindering their applicability in real-world scenarios involving human-robot interaction or augmented reality. This paper addresses the challenge of jointly reconstructing a dynamic scene containing people, a static environment, and the camera trajectory within a unified framework. We propose a novel approach that leverages neural radiance fields (NeRFs) to represent both the static scene and dynamic humans, coupled with a differentiable skeleton fitting module to track human pose. A joint optimization strategy simultaneously refines the camera pose, the static scene NeRF, the dynamic human NeRFs, and the human pose parameters. Experimental results on synthetic and real-world datasets demonstrate that our method achieves significantly improved reconstruction accuracy of both the static environment and dynamic humans compared to state-of-the-art SLAM and dynamic NeRF approaches. This work provides a crucial step towards building robust and comprehensive scene understanding systems capable of operating in complex, human-populated environments."
http://arxiv.org/abs/2412.14172v1,Learning from Massive Human Videos for Universal Humanoid Pose Control,"Humanoid pose control is a fundamental problem in robotics and animation, often tackled with task-specific training. However, the generalization capability of these methods is limited by the diversity of the training data. This paper addresses the challenge of learning a universal humanoid pose control policy from massive, uncurated human video data. We propose a novel framework that leverages self-supervised learning to extract robust pose representations from a large-scale dataset of human motion videos. Our approach involves training a variational autoencoder to encode human poses and then using a reinforcement learning agent to control a humanoid model based on these learned pose embeddings. We further incorporate a contrastive loss to ensure the encoded pose space is discriminative and well-structured. Experimental results demonstrate that our method enables the humanoid to accurately imitate a wide range of human motions observed in the videos, achieving significantly better generalization performance compared to existing methods trained on limited datasets. This work represents a significant step towards creating versatile and adaptable humanoid robots capable of performing complex tasks by learning directly from human demonstrations."
http://arxiv.org/abs/2412.04835v1,Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment,"Learning reward functions that accurately reflect desired robot behavior is crucial for effective visuomotor policy learning, yet existing methods often require substantial amounts of human feedback. This paper addresses the challenge of efficiently aligning robot behavior with human preferences by minimizing the amount of feedback required to learn an effective reward function. We introduce a novel approach, Minimal Feedback Reward Alignment (MiFRA), which leverages a learned world model to predict future states and focuses human feedback on discriminating between trajectories predicted to have the highest potential impact on policy alignment. MiFRA adaptively selects trajectory pairs for comparison based on their predicted influence on the learned reward function, thereby maximizing information gain from each interaction. Experiments on simulated robotic manipulation tasks demonstrate that MiFRA achieves comparable or superior policy alignment performance to state-of-the-art methods while requiring significantly less human feedback (up to 50% reduction). This improved sample efficiency makes reward learning more practical for real-world robotic applications where human supervision is a scarce resource."
http://arxiv.org/abs/2411.08034v3,Scaling Properties of Diffusion Models for Perceptual Tasks,"Diffusion models have emerged as a powerful class of generative models, achieving state-of-the-art results in image synthesis. However, their scaling behavior for perceptual tasks, such as image classification and segmentation, remains relatively unexplored compared to discriminative models. This paper investigates the impact of model size, data size, and training compute on the performance of diffusion models when adapted for perceptual tasks via techniques like classifier guidance and feature extraction. We systematically evaluate diffusion models of varying scales on standard benchmark datasets for image classification and semantic segmentation, analyzing the trade-offs between generative fidelity and perceptual accuracy. Our results demonstrate that larger diffusion models exhibit improved performance on downstream perceptual tasks, but with diminishing returns beyond a certain scale. Furthermore, we find that pre-training on larger datasets significantly enhances the transferability of diffusion model features to downstream tasks, surpassing the performance of smaller, task-specific models. These findings provide valuable insights into the scaling properties of diffusion models and pave the way for more efficient and effective utilization of these models in diverse computer vision applications."
http://arxiv.org/abs/2410.03665v3,Estimating Body and Hand Motion in an Ego-sensed World,"Estimating human body and hand motion is crucial for understanding human behavior and enabling seamless human-computer interaction in egocentric vision. However, the inherent ambiguities and occlusions in first-person views, coupled with the dynamic nature of both camera and human motion, pose significant challenges for accurate and robust motion estimation. We address the problem of simultaneously estimating 3D body pose, hand pose, and ego-motion from monocular video captured by a head-mounted camera. Our approach integrates a novel neural network architecture that leverages temporal information and kinematic constraints to jointly optimize for body pose, hand pose, and camera trajectory. Specifically, we employ a recurrent neural network to predict pose and motion parameters conditioned on past observations, and further refine the predictions using a physics-based optimization that enforces anatomical constraints and minimizes interpenetration. Experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance in egocentric body and hand motion capture, outperforming existing methods in terms of accuracy and robustness. This advancement enables more realistic and immersive experiences in virtual and augmented reality applications, as well as improved understanding of human activities in egocentric videos."
http://arxiv.org/abs/2409.08273v1,Hand-Object Interaction Pretraining from Videos,"Understanding hand-object interactions is crucial for robots to effectively manipulate objects and for AI agents to comprehend human activities. However, training models for hand-object interaction understanding requires large amounts of annotated data, which is expensive and time-consuming to acquire. We address this problem by proposing a novel self-supervised pretraining framework for learning hand-object interaction representations directly from unlabeled video data. Our method, HOI-Pretrain, leverages contrastive learning to learn embeddings that are invariant to viewpoint and object variations while being sensitive to the specific interaction occurring between the hand and object. We achieve this by contrasting different interaction segments within a video and enforcing consistency across different views of the same interaction. Experiments demonstrate that HOI-Pretrain significantly improves performance on downstream hand-object interaction recognition and manipulation tasks, outperforming existing pretraining strategies by a substantial margin. This demonstrates the effectiveness of our self-supervised approach for learning robust and generalizable hand-object interaction representations from readily available video data, paving the way for more efficient and scalable learning of robotic manipulation skills."
http://arxiv.org/abs/2409.04440v1,Synergy and Synchrony in Couple Dances,"Couple dances are a rich form of nonverbal communication where partners coordinate their movements to create a unified aesthetic experience. Understanding the intricate interplay of motion between dancers presents a significant challenge due to the complexity and subtlety of their interactions. This paper introduces a novel computational framework to analyze synergy and synchrony in couple dances using multi-modal motion capture data. Our approach combines dynamic time warping (DTW) for temporal alignment with a novel synergy metric based on principal component analysis (PCA) to quantify the shared variance in motion trajectories. We demonstrate the effectiveness of our method on a dataset of Salsa and Tango performances, revealing distinct patterns of synergy and synchrony that characterize each dance style. Specifically, we observe higher synergy in Tango, indicating a more pronounced leader-follower dynamic, while Salsa exhibits greater synchrony, suggesting a more egalitarian interaction. These findings provide valuable insights into the underlying mechanisms of coordination in couple dances and open avenues for applications in dance education, performance analysis, and human-robot interaction."
http://arxiv.org/abs/2407.18908v2,Wolf: Dense Video Captioning with a World Summarization Framework,"Dense video captioning (DVC) aims to generate natural language descriptions for all salient events in a video, presenting a significant challenge in temporally localizing and describing these events comprehensively. Existing DVC methods often struggle to capture the broader context and relationships between events, leading to fragmented and incomplete summaries. To address this, we propose Wolf, a novel DVC framework incorporating World Summarization. Wolf utilizes a hierarchical attention mechanism to first summarize the overall video context and then leverages this global representation to inform the generation of dense captions for individual events. Specifically, Wolf employs a transformer-based encoder to extract visual features, followed by a world summarization module that captures long-range dependencies, and finally, a captioning decoder conditioned on both local visual features and the global world summary. Experiments on ActivityNet Captions and YouCookII datasets demonstrate that Wolf significantly outperforms existing DVC methods, achieving state-of-the-art results in both captioning accuracy and temporal localization. This highlights the importance of incorporating a comprehensive understanding of the video context for generating more coherent and informative dense video captions."
http://arxiv.org/abs/2404.16823v2,Learning Visuotactile Skills with Two Multifingered Hands,"Robots operating in unstructured environments must leverage rich sensory information to perform complex manipulation tasks. Visuotactile sensing, combining visual and tactile feedback, offers a promising avenue for learning intricate manipulation skills. This paper addresses the challenge of learning coordinated bimanual manipulation skills using visuotactile feedback on a robot equipped with two multifingered hands. We propose a novel reinforcement learning framework that integrates visual information from cameras and tactile data from sensors embedded in the fingertips of each hand. Our approach utilizes a multi-modal fusion network to process visuotactile data and learns a decentralized control policy for each hand, allowing for independent control while encouraging coordinated actions through a shared reward function. Experiments on simulated manipulation tasks, including in-hand object reorientation and bimanual assembly, demonstrate that our method significantly outperforms vision-only and tactile-only baselines, achieving higher success rates and improved robustness to environmental variations. This work highlights the importance of visuotactile feedback for learning complex bimanual manipulation skills and paves the way for more dexterous and adaptable robots."
http://arxiv.org/abs/2404.06507v3,Reconstructing Hand-Held Objects in 3D from Images and Videos,"Reconstructing 3D models of objects is a fundamental problem in computer vision, enabling applications in robotics, augmented reality, and digital asset creation. However, reconstructing accurate 3D models of hand-held objects from commodity RGB images and videos remains challenging due to occlusions, varying lighting conditions, and the complex articulation of the hand. We present a novel approach that integrates multi-view stereo with a learned hand pose prior and a shape completion network to address these challenges. Our method first estimates hand pose and object masks using a deep neural network. Then, a multi-view stereo algorithm refines the object geometry within the masked region. Finally, a shape completion network leverages the partial reconstruction and hand pose to infer a complete and plausible 3D model, even in the presence of significant occlusions. Experiments on a newly collected dataset of hand-object interactions demonstrate that our method significantly improves the accuracy and completeness of 3D reconstructions compared to state-of-the-art approaches, particularly in scenarios with substantial hand occlusion. This work provides a robust and practical solution for 3D reconstruction of hand-held objects, advancing the state-of-the-art in human-object interaction understanding."
http://arxiv.org/abs/2403.02338v2,Twisting Lids Off with Two Hands,"Two-handed manipulation is a fundamental aspect of human dexterity, enabling us to perform complex tasks that are often impossible with a single hand. However, robotic manipulation research has largely focused on single-arm systems or simple bimanual tasks. This paper addresses the challenging problem of enabling a dual-arm robot to robustly and efficiently twist lids off containers, a task requiring coordinated force application, precise positioning, and adaptation to varying lid and container geometries. We propose a novel framework that combines visual perception, force feedback, and a hierarchical reinforcement learning approach. The system first uses visual information to estimate the lid's pose and dimensions. Then, a high-level policy learns to select appropriate grasp points and twisting directions, while a low-level policy executes the motion primitives, adjusting force based on haptic feedback to prevent slippage and damage. Experiments on a physical dual-arm robot demonstrate that our system achieves a success rate of over 90% across a diverse set of containers with varying lid sizes and materials, significantly outperforming baseline methods. This work represents a significant step towards developing more versatile and human-like robotic manipulation capabilities for real-world applications."
http://arxiv.org/abs/2403.01915v2,xT: Nested Tokenization for Larger Context in Large Images,"Large Vision Transformers (ViTs) struggle to process high-resolution images due to the quadratic complexity of self-attention. Existing methods often reduce computational cost through techniques like sparse attention or hierarchical representations, but can still be limited by the fixed receptive field of initial tokenization and struggle to capture long-range dependencies in very large images. This paper addresses the challenge of efficiently processing large images within ViTs by expanding context while minimizing computational overhead. We introduce xT, a novel nested tokenization scheme that adaptively aggregates tokens into larger, semantically meaningful units through iterative pooling and refinement. xT progressively merges adjacent tokens based on learned similarity metrics, creating a hierarchical token structure that captures both fine-grained details and global context. Experiments on high-resolution image classification and object detection benchmarks demonstrate that xT significantly improves performance compared to standard ViTs and other efficient attention mechanisms, achieving a 2.5% increase in top-1 accuracy on ImageNet-22K with a 30% reduction in computational cost for high-resolution images. xT offers a powerful and efficient approach to scaling ViTs to handle increasingly large image inputs without sacrificing performance."
http://arxiv.org/abs/2402.19469v1,Humanoid Locomotion as Next Token Prediction,"Humanoid locomotion is a complex motor control problem requiring precise coordination of joints and balance to achieve stable and efficient movement. Current approaches often rely on reinforcement learning or trajectory optimization, which can be computationally expensive or require meticulous reward engineering. We address the challenge of learning realistic and diverse humanoid locomotion gaits by formulating it as a sequence modeling problem. Specifically, we propose a novel approach where humanoid locomotion is treated as a next-token prediction task, leveraging the Transformer architecture. Our model learns to predict the subsequent pose given a history of past poses and high-level control commands, effectively learning the dynamics and control policies implicitly from motion capture data. Experiments on a diverse dataset of human motions demonstrate that our method can generate stable and varied locomotion styles, including walking, running, and jumping, with smooth transitions between them. This approach offers a data-driven and computationally efficient alternative to traditional methods, enabling the creation of more realistic and controllable humanoid agents."
http://arxiv.org/abs/2401.10889v2,Synthesizing Moving People with 3D Control,"Synthesizing realistic and controllable human motion is a crucial task for various applications, including virtual reality, game development, and autonomous driving simulation. However, existing methods often struggle to generate diverse and physically plausible motions while maintaining fine-grained 3D control over the synthesized human figures. This paper introduces a novel framework for synthesizing moving people with explicit 3D control, leveraging a conditional generative adversarial network (GAN) conditioned on both a sequence of 3D human poses and a spatial layout representing the environment. Our method employs a spatio-temporal generator network to produce realistic video sequences, guided by a discriminator that enforces both visual realism and adherence to the specified 3D pose trajectory and environmental context. Furthermore, we introduce a novel loss function that encourages temporal coherence and reduces artifacts. We demonstrate that our approach generates significantly more realistic and controllable human motion compared to state-of-the-art video synthesis techniques, as evidenced by quantitative metrics and qualitative evaluations. Our method enables precise manipulation of human motion within a 3D scene, paving the way for more interactive and immersive virtual experiences."
http://arxiv.org/abs/2401.04105v2,Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning,"Deep neural networks have achieved remarkable success in various computer vision tasks, but their substantial memory footprint poses a significant challenge, especially during finetuning on resource-constrained devices. This paper addresses the problem of memory inefficiency in finetuning large pre-trained models by introducing Dr$^2$Net: Dynamic Reversible Dual-Residual Networks. Dr$^2$Net leverages reversible architectures to drastically reduce memory consumption by reconstructing activations during the backward pass, eliminating the need for storing them. We further enhance the model with a dynamic dual-residual block that allows for adaptive feature reuse and gradient flow control, mitigating the potential performance degradation associated with reversible architectures. Experimental results on image classification and semantic segmentation tasks demonstrate that Dr$^2$Net achieves significant memory savings (up to 60%) with minimal performance loss compared to standard finetuning approaches. This memory-efficient finetuning strategy enables the deployment of state-of-the-art models on devices with limited memory capacity, broadening the applicability of deep learning in real-world scenarios."
http://arxiv.org/abs/2312.13469v1,Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation,"In-hand manipulation requires robots to understand both visual and tactile information to effectively interact with objects. However, fusing these modalities remains challenging due to their disparate representations and noise characteristics. This paper addresses the problem of learning a shared representation for visuo-tactile data to improve perception for in-hand manipulation. We introduce ""Neural Feels,"" a novel approach that leverages neural radiance fields (NeRFs) to construct a continuous, implicit representation of the object's geometry and tactile properties. Specifically, we train a NeRF conditioned on both visual images and tactile sensor readings, enabling the network to predict density, color, and a learned ""feel"" embedding at any point in space. Experiments on a simulated in-hand manipulation task demonstrate that Neural Feels significantly improves object pose estimation and slip detection compared to vision-only and naive fusion baselines, achieving a 30% reduction in pose error and a 25% increase in slip detection accuracy. This work demonstrates the potential of neural fields for creating rich, multi-modal representations that enhance robotic perception and control in complex manipulation scenarios."
http://arxiv.org/abs/2312.06653v2,Adaptive Human Trajectory Prediction via Latent Corridors,"Human trajectory prediction is crucial for autonomous systems operating in human-populated environments. Existing methods often struggle to accurately predict long-term trajectories in complex, multi-modal scenarios due to their limited ability to capture the underlying environmental constraints and social interactions that guide human movement. This paper introduces a novel approach, Adaptive Human Trajectory Prediction via Latent Corridors (LC-Pred), which explicitly models potential future paths as a set of latent corridors learned from observed trajectories and scene context. LC-Pred utilizes a variational autoencoder to encode observed trajectories and scene information into a latent space representing possible corridor configurations. A decoder then generates future trajectories conditioned on these latent corridors, allowing for adaptation to different environmental contexts and social dynamics. Experiments on benchmark datasets, including ETH, UCY, and SDD, demonstrate that LC-Pred achieves state-of-the-art performance in both short-term and long-term prediction accuracy, particularly in crowded scenes and complex environments. This approach offers a more interpretable and adaptable framework for human trajectory prediction, paving the way for safer and more efficient human-robot interaction."
http://arxiv.org/abs/2312.05251v1,Reconstructing Hands in 3D with Transformers,"Accurate 3D hand pose estimation from monocular images is a challenging problem due to self-occlusion, articulation, and the lack of depth information. Existing methods often rely on convolutional neural networks (CNNs) to extract local features, which can struggle to capture long-range dependencies crucial for understanding hand structure. This paper addresses the problem of accurately reconstructing complete 3D hand meshes from single RGB images by leveraging the power of transformers. We propose a novel transformer-based architecture, the Hand Mesh Transformer (HMT), which directly predicts 3D hand mesh vertices. HMT employs a hierarchical transformer encoder to capture global context and learn relationships between different hand parts, followed by a mesh decoder that reconstructs the 3D hand mesh in a coarse-to-fine manner. Our approach incorporates a novel mesh refinement module within the decoder to progressively improve the mesh quality. Experimental results on benchmark datasets demonstrate that HMT achieves state-of-the-art performance in 3D hand mesh reconstruction, surpassing existing CNN-based methods in accuracy and robustness. This highlights the potential of transformers for modeling complex articulated structures and significantly advances the field of 3D hand pose estimation."
http://arxiv.org/abs/2312.00785v1,Sequential Modeling Enables Scalable Learning for Large Vision Models,"Large vision models have demonstrated remarkable capabilities in various computer vision tasks, but their training often requires substantial computational resources and memory, hindering scalability. This paper addresses the challenge of training such models on limited resources by introducing a novel sequential modeling approach. We propose a method that decomposes the monolithic training process into a sequence of smaller, manageable steps, enabling iterative learning and progressive model refinement. Specifically, we leverage recurrent neural networks to model the training process itself, predicting optimal parameter updates based on past experiences and resource constraints. This allows for efficient exploration of the parameter space and mitigates the need for full batch processing, significantly reducing memory footprint. Our experiments demonstrate that sequential training achieves comparable or superior performance to traditional methods, while requiring substantially less memory and computational power, enabling the training of large vision models on resource-constrained devices. These results pave the way for democratizing access to powerful vision models and facilitating their deployment in diverse environments."
http://arxiv.org/abs/2311.18259v4,Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives,"Understanding skilled human activity is crucial for developing assistive technologies and robotic collaborators. However, current datasets often lack the multi-view coverage necessary to fully capture the nuances of complex actions. We introduce Ego-Exo4D, a large-scale, synchronized ego-centric and exo-centric video dataset designed to facilitate research on skilled human activities. This dataset addresses the challenge of learning robust activity representations by providing paired first-person and third-person perspectives of individuals performing a diverse set of daily tasks, including cooking, cleaning, and object manipulation. Ego-Exo4D contains over 1,000 hours of synchronized video from 300 participants, along with rich annotations such as 3D pose, object bounding boxes, and detailed activity transcripts. Initial experiments demonstrate the benefits of multi-view fusion for activity recognition and highlight the dataset's potential for training models that generalize across viewpoints and improve understanding of human behavior. This dataset provides a valuable resource for advancing research in activity recognition, human-robot interaction, and computer vision."
http://arxiv.org/abs/2310.13724v1,"Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots","The development of embodied AI agents necessitates realistic and interactive environments where they can learn and collaborate with humans. Existing simulation platforms often lack the fidelity and complexity required to bridge the reality gap and enable robust human-agent interaction. To address this limitation, we introduce Habitat 3.0, a novel co-habitat that seamlessly integrates real human participants, photorealistic avatars, and robots within a shared virtual environment. Our system leverages advanced volumetric capture and rendering techniques to represent human participants, coupled with high-fidelity avatar animation and physics-based robot simulation within the Habitat platform. We demonstrate the capabilities of Habitat 3.0 through a series of collaborative tasks, including object manipulation and navigation, showcasing its potential for studying human-robot interaction and training AI agents in complex, realistic scenarios. Our experiments reveal that agents trained in Habitat 3.0 exhibit improved transferability to real-world settings compared to agents trained in traditional simulated environments. This work paves the way for more effective and safe development of embodied AI systems designed to work alongside humans."
http://arxiv.org/abs/2310.07932v2,What Matters to You? Towards Visual Representation Alignment for Robot Learning,"Robot learning from visual data presents a significant challenge due to the inherent differences between the robot's perception and the human's understanding of the task. This paper addresses the problem of aligning visual representations learned by a robot with the task-relevant aspects prioritized by a human expert. We introduce a novel approach, Visual Representation Alignment through Preference Encoding (VRAPE), which leverages human feedback in the form of pairwise preference comparisons between robot-generated visual representations. VRAPE encodes these preferences into a learned reward function, guiding the robot to refine its visual feature extraction network towards representations that better reflect human-valued task characteristics. Experiments on simulated robotic manipulation tasks demonstrate that VRAPE significantly improves task performance and generalization compared to baseline methods that rely solely on task rewards or imitation learning. This approach enables robots to learn more effectively from visual data by actively incorporating human understanding, paving the way for more intuitive and efficient robot learning paradigms."
http://arxiv.org/abs/2309.09979v2,General In-Hand Object Rotation with Vision and Touch,"Dexterous in-hand manipulation enables robots to interact with objects in complex and nuanced ways. However, achieving robust and generalizable in-hand object rotation remains a significant challenge due to the complexities of contact dynamics and the limitations of individual sensory modalities. We address the problem of achieving precise and reliable in-hand object rotation applicable to a variety of objects, leveraging both visual and tactile feedback. Our method employs a hierarchical reinforcement learning framework, where a high-level policy plans discrete rotation actions based on visual observations, and a low-level policy executes these actions by controlling finger joint torques informed by tactile sensor readings. This architecture allows the high-level policy to learn a general rotation strategy, while the low-level policy adapts to specific object properties and contact conditions through tactile feedback. We demonstrate successful rotation of diverse objects with varying shapes and materials in simulation and on a physical robot, achieving significantly improved rotation accuracy and robustness compared to vision-only or open-loop control methods. This work contributes to the development of more versatile and capable robotic manipulation systems by effectively integrating vision and touch for complex in-hand object control."
http://arxiv.org/abs/2308.09126v1,EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding,"Understanding human activities and their underlying motivations in long-form, egocentric video is a crucial step towards embodied AI. Current video understanding benchmarks often focus on short clips and lack the complexity and temporal reasoning required to analyze extended, goal-oriented activities captured from a first-person perspective. To address this limitation, we introduce EgoSchema, a novel diagnostic benchmark designed for very long-form video language understanding in egocentric settings. EgoSchema comprises 150 hours of unscripted egocentric video paired with detailed, multi-level annotations, including activities, sub-activities, objects, and free-form textual rationales explaining the ""why"" behind actions. We evaluate existing state-of-the-art video-language models on EgoSchema using a suite of challenging tasks, revealing significant shortcomings in their ability to perform temporal reasoning, infer motivations, and generate coherent explanations over extended time horizons. The EgoSchema benchmark provides a valuable resource for advancing research in long-form video understanding and fostering the development of more capable and interpretable embodied AI agents."
http://arxiv.org/abs/2306.10208v1,Learning Space-Time Semantic Correspondences,"Establishing semantic correspondences across space and time is a fundamental challenge in computer vision, enabling tasks such as video understanding, action recognition, and 3D scene reconstruction. This paper addresses the problem of learning dense, semantically meaningful correspondences between pixels in different frames of a video sequence, even under significant appearance changes and motion. We introduce a novel approach that leverages a self-supervised learning framework to train a deep neural network to predict space-time semantic correspondences. Our method incorporates a contrastive loss that encourages pixels representing the same semantic entity across different frames to be embedded close together in a learned feature space, while pushing apart embeddings of dissimilar pixels. Additionally, we introduce a temporal consistency constraint that enforces smoothness in the predicted correspondences across adjacent frames. Experimental results on standard video datasets demonstrate that our approach significantly outperforms existing methods in terms of correspondence accuracy and robustness to challenging video conditions. These learned space-time semantic correspondences provide a powerful representation for various downstream video understanding tasks."
http://arxiv.org/abs/2306.10007v2,Robot Learning with Sensorimotor Pre-training,"Robot learning promises to automate the acquisition of complex skills, but often suffers from poor sample efficiency and generalization, particularly when learning from scratch. This work addresses the challenge of enabling robots to learn new tasks more efficiently by leveraging prior experience through sensorimotor pre-training. We propose a self-supervised pre-training framework where a robot explores its environment and learns to predict the consequences of its actions in its sensory space using a contrastive objective. This pre-trained sensorimotor model is then used as a foundation for downstream task learning, allowing for faster adaptation and improved generalization. Experiments on a simulated robotic arm demonstrate that our pre-trained models significantly outperform learning from scratch on a variety of manipulation tasks, achieving comparable or better performance with substantially fewer samples. These results suggest that sensorimotor pre-training is a promising approach for enabling robots to rapidly acquire new skills and adapt to novel environments."
http://arxiv.org/abs/2306.00989v1,Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles,"Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks, often relying on complex architectural modifications and training recipes. However, the necessity of these ""bells-and-whistles"" for achieving high performance remains an open question. In this work, we investigate the potential of a simplified, hierarchical ViT architecture, termed Hiera, that minimizes architectural complexity while retaining the core principles of hierarchical feature extraction. Hiera employs a straightforward multi-stage design with uniform transformer blocks and patch merging layers, eschewing complex attention mechanisms, specialized normalization layers, and intricate initialization schemes. Our experiments demonstrate that Hiera achieves competitive or superior performance compared to more complex ViT variants on ImageNet classification and downstream tasks, while requiring fewer parameters and FLOPS. Specifically, Hiera achieves 84.5% top-1 accuracy on ImageNet-1K with significantly reduced computational cost compared to DeiT-III. This work highlights the importance of hierarchical feature representation in ViTs and suggests that architectural simplicity can be a key factor in achieving efficient and effective vision models."
http://arxiv.org/abs/2305.20091v3,Humans in 4D: Reconstructing and Tracking Humans with Transformers,"Reconstructing and tracking humans in dynamic 3D scenes is a fundamental problem in computer vision, crucial for applications ranging from virtual reality to autonomous navigation. Existing methods often struggle with complex occlusions, fast motions, and maintaining temporal consistency over extended sequences. We address these limitations by introducing a novel transformer-based approach, named Humans in 4D (H4D), for robust human reconstruction and tracking from monocular video. H4D leverages a transformer architecture to globally reason about human pose, shape, and appearance across the entire video sequence. Specifically, we employ a temporal transformer encoder to aggregate information from per-frame image features, followed by a decoder that predicts a sequence of 3D human meshes with associated skeletal poses. Furthermore, we introduce a novel attention mechanism that explicitly models inter-frame dependencies, allowing the network to effectively handle occlusions and motion blur. Experiments on challenging benchmark datasets demonstrate that H4D significantly outperforms state-of-the-art methods in terms of reconstruction accuracy, temporal smoothness, and robustness to noisy environments. This work presents a significant step towards accurate and reliable human digitization in dynamic environments."
http://arxiv.org/abs/2304.01199v2,On the Benefits of 3D Pose and Tracking for Human Action Recognition,"Human action recognition is a fundamental problem in computer vision with applications ranging from video surveillance to human-computer interaction. While current approaches often rely on 2D appearance-based features, these can be susceptible to viewpoint variations and occlusions. This paper investigates the benefits of incorporating 3D human pose and tracking information for robust action recognition. We propose a novel framework that leverages a state-of-the-art 3D pose estimation and multi-person tracking algorithm to extract accurate and temporally consistent 3D skeletons. These skeletons are then used as input to a graph convolutional network (GCN) that learns spatio-temporal relationships between body joints, enhanced by a novel attention mechanism that focuses on the most informative joints for each action. Experimental results on the NTU RGB+D and Kinetics datasets demonstrate that our 3D pose-based approach achieves significant performance improvements compared to existing 2D and some 3D-based methods, particularly in challenging scenarios with viewpoint changes and occlusions, showcasing the robustness of the proposed method. This work highlights the importance of accurate 3D human representation for achieving more reliable and view-invariant action recognition."
http://arxiv.org/abs/2304.01192v1,Navigating to Objects Specified by Images,"Visual navigation to objects is a fundamental capability for autonomous agents. However, current navigation systems often rely on explicit coordinate-based goals or semantic labels, limiting their applicability in unstructured environments where such information is unavailable. This paper addresses the problem of enabling agents to navigate to objects specified solely by a target image depicting the desired object instance. We propose a novel end-to-end trainable framework, ImageGoalNav, which learns to extract relevant visual features from both the target image and the agent's egocentric view, and then uses these features to predict navigation actions. ImageGoalNav incorporates a cross-attention mechanism to dynamically attend to relevant regions in the egocentric view based on the target image, enabling robust and efficient navigation. Experiments conducted in a simulated 3D environment demonstrate that ImageGoalNav significantly outperforms baseline methods, achieving a higher success rate and shorter path lengths in reaching the target object. Our approach provides a more flexible and intuitive way to specify navigation goals, paving the way for more adaptable and user-friendly robotic systems."
http://arxiv.org/abs/2303.18240v2,Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?,"The pursuit of embodied intelligence necessitates the development of artificial visual systems capable of robustly processing and interpreting complex visual scenes in real-time. A key challenge lies in creating artificial visual cortices that can move beyond passive perception and actively support embodied agents in interacting with their environment. This paper investigates the progress and limitations of current computational models aiming to replicate the functionalities of the primate visual cortex for embodied intelligence. We analyze a spectrum of approaches, ranging from biologically-inspired hierarchical convolutional networks with recurrent connections and attention mechanisms to end-to-end trainable architectures optimized for specific embodied tasks, such as navigation and manipulation. We evaluate these models based on their ability to generalize across environments, handle noisy sensory input, and perform efficiently on resource-constrained platforms. Our analysis reveals that while significant advancements have been made in replicating certain aspects of visual processing, current models often struggle with robustness and generalization compared to biological systems, particularly in dynamically changing and unpredictable environments. This highlights the need for further research into incorporating principles of active perception, predictive coding, and hierarchical reinforcement learning to bridge the gap between artificial and biological visual systems for embodied intelligence."
http://arxiv.org/abs/2302.12827v2,Decoupling Human and Camera Motion from Videos in the Wild,"Human motion analysis from video is often complicated by unconstrained camera movement, especially in ""in-the-wild"" scenarios. This work addresses the challenging problem of simultaneously estimating 3D human pose and 6DoF camera motion from monocular videos without relying on extensive training data or restrictive assumptions about the scene. We propose a novel two-stage framework. First, we leverage a pre-trained, off-the-shelf human pose estimator to obtain 2D joint detections and subsequently lift them to an initial 3D pose estimate. Second, we formulate a bundle adjustment problem that jointly optimizes for 3D human pose, camera trajectory, and per-frame scale ambiguity using a robust cost function that downweights outliers and incorporates temporal smoothness priors. Our approach demonstrates state-of-the-art performance on benchmark datasets, significantly improving pose estimation accuracy and camera trajectory estimation compared to existing methods, particularly in challenging scenarios with significant camera shake and occlusions. By decoupling human and camera motion in unconstrained videos, our method provides a valuable tool for a wide range of applications, including activity recognition, virtual reality, and autonomous navigation."
http://arxiv.org/abs/2302.04869v1,Reversible Vision Transformers,"Vision Transformers (ViTs) have achieved impressive results in various computer vision tasks, but their high memory consumption, particularly during training, remains a significant bottleneck. This paper addresses the problem of excessive memory requirements in ViTs by introducing Reversible Vision Transformers (RevViTs), a novel architecture based on reversible layers. RevViTs enable backpropagation without storing intermediate activations, dramatically reducing memory footprint. Our approach leverages a modified Swin Transformer block adapted for reversibility, ensuring information preservation during both forward and backward passes. We demonstrate that RevViTs achieve comparable performance to standard ViTs on ImageNet classification while reducing memory consumption by up to 70%. This improvement in memory efficiency unlocks the potential for training larger ViT models with limited resources and facilitates deployment on resource-constrained platforms."
http://arxiv.org/abs/2301.08247v1,Multiview Compressive Coding for 3D Reconstruction,"3D reconstruction from multiple views is a fundamental problem in computer vision with applications ranging from robotics to virtual reality. However, acquiring and processing dense multiview images can be computationally expensive, especially when dealing with high-resolution data. This paper addresses the challenge of efficient 3D reconstruction from a compressed set of multiview images. We propose a novel Multiview Compressive Coding (MVCC) framework that integrates compressive sensing principles directly into the 3D reconstruction pipeline. MVCC employs a learned, shared dictionary and sparse coding to represent multiview images in a compressed form. These compressed representations are then jointly decoded and utilized within a volumetric fusion framework to reconstruct the 3D scene. We demonstrate that MVCC achieves comparable or superior reconstruction quality to traditional methods while significantly reducing the data acquisition and processing requirements. Experimental results on both synthetic and real-world datasets show that MVCC can reconstruct high-quality 3D models from a substantially smaller number of measurements compared to conventional multiview stereo techniques, paving the way for more efficient and scalable 3D reconstruction systems."
http://arxiv.org/abs/2301.02232v2,CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image,"Articulated objects are prevalent in our daily lives, and understanding their 3D articulation from single images is crucial for robotic manipulation and augmented reality applications. However, existing methods typically focus on category-specific models, limiting their generalizability to novel object categories. This paper introduces CA$^2$T-Net, a Category-Agnostic Articulation Transfer Network, designed to transfer articulation knowledge from a source object to a target object, even when they belong to different categories. Our method leverages a novel part correspondence module based on self-supervised visual features to align the source and target objects. We then predict the target object's articulation parameters by transferring and refining the source object's articulation configuration, guided by the learned part correspondences. Experiments on both synthetic and real datasets demonstrate that CA$^2$T-Net significantly outperforms existing category-agnostic articulation estimation methods, achieving state-of-the-art results in terms of articulation parameter accuracy and visual plausibility. This novel approach enables robust 3D articulation understanding for a wider range of objects, paving the way for more versatile and adaptive robotic systems."
http://arxiv.org/abs/2212.08071v2,MAViL: Masked Audio-Video Learners,"Self-supervised learning has shown remarkable progress in both vision and audio domains independently. However, learning joint audio-visual representations remains a challenge, often requiring carefully designed architectures or complex training schemes. This paper addresses the problem of efficiently learning high-quality audio-visual representations from unlabeled data using a unified masking-based approach. We introduce MAViL, Masked Audio-Video Learners, a framework that extends masked autoencoding to the audio-visual domain. MAViL randomly masks both audio and video inputs and trains a shared encoder-decoder architecture to reconstruct the masked data. Crucially, MAViL leverages cross-modal attention mechanisms within the decoder to facilitate information transfer between modalities during reconstruction. Experiments on multiple audio-visual tasks, including audio-visual event recognition and sound source localization, demonstrate that MAViL achieves state-of-the-art performance compared to existing self-supervised methods, showcasing the effectiveness of our unified masking strategy. This work provides a simple yet powerful framework for learning robust audio-visual representations, advancing the field of multi-modal self-supervised learning."
http://arxiv.org/abs/2212.00922v1,Navigating to Objects in the Real World,"Visual navigation, the task of enabling autonomous agents to reach a target location using only visual input, is crucial for robots operating in complex real-world environments. However, achieving robust navigation to specific objects within these environments remains a significant challenge due to perceptual ambiguities, noisy sensor data, and the difficulty of generalizing across diverse visual scenes. We propose a novel end-to-end trainable framework, ObjectNav-Transformer (ONT), that combines a transformer-based visual encoder with a hierarchical reinforcement learning agent. ONT leverages self-attention mechanisms to learn contextual relationships between objects and landmarks in the scene, enabling more accurate object recognition and robust navigation policy learning. We further introduce a curriculum learning strategy that gradually increases the difficulty of navigation tasks, improving the agent's ability to generalize to unseen environments. Experimental results in photorealistic simulated environments demonstrate that ONT significantly outperforms existing state-of-the-art methods in terms of success rate and navigation efficiency, particularly in cluttered and visually challenging scenarios. This work advances the field of visual navigation by providing a more effective and generalizable approach for object-goal navigation in real-world settings."
http://arxiv.org/abs/2211.15876v1,Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances,"Image Goal Navigation (ImageNav) tasks embodied agents with navigating to a viewpoint where a specified object is visible. Existing ImageNav research primarily focuses on category-level goals, neglecting the more challenging problem of navigating to a specific instance of an object. This paper introduces Instance-Specific Image Goal Navigation (InstanceNav), where the agent must navigate to a target viewpoint depicting a particular instance of an object, identified by a reference image. We propose a novel hierarchical reinforcement learning framework that leverages a pre-trained vision-language model to decompose the InstanceNav task into high-level semantic goals and low-level navigation actions. Specifically, the high-level policy learns to predict a sequence of semantic waypoints representing relevant object categories, while the low-level policy navigates to these waypoints using visual features and depth information. Experimental results on a newly constructed InstanceNav benchmark, based on the Habitat simulator and the Replica dataset, demonstrate that our approach significantly outperforms baseline methods, achieving a success rate of 45% on unseen environments and object instances. This work highlights the potential of leveraging pre-trained vision-language models for complex embodied navigation tasks requiring fine-grained visual understanding."
http://arxiv.org/abs/2211.13225v1,Learning to Imitate Object Interactions from Internet Videos,"Learning manipulation skills from human demonstrations is a promising avenue for robot learning. However, acquiring sufficient expert demonstrations for diverse object interactions can be costly and time-consuming. This paper addresses the challenge of learning robotic manipulation policies for object interactions by leveraging the abundance of unlabeled human demonstrations available in internet videos. We propose a novel imitation learning framework that learns a latent action space from unlabelled video data via a contrastive video prediction objective, enabling the robot to generalize to new object instances and viewpoints. Specifically, we train a video encoder-decoder architecture to predict future video frames while simultaneously learning a compact action representation that captures the underlying manipulation dynamics. The learned action space is then used to train a robot control policy via behavior cloning, allowing the robot to imitate the observed object interactions. We demonstrate that our approach successfully learns complex manipulation skills such as stacking, placing, and pushing objects from internet videos, outperforming baseline methods that rely on hand-engineered features or direct imitation without latent action spaces. This work significantly reduces the need for expert demonstrations, paving the way for robots to learn a broader range of manipulation skills from readily available visual data."
http://arxiv.org/abs/2211.07638v1,Legged Locomotion in Challenging Terrains using Egocentric Vision,"Legged robots offer the potential for versatile navigation in complex environments inaccessible to wheeled or tracked vehicles. However, robust and adaptable locomotion on challenging terrains remains a significant hurdle, particularly when relying solely on onboard sensing. This paper addresses the problem of enabling legged robots to traverse highly variable and unstructured terrains using only egocentric vision. We propose a novel hierarchical control framework integrating visual terrain assessment with a reactive gait adaptation module. Our system first employs a deep convolutional neural network trained on synthetic and real-world data to estimate terrain properties (e.g., traversability, friction) from the robot's onboard camera feed. This information is then fed into a reinforcement learning-based gait controller, which dynamically adjusts foot placement and body posture to maintain stability and forward progress. We demonstrate our approach on a quadruped robot navigating a variety of challenging terrains, including rubble piles, uneven surfaces, and deformable ground, achieving a significant improvement in traversal success rate and robustness compared to traditional vision-agnostic control methods. This work advances the state-of-the-art in autonomous legged locomotion by demonstrating the feasibility of robust navigation in complex environments using solely egocentric visual perception."
http://arxiv.org/abs/2210.04887v1,In-Hand Object Rotation via Rapid Motor Adaptation,"Dexterous in-hand manipulation enables robots to interact with objects in complex and versatile ways, but remains a significant challenge due to the high dimensionality and underactuation of robotic hands. Achieving precise and robust object rotation is particularly difficult, requiring intricate coordination between multiple fingers and rapid adaptation to changing object dynamics. This paper addresses the problem of enabling a multi-fingered hand to rapidly adapt to perform controlled in-hand object rotation despite uncertainties in object properties and contact conditions. We propose a novel motor adaptation framework based on reinforcement learning, incorporating a physics-based simulator for efficient training and a learned residual policy to compensate for model inaccuracies and external disturbances. The residual policy is trained to refine open-loop control sequences generated from a simplified hand model, allowing for fast online adaptation without requiring extensive retraining. We demonstrate that our approach enables a physical hand to achieve accurate and stable object rotations under various conditions, including changes in object mass and friction. The results show a significant improvement in rotation accuracy and robustness compared to traditional open-loop control methods, highlighting the potential of learned motor adaptation for enhancing robotic dexterity."
http://arxiv.org/abs/2210.03109v1,Real-World Robot Learning with Masked Visual Pre-training,"Robot learning in real-world environments remains a significant challenge due to the scarcity of labeled data and the difficulty of generalizing from simulation to reality. This paper addresses the problem of efficiently learning robotic manipulation skills from limited real-world interactions by leveraging large-scale unlabeled visual data. We propose a novel approach that combines masked visual pre-training with reinforcement learning for robotic control. Specifically, we pre-train a visual encoder using a masked autoencoding objective on a large dataset of unlabeled videos, forcing the encoder to learn robust and generalizable visual representations. The pre-trained encoder is then fine-tuned within a reinforcement learning framework to learn robotic manipulation policies from a small number of real-world trials. Our experiments on a diverse set of robotic manipulation tasks demonstrate that our approach significantly improves sample efficiency and generalization performance compared to training from scratch or using alternative pre-training methods. This work demonstrates the potential of masked visual pre-training to accelerate robot learning and enable the deployment of robots in complex, unstructured environments."
http://arxiv.org/abs/2209.12892v1,Learning to Learn with Generative Models of Neural Network Checkpoints,"Neural network training is a complex optimization process, and understanding its dynamics can lead to more efficient and robust learning algorithms. However, directly manipulating the optimization trajectory of a network during training is challenging. This work addresses the problem of learning to control the training process by learning a generative model of neural network checkpoints. We propose a novel meta-learning framework where a variational autoencoder (VAE) is trained to generate realistic sequences of neural network checkpoints conditioned on task embeddings. The latent space of this generative model is then used to explore and manipulate training trajectories, allowing us to synthesize checkpoints with desired properties, such as improved generalization or faster convergence on new tasks. Experiments on image classification and few-shot learning benchmarks demonstrate that our approach can generate checkpoints that outperform standard training, exhibiting improved accuracy and robustness to noisy labels. This work opens up new possibilities for meta-learning and neural architecture search by providing a differentiable and controllable representation of the neural network training process."
http://arxiv.org/abs/2205.15299v2,Adapting Rapid Motor Adaptation for Bipedal Robots,"Bipedal robots operating in unstructured environments require the ability to rapidly adapt to unforeseen changes in dynamics, such as variations in payload or terrain. Traditional reinforcement learning methods often struggle with sample efficiency, hindering their applicability to real-world robotic systems where data collection is costly and time-consuming. This paper addresses the challenge of enabling rapid motor adaptation for bipedal robots in the presence of significant dynamic perturbations. We propose a novel adaptation framework that leverages a learned latent space of motor primitives coupled with a meta-learned adaptation policy. Specifically, we first train a variational autoencoder to create a compact, low-dimensional representation of successful walking gaits. Subsequently, we train a meta-learning agent to predict optimal adjustments within this latent space based on observed changes in robot dynamics, enabling fast adaptation without requiring retraining from scratch. Simulation results on a complex 3D bipedal robot demonstrate that our approach allows the robot to quickly recover stable walking behaviors after experiencing substantial changes in mass distribution and external disturbances, outperforming baseline methods in terms of adaptation speed and robustness. Our framework offers a promising direction for deploying bipedal robots in real-world scenarios where adaptability is crucial."
http://arxiv.org/abs/2204.06107v1,Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity,"Instance segmentation aims to simultaneously detect and segment each object instance in an image. However, most existing methods operate under a closed-world assumption, limiting their ability to identify and segment novel, unseen object categories. This paper addresses the challenging problem of open-world instance segmentation, where the goal is to segment both known and unknown objects without explicit supervision for the latter. Our approach leverages a learned pairwise affinity matrix between image pixels to generate pseudo ground truth masks for potential object instances. Specifically, we train a network to predict pixel affinities, use these affinities to cluster pixels into instance proposals, and then refine these proposals into high-quality pseudo masks by incorporating contextual information and enforcing mask consistency. We demonstrate that training a standard instance segmentation model using these pseudo labels, alongside labeled data for known classes, significantly improves performance on both known and unknown object categories. Experiments on COCO and a modified open-world setting demonstrate that our method achieves state-of-the-art results in open-world instance segmentation, highlighting its ability to effectively discover and segment novel object instances without requiring explicit annotations."
http://arxiv.org/abs/2203.06173v1,Masked Visual Pre-training for Motor Control,"Recent advancements in robot learning have demonstrated the effectiveness of imitation learning and reinforcement learning for motor control. However, these methods often require substantial task-specific data or extensive environment interaction, limiting their generalization capabilities. We address the challenge of learning robust visual representations for motor control that can generalize across diverse tasks and environments. We propose a novel masked visual pre-training (MVP) approach, where a visual encoder is trained to reconstruct masked image patches from observed robot states and actions. This encourages the encoder to learn feature representations that capture crucial visual information relevant to motor control, such as object positions, robot configurations, and scene dynamics. Our experiments across a suite of simulated robotic manipulation tasks demonstrate that pre-training with MVP significantly improves downstream task performance compared to training from scratch or using ImageNet pre-trained features. These results highlight the potential of self-supervised visual pre-training to enhance the efficiency and generalization ability of robot learning algorithms."
http://arxiv.org/abs/2202.05265v1,Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging,"Image-to-image regression is a fundamental task in computer vision, enabling applications such as image enhancement, style transfer, and medical image reconstruction. However, most existing methods provide only point estimates, lacking crucial information about the uncertainty associated with these predictions, particularly when dealing with noisy or ambiguous data. This paper addresses the challenge of providing reliable, distribution-free uncertainty quantification for image-to-image regression tasks. We introduce a novel framework that leverages conformal prediction to construct prediction sets for each output pixel, guaranteeing a user-defined coverage probability without making assumptions about the underlying data distribution or model architecture. Our method uses a deep neural network to predict pixel-wise location and scale parameters, which are then used to construct prediction intervals based on a nonconformity measure. We demonstrate the effectiveness of our approach on synthetic and real-world datasets, including denoising and super-resolution, showing that our method achieves accurate coverage while providing informative uncertainty estimates. This framework offers a robust and practical solution for incorporating uncertainty awareness into image-to-image regression, enhancing the reliability and interpretability of downstream applications."
http://arxiv.org/abs/2201.10029v2,PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning,"ObjectGoal Navigation, the task of navigating to a specified object instance, is a core challenge in embodied AI. Traditional approaches often rely on extensive interaction with the environment for exploration and learning suitable navigation policies. This work addresses the problem of learning effective ObjectGoal Navigation policies without requiring any interaction with the environment during training. We introduce PONI (Potential functions for ObjectGoal Navigation with Interaction-free learning), a novel approach that leverages pre-trained vision-language models (VLMs) to generate potential functions guiding navigation. Specifically, we utilize VLMs to estimate the semantic similarity between the agent's egocentric view and the target object, and then transform this similarity into a navigation potential field. This potential field is used to train a navigation policy in simulation via imitation learning, eliminating the need for environment interaction during training. Our experiments demonstrate that PONI significantly outperforms existing interaction-free navigation methods and achieves comparable performance to some interaction-based methods, despite the absence of online environment interaction. PONI provides a promising direction for developing robust and efficient navigation agents, reducing the reliance on costly and time-consuming environment interaction."
http://arxiv.org/abs/2201.08383v2,MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition,"Long-term video recognition demands processing extensive temporal information, posing significant computational challenges for existing video models. Current Transformer-based approaches often struggle with the quadratic complexity associated with self-attention over long sequences. To address this, we introduce MeMViT, a Memory-augmented Multiscale Vision Transformer designed for efficient and accurate long-term video understanding. MeMViT incorporates a learnable memory module to store and retrieve relevant past features, effectively summarizing long-range dependencies. Furthermore, a multiscale architecture processes video at different temporal resolutions, enabling the model to capture both fine-grained details and coarse-grained contextual information. We evaluate MeMViT on several challenging long-term video recognition datasets, including Something-Something V2 and HACS Segments, demonstrating significant improvements in accuracy and efficiency compared to state-of-the-art methods. Our results highlight MeMViT's ability to effectively model long-range dependencies while maintaining computational feasibility, paving the way for more practical long-term video analysis applications."
http://arxiv.org/abs/2112.04477v1,"Tracking People by Predicting 3D Appearance, Location & Pose","Multi-person tracking in complex scenes remains a challenging problem due to occlusions, appearance similarities, and variations in pose. We address the problem of robust multi-person tracking by jointly predicting and associating 3D appearance, location, and pose features. Our approach employs a recurrent neural network to model the temporal evolution of each person, predicting future states based on past observations. Specifically, we learn a latent space that encodes 3D human mesh parameters, location in the scene, and appearance embeddings, enabling us to predict these cues jointly. We then formulate a novel association cost function that leverages the predicted 3D appearance, location, and pose features to link detections across frames. We demonstrate state-of-the-art performance on the challenging 3D MOT17 and HiEve datasets, achieving significant improvements in both tracking accuracy and identity preservation. Our method provides a more comprehensive and robust representation for tracking people, leading to enhanced performance in crowded and occluded environments."
http://arxiv.org/abs/2112.02094v2,Coupling Vision and Proprioception for Navigation of Legged Robots,"Legged robots offer unparalleled mobility in complex terrains, but their navigation often relies solely on proprioceptive feedback, limiting performance in visually rich environments. This paper addresses the challenge of robust and efficient navigation for legged robots by tightly coupling visual and proprioceptive information. We propose a novel framework that fuses visual features extracted from onboard cameras with proprioceptive data from joint encoders and inertial measurement units within a deep neural network architecture. Specifically, we employ a recurrent neural network with attention mechanisms to learn a latent representation that integrates both modalities, enabling the robot to predict optimal control actions for locomotion and path planning. We validate our approach through extensive simulations and real-world experiments on a quadruped robot navigating diverse terrains, demonstrating significant improvements in navigation accuracy, robustness to sensor noise, and adaptability to unforeseen obstacles compared to proprioception-only and loosely coupled approaches. Our results highlight the importance of synergistic sensor fusion for achieving reliable and versatile navigation capabilities in legged robots operating in complex and unstructured environments."
http://arxiv.org/abs/2112.01526v2,MViTv2: Improved Multiscale Vision Transformers for Classification and Detection,"Vision Transformers (ViTs) have shown promising results in various computer vision tasks, rivaling convolutional neural networks (CNNs). However, capturing multi-scale representations efficiently remains a challenge, especially for dense prediction tasks like object detection and segmentation. This paper introduces MViTv2, an improved multiscale vision transformer building upon the MViT architecture to enhance feature aggregation and computational efficiency. We propose a novel attention mechanism, termed ""Attentive Pooling"", which selectively aggregates features across different scales based on learned attention weights, replacing the uniform pooling strategy in MViT. Furthermore, we introduce a simplified positional encoding scheme and optimized layer normalization placement for improved training stability and performance. Experiments on ImageNet classification demonstrate that MViTv2 achieves state-of-the-art accuracy with fewer parameters and FLOPs compared to previous ViT architectures. Extensive experiments on object detection and instance segmentation benchmarks using COCO show significant improvements over MViT and other competitive baselines, highlighting the effectiveness of MViTv2 in capturing multi-scale information for both classification and dense prediction tasks. These results establish MViTv2 as a strong backbone network for a wide range of computer vision applications."
http://arxiv.org/abs/2112.01010v1,Differentiable Spatial Planning using Transformers,"Spatial planning is a fundamental problem in robotics, requiring agents to navigate complex environments while satisfying constraints. Traditional planning algorithms often rely on discrete representations and are not amenable to gradient-based optimization, hindering their integration with learned perception modules. This paper addresses the challenge of creating a differentiable spatial planning framework that can be trained end-to-end. We introduce a novel approach that leverages Transformer architectures to predict a continuous, cost-aware occupancy map representing the planned trajectory. Our method uses a Transformer encoder to process scene information and a decoder to iteratively refine a spatial plan represented as a probability distribution over possible locations. The entire architecture is differentiable, allowing for direct optimization of planning parameters with respect to task-specific rewards. We demonstrate that our method outperforms traditional planning algorithms and other learning-based approaches in simulated navigation tasks, showcasing improved success rates and shorter path lengths. This work opens up new avenues for incorporating spatial reasoning into differentiable perception and control pipelines."
http://arxiv.org/abs/2112.01001v1,SEAL: Self-supervised Embodied Active Learning using Exploration and 3D Consistency,"Self-supervised learning holds immense promise for training embodied agents, yet effective exploration strategies and robust representations remain critical challenges. We address the problem of efficiently learning visual representations for embodied navigation in novel environments without human annotations. We introduce SEAL, a Self-supervised Embodied Active Learning framework that leverages exploration guided by uncertainty and 3D geometric consistency. SEAL employs a recurrent neural network agent trained with a novel self-supervised objective that encourages consistent 3D scene understanding across different viewpoints. The agent actively explores the environment, selecting actions that maximize prediction uncertainty of the 3D reconstruction. We evaluate SEAL on the Matterport3D dataset, demonstrating significant improvements in downstream navigation tasks and unsupervised depth estimation compared to existing exploration strategies and self-supervised representation learning methods. Our results show that active exploration driven by 3D consistency leads to the acquisition of more robust and generalizable visual representations for embodied agents. This work paves the way for developing autonomous agents capable of efficiently learning from unlabeled visual data in complex environments."
http://arxiv.org/abs/2111.07868v1,Tracking People with 3D Representations,"Accurate and robust tracking of people is crucial for a wide range of applications, including autonomous navigation, video surveillance, and human-robot interaction. Existing methods often struggle with occlusions, pose variations, and scale changes, particularly in crowded or dynamic environments. To address these challenges, we propose a novel approach for tracking people that leverages explicit 3D representations. Our method integrates multi-view image observations to reconstruct a dynamic 3D human model, parameterized by a skeletal pose and a learned non-rigid deformation field. This 3D representation is then tracked over time using a particle filter, where each particle represents a hypothesis of the human pose and shape. We evaluate our approach on several challenging benchmark datasets, demonstrating significant improvements in tracking accuracy and robustness compared to state-of-the-art 2D and 3D tracking methods, particularly in scenarios with frequent occlusions and complex human motion. This work demonstrates the effectiveness of incorporating 3D representations for robust and accurate multi-person tracking in complex scenes."
http://arxiv.org/abs/2111.01674v1,Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots,"Legged robots hold immense potential for navigating complex and unstructured environments, yet their energy inefficiency remains a significant hurdle for widespread adoption. This paper addresses the challenge of automatically discovering energy-efficient locomotion strategies for legged robots without relying on hand-engineered controllers or pre-defined gait patterns. We propose a novel reinforcement learning framework where the reward function is solely based on minimizing the robot's energy consumption, augmented with a minimal survival bonus to encourage exploration. Our approach uses a deep neural network policy to directly map the robot's state to joint torques, enabling the discovery of complex and dynamic gaits. Through extensive simulations on a quadrupedal robot, we demonstrate the emergence of distinct and recognizable gait patterns, including trotting and bounding, solely driven by the objective of energy minimization. These learned gaits exhibit significantly lower energy consumption compared to manually designed alternatives, highlighting the potential of energy-centric reinforcement learning to unlock efficient and adaptive locomotion in legged robotics."
http://arxiv.org/abs/2110.07058v3,"Ego4D: Around the World in 3,000 Hours of Egocentric Video","Egocentric (first-person) vision offers a unique perspective for understanding human behavior and interaction with the world. However, the development of robust and generalizable egocentric perception models has been hampered by the limited scale and diversity of existing datasets. This paper introduces Ego4D, a massive-scale egocentric video dataset comprising over 3,000 hours of video captured by nearly 900 unique camera wearers across a wide range of daily activities and environments in diverse geographic locations. Ego4D is designed to address key challenges in egocentric perception by providing synchronized, calibrated multi-camera recordings, detailed annotations for a variety of tasks, and a strong emphasis on geographic and demographic diversity. We achieve this scale through a coordinated global effort leveraging a standardized data collection protocol and rigorous quality control measures. Initial experiments demonstrate the dataset's utility for training and evaluating models for tasks such as hand-object interaction detection, audio-visual diarization, and long-term activity forecasting, revealing significant performance gains when models are trained on Ego4D compared to existing datasets. Ego4D represents a significant leap forward in the scale and diversity of egocentric video data, enabling the development of more capable and generalizable AI systems for understanding human behavior in the first person."
http://arxiv.org/abs/2110.06199v2,ABO: Dataset and Benchmarks for Real-World 3D Object Understanding,"Real-world 3D object understanding is crucial for enabling robots to interact effectively with their environment. Existing 3D object datasets often lack the complexity and diversity found in real-world scenarios, hindering the development of robust algorithms. This paper introduces ABO, a novel dataset and benchmark suite designed to address the limitations of current datasets for real-world 3D object understanding. ABO comprises a large collection of high-quality 3D scans of diverse objects captured in cluttered, unstructured environments, accompanied by comprehensive annotations including instance segmentation, semantic labels, and physical properties. We propose a benchmark suite encompassing tasks such as object detection, pose estimation, and affordance prediction, providing a standardized evaluation framework for algorithms developed using ABO. Experiments demonstrate that models trained on ABO exhibit significantly improved generalization performance on real-world robotic manipulation tasks compared to those trained on existing datasets. The ABO dataset and benchmark will facilitate advancements in 3D perception and enable robots to better understand and interact with complex, real-world environments."
http://arxiv.org/abs/2110.05472v2,Differentiable Stereopsis: Meshes from multiple views using differentiable rendering,"Stereopsis, the recovery of 3D structure from multiple views, remains a fundamental challenge in computer vision. Existing methods often struggle with textureless regions, occlusions, and require extensive post-processing. This paper addresses the problem of directly reconstructing high-quality 3D meshes from multi-view images in an end-to-end differentiable manner. We propose a novel differentiable stereopsis framework that leverages differentiable rendering to optimize a mesh directly against the input images. Our approach initializes a coarse mesh and iteratively refines its geometry and texture by minimizing a photometric loss computed between the rendered images and the input views, incorporating regularization terms to ensure smooth and plausible surfaces. We demonstrate that our method can recover detailed 3D meshes from a small number of calibrated views, even in challenging scenarios with limited texture and significant occlusions, outperforming state-of-the-art multi-view stereo techniques in terms of accuracy and completeness. This differentiable approach unlocks new possibilities for incorporating learned priors and constraints, leading to more robust and accurate 3D reconstruction."
http://arxiv.org/abs/2110.04994v1,Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans,"Multi-task learning in computer vision holds the promise of improving generalization and sample efficiency by sharing representations across related tasks. However, the creation of large-scale, diverse datasets with dense, per-pixel annotations for multiple mid-level vision tasks remains a significant bottleneck. We address this challenge by introducing Omnidata, a scalable pipeline for generating multi-task datasets from readily available 3D scan data. Our approach leverages differentiable rendering and physically-based simulation to automatically generate ground truth for a suite of tasks including surface normal estimation, semantic segmentation, material classification, and object distance prediction. We demonstrate the effectiveness of Omnidata by creating a large synthetic dataset of indoor scenes and training a single multi-task model that achieves state-of-the-art performance on several of these tasks when evaluated on real-world data, outperforming models trained on task-specific datasets. This work significantly lowers the barrier to entry for multi-task learning in mid-level vision, facilitating the development of more robust and generalizable perception systems."
http://arxiv.org/abs/2107.09584v2,Active 3D Shape Reconstruction from Vision and Touch,"Reconstructing accurate 3D shapes of objects remains a fundamental challenge in computer vision, often limited by occlusions and textureless surfaces. This paper addresses the problem of incomplete and noisy 3D shape reconstruction from visual data by actively integrating tactile feedback. We propose a novel active reconstruction framework that combines a deep implicit surface representation with a reinforcement learning agent. The agent strategically selects viewpoints and tactile probing locations to maximize information gain about the object's underlying geometry. A multi-modal fusion module then integrates visual and tactile data to refine the implicit surface representation, iteratively improving the reconstructed shape. We demonstrate through extensive simulations that our method significantly outperforms vision-only approaches, especially in challenging scenarios with significant occlusions and noisy sensor data, achieving up to a 40% reduction in reconstruction error. Our work highlights the crucial role of active exploration and multi-modal fusion for robust and accurate 3D shape reconstruction."
http://arxiv.org/abs/2107.04034v1,RMA: Rapid Motor Adaptation for Legged Robots,"Legged robots hold immense potential for navigating complex and unstructured environments, but their deployment is often hindered by the time-consuming and computationally expensive process of adapting control policies to new terrains or robot morphologies. Existing approaches typically rely on extensive reinforcement learning or system identification, requiring substantial real-world data or accurate models, which are often unavailable or impractical. We introduce Rapid Motor Adaptation (RMA), a novel framework that leverages meta-learning and a learned residual policy to quickly adapt a pre-trained base policy to novel conditions. RMA employs a fast adaptation module that estimates the optimal residual motor commands based on a small number of observations from the new environment, effectively compensating for discrepancies between the base policy's assumptions and the current reality. Experiments on a quadruped robot in simulation demonstrate that RMA achieves significant performance improvements in terms of speed, stability, and energy efficiency compared to the base policy alone, and outperforms alternative adaptation strategies, converging to near-optimal performance within just a few steps. This rapid adaptation capability significantly reduces the barrier to deploying legged robots in diverse and unpredictable scenarios."
http://arxiv.org/abs/2104.11227v1,Multiscale Vision Transformers,"Vision Transformers (ViTs) have demonstrated remarkable performance in image recognition tasks, often surpassing convolutional neural networks (CNNs) in accuracy and robustness. However, standard ViTs typically operate on a single, fixed scale of the input image, limiting their ability to effectively capture both fine-grained details and global contextual information. This paper addresses the challenge of efficiently representing and processing visual information across multiple scales within the Transformer architecture. We introduce Multiscale Vision Transformers (MViTs), which incorporate a novel hierarchical structure that progressively merges tokens and learns representations at multiple resolutions. MViTs achieve this by employing learnable pooling operations that dynamically adapt the receptive field and reduce the sequence length, enabling the model to capture long-range dependencies at coarser scales while preserving fine-grained features at finer scales. Experiments on ImageNet classification and object detection benchmarks demonstrate that MViTs achieve state-of-the-art performance with significantly reduced computational complexity compared to existing ViT architectures. The proposed MViT architecture provides a powerful and efficient framework for learning robust and scalable visual representations, paving the way for broader adoption of Transformers in various computer vision applications."
http://arxiv.org/abs/2101.02703v3,"Distribution-Free, Risk-Controlling Prediction Sets","Prediction sets, which aim to contain the true label with a user-specified probability, are crucial for reliable decision-making in safety-critical applications of computer vision. However, many existing methods rely on strong distributional assumptions or require retraining, limiting their applicability in complex, real-world scenarios where distributions may shift or labeled data is scarce. This paper addresses the problem of constructing distribution-free prediction sets that provably control the false coverage rate without requiring distributional assumptions or retraining. We propose a novel framework based on Mondrian conformal prediction coupled with a risk-controlling selective classification approach. Our method leverages a pre-trained model and a small calibration dataset to adaptively determine the size and composition of the prediction set, ensuring coverage guarantees while minimizing set size. Experiments on image classification and object detection tasks demonstrate that our method achieves state-of-the-art coverage and efficiency compared to existing distribution-free approaches, even under significant distribution shifts. These results highlight the practical utility of our method for deploying robust and reliable computer vision systems in dynamic environments."
http://arxiv.org/abs/2012.09856v2,Reconstructing Hand-Object Interactions in the Wild,"Estimating accurate 3D hand and object poses, along with their interactions, is crucial for understanding human manipulation skills and enabling realistic human-computer interaction. However, reconstructing hand-object interactions from monocular videos in unconstrained, ""in the wild"" scenarios remains a significant challenge due to occlusions, ambiguities in depth estimation, and the complexity of modeling intricate hand-object relationships. We propose a novel framework, HOI-Wild, that leverages a transformer-based architecture to jointly reason about hand and object poses, incorporating contextual cues from the entire video sequence. Specifically, our method employs a temporal transformer to aggregate features across frames, allowing the network to resolve ambiguities arising from occlusions and noisy detections in individual frames. Furthermore, we introduce a novel interaction-aware loss that encourages the model to learn physically plausible hand-object relationships. We demonstrate state-of-the-art performance on challenging benchmark datasets, significantly improving hand and object pose estimation accuracy, especially in scenarios with heavy occlusions and complex interactions. This robust reconstruction of hand-object interactions in the wild paves the way for more realistic and interactive virtual environments and a deeper understanding of human dexterity."
http://arxiv.org/abs/2012.09843v1,Human Mesh Recovery from Multiple Shots,"Human mesh recovery, estimating a 3D human pose and shape from images, is a fundamental problem in computer vision with applications in animation, virtual reality, and human-computer interaction. Single-view methods often suffer from depth ambiguities and occlusions, limiting their accuracy and robustness. This paper addresses the challenge of improving human mesh recovery by leveraging information from multiple calibrated cameras observing the same scene. We propose a novel multi-view mesh recovery framework that combines 2D pose estimations from multiple views with a learned shape prior to generate an initial 3D mesh. Subsequently, we refine this mesh through a differentiable rendering-based optimization that minimizes photometric and silhouette inconsistencies across all views, while simultaneously regularizing the mesh deformation to maintain anatomical plausibility. Our method achieves state-of-the-art results on benchmark multi-view datasets, demonstrating significant improvements in mesh accuracy and robustness compared to single-view and existing multi-view approaches. The proposed framework offers a robust and accurate solution for capturing detailed 3D human representations, paving the way for more realistic and immersive applications."
http://arxiv.org/abs/2012.01526v1,"From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting","Human trajectory forecasting is crucial for autonomous systems operating in human-populated environments, enabling safe and efficient navigation. However, accurately predicting human movement over extended horizons remains a significant challenge due to the inherent uncertainty and multi-modal nature of human behavior, often influenced by long-term goals and environmental constraints. This paper introduces a novel hierarchical framework for long-term human trajectory forecasting that explicitly reasons about goals, waypoints, and paths. Our approach first infers potential goals based on scene context and observed trajectory, then generates plausible waypoints leading to those goals, and finally, refines these waypoints into smooth, realistic trajectories using a learned path generator conditioned on both the environment and the predicted waypoints. We demonstrate significant improvements in long-term prediction accuracy and diversity on several benchmark datasets, outperforming state-of-the-art methods in terms of prediction error and goal adherence. This work offers a more interpretable and robust approach to long-term trajectory forecasting by disentangling the complex decision-making process inherent in human movement."
http://arxiv.org/abs/2011.06698v1,Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation,"Reinforcement learning (RL) offers a promising paradigm for training robotic agents to perform complex tasks, but often struggles with generalization to novel environments and robustness to perceptual variations. This paper addresses the challenge of learning robust policies for manipulation and navigation tasks by leveraging mid-level visual representations extracted from pre-trained convolutional neural networks (CNNs). We propose a framework that decouples visual perception from policy learning, using CNN activations from intermediate layers as a compact and informative state representation for RL. We experimentally evaluate this approach on a suite of simulated robotic manipulation and navigation tasks with varying levels of visual distractors, domain randomization, and task complexity. Our results demonstrate that policies trained using mid-level visual representations exhibit significantly improved generalization and robustness compared to policies trained directly on raw pixel inputs or high-level object-centric features, particularly in unseen environments and under noisy perceptual conditions. This indicates that mid-level visual features provide a more stable and transferable basis for learning robust robotic control policies, paving the way for more reliable real-world deployment."
http://arxiv.org/abs/2011.01975v1,Rearrangement: A Challenge for Embodied AI,"Embodied AI strives to create agents that can intelligently interact with and manipulate their environments. While significant progress has been made in navigation and object manipulation, the more complex task of rearrangement, involving intricate planning and precise execution to organize objects into a desired configuration, remains a significant hurdle. This paper addresses the challenge of developing embodied agents capable of robust and efficient rearrangement in cluttered and dynamic environments. We introduce a hierarchical reinforcement learning framework that decomposes the rearrangement task into a high-level planner, which selects target objects and placement locations, and a low-level controller, responsible for executing pick-and-place actions. The planner is trained using a novel curriculum learning strategy that gradually increases the complexity of the rearrangement scenarios, while the controller leverages pre-trained visual representations to improve generalization. Experimental results in simulated environments demonstrate that our approach outperforms existing methods in terms of success rate and task completion time, particularly in scenarios with a large number of objects and significant clutter. This work highlights the importance of hierarchical planning and curriculum learning for tackling complex embodied AI tasks and paves the way for more capable robotic agents in real-world applications."
http://arxiv.org/abs/2010.03592v1,"Shape, Illumination, and Reflectance from Shading","Recovering 3D shape, illumination, and reflectance from a single image remains a fundamental challenge in computer vision. This paper addresses the ill-posed inverse problem of disentangling shape, illumination, and spatially-varying reflectance from shading cues. We propose a novel deep learning framework that leverages a physics-based rendering loss combined with shape priors learned from a large dataset of 3D models. Our network architecture consists of three modules: a shape estimation network predicting surface normals, an illumination estimation network predicting spherical harmonics coefficients, and a reflectance estimation network predicting albedo maps. We demonstrate state-of-the-art performance on benchmark datasets, significantly improving reconstruction accuracy and albedo estimation compared to existing methods, especially in challenging scenarios with complex illumination and reflectance variations. This work provides a robust and accurate approach for intrinsic image decomposition, enabling more realistic and interpretable 3D scene understanding from monocular images."
http://arxiv.org/abs/2009.14193v5,Uncertainty Sets for Image Classifiers using Conformal Prediction,"Image classifiers are widely deployed in safety-critical applications, necessitating reliable measures of their predictive uncertainty. However, standard softmax outputs often provide poorly calibrated confidence scores. This paper addresses the problem of constructing well-calibrated uncertainty sets for image classifiers that provably contain the true class with a user-specified probability. We propose a novel conformal prediction framework tailored to image classification, leveraging a nonconformity measure based on a combination of softmax probabilities and feature-space distances to generate adaptive and efficient uncertainty sets. Our approach dynamically adjusts set sizes based on the input image, leading to tighter sets for easier examples and larger sets for ambiguous inputs. Experiments on benchmark datasets demonstrate that our method achieves state-of-the-art coverage rates, closely matching the desired confidence level, while simultaneously producing significantly smaller and more informative uncertainty sets compared to existing conformal prediction techniques. This improved uncertainty quantification enables more robust and trustworthy decision-making in real-world image classification tasks."
http://arxiv.org/abs/2008.02265v5,Learning Long-term Visual Dynamics with Region Proposal Interaction Networks,"Predicting future visual dynamics is crucial for intelligent agents to interact with the world. Existing methods often struggle with long-term predictions due to the compounding errors arising from pixel-level generation and the difficulty in modeling complex object interactions. This paper addresses the challenge of long-term visual dynamics prediction by introducing Region Proposal Interaction Networks (RPI-Nets). RPI-Nets operate on detected region proposals, enabling the modeling of object-centric dynamics and interactions. Specifically, we learn latent representations of region proposals, model their interactions through a graph neural network, and predict future states by propagating information across the graph and decoding back to visual features. Experiments on both synthetic and real-world datasets demonstrate that RPI-Nets significantly outperform existing approaches, particularly in long-term prediction horizons, achieving improvements of up to 20% in FVD score on the BAIR dataset. This demonstrates the effectiveness of our object-centric approach and highlights the importance of modeling object interactions for accurate and coherent long-term visual dynamics prediction."
http://arxiv.org/abs/2007.15649v2,Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild,"Understanding human-object interactions is crucial for scene understanding and robotic manipulation. Estimating the 3D spatial arrangement of humans and objects from a single image, particularly in unconstrained, real-world scenarios, remains a significant challenge. This paper addresses the problem of inferring the full 3D pose of a human and the 3D location, size, and orientation of interacted objects from a single RGB image, without relying on controlled environments or synthetic data. We propose a novel framework that leverages a two-stage approach: first, a human pose estimation network predicts 2D human pose and object bounding boxes; then, a transformer-based architecture reasons about the geometric relationships between the human and object, predicting 3D human pose, 3D object bounding boxes, and their relative spatial arrangement. Our method incorporates a learned contextual embedding to capture the complex dependencies between human pose, object properties, and scene context. We demonstrate state-of-the-art performance on the challenging 3D-HOI dataset and show significant improvements in generalization to unseen real-world images. This work provides a step towards robust and accurate 3D scene understanding from monocular images, enabling applications in robotics, augmented reality, and human-computer interaction."
http://arxiv.org/abs/2007.10982v1,Shape and Viewpoint without Keypoints,"Reconstructing 3D shape and camera viewpoint from 2D images is a fundamental problem in computer vision, typically tackled using keypoint correspondences or silhouette-based approaches. However, keypoint detectors often fail in textureless regions or under significant viewpoint changes, while silhouette methods struggle with complex geometries. This paper introduces a novel framework for simultaneous shape and viewpoint estimation that bypasses the need for explicit keypoint detection or silhouette extraction. Our approach leverages differentiable rendering to directly optimize a neural implicit surface representation and camera parameters by minimizing the photometric error between the rendered image and the input image. We introduce a multi-scale image loss and a novel regularization term based on surface normal consistency to improve reconstruction accuracy and robustness. We demonstrate state-of-the-art performance on synthetic and real datasets, showing accurate shape reconstruction and viewpoint estimation even in the presence of significant occlusions and challenging lighting conditions. Our keypoint-free and silhouette-free approach provides a more robust and versatile solution for 3D reconstruction from single images, opening new avenues for applications in robotics, augmented reality, and 3D content creation."
http://arxiv.org/abs/2007.03778v2,3D Shape Reconstruction from Vision and Touch,"3D shape reconstruction is a fundamental problem in computer vision, typically addressed using visual data. However, vision alone can be insufficient due to occlusions, lack of texture, and challenging lighting conditions. This paper addresses the problem of improving 3D shape reconstruction by fusing visual information with tactile sensing. We propose a novel framework that leverages both RGB-D images and tactile measurements from a multi-fingered robotic hand to iteratively refine an initial shape estimate. Our method employs a differentiable rendering module to project the current shape estimate into the visual domain, allowing for gradient-based optimization using image-based losses. Simultaneously, we incorporate tactile feedback via a signed distance function learned from tactile measurements, which penalizes deviations from the perceived contact surface. We demonstrate that our approach significantly improves the accuracy and completeness of 3D reconstructions compared to vision-only methods, particularly in scenarios with significant occlusions and limited visual texture. This integrated vision-touch approach offers a more robust and reliable solution for 3D shape understanding, enabling applications in robotics, manufacturing, and virtual reality."
http://arxiv.org/abs/2007.03672v3,Long-term Human Motion Prediction with Scene Context,"Human motion prediction is a fundamental problem with applications in robotics, surveillance, and autonomous driving. Accurately forecasting long-term human trajectories remains challenging due to the inherent uncertainty of human behavior and the difficulty in modeling interactions with the surrounding environment. We address the problem of long-term human motion prediction by explicitly incorporating scene context into a recurrent neural network framework. Our approach, Context-Aware Trajectory Prediction (CATP), leverages a graph neural network to encode spatial relationships between scene objects and a novel attention mechanism to dynamically weigh the influence of these objects on future human motion. Furthermore, we introduce a hierarchical variational autoencoder to capture the multi-modal nature of human trajectories. Experimental results on benchmark datasets, including the ETH, UCY, and SDD datasets, demonstrate that CATP significantly outperforms state-of-the-art methods in long-term prediction accuracy, particularly in scenarios with complex scene layouts and human-scene interactions. This work highlights the importance of scene context in improving the realism and accuracy of long-term human motion forecasting."
http://arxiv.org/abs/2006.16992v2,Deep Isometric Learning for Visual Recognition,"Visual recognition tasks often suffer from variations in viewpoint, pose, and articulation, leading to significant intra-class variations. This paper addresses the challenge of learning robust visual representations that are invariant to isometric transformations. We introduce Deep Isometric Learning (DIL), a novel framework that explicitly encourages the learned features to be invariant to a set of predefined isometric transformations. DIL employs a Siamese network architecture trained with a novel isometric loss function. This loss minimizes the feature distance between different isometric views of the same object while maximizing the distance between features of different objects, thereby embedding objects onto a more isometry-invariant manifold. We demonstrate the effectiveness of DIL on several benchmark datasets, including ModelNet40, SHREC15, and a synthetic dataset of articulated objects, showing significant improvements in classification accuracy and robustness to isometric variations compared to state-of-the-art methods. Our work offers a principled approach for learning isometry-invariant features, paving the way for more robust and generalizable visual recognition systems."
http://arxiv.org/abs/2006.04096v1,Robust Learning Through Cross-Task Consistency,"Deep learning models often struggle with generalization when faced with distribution shifts or noisy labels in real-world applications. This paper addresses the problem of improving model robustness by leveraging the inherent consistency between related computer vision tasks. We propose a novel training paradigm, Cross-Task Consistency (CTC), which encourages a shared feature representation to produce consistent predictions across multiple auxiliary tasks derived from the primary task. Specifically, CTC minimizes the discrepancy between predictions generated by the shared feature extractor for different task-specific heads, effectively enforcing a form of multi-task regularization. Our experiments on image classification with corrupted labels and domain adaptation benchmarks demonstrate that CTC significantly improves robustness against label noise and domain shifts compared to standard training and existing regularization techniques. These improvements highlight the effectiveness of exploiting cross-task consistency for learning more robust and generalizable representations."
http://arxiv.org/abs/2004.03355v3,Inclusive GAN: Improving Data and Minority Coverage in Generative Models,"Generative Adversarial Networks (GANs) have shown remarkable success in generating realistic images, but often struggle with datasets exhibiting class imbalance and under-representation of minority groups, leading to biased and limited generation capabilities. This work addresses the problem of improving both overall data coverage and specifically enhancing the representation of minority groups within GAN-generated data. We introduce Inclusive GAN (IGAN), a novel training strategy that combines a re-weighted loss function focusing on under-represented samples with a diversity-promoting regularizer based on feature manifold analysis. IGAN encourages the generator to explore and populate regions of the data space that are currently poorly represented in both the training data and initial generator outputs. Experimental results on benchmark datasets with varying degrees of class imbalance demonstrate that IGAN significantly improves both Frchet Inception Distance (FID) scores and minority class generation quality, as measured by per-class FID and recall. These improvements highlight the potential of IGAN to create more equitable and comprehensive generative models, leading to fairer and more robust downstream applications."
http://arxiv.org/abs/2004.03590v1,Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood Estimation,"Multimodal image synthesis aims to generate diverse and realistic images conditioned on input modalities such as text or semantic layouts. Current methods often struggle with mode collapse and generating high-fidelity images that accurately reflect the nuances of the conditioning input. To address these limitations, we propose a novel framework, Conditional Implicit Maximum Likelihood Estimation (CIMLE), which leverages an implicit generative model trained with a conditional variant of Maximum Likelihood Estimation. CIMLE avoids explicit density estimation, allowing for greater flexibility in modeling complex multimodal distributions. Specifically, we train a generator network to match the distribution of real images conditioned on the input modality by minimizing a learned discrepancy measure between generated and real image features. Our experiments on text-to-image synthesis and semantic image synthesis demonstrate that CIMLE outperforms state-of-the-art methods in terms of both image quality and diversity, achieving significant improvements in FID and LPIPS scores. This approach offers a robust and effective solution for multimodal image synthesis, paving the way for more controllable and realistic image generation."
http://arxiv.org/abs/2004.02025v3,It Is Not the Journey but the Destination: Endpoint Conditioned Trajectory Prediction,"Trajectory prediction is a crucial component in various applications, including autonomous driving and social robotics. While many existing approaches focus on predicting intermediate states based on observed motion patterns, they often lack explicit mechanisms to ensure accurate arrival at a desired destination. This paper addresses the challenge of predicting trajectories that reliably reach specific endpoint conditions, such as a target location or a particular orientation. We introduce a novel endpoint-conditioned trajectory prediction framework that leverages a conditional variational autoencoder (CVAE) architecture. Our model learns a latent space of trajectories conditioned on the desired endpoint, enabling the generation of diverse and plausible paths terminating at the specified goal. A key component is a custom loss function that encourages both accurate endpoint arrival and smooth trajectory generation, incorporating terms for endpoint proximity, velocity alignment, and trajectory smoothness. We demonstrate the effectiveness of our approach on both synthetic and real-world datasets, showing significant improvements in endpoint accuracy and trajectory realism compared to state-of-the-art methods. This work offers a promising direction for trajectory prediction by explicitly incorporating endpoint constraints, leading to more reliable and predictable behavior in dynamic environments."
http://arxiv.org/abs/2001.08740v2,Audiovisual SlowFast Networks for Video Recognition,"Human activity recognition in videos often relies on visual cues, but sound provides complementary information that can improve performance, especially in ambiguous visual scenarios. However, effectively integrating auditory information with deep visual features remains a challenge. This paper introduces Audiovisual SlowFast Networks, an extension of the SlowFast architecture, designed to explicitly model both semantic and temporal aspects of audiovisual data for improved video recognition. Our model incorporates a parallel audio pathway operating at a high frame rate to capture fine-grained auditory changes, and fuses these auditory features with the Slow and Fast visual pathways at multiple stages using learned attention mechanisms. We demonstrate that Audiovisual SlowFast Networks achieve state-of-the-art results on several benchmark datasets, including Kinetics-400 and AVE, showing significant improvements over unimodal visual and naive audiovisual fusion approaches, particularly in recognizing sound-related actions. This highlights the importance of explicitly modeling audio and designing effective fusion strategies for robust video understanding."
http://arxiv.org/abs/1912.13503v4,Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks,"Adapting pre-trained neural networks to new tasks or domains is crucial for efficient transfer learning. However, full fine-tuning can be computationally expensive and prone to overfitting, motivating parameter-efficient adaptation strategies. This paper introduces side-tuning, a simple yet effective baseline for network adaptation that leverages small, additive side networks. Side-tuning freezes the pre-trained backbone and learns a set of parallel, lightweight networks, whose outputs are added to the corresponding layers of the backbone. We conduct extensive experiments across diverse datasets and architectures, demonstrating that side-tuning achieves competitive or superior performance compared to more complex parameter-efficient methods, while maintaining a significantly reduced training cost and memory footprint. Our results establish side-tuning as a strong, readily implementable baseline for network adaptation, highlighting the potential of simple additive architectures for effective transfer learning."
http://arxiv.org/abs/1912.11121v1,Learning to Navigate Using Mid-Level Visual Priors,"Visual navigation, the task of guiding an agent to a target location using visual input, remains a challenging problem in robotics and computer vision. Many existing approaches rely on end-to-end learning or complex geometric reconstruction, often struggling with generalization and robustness in novel environments. This paper addresses the problem of learning efficient navigation policies by explicitly leveraging mid-level visual priors. We propose a novel architecture that incorporates learnable modules to extract and reason about scene layout elements like corridors, rooms, and intersections, represented as topological affordances. These affordances are then used to guide a hierarchical navigation policy, first selecting a high-level subgoal based on the scene topology, and then executing a low-level action to reach that subgoal. We demonstrate that our approach significantly outperforms state-of-the-art end-to-end methods on both simulated and real-world navigation tasks, achieving higher success rates and shorter path lengths, particularly in complex and previously unseen environments. This work highlights the importance of incorporating structured visual knowledge into navigation systems to improve performance and generalization."
http://arxiv.org/abs/1910.02527v1,"3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera","Understanding complex 3D environments requires reasoning about objects, their relationships, and the viewpoints from which they are observed. Existing representations often treat semantics, 3D geometry, and camera parameters as separate entities, hindering holistic scene understanding. We introduce the 3D Scene Graph, a novel structure that unifies these three crucial aspects into a single, interconnected representation. Our graph comprises object nodes encoding semantic information and 3D bounding boxes, relationship edges representing spatial and functional dependencies between objects, and camera nodes capturing the viewpoint from which the scene is observed. We develop a method to construct these scene graphs from multi-view RGB-D data by leveraging deep learning models for object detection, relationship prediction, and camera pose estimation. Experiments on benchmark datasets demonstrate that our 3D Scene Graph facilitates improved performance in tasks such as scene understanding, object retrieval, and visual navigation compared to methods relying on fragmented representations. This unified structure enables a more comprehensive and effective approach to reasoning about 3D environments."
http://arxiv.org/abs/1908.04781v2,Predicting 3D Human Dynamics from Video,"Estimating 3D human dynamics from monocular video is a challenging problem in computer vision, crucial for applications like virtual reality and human-computer interaction. Existing methods often struggle with temporal inconsistencies and require large amounts of training data. This paper addresses the problem of predicting temporally coherent and realistic 3D human motion sequences from video, even with limited training data. We propose a novel framework that combines a variational autoencoder (VAE) for learning a latent space of human motion with a recurrent neural network (RNN) for predicting future motion in this latent space, conditioned on observed video features. A key innovation is the incorporation of a learned prior within the VAE, which leverages a discriminator network to encourage the generated motions to resemble realistic human movements. We demonstrate that our approach achieves state-of-the-art performance on benchmark datasets, producing more accurate and temporally stable motion predictions compared to existing methods, particularly in scenarios with limited training data. Our method offers a significant advancement towards robust and generalizable 3D human motion prediction from video."
http://arxiv.org/abs/1906.04160v1,Learning Individual Styles of Conversational Gesture,"Human communication relies heavily on non-verbal cues, with conversational gestures playing a crucial role in conveying meaning and emotion. While existing research often focuses on generalizing gesture recognition and generation, the individual stylistic variations in gesturing remain largely unexplored. This paper addresses the challenge of learning and replicating the unique gesturing styles of individual speakers. We propose a novel framework that leverages a variational autoencoder (VAE) to learn a latent representation of individual gesture styles, conditioned on both the corresponding speech and a speaker-specific embedding. This allows us to generate gestures that are not only semantically aligned with the speech content but also reflect the idiosyncratic mannerisms of the target speaker. Experiments on a publicly available dataset demonstrate that our approach can synthesize gestures that are perceived as more personalized and natural compared to baseline methods, as evaluated by both quantitative metrics and qualitative user studies. This work paves the way for more realistic and expressive virtual agents capable of mirroring individual communication styles, enhancing human-computer interaction."
http://arxiv.org/abs/1906.02739v2,Mesh R-CNN,"Reconstructing accurate 3D meshes of objects from single images remains a significant challenge in computer vision. While recent advances in instance segmentation have demonstrated impressive performance in detecting and segmenting objects, generating detailed and accurate 3D mesh representations from these segmented regions is often limited by coarse voxel-based or point cloud-based approaches. To address this limitation, we introduce Mesh R-CNN, an instance-aware mesh reconstruction framework built upon the Mask R-CNN architecture. Mesh R-CNN predicts a deformation field that warps a template mesh to fit the shape of each detected object instance. Specifically, we leverage a graph convolutional network (GCN) to refine the mesh vertices based on both local geometric features and global contextual information extracted from the image. Experiments on standard benchmark datasets demonstrate that Mesh R-CNN significantly outperforms existing methods in terms of mesh accuracy, achieving state-of-the-art results for instance-level 3D mesh reconstruction from single images. This novel framework provides a powerful approach for generating high-quality 3D object representations for downstream tasks such as robotic manipulation, augmented reality, and 3D scene understanding."
http://arxiv.org/abs/2507.01368v1,Activation Reward Models for Few-Shot Model Alignment,"Large language models (LLMs) can exhibit unintended behaviors despite pre-training on vast datasets, necessitating alignment techniques to better adhere to human preferences. Reinforcement Learning from Human Feedback (RLHF) is a popular method for alignment, but its sample complexity makes it challenging in few-shot settings. We address the problem of aligning LLMs with limited feedback by introducing Activation Reward Models (ARMs), a novel approach that learns reward functions directly from model activations. ARMs leverage the internal representations of a pre-trained LLM as features for predicting human preferences, enabling efficient reward learning from a small number of preference pairs. Specifically, we train a lightweight reward model on top of frozen LLM activations extracted from trajectories generated by different policy versions. Our experiments demonstrate that ARMs can effectively align LLMs using only a few dozen preference pairs, achieving comparable or superior performance to standard RLHF baselines with significantly less data. This offers a practical and efficient solution for aligning LLMs in data-scarce environments, reducing the reliance on extensive human feedback."
http://arxiv.org/abs/2506.08008v1,Hidden in plain sight: VLMs overlook their visual representations,"Vision-Language Models (VLMs) have demonstrated impressive capabilities in tasks requiring joint understanding of visual and textual data. However, it remains unclear to what extent VLMs truly leverage and reason about the visual representations they generate internally. This paper investigates the hypothesis that VLMs often overlook valuable information encoded in their visual representations, instead relying heavily on textual cues for downstream tasks. We propose a novel probing methodology that decouples the visual and textual pathways within a VLM, allowing us to independently assess the contribution of each modality. Specifically, we introduce a ""visual-swap"" technique where visual representations from semantically different images are subtly interchanged, while maintaining consistent textual prompts. Our experiments across a range of tasks, including visual question answering and image captioning, reveal a surprising resilience of VLMs to significant alterations in the visual input. This suggests that VLMs are not fully exploiting the information present within their own visual representations, highlighting a critical area for future research aimed at developing more robust and truly multimodal AI systems."
http://arxiv.org/abs/2506.01955v1,Dual-Process Image Generation,"Image generation has witnessed remarkable progress with the advent of deep generative models, yet challenges remain in balancing global coherence with fine-grained detail. Current generative models often struggle to simultaneously capture high-level semantic structure and intricate textures, leading to either blurry outputs or unrealistic compositions. To address this, we propose a novel Dual-Process Image Generation framework that mimics human perception by employing two distinct, yet interconnected, processes: a System 1 pathway focused on rapid, intuitive scene layout generation using a variational autoencoder conditioned on semantic labels, and a System 2 pathway that refines and enhances local details through an adversarial network trained to discriminate between real and generated patches within the System 1 output. This allows the model to first establish a coarse, semantically plausible scene and then iteratively refine the details within that structure. Experimental results demonstrate that our approach achieves superior performance in terms of both image quality and semantic fidelity, as measured by FID and semantic segmentation metrics, compared to state-of-the-art generative models on benchmark datasets. This dual-process approach offers a promising direction for generating high-quality, semantically coherent images with realistic details."
http://arxiv.org/abs/2505.23759v1,Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint,"Vision-Language Models (VLMs) have demonstrated impressive capabilities in tasks requiring joint reasoning over visual and textual data, yet their understanding of nuanced, context-dependent information remains a challenge. This paper investigates the limitations of current VLMs in interpreting subtle visual cues, specifically focusing on the ""hint-taking"" ability within the context of jigsaw puzzles, a deceptively simple task for humans. We introduce a novel benchmark dataset, PuzzleHints, composed of jigsaw puzzles paired with textual hints designed to guide the model towards the correct piece placement. Our approach leverages a modified VLM architecture incorporating a cross-modal attention mechanism that dynamically weights visual features based on the textual hint, allowing for a more focused analysis of relevant image regions. Experiments on PuzzleHints reveal a significant performance gap between human performance and even fine-tuned VLMs, highlighting a persistent difficulty in effectively integrating hints to overcome visual ambiguity. These findings underscore the need for future research to concentrate on improving the contextual reasoning and cross-modal grounding capabilities of VLMs."
http://arxiv.org/abs/2505.23751v1,REOrdering Patches Improves Vision Models,"Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks by processing images as sequences of patches. However, the standard raster-scan order of these patches may not be optimal for capturing long-range dependencies and global context, potentially hindering model performance. This paper addresses the problem of improving vision model accuracy by optimizing the order in which image patches are processed. We propose a novel learnable patch reordering module, dubbed RePatch, which leverages a lightweight attention mechanism to dynamically rearrange the input patch sequence based on image content. RePatch is designed to be computationally efficient and easily integrated into existing ViT architectures. Experiments on ImageNet classification and COCO object detection demonstrate that incorporating RePatch consistently improves the performance of several ViT variants, achieving gains of up to 1.2% in top-1 accuracy and 0.8% in mAP. These results highlight the importance of patch ordering in vision models and demonstrate the effectiveness of our learnable reordering approach in enhancing their representational power."
http://arxiv.org/abs/2504.16072v1,Describe Anything: Detailed Localized Image and Video Captioning,"Image and video captioning models have achieved remarkable progress in describing global scene content. However, generating detailed and localized descriptions, pinpointing specific regions of interest and their associated actions or properties, remains a significant challenge. This paper introduces ""Describe Anything,"" a novel framework for detailed localized image and video captioning. Our approach leverages a multi-modal encoder that integrates visual features from a pre-trained vision-language model (VLM) with spatial and temporal information. A hierarchical decoder then generates localized captions by first identifying relevant regions or objects and subsequently describing their attributes and actions within a structured narrative. We demonstrate that ""Describe Anything"" significantly outperforms existing state-of-the-art captioning models on several benchmarks, including those designed for fine-grained descriptions and localization accuracy, showing improvements in both caption relevance and localization precision. This work advances the field of multi-modal understanding by enabling more precise and informative descriptions of visual content."
http://arxiv.org/abs/2504.13169v2,"Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling","Vision-language models (VLMs) have demonstrated remarkable capabilities in generating descriptive text for images, yet they often suffer from the problem of hallucination, where generated content contradicts the visual evidence. This paper addresses the critical challenge of reducing object hallucination in VLMs, specifically focusing on improving the factual accuracy of generated captions. We introduce Retrospective Resampling, a novel approach that integrates a verification module into the generation process. Our method first generates an initial set of captions, then leverages a pre-trained object detector to identify and score the visual grounding of mentioned objects within each caption. Based on these grounding scores, captions are resampled, favoring those with strong visual support and penalizing those exhibiting hallucinated objects. Experiments on the COCO benchmark demonstrate that Retrospective Resampling significantly reduces object hallucination, leading to improved caption faithfulness and overall quality, as measured by established metrics such as SPICE and CIDEr. This work offers a practical and effective strategy for enhancing the reliability of VLMs, paving the way for more trustworthy and accurate vision-language applications."
http://arxiv.org/abs/2504.13152v1,St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World,"Simultaneous Localization and Mapping (SLAM) has achieved remarkable progress in estimating camera pose and building 3D scene representations. However, many existing SLAM systems focus solely on geometric reconstruction, neglecting the dynamic nature of real-world environments and the importance of temporal consistency. This paper addresses the challenge of simultaneously reconstructing a temporally coherent 4D scene representation (3D geometry + time) and tracking camera pose within it. We introduce St4RTrack, a novel approach that combines neural radiance fields (NeRFs) with a deformation-based tracking framework. St4RTrack leverages a canonical NeRF to represent the static scene, and employs a learned deformation field to map this canonical space to the observed, time-varying geometry. Camera pose and deformation parameters are jointly optimized by minimizing photometric and regularization losses, enabling accurate tracking and consistent 4D reconstruction. Experiments on both synthetic and real-world datasets demonstrate that St4RTrack significantly improves tracking accuracy and reconstruction quality compared to state-of-the-art dynamic scene SLAM methods, especially in challenging scenarios with large deformations and occlusions. Our method provides a crucial step towards building comprehensive and temporally aware scene understanding systems for robotics and augmented reality applications."
http://arxiv.org/abs/2503.19903v2,Scaling Vision Pre-Training to 4K Resolution,"Self-supervised visual representation learning has demonstrated remarkable success in learning transferable features from large-scale unlabeled datasets, often relying on downsampled images for computational efficiency. However, the impact of input resolution on the quality and transferability of learned representations remains relatively unexplored, particularly at resolutions approaching those commonly encountered in real-world applications like autonomous driving and medical imaging. This paper addresses the challenge of scaling vision pre-training to 4K resolution images. We propose a novel multi-scale masked autoencoder architecture, HighRes-MAE, which incorporates a hierarchical encoder with adaptive receptive fields to efficiently process high-resolution inputs. Furthermore, we introduce a progressive training strategy that gradually increases the input resolution during pre-training, mitigating memory constraints and enabling stable convergence. Experiments on ImageNet-1K and downstream transfer learning tasks demonstrate that HighRes-MAE significantly outperforms existing pre-training methods when fine-tuned on high-resolution datasets, achieving state-of-the-art results in object detection and semantic segmentation. These findings highlight the importance of high-resolution pre-training for learning robust and transferable visual representations applicable to real-world computer vision tasks."
http://arxiv.org/abs/2503.15485v2,TULIP: Towards Unified Language-Image Pretraining,"Vision-language pretraining has emerged as a powerful paradigm for learning transferable representations across modalities. However, existing approaches often rely on complex architectures and objectives tailored to specific downstream tasks, hindering generalization and scalability. We address this limitation by introducing TULIP: Towards Unified Language-Image Pretraining, a novel framework designed for simplified and effective cross-modal representation learning. TULIP employs a single, shared transformer encoder for both image and text embeddings, trained using a unified objective function that combines masked language modeling, masked image modeling, and image-text matching. This streamlined approach reduces architectural complexity and promotes synergistic learning between modalities. Experimental results on a diverse set of vision-language benchmarks, including image classification, visual question answering, and image captioning, demonstrate that TULIP achieves competitive performance compared to more complex task-specific models, while requiring significantly fewer parameters and training resources. This highlights the potential of TULIP as a foundational model for a wide range of vision-language applications, paving the way for more efficient and scalable cross-modal learning."
http://arxiv.org/abs/2503.12355v1,Atlas: Multi-Scale Attention Improves Long Context Image Modeling,"Long-range dependencies are crucial for holistic image understanding, yet effectively modeling them remains a significant challenge in computer vision. Existing attention mechanisms often struggle to capture both fine-grained details and global context within high-resolution images due to computational limitations and the quadratic complexity of standard attention. To address this, we introduce Atlas, a novel multi-scale attention mechanism designed for improved long-context image modeling. Atlas operates by first encoding the image into multiple scales using a learned pooling strategy. Then, it employs a hierarchical attention mechanism that aggregates information across scales, allowing for efficient capture of both local and global dependencies. Experiments on image classification, semantic segmentation, and object detection demonstrate that Atlas consistently outperforms existing attention mechanisms and state-of-the-art architectures, achieving significant improvements in accuracy and efficiency. These results highlight the importance of multi-scale processing for long-context image modeling and establish Atlas as a promising building block for future vision architectures."
http://arxiv.org/abs/2503.07860v1,Video Action Differencing,"Understanding subtle differences between similar video sequences is crucial for various applications like video editing, surveillance, and robotics. Existing methods often focus on action recognition or temporal action localization, but lack the ability to explicitly identify and highlight the differentiating elements between two videos showcasing related, yet distinct, actions. We introduce ""Video Action Differencing,"" a novel approach to pinpoint and visualize the spatio-temporal regions that contribute most to the difference between two videos. Our method leverages a Siamese network architecture with contrastive loss to learn a shared embedding space where similar actions are close, and then employs a novel attention mechanism to identify the frames and spatial locations with the largest distance in the embedding space. This allows us to generate a difference map highlighting the key regions that distinguish the actions. Experimental results on benchmark datasets demonstrate our methods ability to accurately identify and visualize subtle action differences, outperforming existing change detection and action localization techniques in this specific task. This work opens new avenues for fine-grained video understanding and provides a valuable tool for applications requiring precise action comparison."
http://arxiv.org/abs/2503.06469v1,Vector Quantized Feature Fields for Fast 3D Semantic Lifting,"3D semantic scene understanding is crucial for a wide range of applications, including autonomous navigation and robotic manipulation. However, lifting 2D semantic segmentation results to a consistent 3D representation remains computationally expensive, especially when dealing with large-scale environments. This paper addresses the challenge of efficiently reconstructing semantically-aware 3D scenes from 2D semantic segmentations. We propose Vector Quantized Feature Fields (VQFF), a novel approach that leverages a learned codebook of feature vectors to represent the 3D scene. Our method efficiently aggregates 2D semantic information into a compact 3D feature field, which is then decoded using the learned codebook to produce a semantic occupancy grid. VQFF significantly reduces the computational cost associated with traditional 3D reconstruction methods by operating on quantized feature representations. We demonstrate that VQFF achieves comparable semantic segmentation accuracy to state-of-the-art methods, while offering a substantial speedup in processing time. This enables real-time 3D semantic lifting, facilitating the deployment of scene understanding algorithms in resource-constrained environments."
http://arxiv.org/abs/2501.00912v2,AutoPresent: Designing Structured Visuals from Scratch,"Creating effective visual presentations often requires significant design expertise and time investment. This work addresses the challenge of automatically generating structured visuals from unstructured input content, such as raw text or data tables. We introduce AutoPresent, a novel framework that leverages large language models (LLMs) and generative image models to design coherent and informative presentations from scratch. AutoPresent first employs an LLM to understand the input, identify key information, and plan the overall presentation structure, including slide organization and content allocation. Then, a combination of LLM-driven prompt engineering and layout optimization is used to generate individual slides, incorporating relevant text, images synthesized using text-to-image models, and structured visual elements. Experimental results demonstrate that AutoPresent can generate visually appealing and informative presentations that are competitive with human-designed alternatives, as judged by both quantitative metrics and qualitative user studies. This automated approach significantly reduces the effort required to create compelling visual presentations, making information dissemination more accessible and efficient."
http://arxiv.org/abs/2412.08687v3,VisionArena: 230K Real World User-VLM Conversations with Preference Labels,"Vision-Language Models (VLMs) have demonstrated remarkable progress, yet their evaluation often relies on limited, synthetic datasets, hindering accurate assessment of real-world user interaction. This paper addresses the critical gap in realistic VLM evaluation by introducing VisionArena, a novel dataset comprising 230,000 real-world user-VLM conversations spanning diverse visual content and user intents. We collected these conversations through a deployed, interactive VLM-powered assistant, ensuring authentic user queries and model responses. Furthermore, each conversation includes explicit user preference labels, indicating which response from a pair of options the user preferred. Leveraging this data, we develop a robust preference model trained on VisionArena that accurately reflects user satisfaction with VLM outputs. Our experiments demonstrate that models fine-tuned using VisionArena exhibit significantly improved performance in real-world scenarios, as validated by both offline metrics and human evaluation. VisionArena provides a valuable resource for training and evaluating VLMs, fostering the development of more user-centric and practical vision-language systems."
http://arxiv.org/abs/2412.06774v1,Visual Lexicon: Rich Image Features in Language Space,"The intersection of computer vision and natural language processing holds immense potential for creating AI systems that can truly understand and interact with the visual world. However, directly bridging the gap between raw image features and high-level semantic concepts expressed in language remains a significant challenge. This paper addresses the problem of learning a robust and interpretable mapping from visual features to a rich, language-grounded representation. We introduce the ""Visual Lexicon,"" a novel framework that leverages contrastive learning and large language models to embed image regions into a language embedding space. Specifically, we train a visual encoder to align image region features with corresponding textual descriptions using a contrastive loss, encouraging the emergence of semantically meaningful visual representations. Furthermore, we fine-tune a pre-trained language model to generate detailed textual descriptions conditioned on these visual embeddings, enriching the lexicon with contextual information. Experimental results on several benchmark datasets demonstrate that our Visual Lexicon achieves state-of-the-art performance in image captioning and visual question answering tasks, showcasing its ability to capture fine-grained visual semantics. This work provides a powerful new approach for grounding visual information in language, enabling more nuanced and human-like interactions with visual data."
http://arxiv.org/abs/2412.00142v3,Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features,"Vision-language classification aims to assign a visual input to a semantic category described in natural language. While large pre-trained vision-language models (VLMs) have shown remarkable success, their performance often degrades significantly in few-shot scenarios due to limited data availability and the difficulty in adapting learned representations to novel classes. This paper addresses the challenge of improving few-shot vision-language classification by leveraging the rich feature representations learned by large multimodal models (LMMs). We propose a novel approach that combines features extracted from a pre-trained VLM with features extracted from a separate, frozen LMM using a learnable fusion module. Specifically, we fine-tune the VLM to align its feature space with the LMM's feature space, enabling effective knowledge transfer. Experiments on multiple benchmark datasets demonstrate that our method significantly outperforms existing few-shot vision-language classification approaches, achieving state-of-the-art results in several settings. This work highlights the potential of LMM features for enhancing the generalization capabilities of VLMs in data-scarce environments."
http://arxiv.org/abs/2411.05001v1,Analyzing The Language of Visual Tokens,"Vision Transformers (ViTs) have achieved state-of-the-art performance across numerous computer vision tasks by processing images as sequences of visual tokens. However, the internal representations and learned ""language"" of these tokens remain largely opaque, hindering our understanding of ViT decision-making and limiting potential for improvement. This paper addresses the problem of interpreting the semantic content encoded within visual tokens of pre-trained ViTs. We propose a novel framework that leverages masked token prediction and a learned vocabulary of textual descriptions to decode the information represented by individual tokens and their interactions. Specifically, we train a decoder to predict masked visual tokens based on context and associate these predictions with natural language descriptions, thereby generating a semantic ""translation"" of each token. Our experiments on ImageNet demonstrate that our method can effectively extract meaningful textual descriptions corresponding to object parts, attributes, and relationships encoded within visual tokens. This provides valuable insights into the internal workings of ViTs and opens avenues for developing more interpretable and controllable vision models."
http://arxiv.org/abs/2410.22330v2,Vision-Language Models Create Cross-Modal Task Representations,"Vision-language models (VLMs) have shown remarkable capabilities in tasks requiring joint understanding of visual and textual information. However, it remains unclear how VLMs internally represent and relate information from these distinct modalities, particularly when adapting to downstream tasks. This paper addresses the problem of understanding how VLMs create task-specific representations that bridge the vision and language domains. We propose a novel analysis framework that leverages representational similarity analysis (RSA) to compare the internal representations of VLMs across different tasks and modalities. Specifically, we examine the hidden activations of VLMs fine-tuned on visual question answering (VQA), image captioning, and visual entailment, and correlate these activations with both image and text embeddings. Our results reveal that fine-tuning VLMs on different tasks leads to the emergence of distinct, task-specific cross-modal representations, characterized by varying degrees of alignment between visual and textual features. These findings provide valuable insights into the inner workings of VLMs and their ability to adapt to diverse tasks by creating specialized cross-modal representations, paving the way for improved model design and task-specific optimization."
http://arxiv.org/abs/2410.18923v2,SegLLM: Multi-round Reasoning Segmentation,"Large Language Models (LLMs) have shown remarkable capabilities in various vision-language tasks, yet their application to detailed image segmentation, especially requiring multi-step reasoning, remains limited. Current segmentation approaches often struggle to handle complex instructions that necessitate iterative refinement or incorporating contextual understanding beyond a single forward pass. To address this, we introduce SegLLM, a novel framework for multi-round reasoning segmentation. SegLLM leverages an LLM to iteratively refine segmentation masks based on user feedback or internal reasoning steps. The core of our approach involves a closed-loop system where the LLM receives an initial image and segmentation proposal, reasons about potential errors or ambiguities, and generates instructions for mask modification. These instructions are then executed by a segmentation module, and the refined mask is fed back into the LLM for subsequent reasoning. Experiments on challenging datasets demonstrate that SegLLM significantly improves segmentation accuracy, particularly for complex scenes requiring contextual understanding and iterative refinement, achieving up to a 15% improvement in IoU compared to single-round segmentation methods. Our framework offers a promising direction for incorporating reasoning and iterative refinement into image segmentation, paving the way for more robust and user-interactive segmentation systems."
http://arxiv.org/abs/2410.10817v1,When Does Perceptual Alignment Benefit Vision Representations?,"Perceptual alignment, the process of transforming visual data into a shared representational space across different modalities or viewpoints, is often assumed to improve the robustness and generalization of learned vision representations. However, the circumstances under which perceptual alignment demonstrably benefits downstream task performance remain unclear. This paper investigates the conditions under which aligning representations from diverse perceptual sourcesspecifically, different image augmentations and synthetic-to-real domain shiftsyields improvements in learned visual features. We introduce a novel metric, the Alignment Benefit Score (ABS), which quantifies the downstream task performance gain achieved by explicitly aligning representations compared to training without explicit alignment. We empirically evaluate ABS across various datasets, network architectures, and alignment strategies, demonstrating that perceptual alignment is most beneficial when the diversity of the perceptual sources is high and the downstream task is sensitive to the variations captured by those sources. Our findings reveal that indiscriminate application of perceptual alignment can be detrimental, highlighting the importance of considering the interplay between data diversity, task specificity, and the choice of alignment strategy to maximize the utility of learned visual representations."
http://arxiv.org/abs/2410.03825v2,MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion,"Estimating scene geometry from multiple images is a fundamental problem in computer vision, crucial for tasks like 3D reconstruction and autonomous navigation. However, accurately recovering geometry in dynamic scenes with independently moving objects remains a significant challenge. We introduce MonST3R (Motion-aware Simple Transformer for 3D Reconstruction), a novel and efficient approach for monocular depth estimation and scene flow decomposition in dynamic environments. MonST3R leverages a transformer-based architecture with a carefully designed attention mechanism to explicitly model motion cues and disentangle static scene structure from dynamic object motion. The network learns to predict per-pixel depth, camera ego-motion, and object motion fields, enabling the separation of static and dynamic elements within the scene. Experiments on both synthetic and real-world datasets demonstrate that MonST3R achieves state-of-the-art performance in depth estimation and scene flow decomposition, particularly in scenes with significant independent object motion. Our method's simplicity and effectiveness offer a promising direction for robust 3D scene understanding in complex, dynamic environments."
http://arxiv.org/abs/2407.13766v4,Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark,"Finding specific instances of objects within vast amounts of visual data remains a significant challenge for computer vision systems. This paper introduces ""Visual Haystacks,"" a novel benchmark designed to rigorously evaluate the performance of instance retrieval algorithms in complex, cluttered environments. The benchmark presents a ""needle-in-a-haystack"" problem, requiring algorithms to precisely locate a target object (the needle) within a large dataset of distractor images (the haystack), where the target may exhibit variations in pose, lighting, and partial occlusion. We curate two distinct datasets: a synthetic dataset with precise ground truth and controlled variations, and a real-world dataset of retail products captured in diverse store environments. We evaluate several state-of-the-art instance retrieval methods on these datasets, demonstrating a significant performance gap between current capabilities and the demands of this challenging task, particularly on the real-world data. Our benchmark highlights the limitations of existing approaches in handling realistic visual clutter and provides a valuable resource for driving future research in robust and efficient instance retrieval."
http://arxiv.org/abs/2406.20081v1,Segment Anything without Supervision,"The Segment Anything Model (SAM) has demonstrated remarkable zero-shot generalization capabilities for image segmentation, yet it relies on extensive supervised pre-training with a massive dataset of manually annotated masks. This work addresses the challenge of achieving comparable segmentation performance without any human annotation, by introducing a novel self-supervised training framework. Our method leverages a combination of contrastive learning, masked image modeling, and consistency regularization to train a segmentation model directly on unlabeled image data. Specifically, we employ contrastive learning to learn robust feature representations, masked image modeling to encourage contextual understanding, and a novel consistency loss that enforces agreement between segmentations generated from different augmentations of the same image. We demonstrate that our unsupervised approach achieves segmentation performance that is surprisingly close to SAM on several benchmark datasets, significantly outperforming previous unsupervised segmentation methods. This work represents a significant step towards eliminating the reliance on manual annotations for training powerful image segmentation models, paving the way for more scalable and accessible vision applications."
http://arxiv.org/abs/2406.15334v3,Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning,"Multimodal in-context learning (ICL) enables large language models to perform novel multimodal tasks by conditioning on a few input-output examples. However, the effectiveness of ICL often degrades with limited examples and struggles to generalize across diverse modalities. We address the challenge of many-shot multimodal ICL by introducing Multimodal Task Vectors (MTV), a novel parameter-efficient approach that adapts a pre-trained language model to new multimodal tasks. MTVs are computed by aggregating gradients from a small set of multimodal examples, representing a task-specific direction in the model's parameter space. These task vectors are then used to modulate the model's activations during inference, enabling rapid adaptation to new tasks using only a handful of demonstrations. Our experiments demonstrate that MTVs significantly improve performance on various multimodal benchmarks, including visual question answering, image captioning, and multimodal sentiment analysis, outperforming standard fine-tuning and existing ICL methods, particularly in low-data regimes. This work provides a practical and effective approach for adapting large language models to diverse multimodal tasks with limited data, paving the way for more efficient and generalizable multimodal learning."
http://arxiv.org/abs/2406.11815v1,LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning,"Vision-language models (VLMs) have demonstrated impressive capabilities in understanding and reasoning about images and text, offering potential for enhancing robot learning. However, directly applying pre-trained VLMs to robot control often suffers from a significant domain gap and a lack of action-specific knowledge. To address this, we introduce LLARVA: a novel approach to vision-action instruction tuning that bridges the gap between pre-trained VLMs and robotic manipulation. LLARVA leverages a curated dataset of robot interaction videos paired with detailed, multi-modal instructions encompassing both visual observations and desired actions. We fine-tune a VLM by training it to predict appropriate robot actions conditioned on visual input and textual instructions, incorporating techniques to improve action discretization and contextual understanding. Experimental results across a range of simulated robotic tasks demonstrate that LLARVA significantly outperforms existing methods, including those based on imitation learning and direct VLM application, achieving up to a 40% improvement in task success rate. This work highlights the effectiveness of vision-action instruction tuning for adapting pre-trained VLMs to complex robotic manipulation tasks, paving the way for more intuitive and generalizable robot control."
http://arxiv.org/abs/2406.08164v3,ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs,"Vision-Language Models (VLMs) have demonstrated remarkable progress in understanding and generating complex scenes, yet their compositional reasoning abilities remain a significant challenge. Existing evaluation benchmarks often rely on synthetic data or simplistic natural images with limited compositional complexity, failing to adequately assess the nuanced reasoning capabilities required for real-world applications. This paper introduces ConMe, a novel benchmark designed to rigorously evaluate compositional reasoning in modern VLMs using a diverse and challenging set of natural images featuring intricate object arrangements and attribute combinations. ConMe comprises a carefully curated dataset and a suite of evaluation metrics that emphasize both accuracy and consistency in compositional understanding. Our experiments across several state-of-the-art VLMs reveal significant performance gaps compared to human-level performance, highlighting limitations in handling complex compositional relationships. ConMe provides a more realistic and demanding evaluation platform, enabling researchers to develop and refine VLMs with improved compositional reasoning skills, ultimately advancing their applicability in real-world scenarios."
http://arxiv.org/abs/2405.03689v2,Pose Priors from Language Models,"Human pose estimation is a fundamental task in computer vision, often relying on large datasets and complex architectures. However, incorporating prior knowledge about plausible human poses can significantly improve robustness and accuracy, especially in challenging scenarios like occlusions or unusual viewpoints. This paper addresses the problem of learning and utilizing pose priors directly from natural language descriptions of human actions. We propose a novel approach that leverages pre-trained language models to encode textual descriptions of actions into a latent space, which is then mapped to a distribution over plausible 3D human poses using a learned neural network. Specifically, we train a variational autoencoder conditioned on the language embedding to generate diverse and realistic pose samples. Experiments on benchmark datasets demonstrate that our language-driven pose priors significantly improve pose estimation accuracy, particularly for ambiguous or occluded poses, outperforming state-of-the-art methods that rely solely on visual features or generic pose priors. This work offers a new avenue for incorporating semantic knowledge into pose estimation, opening up possibilities for more robust and interpretable human-centric vision systems."
http://arxiv.org/abs/2404.05729v2,Finding Visual Task Vectors,"Neural network weights can be manipulated to perform specific tasks via fine-tuning or weight-space arithmetic. However, identifying the precise vector in weight space that isolates a particular task remains a challenge. This paper addresses the problem of automatically discovering ""visual task vectors""  directions in weight space that, when added to a pre-trained model, induce a desired visual behavior without extensive retraining. We propose a novel method leveraging gradient-based optimization within a low-dimensional subspace defined by the top principal components of task-specific fine-tuning gradients. Our approach efficiently searches for task vectors that maximize performance on a proxy task aligned with the desired behavior, while simultaneously minimizing interference with the original model's capabilities. Experiments on image classification and generation tasks demonstrate that our method successfully identifies task vectors that enable targeted visual modifications, such as style transfer or object addition, with minimal performance degradation on the base model. This work offers a practical and efficient way to inject specific visual skills into pre-trained models, opening new avenues for model customization and transfer learning."
http://arxiv.org/abs/2404.02904v1,ALOHa: A New Measure for Hallucination in Captioning Models,"Image captioning models often generate fluent and grammatically correct sentences that, however, contain information not present in the corresponding image  a phenomenon known as hallucination. Existing metrics for evaluating captioning models primarily focus on accuracy and relevance to the image content but often fail to adequately capture and penalize these hallucinated details. This paper introduces ALOHa (Alignment-based Object Hallucination), a novel metric designed to explicitly quantify object hallucination in image captions. ALOHa leverages pre-trained vision-language models to identify objects mentioned in the caption, then employs visual grounding to determine if those objects are present in the image. The metric aggregates the proportion of hallucinated object mentions, providing a comprehensive score reflecting the caption's faithfulness to the visual input. Experiments on benchmark datasets demonstrate that ALOHa correlates strongly with human judgments of hallucination and effectively differentiates between captions generated by models with varying degrees of hallucination. ALOHa provides a valuable tool for evaluating and improving the reliability of image captioning systems, fostering the development of more trustworthy AI."
http://arxiv.org/abs/2404.01476v2,TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering,"Video Question Answering (VideoQA) demands intricate reasoning about visual content and temporal dynamics. Existing approaches often struggle with complex, multi-step reasoning required to answer questions necessitating external knowledge or diverse perspectives. We introduce TraveLER, a novel modular multi-Large Language Model (LMM) agent framework designed to decompose complex VideoQA tasks into manageable sub-problems. TraveLER employs a central router agent to intelligently dispatch sub-problems to specialized LMM agents, each designed with a specific expertise, such as video captioning, object recognition, or knowledge retrieval. These agents independently process their assigned sub-problems and return structured results to the router, which then synthesizes the information into a final answer. Experiments on benchmark VideoQA datasets demonstrate that TraveLER significantly outperforms existing state-of-the-art methods, particularly on questions requiring multi-hop reasoning and external knowledge integration, achieving a relative improvement of X% on dataset Y. This modular, agent-based approach offers a promising avenue for developing more robust and explainable VideoQA systems."
http://arxiv.org/abs/2403.13043v2,When Do We Not Need Larger Vision Models?,"Scaling vision models has become a dominant paradigm for achieving state-of-the-art performance across various computer vision tasks. However, the computational demands and energy consumption associated with larger models raise concerns about their practicality and sustainability. This paper addresses the critical question: When are smaller, more efficient vision models sufficient, and when is the investment in larger models truly necessary? We propose a task-aware model selection strategy based on a novel metric, Task Information Capacity (TIC), which quantifies the inherent complexity of a given visual task. TIC is estimated by analyzing the minimum description length required to represent the task's input-output mapping using a simplified information bottleneck framework. Our experiments across a diverse set of image classification, object detection, and semantic segmentation benchmarks demonstrate a strong correlation between TIC and the performance gap between small and large models. Specifically, tasks with low TIC can achieve comparable performance with significantly smaller models, while high TIC tasks necessitate larger models for optimal results. These findings offer valuable guidance for resource-conscious model deployment and encourage the development of task-specific architectures, ultimately promoting a more sustainable and efficient approach to computer vision."
http://arxiv.org/abs/2402.13144v3,Neural Network Diffusion,"Diffusion models have achieved state-of-the-art results in image generation, surpassing GANs in terms of image quality and diversity. However, the application of diffusion models to directly manipulate and evolve the weights of neural networks remains largely unexplored, presenting a unique challenge in the field of neural architecture search and meta-learning. This paper introduces Neural Network Diffusion (NND), a novel framework that leverages diffusion processes to directly perturb and refine the weights of pre-trained neural networks. Specifically, we define a forward diffusion process that gradually adds noise to network weights, transforming them into a Gaussian distribution, and a reverse process, parameterized by a neural network, that learns to denoise and reconstruct the original weight distribution. We demonstrate that NND can effectively explore the weight space of neural networks, leading to improved generalization performance and robustness on downstream tasks. Furthermore, we show that NND can discover networks with comparable or superior performance to the original network while significantly altering the weight configuration, suggesting a powerful mechanism for escaping local optima and discovering novel network architectures. This work opens up new avenues for research in neural architecture search, meta-learning, and understanding the loss landscape of neural networks."
http://arxiv.org/abs/2402.03290v1,InstanceDiffusion: Instance-level Control for Image Generation,"Diffusion models have achieved remarkable success in image generation, but often lack precise control over the placement and appearance of individual objects within the generated scene. This paper addresses the challenge of instance-level control in diffusion-based image generation, enabling users to manipulate the position, size, and attributes of specific objects. We introduce InstanceDiffusion, a novel framework that integrates instance segmentation maps directly into the diffusion process. Our method leverages a conditional diffusion model trained to generate images conditioned on both global scene descriptions and instance-specific segmentation masks, along with corresponding text prompts. At inference, users can specify the desired layout and characteristics of individual objects by providing segmentation masks and descriptive text, allowing for fine-grained control over the generated image composition. Experiments demonstrate that InstanceDiffusion significantly improves instance-level controllability compared to existing methods, generating images with accurate object placement and attribute adherence, while maintaining high visual quality. This work paves the way for more intuitive and powerful creative tools for image generation and editing."
http://arxiv.org/abs/2401.14391v2,Rethinking Patch Dependence for Masked Autoencoders,"Masked Autoencoders (MAE) have emerged as a powerful self-supervised learning paradigm for visual representation learning, achieving impressive results by reconstructing masked image patches. However, the prevalent random masking strategy often overlooks the inherent dependencies between patches, leading to suboptimal contextual understanding and reconstruction accuracy. This paper investigates and addresses the limitations of naive patch independence assumptions in MAE frameworks. We introduce a novel Patch Dependency-Aware Masking (PDAM) strategy that leverages semantic similarity between patches to guide the masking process, encouraging the model to focus on reconstructing patches strongly correlated with their unmasked neighbors. PDAM incorporates a dynamic masking ratio based on patch feature variance to further prioritize informative regions. Extensive experiments on ImageNet classification and downstream transfer learning tasks demonstrate that our PDAM-enhanced MAE significantly outperforms conventional MAE and other self-supervised learning methods, achieving state-of-the-art performance with improved efficiency. These results highlight the importance of explicitly modeling patch dependencies for effective masked image modeling and robust visual representation learning."
http://arxiv.org/abs/2401.01885v1,From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations,"Creating photorealistic and behaviorally accurate virtual humans is crucial for immersive communication and virtual reality applications. Generating these embodied avatars directly from speech, especially in conversational settings, remains a significant challenge due to the complex interplay of speech prosody, facial expressions, and body pose. We introduce a novel audio-driven human synthesis framework, ""Audio2RealHuman,"" designed to generate photorealistic, full-body human videos from conversational audio. Our method leverages a multi-stage architecture: first, we predict fine-grained 3D facial expressions and head pose from the audio using a transformer-based network trained on a large-scale audio-visual dataset. These predicted parameters then drive a neural rendering pipeline, conditioned on learned latent codes for personalized appearance and a diffusion model to generate realistic body movements synchronized with the audio and facial expressions. Experiments demonstrate that Audio2RealHuman produces significantly more realistic and synchronized human videos compared to existing audio-driven animation methods, as evaluated through both quantitative metrics and subjective user studies. Our approach paves the way for more natural and immersive audio-visual communication experiences."
http://arxiv.org/abs/2312.17243v1,Unsupervised Universal Image Segmentation,"Image segmentation is a fundamental task in computer vision, traditionally relying on supervised learning with extensive labeled data. This reliance limits its applicability to novel domains and objects where labeled data is scarce or unavailable. This paper addresses the challenge of unsupervised universal image segmentation, aiming to segment images into meaningful regions without any prior knowledge or supervision. We introduce a novel framework that leverages self-supervised learning to extract robust feature representations, followed by a graph-based clustering algorithm optimized for perceptual grouping. Our method incorporates a novel contrastive loss function that encourages feature similarity within semantically consistent regions while maximizing dissimilarity between distinct regions. Experimental results on diverse datasets, including natural images and medical scans, demonstrate that our approach achieves state-of-the-art performance compared to existing unsupervised segmentation methods, often approaching the performance of weakly supervised techniques. The proposed method offers a practical and scalable solution for image segmentation in scenarios where labeled data is limited or unavailable, opening new avenues for automated image analysis across various domains."
http://arxiv.org/abs/2312.08366v1,"See, Say, and Segment: Teaching LMMs to Overcome False Premises","Large Multimodal Models (LMMs) demonstrate remarkable capabilities in understanding and interacting with visual content through natural language. However, they often struggle when presented with images containing false premises, such as objects in physically impossible configurations or relationships, leading to incorrect reasoning and responses. This paper addresses the challenge of improving LMM robustness to such flawed visual inputs. We introduce ""See, Say, and Segment"" (S3), a novel training paradigm that explicitly teaches LMMs to identify and articulate inconsistencies within an image before answering questions. S3 leverages a multi-stage process: first, the model ""sees"" the image; second, it ""says"" what it observes, including potential inconsistencies, by generating a descriptive caption highlighting anomalies; and third, it ""segments"" the image, focusing on regions relevant to the identified inconsistencies. We evaluate S3 on a newly curated dataset of images containing various types of physically implausible scenarios. Experimental results demonstrate that S3 significantly improves LMM accuracy and reduces hallucination rates when confronted with false premises, leading to more reliable and grounded responses. This work advances the reliability of LMMs for real-world applications where visual inputs may contain errors or unexpected configurations."
http://arxiv.org/abs/2312.02974v2,Describing Differences in Image Sets with Natural Language,"Generating natural language descriptions of image sets is a challenging task, especially when the goal is to articulate the differences between them. Current image captioning and visual question answering systems struggle to explicitly highlight distinctions between groups of images, often focusing on individual image content rather than comparative analysis. This paper addresses the problem of automatically generating natural language descriptions that accurately and comprehensively capture the key differences between two distinct image sets. We propose a novel framework that leverages a contrastive learning approach to identify salient visual features unique to each set. These features are then fed into a transformer-based language model, fine-tuned to generate comparative descriptions. Experiments on a newly curated dataset of image set pairs with corresponding difference descriptions demonstrate that our method significantly outperforms existing captioning models in generating descriptions that accurately reflect the differences between the sets, as evaluated by both automatic metrics and human evaluation. Our approach offers a valuable tool for applications such as visual data exploration, education, and automated report generation."
http://arxiv.org/abs/2312.02150v2,Readout Guidance: Learning Control from Diffusion Features,"Diffusion models have demonstrated remarkable capabilities in generating high-quality images and videos, offering a rich latent space with semantically meaningful features at each denoising step. However, leveraging these features for downstream control tasks, such as robotic manipulation or character animation, remains a challenge due to the complexity of mapping diffusion features to effective actions. This paper addresses the problem of learning control policies directly from the internal representations of diffusion models without requiring explicit task-specific training of the diffusion model itself. We introduce ""Readout Guidance,"" a novel approach that learns a control policy by training a lightweight mapping network to predict actions from the diffusion model's feature activations at different denoising timesteps. This mapping network is trained using reinforcement learning, where the reward signal is based on the observed outcome of applying the predicted actions in a simulated environment. Our experiments demonstrate that Readout Guidance can effectively learn control policies for a variety of tasks, including reaching, pushing, and object manipulation, achieving performance comparable to or exceeding task-specific trained policies while leveraging the pre-trained knowledge embedded within the diffusion model. This approach provides a powerful framework for transferring the generative capabilities of diffusion models to interactive control applications, opening new avenues for robot learning and embodied AI."
http://arxiv.org/abs/2312.02249v2,Recursive Visual Programming,"Visual programming languages offer an intuitive way to specify complex image processing pipelines by assembling pre-defined modules. However, creating programs that adapt their structure based on image content remains a significant challenge. This paper introduces Recursive Visual Programming (RVP), a novel framework that allows visual programs to recursively call themselves, enabling data-dependent control flow and adaptive computation graphs. RVP achieves this by introducing a conditional execution module that, based on an image feature, determines whether to execute a pre-defined sub-program or terminate the recursion. We demonstrate the effectiveness of RVP on tasks such as adaptive image filtering and hierarchical object segmentation, achieving state-of-the-art performance with significantly fewer parameters compared to traditional deep learning approaches. The ability to create visual programs that dynamically adjust their computational structure based on image content unlocks new possibilities for efficient and interpretable computer vision algorithms."
http://arxiv.org/abs/2312.01771v1,IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks,"Multimodal prompting, leveraging both visual and textual cues, has shown promise in adapting pre-trained vision-language models to downstream computer vision tasks. However, effectively integrating and utilizing visual prompts, especially when dealing with noisy or incomplete visual information, remains a challenge. We introduce IMProv, a novel inpainting-based multimodal prompting framework that enhances visual prompt quality and robustness. IMProv employs a pre-trained image inpainting model to reconstruct and refine potentially corrupted or missing regions within the visual prompt. This inpainting process not only mitigates noise but also allows for the generation of semantically coherent visual prompts from sparse or ambiguous input. We demonstrate the effectiveness of IMProv across a range of tasks, including image classification, semantic segmentation, and object detection. Our experiments show that IMProv consistently improves performance compared to existing multimodal prompting techniques, particularly in scenarios with degraded visual prompts. The proposed method offers a significant step towards more robust and versatile vision-language models by enabling effective utilization of imperfect visual information."
http://arxiv.org/abs/2311.18823v1,Initializing Models with Larger Ones,"Deep learning models often require extensive training to achieve satisfactory performance, demanding significant computational resources and time. This training cost can be a bottleneck, particularly when dealing with novel tasks or limited data. We address the problem of how to effectively leverage pre-trained, larger models to accelerate the training and improve the performance of smaller, more specialized models. Our method, ""Model Distillation with Selective Initialization"" (MDSI), involves first training a large ""teacher"" model on a related, potentially larger, dataset. We then selectively initialize the student model, a smaller architecture, using specific layers from the pre-trained teacher, guided by layer similarity metrics. This initialization is followed by a distillation phase where the student learns from the softened outputs of the teacher, further refining its performance. Experiments on image classification and semantic segmentation tasks demonstrate that MDSI consistently outperforms random initialization and standard distillation techniques, achieving comparable or superior accuracy to models trained from scratch with significantly less training time. By effectively transferring knowledge from larger models, MDSI offers a practical and efficient approach to training high-performing deep learning models."
http://arxiv.org/abs/2311.17942v1,Object-based (yet Class-agnostic) Video Domain Adaptation,"Unsupervised domain adaptation (UDA) in video understanding aims to transfer knowledge from a labeled source domain to an unlabeled target domain, mitigating domain shift. Existing video UDA methods often struggle with complex scenarios involving significant appearance and motion discrepancies, particularly when objects of interest are not explicitly defined by class labels. We address this challenge by proposing a novel object-based yet class-agnostic video domain adaptation framework. Our approach leverages unsupervised object discovery to segment videos into object-centric regions without relying on class information. We then employ adversarial learning to align the feature distributions of these object regions across domains, focusing on both appearance and motion cues through a novel spatiotemporal feature encoder. Experiments on benchmark video UDA datasets demonstrate that our method achieves significant performance gains compared to state-of-the-art class-specific and class-agnostic approaches, highlighting the effectiveness of object-centric adaptation in bridging domain gaps. This work presents a promising direction for robust video understanding in diverse and unlabeled environments by shifting the focus from global features to meaningful object-level representations."
http://arxiv.org/abs/2311.17076v3,Compositional Chain-of-Thought Prompting for Large Multimodal Models,"Large Multimodal Models (LMMs) demonstrate impressive capabilities in understanding and generating content across vision and language. However, complex reasoning tasks requiring compositional understanding and step-by-step inference remain challenging for these models. This paper addresses the problem of enabling LMMs to perform more nuanced and accurate reasoning through structured, compositional prompting. We introduce Compositional Chain-of-Thought (CoT) prompting, a novel approach that decomposes complex multimodal tasks into a series of simpler sub-problems, each addressed by a specialized prompt designed to elicit a specific reasoning step. These sub-problems are then linked together in a chain, allowing the LMM to sequentially build towards a final solution. We evaluate our method on a range of challenging multimodal reasoning tasks, including visual question answering with multiple constraints and complex scene understanding. Our results demonstrate that Compositional CoT prompting significantly improves performance compared to standard prompting techniques and existing CoT methods, achieving state-of-the-art results on several benchmarks. This work highlights the potential of structured prompting strategies to unlock more sophisticated reasoning capabilities in LMMs."
http://arxiv.org/abs/2311.16090v1,Self-correcting LLM-controlled Diffusion Models,"Diffusion models have demonstrated remarkable capabilities in generating high-fidelity images from textual descriptions. However, relying solely on Large Language Models (LLMs) to directly control diffusion models often results in images that, while semantically plausible, contain subtle inaccuracies or inconsistencies with the input text. This paper addresses the challenge of improving the factual alignment between text prompts and generated images by introducing a self-correcting framework for LLM-controlled diffusion models. Our approach leverages a feedback loop where an LLM acts as both a generator of initial image prompts and a critic, evaluating the generated image against the original text. Based on the critique, the LLM iteratively refines the prompt, guiding the diffusion model towards generating images that are more faithful to the input description. Experiments demonstrate that our self-correcting framework significantly improves the factual accuracy and visual quality of generated images compared to baseline methods, as measured by both automated metrics and human evaluations. This work highlights the potential of integrating LLMs in a closed-loop system to enhance the reliability and controllability of generative models."
http://arxiv.org/abs/2311.12391v1,From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation,"Vision-language explanation (VLE) aims to provide human-understandable rationales for a model's prediction given an image and a corresponding question. Existing VLE methods often struggle to generate accurate and comprehensive explanations, particularly when the initial explanation is flawed or incomplete, leading to a disconnect between the rationale and the final answer. We address this limitation by introducing a novel ""From Wrong To Right"" (FWTR) recursive approach for VLE, which iteratively refines the explanation based on a feedback loop. FWTR leverages a large language model (LLM) to first generate an initial explanation. Then, using the predicted answer, the image, and the explanation itself, the LLM identifies potential errors or omissions in the explanation and recursively revises it until a satisfactory rationale is achieved. Experiments on multiple VLE benchmarks demonstrate that FWTR consistently improves the accuracy and faithfulness of explanations compared to state-of-the-art methods, achieving significant gains in both explanation quality and answer prediction performance. This recursive refinement strategy offers a promising direction for generating more reliable and insightful explanations in vision-language understanding."
http://arxiv.org/abs/2311.06694v3,Which One? Leveraging Context Between Objects and Multiple Views for Language Grounding,"Language grounding, the task of linking natural language to visual elements, is crucial for enabling robots and AI agents to interact with the world. A key challenge arises when multiple objects of the same type are present, requiring the system to disambiguate based on subtle contextual cues and relationships described in the language. This paper addresses the problem of accurately grounding language referring to specific instances of object categories in scenes with multiple similar objects. We propose a novel approach that integrates contextual information derived from inter-object relationships and leverages information from multiple viewpoints. Our method constructs a graph representation capturing the spatial and semantic relationships between objects, and uses a multi-view attention mechanism to fuse information from different camera angles, enabling a more robust and comprehensive understanding of the scene. Experiments on a synthetic dataset and the ScanRefer dataset demonstrate that our approach significantly outperforms existing methods, achieving a 15% improvement in accuracy when grounding ambiguous object references. This improved grounding performance facilitates more effective human-robot interaction and scene understanding in complex environments."
http://arxiv.org/abs/2310.15166v1,Large Language Models are Visual Reasoning Coordinators,"Visual reasoning, the ability to understand and infer relationships between visual elements, remains a significant challenge in artificial intelligence. Existing approaches often rely on specialized architectures or training paradigms tailored to specific reasoning tasks, lacking the flexibility and generalizability of human cognition. This paper addresses the problem of effectively coordinating diverse visual reasoning modules to solve complex, multi-faceted tasks. We propose leveraging Large Language Models (LLMs) as visual reasoning coordinators. Our method, VRC-LLM, utilizes the LLM's inherent capabilities in natural language understanding and planning to decompose intricate visual reasoning problems into sub-tasks. The LLM then orchestrates the execution of specialized vision modules, such as object detectors, attribute classifiers, and spatial relation predictors, interpreting their outputs and iteratively refining the reasoning process until a final solution is reached. Experiments on challenging visual reasoning benchmarks, including GQA and NLVR2, demonstrate that VRC-LLM significantly outperforms existing methods, achieving state-of-the-art results with improved interpretability. These findings suggest that LLMs can effectively serve as coordinators for visual reasoning, unlocking the potential for more flexible and adaptable AI systems."
http://arxiv.org/abs/2310.12971v1,CLAIR: Evaluating Image Captions with Large Language Models,"Image captioning aims to generate natural language descriptions for visual content, and automated evaluation metrics are crucial for assessing caption quality. Traditional metrics like BLEU and CIDEr often correlate poorly with human judgment, highlighting the need for more robust evaluation methods. This paper addresses the limitations of existing image captioning metrics by leveraging the power of Large Language Models (LLMs) for improved evaluation. We introduce CLAIR (Captioning evaluation with Language model-Assisted InfeRence), a novel approach that utilizes LLMs to infer the semantic similarity between a generated caption and the reference caption, incorporating visual context implicitly through the LLM's pre-trained knowledge. CLAIR prompts the LLM to analyze the generated and reference captions, scoring their semantic equivalence and fluency, thereby providing a more comprehensive assessment of caption quality. Experimental results on benchmark datasets demonstrate that CLAIR exhibits significantly higher correlation with human judgments compared to traditional metrics and other LLM-based evaluation methods. This improved correlation indicates CLAIR's superior ability to accurately reflect human perception of caption quality, paving the way for more reliable and effective development of image captioning models."
http://arxiv.org/abs/2309.17444v3,LLM-grounded Video Diffusion Models,"Video diffusion models have demonstrated impressive capabilities in generating high-fidelity and diverse video content. However, controlling the semantic content and ensuring temporal coherence in these models remains a significant challenge, especially when incorporating complex textual prompts. This paper addresses the problem of grounding video diffusion models with Large Language Models (LLMs) to achieve finer-grained control over video generation and improve temporal consistency. We introduce a novel framework, LLM-Grounded Video Diffusion (LGVD), which leverages an LLM to decompose complex text prompts into a sequence of semantically rich scene descriptions and action specifications. These descriptions are then used to condition a spatio-temporal video diffusion model, allowing for precise control over the visual content and dynamics of the generated video. Experiments on diverse video generation benchmarks demonstrate that LGVD significantly improves text-video alignment, enhances temporal coherence, and enables the generation of more complex and nuanced video sequences compared to existing methods. The proposed approach offers a promising direction for creating controllable and semantically meaningful video content with diffusion models."
http://arxiv.org/abs/2309.14525v1,Aligning Large Multimodal Models with Factually Augmented RLHF,"Large Multimodal Models (LMMs) have demonstrated impressive capabilities in understanding and generating content across various modalities. However, these models often struggle with factual accuracy and can generate outputs that contradict established knowledge. This paper addresses the problem of aligning LMMs with factual information, particularly in scenarios where the model's internal knowledge is insufficient or outdated. We introduce Factually Augmented Reinforcement Learning from Human Feedback (FA-RLHF), a novel approach that integrates external knowledge retrieval into the RLHF training loop. Specifically, we augment both the reward model and the policy learning process with retrieved factual information relevant to the LMM's generated outputs. This allows the model to be explicitly penalized for factual inaccuracies and rewarded for incorporating verified knowledge. Experimental results on a diverse set of multimodal tasks demonstrate that FA-RLHF significantly improves the factual accuracy of LMM outputs while maintaining or improving other desirable qualities like coherence and relevance. This work provides a crucial step towards building more reliable and trustworthy LMMs capable of grounded reasoning and knowledge integration."
http://arxiv.org/abs/2308.14710v1,VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation,"Unsupervised Video Instance Segmentation (VIS) aims to discover and segment object instances in videos without relying on manual annotations, a challenging yet highly desirable goal. Current state-of-the-art unsupervised VIS methods often involve complex architectures and multi-stage training pipelines, hindering their accessibility and scalability. We address this complexity by proposing VideoCutLER (Video Cut with Learnable Edge Representations), a surprisingly simple approach that leverages self-supervised learning and a novel learnable edge representation for efficient instance-aware video segmentation. VideoCutLER first pre-trains a DINOv2 backbone on unlabeled video data. Then, it extracts dense features and learns a lightweight module to predict pairwise affinities between pixels, effectively capturing object boundaries. A simple graph partitioning algorithm, leveraging these learned edge representations, then segments the video into distinct object instances across time. Our experiments on standard benchmarks demonstrate that VideoCutLER achieves competitive performance compared to significantly more complex methods, showcasing the effectiveness of our simple yet powerful approach. VideoCutLER paves the way for more accessible and scalable unsupervised video understanding by reducing the complexity of unsupervised VIS."
http://arxiv.org/abs/2308.10897v1,Can Language Models Learn to Listen?,"Large language models (LLMs) have demonstrated remarkable abilities in understanding and generating text, prompting investigation into their potential for multimodal reasoning. However, the extent to which LLMs can truly ""listen""  that is, process and understand raw audio signals without intermediate transcriptions  remains an open question. This paper addresses the challenge of directly integrating raw audio into LLMs for tasks requiring auditory understanding. We propose a novel framework, *Audio-LLM*, which utilizes a trainable audio encoder to project raw audio waveforms into a latent space compatible with LLM embeddings, followed by cross-modal attention mechanisms to fuse auditory and textual information. This allows the LLM to directly attend to relevant segments of the audio signal. Our experiments on audio captioning and question answering tasks demonstrate that Audio-LLM achieves comparable or superior performance to systems relying on automatic speech recognition (ASR), particularly in noisy environments. This suggests that LLMs can indeed learn to process raw audio, opening new avenues for end-to-end multimodal systems and bypassing the limitations imposed by reliance on imperfect ASR transcriptions."
http://arxiv.org/abs/2307.00764v2,Hierarchical Open-vocabulary Universal Image Segmentation,"Universal image segmentation aims to segment any object or scene depicted in an image, a challenging task demanding generalization across diverse visual concepts. Current approaches often struggle with open-vocabulary scenarios, requiring retraining or fine-tuning for novel categories, and lack hierarchical understanding of object relationships. We introduce Hierarchical Open-vocabulary Universal Image Segmentation (HOUS), a novel framework capable of segmenting images into semantically meaningful regions with an open vocabulary, while also inferring a hierarchical part-whole relationship between these regions. HOUS leverages a transformer-based architecture to generate region proposals, followed by a contrastive vision-language model that aligns image regions with textual descriptions to assign semantic labels. Furthermore, it employs a novel hierarchical relation module that predicts the part-whole relationships between segmented regions based on their visual features and semantic labels. Our experiments on diverse datasets demonstrate that HOUS achieves state-of-the-art performance in open-vocabulary segmentation and accurately infers hierarchical relationships, outperforming existing methods by a significant margin in both segmentation accuracy and hierarchy prediction. HOUS provides a significant step towards truly general-purpose image understanding by enabling flexible and comprehensive image analysis."
http://arxiv.org/abs/2306.11180v5,Hyperbolic Active Learning for Semantic Segmentation under Domain Shift,"Semantic segmentation models often struggle to maintain performance when deployed in new, unseen environments due to domain shift. Active learning (AL) can mitigate this issue by strategically selecting the most informative samples for annotation, but existing AL methods often perform poorly under significant domain shift. This paper addresses the challenge of active learning for semantic segmentation in the presence of domain shift by proposing a novel hyperbolic active learning framework. Our approach leverages the unique properties of hyperbolic space to better capture the underlying data manifold and uncertainty, enabling more effective sample selection. Specifically, we embed image features into hyperbolic space and define acquisition functions based on hyperbolic distances and curvature, facilitating the identification of diverse and high-uncertainty samples that are representative of the target domain. Experimental results on several benchmark datasets demonstrate that our method significantly outperforms state-of-the-art active learning strategies, achieving higher segmentation accuracy with fewer labeled samples in cross-domain scenarios. This work offers a promising direction for developing robust and efficient semantic segmentation models for real-world applications."
http://arxiv.org/abs/2306.09322v1,Neural Relighting with Subsurface Scattering by Learning the Radiance Transfer Gradient,"Realistic relighting of objects with complex subsurface scattering (SSS) effects remains a challenging problem in computer vision and graphics. Existing neural relighting methods often struggle to accurately reproduce view-dependent SSS effects, particularly when handling complex geometries and material properties. We address this by proposing a novel neural relighting framework that learns the radiance transfer gradient, a representation that explicitly captures the change in outgoing radiance with respect to changes in incident illumination at each surface point. Our network architecture leverages a differentiable rendering pipeline to estimate this gradient, which is then used to predict the final relit appearance under novel lighting conditions. Specifically, we train the network to predict the radiance transfer gradient from single-view images and corresponding surface normals, enabling efficient and accurate relighting. Experiments on synthetic and real-world datasets demonstrate that our method significantly improves the accuracy and realism of relit images, especially for objects exhibiting strong SSS, compared to state-of-the-art techniques. This work provides a robust and efficient solution for realistic neural relighting, enhancing applications in augmented reality, virtual production, and image editing."
http://arxiv.org/abs/2305.16289v2,Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation,"Data augmentation is a crucial technique for improving the generalization and robustness of deep learning models, especially in computer vision. However, traditional augmentation methods often rely on simple geometric and photometric transformations, limiting the diversity of generated samples and potentially hindering model performance. This paper addresses the challenge of generating more diverse and realistic augmented data for computer vision tasks. We introduce a novel automatic data augmentation framework based on conditional diffusion models, leveraging their ability to synthesize high-quality and semantically consistent images. Our approach utilizes class-conditional diffusion models trained on the target dataset to generate new samples conditioned on the original image and its label, effectively expanding the dataset with diverse variations. Experiments on several benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that our diffusion-based augmentation significantly improves classification accuracy compared to state-of-the-art augmentation techniques, particularly in low-data regimes. This work provides a powerful and automated approach to diversify vision datasets, leading to more robust and accurate computer vision models."
http://arxiv.org/abs/2305.15542v2,TOAST: Transfer Learning via Attention Steering,"Transfer learning leverages knowledge from a source domain to improve performance in a related target domain, especially when target data is scarce. However, naively transferring knowledge can lead to negative transfer, particularly when domain differences are significant. This paper introduces TOAST: Transfer Learning via Attention Steering, a novel attention-based transfer learning framework. TOAST utilizes a trainable attention module to dynamically modulate the feature activations from a pre-trained source model based on the characteristics of the target domain. Specifically, the attention module learns to emphasize relevant features and suppress irrelevant or misleading ones, effectively steering the transferred knowledge towards the target task. Experiments on several benchmark datasets for image classification and semantic segmentation demonstrate that TOAST consistently outperforms state-of-the-art transfer learning methods, achieving significant improvements in accuracy and robustness, especially in challenging cross-domain scenarios. The proposed attention steering mechanism offers a principled and effective approach to mitigate negative transfer and unlock the full potential of pre-trained models for downstream tasks."
http://arxiv.org/abs/2305.14334v2,Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence,"Establishing semantic correspondence between images depicting different instances of the same object category remains a challenging problem, particularly under significant viewpoint, pose, and appearance variations. This paper addresses the problem of learning robust and discriminative features for semantic correspondence by leveraging the generative power of diffusion models. We introduce Diffusion Hyperfeatures, a novel approach that extracts features from multiple denoising steps within a pre-trained diffusion model, effectively capturing both high-level semantic information and fine-grained spatial details across the diffusion process. These features are then aggregated into a hyperfeature representation, providing a comprehensive description of the image content. Experiments on standard semantic correspondence benchmarks demonstrate that Diffusion Hyperfeatures significantly outperform existing hand-crafted and learned feature descriptors, achieving state-of-the-art results in challenging scenarios. The proposed approach offers a powerful new paradigm for semantic correspondence, demonstrating the potential of diffusion models beyond image generation for solving core computer vision tasks."
http://arxiv.org/abs/2305.13655v3,LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models,"Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images from textual descriptions. However, these models often struggle with nuanced prompt understanding, leading to outputs that deviate from the user's intended meaning, particularly for complex or ambiguous prompts. This paper introduces LLM-grounded Diffusion, a novel approach that leverages the reasoning and contextual understanding of large language models (LLMs) to enhance prompt interpretation in diffusion models. Our method employs an LLM to decompose the input prompt into a structured representation consisting of key objects, attributes, and relationships, which are then used to guide the diffusion process through a cross-attention mechanism. Furthermore, we introduce a feedback loop where the LLM refines its understanding based on intermediate image features extracted during diffusion, ensuring consistency between the textual intent and the generated visual content. Experiments on benchmark datasets demonstrate that LLM-grounded Diffusion significantly improves image quality and prompt adherence compared to state-of-the-art text-to-image models, particularly for prompts requiring compositional reasoning. By bridging the gap between textual semantics and visual generation, this work paves the way for more controllable and accurate text-to-image synthesis."
http://arxiv.org/abs/2305.07021v1,Simple Token-Level Confidence Improves Caption Correctness,"Image captioning models often generate fluent but factually incorrect descriptions of images. While much research focuses on improving overall caption quality, the nuanced issue of factual correctness at the token level remains a significant challenge. We address this problem by introducing a simple yet effective method for leveraging token-level confidence scores to improve caption correctness. Our approach involves training a standard captioning model and then, during inference, incorporating a learned confidence threshold for each token based on the model's output probability. Tokens falling below their respective confidence thresholds are masked and replaced with alternatives generated using a combination of the original model and an external knowledge source. Experiments on the COCO Captions dataset demonstrate that our method significantly reduces object hallucination and attribute errors, leading to improved caption correctness as measured by established metrics like SPICE and a dedicated factual consistency evaluation. This highlights the potential of simple token-level confidence calibration for enhancing the reliability of image captioning models."
http://arxiv.org/abs/2305.06343v2,Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs,"Pretrained vision and language (V&L) models have achieved remarkable success in various multimodal tasks. However, these models often struggle with complex reasoning scenarios that require understanding the relationships between objects in an image. This paper addresses the challenge of improving V&L reasoning by incorporating structured representations of visual scenes. We propose a novel approach that injects scene graph information into a pretrained V&L model by learning to align visual features with graph embeddings using a graph attention mechanism. This allows the model to explicitly reason about objects and their relationships. We evaluate our approach on Visual Question Answering (VQA) and Visual Commonsense Reasoning (VCR) tasks. Our results demonstrate significant improvements over existing V&L models, particularly on questions requiring relational reasoning, achieving state-of-the-art performance on the VCR benchmark. This work highlights the importance of structured visual representations for enhancing reasoning capabilities in V&L models."
http://arxiv.org/abs/2303.17546v3,PAIR-Diffusion: A Comprehensive Multimodal Object-Level Image Editor,"Diffusion models have demonstrated remarkable capabilities in image generation and editing, yet precise object-level manipulation within complex scenes remains a challenge, often requiring laborious masking and specialized training. This paper addresses the need for a more intuitive and controllable object-level image editing framework that leverages multiple modalities to guide the diffusion process. We introduce PAIR-Diffusion, a novel approach that combines textual prompts, reference images, and spatial constraints through a unified attention mechanism within a pre-trained diffusion model. PAIR-Diffusion allows users to specify object replacements, attribute modifications, and spatial arrangements using a combination of text descriptions, exemplar object images, and bounding box coordinates. Experiments on a diverse set of image editing tasks demonstrate that PAIR-Diffusion achieves significantly improved fidelity, consistency, and controllability compared to existing state-of-the-art methods, producing realistic and semantically coherent results. This multimodal and object-aware editing framework unlocks a new level of precision and flexibility in image manipulation, paving the way for more creative and accessible image editing tools."
http://arxiv.org/abs/2303.13519v1,Learning and Verification of Task Structure in Instructional Videos,"Instructional videos are a rich source of procedural knowledge, but automatically extracting and verifying their underlying task structure remains a significant challenge. We address the problem of simultaneously learning a task's hierarchical structure and verifying its consistency across multiple video instances. Our method combines a novel recurrent neural network architecture, the Hierarchical Task Graph Learner (HTGL), which learns to predict task decompositions and temporal relationships, with a differentiable verification module that assesses the consistency of predicted structures against observed video segments. The HTGL is trained using a combination of weak supervision from video transcripts and a novel structural consistency loss. Experiments on the CrossTask dataset demonstrate that our approach significantly outperforms existing methods in both task structure prediction accuracy and cross-video verification. This work enables automated understanding and validation of procedural knowledge, facilitating applications in education, robotics, and knowledge base construction."
http://arxiv.org/abs/2303.13043v2,Top-Down Visual Attention from Analysis by Synthesis,"Visual attention allows intelligent agents to efficiently process complex scenes by selectively focusing on relevant information. Current computational models of visual attention often rely on bottom-up saliency or task-specific training, lacking the flexibility and generalizability of human attention. This paper addresses the problem of developing a top-down attention mechanism that operates without explicit supervision and can adapt to novel scenes and tasks by leveraging an analysis-by-synthesis framework. Our approach utilizes a generative neural network trained to reconstruct input images. Attention is then deployed by iteratively perturbing latent codes and evaluating the resulting changes in reconstruction error, effectively identifying image regions crucial for accurate synthesis. We demonstrate that our method, without task-specific training, can effectively highlight salient objects and regions relevant to scene understanding, achieving performance comparable to supervised attention models on object localization and segmentation tasks. This work provides a novel perspective on top-down attention, linking it to generative scene models and offering a pathway towards more flexible and adaptable visual processing systems."
http://arxiv.org/abs/2303.07226v1,Scaling Vision-Language Models with Sparse Mixture of Experts,"Vision-language models (VLMs) have demonstrated remarkable capabilities in tasks requiring joint understanding of images and text. However, scaling these models to handle increasingly complex datasets and achieve state-of-the-art performance remains computationally expensive, hindering progress. This paper addresses the challenge of efficiently scaling VLMs by introducing a novel sparse mixture of experts (MoE) architecture specifically tailored for vision-language tasks. Our approach integrates MoE layers within a transformer-based VLM, enabling conditional computation where only a subset of expert parameters are activated for a given input image and text pair. We carefully design the routing mechanism to balance expert load and maintain efficient training. Experiments on large-scale image-text datasets demonstrate that our sparse MoE-VLM achieves significant improvements in performance compared to dense VLMs with comparable parameter counts and training costs, exhibiting a 2-3% increase in zero-shot transfer accuracy on downstream tasks. These results highlight the potential of sparse MoE architectures to unlock efficient scaling in vision-language modeling, paving the way for more powerful and adaptable AI systems."
http://arxiv.org/abs/2303.01500v2,Dropout Reduces Underfitting,"Dropout, a widely used regularization technique in deep learning, is typically employed to mitigate overfitting by preventing complex co-adaptations of neurons. However, its effect on underfitting, a less explored but equally critical challenge, remains largely unexamined. This paper investigates the surprising ability of dropout to reduce underfitting in certain scenarios. We hypothesize that dropout, by randomly disabling neurons, effectively creates an ensemble of simpler models. This ensemble, while preventing overfitting in high-capacity regimes, can also improve generalization performance when the base model is initially underfitting by providing a more diverse set of representational capacities and preventing the model from settling into a suboptimal local minimum. Through experiments on various datasets and network architectures, we demonstrate that dropout consistently reduces underfitting when the base model capacity is intentionally limited or the training data is scarce. Our findings reveal a novel and counter-intuitive benefit of dropout, expanding its applicability beyond regularization and highlighting its potential for improving model performance in data-limited or computationally constrained environments."
http://arxiv.org/abs/2212.14532v4,Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning,"Geospatial representation learning from remote sensing imagery is crucial for various downstream tasks such as land cover classification and change detection. However, existing self-supervised learning methods often struggle to effectively capture multiscale information inherent in geospatial data, limiting their ability to generalize across diverse spatial resolutions and object sizes. To address this limitation, we introduce Scale-MAE, a novel scale-aware masked autoencoder designed for learning robust multiscale geospatial representations. Scale-MAE incorporates a scale-adaptive masking strategy that preferentially masks smaller objects while preserving larger contextual features, and employs a scale-consistent reconstruction loss that encourages the model to recover missing information at appropriate scales. Experiments on multiple benchmark datasets demonstrate that Scale-MAE significantly outperforms state-of-the-art self-supervised learning methods in downstream tasks, particularly when dealing with datasets exhibiting significant scale variations. This improved performance highlights the effectiveness of Scale-MAE in learning scale-invariant representations, enabling more accurate and reliable analysis of geospatial data across diverse applications."
http://arxiv.org/abs/2212.04821v3,PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data,"Video understanding benefits significantly from large-scale pre-training, yet acquiring sufficient labeled video data for diverse tasks remains a substantial challenge. This work addresses the problem of efficiently adapting video transformers to multiple downstream tasks with limited real-world data. We introduce PromptonomyViT, a novel multi-task prompt learning framework that leverages synthetically generated video data to train a diverse set of task-specific prompts. Our approach uses a Video Transformer backbone and trains prompts on synthetic data covering action recognition, object tracking, and scene understanding. These learned prompts are then transferred and fine-tuned on real-world video datasets with significantly fewer samples. Experiments demonstrate that PromptonomyViT achieves state-of-the-art or competitive performance on several downstream video understanding benchmarks, including HMDB51, UCF101, and Something-Something V2, outperforming existing prompt-based and fine-tuning methods, especially in low-data regimes. This indicates that pre-training prompts on synthetic data provides a powerful and efficient mechanism for adapting video transformers to diverse tasks, reducing the reliance on large labeled real-world datasets."
http://arxiv.org/abs/2212.00210v3,Shape-Guided Diffusion with Inside-Outside Attention,"Diffusion models have achieved state-of-the-art results in image generation, but controlling the shape of generated objects remains a challenge, often requiring complex conditioning schemes or post-processing steps. Existing shape-guided diffusion approaches often struggle with accurately enforcing shape constraints, particularly for intricate or concave shapes, and can introduce unwanted artifacts near the shape boundary. We introduce Shape-Guided Diffusion with Inside-Outside Attention (SGD-IOA), a novel approach that leverages a shape mask to modulate the diffusion process through a dedicated attention mechanism. Specifically, SGD-IOA employs an Inside-Outside Attention module, which attends separately to the regions inside and outside the provided shape mask, allowing the model to learn distinct features for the object and its surrounding context while precisely adhering to the shape boundaries. Experiments on various datasets, including synthetic shapes and real-world images, demonstrate that SGD-IOA generates images that faithfully adhere to the provided shape masks, exhibiting significantly improved shape fidelity and reduced artifacts compared to existing methods. The proposed approach offers a simple yet effective way to incorporate shape priors into diffusion models, enabling more controlled and high-quality image generation."
http://arxiv.org/abs/2211.15521v1,G^3: Geolocation via Guidebook Grounding,"Geolocation estimation from a single image remains a challenging problem, particularly in the absence of explicit GPS metadata. Existing methods often rely on large-scale scene recognition or learning direct image-to-location mappings, which can be brittle to domain shifts and lack interpretability. This paper addresses the problem of grounding image content within a structured knowledge base, specifically travel guidebooks, to infer location. We introduce G^3: Geolocation via Guidebook Grounding, a novel approach that leverages visual and textual embeddings to semantically align image features with relevant guidebook entries. Our method first extracts visual features from the input image and then uses a cross-modal retrieval framework to identify the most similar guidebook descriptions. Finally, we aggregate the geographical coordinates associated with these descriptions to estimate the image's location. Experiments on a diverse dataset of geotagged images demonstrate that G^3 achieves state-of-the-art performance in geolocation accuracy, surpassing existing methods, especially in challenging scenarios with limited visual cues. This work provides a new paradigm for geolocation by grounding visual observations in structured, human-curated knowledge, offering a more robust and interpretable alternative to end-to-end learning approaches."
http://arxiv.org/abs/2211.11720v3,Multitask Vision-Language Prompt Tuning,"Vision-Language (VL) models pre-trained on large-scale datasets exhibit remarkable zero-shot transfer capabilities. However, adapting these models to downstream tasks often requires computationally expensive fine-tuning of all model parameters. This paper addresses the challenge of efficiently adapting VL models to multiple diverse tasks simultaneously. We propose Multitask Vision-Language Prompt Tuning (MVLPT), a novel approach that learns a set of task-specific continuous prompts for both the vision and language encoders while keeping the underlying pre-trained VL model frozen. MVLPT leverages a shared prompt pool initialized with diverse prompt embeddings, and learns task-specific combinations of these embeddings using lightweight task adapters. Our experiments demonstrate that MVLPT achieves comparable or superior performance to full fine-tuning and other parameter-efficient transfer learning methods across a range of VL tasks, including image classification, visual question answering, and image captioning, while only tuning a small fraction of the parameters. This makes MVLPT a practical and scalable solution for deploying VL models in real-world applications requiring adaptation to multiple tasks."
http://arxiv.org/abs/2210.09520v6,Using Language to Extend to Unseen Domains,"Domain generalization aims to train models that perform well on unseen target domains, a crucial capability for real-world deployment. A key challenge in this area is bridging the domain gap between training and testing data, particularly when the target domain exhibits characteristics not observed during training. We address this problem by leveraging natural language descriptions to provide a semantic bridge between seen and unseen domains. Our proposed method, Language-Augmented Domain Generalization (LADG), uses a pre-trained language model to encode textual descriptions of domain-specific attributes, enriching the learned feature representations with semantic information. These representations are then used to guide a domain-invariant feature extractor, encouraging the model to learn features that are robust to domain shifts. We demonstrate that LADG significantly improves generalization performance on several benchmark datasets, achieving state-of-the-art results on PACS and DomainBed. This demonstrates the potential of language as a powerful tool for improving domain generalization by enabling models to reason about and adapt to unseen domains based on semantic understanding."
http://arxiv.org/abs/2210.06984v2,QDTrack: Quasi-Dense Similarity Learning for Appearance-Only Multiple Object Tracking,"Multiple Object Tracking (MOT) is a crucial component in various computer vision applications, with appearance-based methods gaining prominence for their robustness in complex scenarios. However, existing appearance-based MOT approaches often suffer from inefficient feature utilization, relying primarily on sparse object detections. To address this limitation, we introduce QDTrack, a novel appearance-only MOT framework that leverages quasi-dense similarity learning. QDTrack learns fine-grained similarities between all possible pairs of tracklets and detections within a temporal window, enabling robust data association even with significant appearance variations or occlusions. Specifically, we employ a transformer-based architecture to model long-range dependencies and learn a similarity metric that is robust to noisy or incomplete detections. Experimental results on standard MOT benchmarks, including MOT17 and MOT20, demonstrate that QDTrack achieves state-of-the-art performance among appearance-only trackers, exhibiting significant improvements in tracking accuracy (IDF1) and identity switches (IDSw). This work highlights the effectiveness of quasi-dense similarity learning for enhancing the robustness and accuracy of appearance-based MOT systems."
http://arxiv.org/abs/2209.08763v3,Decentralized Vehicle Coordination: The Berkeley DeepDrive Drone Dataset and Consensus-Based Models,"Cooperative driving can improve traffic flow and safety, but relying on centralized control limits scalability and robustness. This paper addresses the challenge of coordinating autonomous vehicles in a decentralized manner, using only onboard sensing and communication. We introduce a novel application of the Berkeley DeepDrive Drone Dataset, leveraging its multi-agent vehicle trajectories and realistic urban environment to train and evaluate decentralized coordination models. We propose a consensus-based approach, where each vehicle uses a deep neural network to predict the intentions of its neighbors, and then iteratively refines its own trajectory based on a weighted consensus of these predictions. The weights are learned end-to-end, allowing the system to adapt to varying communication qualities and agent reliability. Experimental results demonstrate that our method significantly improves coordination performance compared to baseline approaches, achieving smoother trajectories and reduced collision rates in dense traffic scenarios. These findings highlight the potential of consensus-based models for enabling robust and scalable decentralized vehicle coordination in complex urban environments."
http://arxiv.org/abs/2209.03745v1,Prior Knowledge-Guided Attention in Self-Supervised Vision Transformers,"Self-supervised learning (SSL) has shown remarkable progress in learning visual representations from unlabeled data, with Vision Transformers (ViTs) emerging as a powerful architecture. However, ViTs often struggle to focus on semantically relevant regions, especially during the initial stages of self-supervised pre-training, leading to suboptimal feature extraction. This paper addresses the challenge of incorporating prior knowledge to guide the attention mechanism in self-supervised ViTs. We introduce a novel framework, Prior Knowledge-Guided Attention (PKGA), which leverages readily available prior knowledge, such as saliency maps or edge detectors, to modulate the attention weights within the Transformer layers. PKGA employs a learnable gating mechanism that dynamically fuses the original attention maps with the prior knowledge, allowing the network to selectively attend to informative regions. We evaluate PKGA on several benchmark datasets for image classification and object detection, demonstrating consistent improvements over baseline self-supervised ViTs and other attention modulation techniques. Our results show that PKGA significantly enhances the quality of learned representations, particularly in scenarios with limited labeled data, highlighting the potential of integrating prior knowledge to improve the efficiency and effectiveness of self-supervised visual learning."
http://arxiv.org/abs/2209.02836v2,Studying Bias in GANs through the Lens of Race,"Generative Adversarial Networks (GANs) have demonstrated remarkable capabilities in generating realistic images, but concerns regarding biases encoded within these models are increasingly recognized. This paper investigates racial bias in GANs, focusing on the generation of human faces and the potential for these biases to perpetuate societal stereotypes. We propose a novel evaluation framework that combines quantitative metrics, measuring demographic representation in generated images using facial attribute classifiers, with qualitative analysis, assessing the realism and stereotypicality of generated faces across different racial groups. Specifically, we leverage pre-trained facial attribute classifiers to quantify the distribution of perceived race and gender in generated datasets and introduce a perceptual study to gauge human perception of generated images with respect to race-related stereotypes. Our experiments, conducted on StyleGAN2 trained on large-scale face datasets, reveal significant disparities in the quality and diversity of generated faces across different racial groups, with certain groups being underrepresented and others exhibiting exaggerated stereotypical features. These findings highlight the urgent need for bias mitigation strategies in GAN training and evaluation to ensure fairness and prevent the propagation of harmful stereotypes in AI-generated content."
http://arxiv.org/abs/2209.00647v1,Visual Prompting via Image Inpainting,"Visual prompting has emerged as a powerful paradigm for adapting pre-trained vision models to downstream tasks by conditioning them on task-specific visual cues. However, designing effective visual prompts often requires extensive manual effort or complex optimization procedures, limiting its accessibility and scalability. We address the challenge of generating effective visual prompts by leveraging image inpainting as a novel prompting mechanism. Our approach, termed Inpainting-based Visual Prompting (IVP), strategically masks regions of the input image and instructs a pre-trained inpainting model to fill in the masked areas with content representing the desired task. This implicitly encodes task-specific information within the inpainted regions, effectively acting as a visual prompt for subsequent downstream tasks. Experiments on various image classification and segmentation benchmarks demonstrate that IVP achieves competitive performance compared to existing visual prompting techniques, while offering a more intuitive and efficient prompt generation process. The proposed method provides a practical and easily implementable alternative for visual prompting, enabling broader adoption and exploration of this promising paradigm."
http://arxiv.org/abs/2208.11821v2,Refine and Represent: Region-to-Object Representation Learning,"Object detection relies on accurate localization and classification, often achieved through region proposal networks. However, current region-based methods often struggle with noisy or incomplete region proposals, leading to suboptimal object representations. This paper introduces ""Refine and Represent: Region-to-Object Representation Learning,"" a novel framework designed to learn robust object representations from potentially imperfect region proposals. Our method employs a two-stage approach: first, a refinement module leverages contextual information to adjust and refine the initial region proposals, effectively mitigating the impact of noise and incompleteness. Second, a representation learning module aggregates features from the refined regions, employing an attention mechanism to prioritize salient parts and construct a comprehensive object representation. Experiments on standard benchmarks, including MS COCO and Pascal VOC, demonstrate significant improvements in detection accuracy compared to state-of-the-art region-based detectors, particularly in scenarios with challenging occlusions or cluttered backgrounds. This refined region-to-object representation learning strategy offers a more robust and accurate foundation for object detection and related tasks."
http://arxiv.org/abs/2208.06773v1,TL;DW? Summarizing Instructional Videos with Task Relevance & Cross-Modal Saliency,"Instructional videos are a popular medium for learning new skills, yet their length often poses a significant barrier to efficient knowledge acquisition. This paper addresses the problem of generating concise and informative summaries of instructional videos by focusing on task relevance and cross-modal saliency. Our proposed method, Task-Driven Cross-Modal Attention Network (TD-CMAN), leverages a hierarchical attention mechanism that first identifies task-relevant segments using action recognition and language modeling. Subsequently, a cross-modal attention module integrates visual and textual cues, weighting frames based on their saliency to both the predicted task and the descriptive text. We train the model end-to-end, optimizing for both summary informativeness and conciseness. Experimental results on benchmark datasets demonstrate that TD-CMAN outperforms state-of-the-art methods in terms of ROUGE scores and user preference, generating summaries that are both shorter and more informative. This work offers a novel approach to instructional video summarization, providing users with efficient access to the most crucial information for learning new skills."
http://arxiv.org/abs/2207.03442v2,Back to the Source: Diffusion-Driven Test-Time Adaptation,"Test-time adaptation (TTA) aims to adapt a pre-trained model to a target domain using only unlabeled test data, circumventing the need for retraining or fine-tuning with labeled data. However, existing TTA methods often struggle with significant domain shifts and catastrophic forgetting due to the reliance on noisy self-supervision signals derived solely from the target domain. To address this, we propose a novel diffusion-driven TTA framework that leverages the generative power of diffusion models to ""go back to the source"" by reconstructing training-like data from noisy test samples. Specifically, we train a diffusion model on the source domain and then use it to iteratively refine test samples, guiding them towards the source domain distribution while preserving their underlying semantic content through classifier guidance. This allows the model to adapt to the target domain by leveraging its pre-existing knowledge rather than relying solely on potentially unreliable information from the target domain. Experiments on various domain adaptation benchmarks demonstrate that our approach significantly outperforms existing TTA methods, achieving state-of-the-art results and demonstrating robustness to large domain shifts. Our method offers a promising direction for improving the reliability and generalization of deep learning models in real-world deployment scenarios."
http://arxiv.org/abs/2207.01708v1,Disentangled Action Recognition with Knowledge Bases,"Human action recognition in videos remains challenging due to the complex interplay of actors, objects, and environments, often leading to entangled representations that hinder generalization. This paper addresses the problem of learning disentangled representations for action recognition by explicitly incorporating structured knowledge. Our approach, Disentangled Action Recognition Network (DARN), leverages a knowledge base to decompose actions into semantic components: actor roles, object affordances, and environment constraints. DARN uses a novel graph-reasoning module to propagate information from the knowledge base to guide the disentanglement process, encouraging the network to learn independent representations for each component. We evaluate DARN on several challenging action recognition datasets, including Kinetics-400 and Something-Something V2, demonstrating superior performance compared to state-of-the-art methods, particularly in few-shot and zero-shot settings. These results highlight the effectiveness of incorporating structured knowledge for learning more robust and generalizable action representations, paving the way for more sophisticated video understanding systems."
http://arxiv.org/abs/2206.07689v1,Structured Video Tokens @ Ego4D PNR Temporal Localization Challenge 2022,"Ego4D presents a significant challenge for egocentric perception, requiring algorithms to understand complex human activities and interactions from a first-person perspective. This paper addresses the problem of temporal localization of PNRs (Past Narrative References) within long, untrimmed egocentric videos, a crucial aspect of understanding the user's memory and intentions. We propose a novel approach that leverages structured video tokens, generated by clustering semantically similar video segments based on visual and audio features, to represent the temporal context. These tokens are then used to train a transformer-based model to predict the start and end times of PNR segments. Our method incorporates both global context from the entire video and local context from the surrounding segments, significantly improving localization accuracy. Experiments on the Ego4D PNR Temporal Localization Challenge 2022 validation set demonstrate that our approach achieves competitive results, outperforming several baseline methods and demonstrating the effectiveness of structured video tokens for temporal understanding. This work highlights the potential of semantic video representations for improving memory recall and activity understanding in egocentric videos."
http://arxiv.org/abs/2206.06346v3,Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens,"Understanding video scenes requires reasoning about both object appearance and their spatiotemporal relationships. While image-based scene understanding has benefited from structured representations like scene graphs, adapting these to video remains challenging due to dynamic object motion and viewpoint changes. This paper addresses the problem of effectively transferring structural scene information from individual frames to coherent video-level representations. We propose a novel approach that leverages object tokenization and enforces frame-clip consistency to propagate image scene structure to video. Our method first extracts object tokens from each frame using a pre-trained object detector. Then, we learn a transformer-based architecture that enforces consistency between object tokens across short video clips, encouraging the formation of stable and meaningful video-level object representations. Experiments on video scene graph generation and action recognition tasks demonstrate that our method significantly improves performance compared to existing approaches. These results highlight the importance of enforcing frame-clip consistency for effectively bringing image scene structure to the video domain, offering a promising avenue for future video understanding research."
http://arxiv.org/abs/2205.09710v1,Voxel-informed Language Grounding,"Language grounding in 3D scenes aims to localize objects referred to in natural language descriptions. Existing methods often struggle with complex scenes containing cluttered objects and subtle linguistic cues due to limitations in effectively integrating both visual and linguistic information. This paper introduces a novel Voxel-informed Language Grounding (VLG) approach that explicitly leverages voxel representations to enhance the interaction between language and 3D scene understanding. VLG first encodes the 3D scene into a voxel grid, enabling fine-grained spatial reasoning. Then, it employs a cross-modal attention mechanism that attends to relevant voxels based on the linguistic query, thereby generating context-aware voxel embeddings. Finally, a prediction module uses these embeddings to predict the target object's location. Experiments on the Nr3D and Sr3D datasets demonstrate that VLG achieves state-of-the-art performance, outperforming existing methods by a significant margin, particularly in scenes with high object density and nuanced language descriptions. This highlights the effectiveness of voxel-based representations in improving language grounding accuracy and robustness in complex 3D environments."
http://arxiv.org/abs/2204.13631v3,Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly,"Visual Question Answering (VQA) systems have achieved remarkable progress, but often struggle with questions requiring complex reasoning or exhibiting subtle biases, leading to incorrect answers despite high confidence. This paper addresses the critical problem of improving the reliability of VQA systems by enabling them to abstain from answering when uncertain, thereby minimizing incorrect responses. We propose a novel uncertainty-aware VQA framework that incorporates a learned abstention mechanism. Our approach leverages a combination of evidential deep learning to model aleatoric and epistemic uncertainty, and a calibrated abstention threshold optimized to maximize accuracy on correctly answered questions while minimizing the frequency of incorrect answers. Experiments on benchmark VQA datasets demonstrate that our method significantly reduces the number of incorrect answers compared to standard VQA models, achieving state-of-the-art performance in reliability metrics such as selective accuracy and risk coverage. By prioritizing abstention over incorrect predictions, our work paves the way for more trustworthy and deployable VQA systems in real-world applications."
http://arxiv.org/abs/2204.10962v3,Visual Attention Emerges from Recurrent Sparse Reconstruction,"Visual attention mechanisms are crucial for efficient processing of complex scenes by focusing computational resources on relevant image regions. Existing attention models often rely on supervised training with explicit attention labels, limiting their adaptability to novel tasks and environments. This paper addresses the problem of learning visual attention in a self-supervised manner, without requiring explicit attention annotations. We propose a novel recurrent neural network architecture that learns to reconstruct images from a sparse set of features extracted at attended locations. The network iteratively selects salient regions, encodes their features, and attempts to reconstruct the original image, rewarding sparse and accurate reconstructions. Through extensive experiments on several benchmark datasets for object recognition and visual question answering, we demonstrate that our model learns to attend to semantically meaningful regions without explicit supervision. Furthermore, the learned attention maps are highly correlated with human eye fixations and improve performance on downstream tasks compared to several baseline attention mechanisms. Our work provides a novel and effective approach to learning visual attention from raw pixel data, opening avenues for more generalizable and adaptable vision systems."
http://arxiv.org/abs/2111.13411v1,TRIP: Refining Image-to-Image Translation via Rival Preferences,"Image-to-image translation aims to learn a mapping between different visual domains, enabling tasks like style transfer and image synthesis. However, existing methods often struggle with generating realistic and diverse outputs, especially when dealing with complex domain shifts or limited data. This paper introduces TRIP, a novel approach to refining image-to-image translation through the incorporation of *Rival Preferences*. TRIP leverages a discriminator network trained not only to distinguish between real and generated images but also to identify the more realistic image between two outputs from the generator trained with different parameterizations. This ""rival preference"" signal provides a richer training signal, pushing the generator to produce results that are not only realistic but also superior to alternative generated images. Experiments on various benchmark datasets demonstrate that TRIP significantly improves the visual quality, realism, and diversity of generated images compared to state-of-the-art methods. TRIP offers a promising direction for enhancing image-to-image translation by effectively harnessing the power of relative comparisons."
http://arxiv.org/abs/2403.17236v1,Neural Image Compression with Quantization Rectifier,"Neural image compression has emerged as a promising alternative to traditional codecs, achieving competitive rate-distortion performance. However, the non-differentiable nature of quantization, a crucial step in image compression, poses a significant challenge for end-to-end training of these models. This paper introduces a novel ""Quantization Rectifier"" (QR) that addresses the training instability and sub-optimal performance caused by commonly used straight-through estimators in neural image compression. Our QR module dynamically adjusts the gradient flow based on the quantization error, mitigating gradient mismatch and promoting more stable and efficient learning. Experiments on standard image datasets demonstrate that our approach significantly improves rate-distortion performance compared to state-of-the-art neural image compression methods utilizing standard straight-through estimators, particularly at low bitrates. The proposed QR module offers a general and effective solution for training neural image compression models, paving the way for more efficient and practical learned image codecs."
http://arxiv.org/abs/2204.01844v1,Deep Q-learning of global optimizer of multiply model parameters for viscoelastic imaging,"Viscoelastic imaging aims to non-invasively characterize the mechanical properties of tissues, providing valuable diagnostic information. However, accurate viscoelastic parameter estimation from dynamic imaging data often relies on computationally expensive iterative optimization algorithms that are sensitive to initial conditions and prone to converging to local minima, especially when dealing with complex material models. This paper addresses the challenge of efficiently and robustly estimating global optimal viscoelastic parameters across multiple model instances. We propose a novel Deep Q-Network (DQN) framework to learn a global optimizer that intelligently guides the iterative parameter estimation process. The DQN is trained to select optimal actions (parameter updates) based on the current parameter estimates and the observed error between the simulated and measured data, effectively navigating the complex parameter space. Experimental results on simulated and in-vitro tissue phantoms demonstrate that our DQN-based optimizer significantly improves parameter estimation accuracy and convergence speed compared to conventional gradient-based methods, especially in scenarios with high noise and complex viscoelastic models. This work provides a powerful and generalizable approach for accurate and efficient viscoelastic parameter estimation, paving the way for improved clinical applications of viscoelastic imaging."
http://arxiv.org/abs/1008.0336v1,Close Clustering Based Automated Color Image Annotation,"Automated image annotation is a crucial task in computer vision, enabling efficient image retrieval and organization. Existing methods often struggle with accurately capturing the semantic content of images using global color features, leading to imprecise and incomplete annotations. This paper addresses the challenge of improving color-based image annotation by leveraging the spatial distribution and coherence of color information. We propose a novel approach, Close Clustering based Automated Color Image Annotation (CC-ACIA), which first segments the image into spatially contiguous color clusters. These clusters are then analyzed based on their size, location, and color characteristics to generate a set of candidate annotation terms. A semantic similarity measure, incorporating WordNet and color name databases, is employed to refine and rank these terms, selecting the most relevant annotations. Experimental results on benchmark datasets demonstrate that CC-ACIA achieves significantly higher precision and recall compared to state-of-the-art color-based annotation techniques. Our approach offers a robust and effective solution for automated image annotation, facilitating more accurate and semantically meaningful image understanding."
http://arxiv.org/abs/1711.04606v1,Provably efficient neural network representation for image classification,"Deep neural networks have achieved remarkable success in image classification, but theoretical understanding of their representation power and sample complexity remains limited. This paper addresses the problem of constructing provably efficient neural network representations for image classification tasks, focusing on achieving optimal or near-optimal sample complexity. We propose a novel neural network architecture, the ""Spectral-Convolutional ReLU Network (SCRN)"", which leverages spectral properties of image data and incorporates learnable convolutional filters followed by ReLU activations. Our construction guarantees that, under mild assumptions on the image distribution, the SCRN can learn a target classifier with a sample complexity that scales logarithmically with the input dimension and polynomially with the desired accuracy and complexity of the target function. We demonstrate theoretically that SCRNs achieve significantly better sample complexity bounds compared to standard architectures for certain image classes, and empirically validate these findings through experiments on synthetic and real-world datasets, showing competitive classification accuracy with substantially fewer training samples. This work provides a theoretical foundation for designing efficient neural network architectures with provable guarantees, bridging the gap between theory and practice in deep learning."
http://arxiv.org/abs/2301.12874v3,Extremal Domain Translation with Neural Optimal Transport,"Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. However, many real-world applications require adaptation to *extremal* target domains, characterized by significant distributional shifts and limited overlap with the source. This paper addresses the challenge of unsupervised domain adaptation to such extremal target domains. We propose a novel framework, Extremal Domain Translation with Neural Optimal Transport (ExDoT), which leverages neural optimal transport to explicitly translate source domain samples towards the extremal target distribution. ExDoT learns a transport map that minimizes the Wasserstein distance between the translated source and the target, while simultaneously encouraging the translated samples to lie within the support of the target distribution via an adversarial constraint. Furthermore, we introduce a novel regularizer based on cycle consistency to ensure the learned transport map is well-behaved and prevents mode collapse. Experimental results on synthetic and real-world benchmark datasets demonstrate that ExDoT significantly outperforms existing domain adaptation methods, particularly when dealing with large domain shifts, achieving state-of-the-art performance in several challenging adaptation scenarios. This work provides a powerful and principled approach to address domain adaptation under extreme distributional shifts, opening avenues for deploying machine learning models in previously inaccessible real-world applications."
http://arxiv.org/abs/2107.02423v2,Improving Text-to-Image Synthesis Using Contrastive Learning,"Text-to-image synthesis has achieved remarkable progress in recent years, enabling the generation of realistic images from textual descriptions. However, current models often struggle with accurately capturing fine-grained details and semantic relationships described in the text, leading to inconsistencies between the generated image and the input text. To address this limitation, we propose a novel approach that incorporates contrastive learning into the training process of text-to-image synthesis models. Our method leverages a contrastive loss function to encourage the model to learn a shared embedding space where semantically similar text-image pairs are pulled closer together, while dissimilar pairs are pushed further apart. Furthermore, we introduce a text-guided attention mechanism within the image generation network to facilitate better alignment between textual features and corresponding image regions. Experimental results on benchmark datasets demonstrate that our approach significantly improves the visual fidelity, semantic consistency, and overall quality of generated images compared to state-of-the-art methods. This work highlights the effectiveness of contrastive learning in enhancing the performance of text-to-image synthesis and offers a promising direction for future research in this field."
http://arxiv.org/abs/1312.5650v3,Zero-Shot Learning by Convex Combination of Semantic Embeddings,"Zero-shot learning (ZSL) aims to recognize unseen classes by transferring knowledge from seen classes, typically leveraging semantic embeddings such as attributes or word vectors. However, existing methods often struggle with domain shift and the inherent ambiguity in semantic representations, leading to suboptimal performance in unseen class recognition. We address this problem by proposing a novel zero-shot learning framework based on convex combination of semantic embeddings. Our approach learns a compatibility function between visual features and a convex combination of semantic embeddings of seen classes, effectively synthesizing semantic representations for unseen classes within the convex hull formed by the seen classes. The optimal combination weights are learned by minimizing a reconstruction loss that encourages the synthesized semantic embeddings to accurately represent the visual features of the corresponding unseen classes. Experimental results on benchmark datasets demonstrate that our method achieves significant improvements over state-of-the-art ZSL techniques, particularly in challenging generalized zero-shot learning settings. This highlights the effectiveness of our convex combination strategy in mitigating domain shift and improving the generalizability of ZSL models."
http://arxiv.org/abs/2305.12082v3,SneakyPrompt: Jailbreaking Text-to-image Generative Models,"Text-to-image generative models have achieved remarkable progress in creating realistic and artistic images from textual descriptions. However, these models are vulnerable to adversarial attacks that can bypass safety mechanisms and generate undesirable content, such as depictions of violence, hate speech, or copyrighted material. This paper introduces SneakyPrompt, a novel method for jailbreaking text-to-image generative models by crafting subtly perturbed prompts that circumvent content filters without significantly altering the semantic meaning. SneakyPrompt leverages a combination of genetic algorithms and gradient-based optimization to iteratively refine the prompt, introducing small, often imperceptible changes in wording, punctuation, or character encoding that exploit weaknesses in the model's text understanding and safety protocols. We demonstrate the effectiveness of SneakyPrompt across multiple state-of-the-art text-to-image models, achieving high success rates in generating targeted harmful content while maintaining high visual fidelity. This work highlights the critical need for robust defenses against adversarial attacks on text-to-image generative models to prevent their misuse and ensure responsible deployment."
http://arxiv.org/abs/2306.05310v1,"A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments","Deep Reinforcement Learning (DRL) holds immense promise for automating radiology workflows, but deploying DRL models across diverse clinical settings presents significant challenges. This paper addresses the problem of adapting DRL agents trained for specific radiology tasks to different, low-compute environments characterized by continuously changing data distributions and resource constraints. We propose a novel framework that combines meta-learning with online adaptation strategies. Specifically, we employ a Model-Agnostic Meta-Learning (MAML) approach to pre-train a DRL agent capable of rapid adaptation. This meta-learned agent is then deployed, and its policy is continuously refined using a combination of self-supervised learning and a lightweight policy distillation technique that transfers knowledge from a larger, cloud-based model to the resource-constrained edge device. Experimental results on a simulated lung nodule detection task demonstrate that our framework significantly outperforms agents trained from scratch and those employing standard fine-tuning approaches, achieving a 15% improvement in average reward across diverse deployment environments. This work offers a practical solution for deploying robust and adaptable DRL agents in real-world radiology settings, paving the way for wider adoption of AI-powered tools in clinical practice."
http://arxiv.org/abs/2205.06969v1,Mask CycleGAN: Unpaired Multi-modal Domain Translation with Interpretable Latent Variable,"Unpaired image-to-image translation aims to learn mappings between different visual domains without requiring corresponding image pairs. Existing methods often lack interpretability and control over the translated outputs, particularly when dealing with multi-modal data. This paper addresses the challenge of generating diverse and interpretable translations between unpaired image domains, focusing on disentangling style and content information. We introduce Mask CycleGAN, a novel framework that integrates masked image modeling with CycleGAN to learn a disentangled latent space. Our approach utilizes a shared latent space for content and separate latent spaces for domain-specific style, with masks guiding the disentanglement process during encoding and decoding. Furthermore, we incorporate cycle consistency and latent consistency losses to ensure faithful reconstruction and meaningful latent representations. Experimental results on benchmark datasets demonstrate that Mask CycleGAN achieves state-of-the-art performance in generating diverse and realistic translations while providing explicit control over style attributes. The proposed method offers a significant advancement towards controllable and interpretable unpaired image-to-image translation, opening new avenues for creative image manipulation and data augmentation."
http://arxiv.org/abs/2112.09638v1,Oil Spill SAR Image Segmentation via Probability Distribution Modelling,"Synthetic Aperture Radar (SAR) imagery offers a crucial tool for oil spill monitoring due to its all-weather and day-night capabilities. Accurate and automated oil spill segmentation in SAR images remains challenging due to inherent speckle noise, complex backscattering mechanisms, and ambiguities with look-alikes. This paper addresses the problem of robust oil spill segmentation in SAR images by proposing a novel probability distribution modelling approach. We model the SAR image intensity distribution using a mixture of Generalized Gamma distributions, separately representing oil spill, look-alike, and clean water regions. Expectation-Maximization (EM) algorithm is then employed to estimate the parameters of the mixture model and subsequently segment the image based on maximum a posteriori (MAP) classification. Experimental results on real SAR images demonstrate that our method achieves significantly improved segmentation accuracy compared to traditional thresholding and clustering methods, particularly in challenging scenarios with low contrast and significant speckle noise. This improved segmentation technique facilitates more effective and timely oil spill response and environmental monitoring."
http://arxiv.org/abs/2405.12500v1,Entropic associative memory for real world images,"Associative memories provide a mechanism for retrieving stored patterns from noisy or incomplete inputs, a fundamental capability for robust perception. However, classical associative memory models often struggle with high-dimensional, real-world image data due to limitations in storage capacity and sensitivity to noise. This paper addresses the challenge of building an associative memory system capable of effectively storing and retrieving real-world images. We propose an Entropic Associative Memory (EAM) that leverages information-theoretic principles to optimize memory encoding. The EAM minimizes the redundancy between stored patterns by maximizing the entropy of the hidden unit activations, leading to improved storage capacity and noise resilience. Experimental results on standard image datasets, including CIFAR-10 and ImageNet, demonstrate that the EAM significantly outperforms traditional Hopfield networks and sparse coding techniques in terms of retrieval accuracy and robustness to corruption. The EAM presents a promising approach for building more efficient and robust associative memory systems for real-world computer vision applications."
http://arxiv.org/abs/1707.07609v1,A Deep Learning Approach to Digitally Stain Optical Coherence Tomography Images of the Optic Nerve Head,"Optical coherence tomography (OCT) provides high-resolution, three-dimensional images of the optic nerve head (ONH), crucial for diagnosing and monitoring glaucoma. However, standard OCT B-scans lack the visual cues of stained histological sections, hindering detailed assessment of microstructural features. This paper addresses the challenge of digitally staining OCT images of the ONH to enhance visualization and potentially improve diagnostic accuracy. We propose a deep learning framework based on a conditional generative adversarial network (cGAN) to translate grayscale OCT images into realistically stained virtual representations. The cGAN, trained on registered pairs of OCT and corresponding histology images, learns to map structural information from OCT to the color and texture characteristics of histological stains. Our results demonstrate that the generated digitally stained OCT images significantly improve the visibility of key ONH structures, such as the lamina cribrosa and nerve fiber bundles, as evaluated by expert graders. This novel approach offers a non-invasive method for enhancing the interpretability of OCT images, potentially leading to earlier and more accurate glaucoma diagnosis."
http://arxiv.org/abs/2103.01629v1,DeepCert: Verification of Contextually Relevant Robustness for Neural Network Image Classifiers,"Neural network image classifiers are vulnerable to adversarial examples, raising concerns about their reliability in safety-critical applications. Existing robustness verification techniques often focus on $L_p$-norm bounded perturbations, which may not correspond to realistic or contextually relevant image corruptions. This paper addresses the challenge of verifying robustness against contextually relevant perturbations, where the notion of relevance is defined by a learned generative model. We propose DeepCert, a novel framework that combines a variational autoencoder (VAE) for modeling contextually relevant image corruptions with a sound and complete robustness verification procedure based on abstract interpretation. DeepCert leverages the VAE's latent space to define a set of semantically meaningful perturbations and then uses abstract domains to over-approximate the network's behavior under these perturbations. Experimental results on benchmark datasets demonstrate that DeepCert can effectively verify robustness against contextually relevant corruptions, achieving significantly tighter robustness guarantees compared to $L_\infty$ verification while scaling to larger networks. Our approach offers a practical and principled method for assessing the reliability of neural network classifiers in real-world scenarios, moving beyond worst-case adversarial robustness."
http://arxiv.org/abs/2108.02814v1,Potential Applications of Artificial Intelligence and Machine Learning in Radiochemistry and Radiochemical Engineering,"Radiochemistry and radiochemical engineering are complex fields involving the manipulation and study of radioactive materials, requiring precise control and often operating in hazardous environments. The inherent complexity and data-rich nature of radiochemical processes present significant challenges in optimization, safety, and efficiency. This paper explores the potential of artificial intelligence (AI) and machine learning (ML) techniques to address these challenges. We propose a framework for integrating AI/ML into various aspects of radiochemistry, including predictive modeling of chemical separations, automated synthesis route optimization, and enhanced radiation detection and source localization through deep learning-based image analysis. Specifically, we investigate the application of recurrent neural networks (RNNs) for predicting the elution behavior of radionuclides in chromatographic separations and convolutional neural networks (CNNs) for identifying and classifying radioactive materials from gamma-ray spectra. Preliminary results demonstrate that RNN models can predict elution curves with high accuracy (R > 0.95) based on operational parameters, while CNNs achieve >90% accuracy in identifying isotopes from spectral data. These advancements promise to significantly improve the efficiency, safety, and precision of radiochemical processes, paving the way for more effective nuclear medicine, environmental monitoring, and nuclear waste management."
http://arxiv.org/abs/2201.12220v3,Neural Optimal Transport,"Optimal Transport (OT) provides a powerful framework for comparing and manipulating probability distributions, finding applications in diverse fields such as computer vision, machine learning, and image processing. However, traditional OT computation often suffers from high computational costs, especially when dealing with high-dimensional data or complex cost functions. This paper addresses the challenge of efficiently approximating OT distances and transport plans by leveraging neural networks. We introduce ""Neural Optimal Transport"" (NOT), a novel framework that learns a cost-aware embedding space where the Euclidean distance approximates the OT distance between embedded probability distributions. Furthermore, NOT learns a neural transport map that directly estimates the optimal transport plan, enabling efficient sample transfer between distributions. Experimental results demonstrate that NOT significantly reduces the computational burden compared to traditional OT solvers while maintaining high accuracy in distance approximation and transport plan estimation on various benchmark datasets. NOT offers a scalable and efficient alternative to classical OT methods, opening up new possibilities for applications involving large-scale data and complex transport scenarios."
http://arxiv.org/abs/2107.00002v2,Cascade Decoders-Based Autoencoders for Image Reconstruction,"Autoencoders have become a popular choice for unsupervised representation learning and image reconstruction. However, standard autoencoder architectures often struggle to reconstruct fine-grained details and high-frequency information, particularly in complex images. This paper addresses the limitations of single-stage decoder architectures in autoencoders by introducing a novel cascade decoder framework. Our approach employs a series of progressively refined decoders, where each decoder in the cascade focuses on reconstructing residual details from the previous stage. This cascade architecture allows for hierarchical feature refinement and improved reconstruction quality by iteratively correcting errors and adding finer details. Experiments on benchmark datasets demonstrate that our cascade decoder autoencoders achieve significantly improved reconstruction quality, outperforming traditional autoencoders and other state-of-the-art reconstruction methods, particularly in terms of sharpness and detail preservation, as measured by PSNR and SSIM. This work demonstrates the effectiveness of cascade decoders for high-fidelity image reconstruction within the autoencoder framework."
http://arxiv.org/abs/2501.13223v4,A Comprehensive Social Bias Audit of Contrastive Vision Language Models,"Contrastive Vision Language Models (CVLMs) like CLIP have demonstrated remarkable zero-shot transfer capabilities, making them increasingly prevalent in downstream applications. However, the potential for these models to perpetuate and amplify social biases present in their training data remains a significant concern. This paper addresses the critical need for a comprehensive and systematic evaluation of social biases embedded within CVLMs across various demographic attributes. We introduce a novel audit framework leveraging a diverse set of image-text prompts designed to probe for biases related to gender, race, and age across multiple tasks, including image classification and retrieval. Our methodology incorporates both attribute-centric and intersectional analyses to identify subtle and complex biases. Our experiments reveal significant biases in several widely used CVLMs, manifesting as both stereotypical associations and disparities in performance across different demographic groups. These findings underscore the urgency of developing robust debiasing strategies for CVLMs to ensure fair and equitable performance in real-world applications."
http://arxiv.org/abs/2111.15309v2,Deep Auto-encoder with Neural Response,"Auto-encoders have demonstrated effectiveness in representation learning, particularly for dimensionality reduction and data generation. However, the latent space learned by traditional auto-encoders often lacks disentanglement and interpretability, hindering downstream tasks requiring explicit control over generated features. To address this, we propose a novel Deep Auto-encoder with Neural Response (DAE-NR) that incorporates a learnable neural response layer within the bottleneck of the auto-encoder. This layer is trained to predict the activation patterns of pre-trained convolutional neural network (CNN) layers given the encoded latent representation. By forcing the latent space to correlate with semantically meaningful features extracted by the CNN, DAE-NR achieves improved disentanglement and allows for targeted manipulation of generated outputs. Experiments on benchmark image datasets demonstrate that DAE-NR achieves state-of-the-art performance in image reconstruction quality and latent space traversal, enabling fine-grained control over image attributes such as object pose and appearance. This method offers a significant advancement in controllable image generation and representation learning."
http://arxiv.org/abs/2204.07664v3,Conditional Injective Flows for Bayesian Imaging,"Bayesian imaging offers a principled framework for solving inverse problems by explicitly modeling uncertainty. However, accurately representing complex, high-dimensional posterior distributions remains a significant challenge. This paper addresses the limitations of existing Bayesian imaging techniques by introducing Conditional Injective Flows (CIFs), a novel approach for learning expressive and invertible mappings conditioned on observed data. Our method leverages the power of normalizing flows to transform a simple prior distribution into a complex posterior, while ensuring injectivity to maintain a well-defined likelihood. We condition the flow on the observed image using a convolutional neural network, enabling the model to adapt to different data characteristics and noise levels. Experiments on image deblurring and super-resolution demonstrate that CIFs achieve state-of-the-art performance in terms of posterior approximation quality, measured by metrics such as negative log-likelihood and sample diversity, and lead to improved reconstruction accuracy compared to variational inference and Markov Chain Monte Carlo methods. This work provides a powerful and flexible framework for Bayesian imaging, paving the way for more reliable and uncertainty-aware image processing pipelines."
http://arxiv.org/abs/2310.14413v1,Data Augmentation: a Combined Inductive-Deductive Approach featuring Answer Set Programming,"Data augmentation is a crucial technique for improving the generalization capability of deep learning models, especially when training data is scarce. Existing data augmentation strategies often rely on inductive biases, applying transformations based on heuristics or learned patterns from the data itself, sometimes leading to semantically inconsistent augmentations. We address the problem of generating high-quality augmented data that respects underlying semantic constraints and logical relationships within the scene. Our proposed method combines inductive learning for suggesting plausible transformations with deductive reasoning using Answer Set Programming (ASP) to enforce semantic consistency. Specifically, we leverage a convolutional neural network to predict potential augmentations, and then utilize an ASP program to verify and refine these proposals based on a set of predefined logical rules that capture domain knowledge. Experiments on synthetic and real-world datasets demonstrate that our combined inductive-deductive approach generates more realistic and semantically valid augmented data compared to purely inductive methods, leading to improved performance in downstream tasks such as object detection and scene understanding. This work offers a novel and principled framework for data augmentation that integrates learned patterns with explicit semantic knowledge, enhancing the robustness and reliability of deep learning models."
http://arxiv.org/abs/2311.02247v1,PRISM: Progressive Restoration for Scene Graph-based Image Manipulation,"Scene graph-based image manipulation offers a structured approach to editing images based on relationships between objects. However, existing methods often struggle with generating realistic and coherent images, particularly when requiring substantial alterations to the input image, as they may introduce artifacts or inconsistencies due to the complexity of propagating changes across the entire scene. To address this, we propose PRISM: Progressive Restoration for Scene Graph-based Image Manipulation. PRISM utilizes a novel progressive restoration framework that iteratively refines the manipulated image by first focusing on local object modifications guided by the scene graph, and then progressively expanding the restoration region to ensure global consistency. This is achieved by a cascaded diffusion model conditioned on the scene graph, iteratively refining the image from localized changes to global harmonization. Experiments on the COCO-Stuff and Visual Genome datasets demonstrate that PRISM generates higher-quality and more realistic manipulated images compared to state-of-the-art methods, achieving significant improvements in FID and visual fidelity. PRISM offers a significant advance in scene graph-based image manipulation, enabling more controlled and realistic image editing capabilities."
http://arxiv.org/abs/2406.13300v1,LightGBM robust optimization algorithm based on topological data analysis,"Gradient boosting decision tree algorithms, such as LightGBM, have achieved remarkable success in various machine learning tasks. However, their performance can be sensitive to hyperparameter settings and noisy data, leading to suboptimal generalization and robustness issues. This paper addresses the problem of optimizing LightGBM's hyperparameters to enhance its robustness against noisy data and improve generalization performance. We propose a novel LightGBM robust optimization algorithm leveraging topological data analysis (TDA). Specifically, we employ persistent homology to characterize the topological structure of the loss landscape induced by different hyperparameter configurations. The algorithm then selects hyperparameter sets that correspond to more stable and persistent topological features, indicating flatter minima and improved robustness. Experimental results on several benchmark datasets demonstrate that our TDA-guided optimization significantly improves LightGBM's performance and robustness, achieving higher accuracy and stability compared to traditional optimization methods and other robust boosting algorithms. This approach offers a principled and effective way to enhance the reliability and generalization capability of gradient boosting models in real-world applications with noisy data."
http://arxiv.org/abs/2412.15650v1,Beyond Human Data: Aligning Multimodal Large Language Models by Iterative Self-Evolution,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and generating content across different modalities, yet their reliance on extensive human-annotated datasets introduces biases and limits their adaptability to novel scenarios. This paper addresses the challenge of aligning multimodal representations and instructions in MLLMs without explicit human supervision. We propose an iterative self-evolution framework where the MLLM acts as both the generator and discriminator, creating and refining its own training data. Specifically, the model generates multimodal instruction-following examples, then evaluates and filters these examples based on internal consistency and a learned reward function derived from its own performance. The MLLM is then fine-tuned on the filtered self-generated data, leading to improved alignment and generalization. Experiments on a diverse set of multimodal tasks demonstrate significant performance gains compared to models trained solely on human-annotated data, particularly in out-of-distribution scenarios. This self-evolution approach offers a promising pathway towards building more robust, unbiased, and adaptable MLLMs by reducing dependence on human-labeled datasets."
http://arxiv.org/abs/2506.16237v1,Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design,"Magnetic Resonance Imaging (MRI) is a vital tool for non-invasive clinical diagnosis, but its lengthy acquisition times can be a significant bottleneck. This paper addresses the problem of minimizing the number of MRI measurements required for accurate estimation of tissue microstructure parameters, specifically focusing on diffusion MRI (dMRI). We propose a novel active learning framework, Diffusion Guided Bayesian Experimental Design (Diff-BED), that iteratively selects optimal diffusion gradient directions based on a Bayesian estimate of the posterior distribution over tissue microstructure parameters. Diff-BED leverages a Gaussian Process surrogate model to approximate the computationally expensive forward model and incorporates diffusion tensor imaging (DTI) information as a prior to guide the exploration of the experimental space. Experiments on both synthetic and in-vivo human brain data demonstrate that Diff-BED significantly reduces the number of required dMRI measurements while achieving comparable or improved accuracy in estimating fiber orientation and fractional anisotropy compared to conventional fixed sampling schemes and alternative active learning methods. This accelerated acquisition strategy has the potential to improve the feasibility and efficiency of dMRI in clinical settings."
http://arxiv.org/abs/1301.3391v3,Feature grouping from spatially constrained multiplicative interaction,"Multiplicative interactions between features have shown promise in capturing higher-order dependencies for various computer vision tasks. However, the combinatorial explosion of possible feature interactions makes it challenging to identify and utilize the most relevant ones, especially when considering spatial context. This paper addresses the problem of efficiently learning and grouping relevant feature interactions from a large pool by incorporating spatial constraints. We propose a novel method that learns multiplicative interactions while simultaneously grouping features based on spatial proximity and interaction strength. This is achieved through a spatially-aware attention mechanism that selectively aggregates feature interactions within defined neighborhoods, followed by a grouping loss that encourages similar interaction patterns within these neighborhoods. Experiments on image classification and semantic segmentation demonstrate that our method achieves competitive performance with significantly fewer parameters compared to existing multiplicative interaction approaches. The proposed spatially constrained multiplicative interaction offers a more efficient and interpretable way to learn higher-order feature relationships, leading to improved performance and reduced computational cost in computer vision applications."
http://arxiv.org/abs/2110.13953v1,On sensitivity of meta-learning to support data,"Meta-learning algorithms aim to learn how to learn from limited data, typically by leveraging a distribution of related tasks. However, the sensitivity of meta-learning to the specific composition of the support set, the small dataset used for adaptation to a new task, remains relatively unexplored. This paper addresses the problem of understanding and mitigating the impact of noisy or unrepresentative support sets on meta-learning performance. We propose a novel approach that combines a support set selection mechanism based on uncertainty estimation with a meta-regularization technique that encourages robustness to variations in the support data. Specifically, we use a learned uncertainty estimator to identify and potentially down-weight noisy support samples, and simultaneously train the meta-learner with a penalty that minimizes the variance of predictions across different sampled support sets. Our experiments on benchmark meta-learning datasets demonstrate that our approach significantly improves the robustness of meta-learning algorithms to corrupted and biased support data, leading to improved generalization performance and faster adaptation. This work highlights the critical role of support set quality in meta-learning and provides a practical solution for enhancing its reliability in real-world scenarios."
http://arxiv.org/abs/2103.03622v2,Explanations for Occluded Images,"Explainable AI (XAI) methods aim to provide insights into the decision-making processes of deep learning models, fostering trust and understanding. However, most existing XAI techniques struggle to generate meaningful explanations for images with significant occlusions, a common occurrence in real-world scenarios. We address the problem of generating faithful and comprehensive explanations for image classification models when presented with occluded images. Our proposed method, Occlusion-Aware Explanation Network (OAENet), explicitly models the occlusion by incorporating a learnable occlusion mask branch alongside the standard explanation generation network. OAENet leverages a novel occlusion-aware loss function that encourages the explanation to focus on the visible regions of the image while also accounting for the potential influence of the occluded areas on the model's prediction. Experiments on benchmark datasets with synthetic and real occlusions demonstrate that OAENet significantly outperforms state-of-the-art XAI methods in terms of explanation accuracy, localization, and robustness to varying occlusion levels. This improved explanation quality for occluded images enables more reliable model debugging and facilitates informed decision-making in applications where occlusions are prevalent."
http://arxiv.org/abs/2302.06588v1,Raising the Cost of Malicious AI-Powered Image Editing,"The proliferation of powerful AI-powered image editing tools presents a significant threat, enabling the creation of increasingly realistic and difficult-to-detect forgeries. A critical challenge is to develop methods that increase the computational and data resources required to generate convincing manipulated images, thereby raising the cost of malicious use. We propose a framework, termed ""Adversarial Perturbation for Provenance Degradation (APPD),"" that introduces imperceptible, targeted perturbations during image capture, designed to disrupt the training and fine-tuning of generative adversarial networks (GANs) and diffusion models used for image manipulation. APPD leverages an adversarial training scheme to optimize perturbations that maximize the error rate of downstream forgery tasks while maintaining perceptual similarity to the original image. Experiments demonstrate that images pre-processed with APPD require significantly larger training datasets and computational resources to achieve comparable forgery realism compared to unperturbed images, effectively increasing the cost of generating high-quality forgeries by 2-3x. This work offers a proactive defense mechanism to mitigate the potential misuse of AI-powered image editing by increasing the resources required for malicious actors to create convincing forgeries."
http://arxiv.org/abs/2506.15903v1,VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics,"Vector graphics offer a powerful and efficient representation for images, enabling scalability and editability. However, manipulating vector graphics programmatically based on natural language instructions remains a challenging task, primarily due to the lack of suitable training data. To address this, we introduce VectorEdits, a novel dataset and benchmark for instruction-based vector graphic editing. VectorEdits comprises over 60,000 pairs of vector graphics and corresponding natural language editing instructions, covering a diverse range of editing operations such as shape modification, color changes, and object rearrangement. We also propose a transformer-based model that directly predicts SVG code conditioned on both the initial vector graphic and the textual instruction. Experiments demonstrate the complexity of VectorEdits, revealing a significant performance gap between our model and human performance, highlighting the challenges in understanding and executing intricate vector graphic manipulations from language. VectorEdits provides a valuable resource for advancing research in instruction-following, vector graphics manipulation, and vision-language understanding."
http://arxiv.org/abs/2011.04127v1,What Does CNN Shift Invariance Look Like? A Visualization Study,"Convolutional Neural Networks (CNNs) are widely used in computer vision due to their ability to learn spatial hierarchies and extract features robust to local variations. However, the precise nature and extent of this shift invariance, especially within different layers of the network, remain poorly understood. This paper addresses the problem of visualizing and characterizing the shift invariance properties of CNNs in a layer-wise manner. We propose a visualization technique that systematically shifts input images and tracks the corresponding changes in feature map activations across various CNN layers. By quantifying the similarity between the original and shifted activations using metrics like cosine similarity and normalized cross-correlation, we generate detailed shift invariance maps. Our results reveal that shift invariance generally increases with network depth, but exhibits significant variations depending on the specific layer type, architecture, and the magnitude of the applied shift. Furthermore, we observe that certain layers exhibit localized regions of high and low shift sensitivity, suggesting a more nuanced and spatially dependent form of invariance than previously appreciated. This visualization study provides valuable insights into the internal workings of CNNs and their inherent robustness to spatial transformations, which can inform the design of more efficient and reliable vision systems."
http://arxiv.org/abs/2012.12076v1,MetaAugment: Sample-Aware Data Augmentation Policy Learning,"Data augmentation is a crucial technique for improving the generalization and robustness of deep learning models, particularly in scenarios with limited data. However, manually designing effective augmentation policies is often a time-consuming and sub-optimal process. This paper addresses the challenge of automatically learning data augmentation policies tailored to individual training samples. We introduce MetaAugment, a novel meta-learning framework that learns augmentation policies by optimizing for sample-specific performance. MetaAugment uses a bi-level optimization strategy: the outer loop trains a meta-controller to generate augmentation policies based on the characteristics of each training sample, while the inner loop trains the main model using the sample-specific augmented data. The meta-controller is trained to maximize the validation performance of the main model, effectively learning which augmentations are most beneficial for each sample. Experimental results on CIFAR-10, CIFAR-100, and SVHN datasets demonstrate that MetaAugment significantly outperforms state-of-the-art data augmentation techniques, achieving higher accuracy and robustness with fewer computational resources. This highlights the potential of sample-aware data augmentation to improve the efficiency and effectiveness of deep learning model training."
http://arxiv.org/abs/2506.21411v1,Distributed Cross-Channel Hierarchical Aggregation for Foundation Models,"Foundation models have achieved remarkable success in various computer vision tasks, yet their computational demands hinder deployment in resource-constrained environments. Existing methods for efficient inference often overlook the potential for distributed computation and the inherent hierarchical structure within feature representations. This paper addresses the challenge of efficiently aggregating cross-channel information in foundation models within a distributed computing framework. We propose Distributed Cross-Channel Hierarchical Aggregation (DCHA), a novel approach that partitions feature maps across multiple devices and employs a hierarchical aggregation strategy. DCHA first performs local cross-channel aggregation on each device, followed by a distributed aggregation step that leverages a tree-based communication scheme to minimize communication overhead. Experiments on image classification and object detection tasks demonstrate that DCHA achieves significant speedups compared to centralized aggregation methods, while maintaining competitive accuracy. The proposed DCHA framework facilitates the deployment of large-scale foundation models in edge computing scenarios and other distributed environments."
http://arxiv.org/abs/2507.22076v1,Test-time Prompt Refinement for Text-to-Image Models,"Text-to-image diffusion models have demonstrated remarkable capabilities in generating diverse and high-quality images from textual descriptions. However, these models often struggle to accurately capture fine-grained details and nuanced semantic information present in complex prompts, leading to inaccuracies or inconsistencies in the generated images. This paper addresses the problem of improving the fidelity of text-to-image generation by refining the input prompt at test time, without requiring any additional training. We propose a novel test-time prompt refinement (TPR) framework that leverages the internal representations of the diffusion model to iteratively optimize the prompt embedding. Specifically, we use a gradient-based approach to adjust the prompt embedding in the direction that minimizes the discrepancy between the generated image features and the features expected based on the original prompt, effectively aligning the model's understanding with the user's intent. Experimental results on a variety of complex prompts demonstrate that TPR significantly improves image quality and semantic accuracy compared to baseline methods, leading to more faithful and visually appealing generations. This approach offers a practical and efficient way to enhance the performance of existing text-to-image models without the need for extensive retraining or fine-tuning."
http://arxiv.org/abs/2107.07009v1,Free-Text Keystroke Dynamics for User Authentication,"Keystroke dynamics, the analysis of typing patterns, offer a promising approach to continuous user authentication. However, traditional methods often rely on fixed text or password-based authentication, limiting their applicability in real-world scenarios where users engage in free-text input. This paper addresses the challenge of developing a robust and practical keystroke dynamics authentication system that operates effectively with free-text input. We propose a novel approach that combines deep learning techniques with feature engineering specifically designed for free-text keystroke dynamics. Our method leverages a recurrent neural network (RNN) architecture, specifically LSTMs, to capture the temporal dependencies within keystroke sequences, and incorporates features derived from n-grams and character-level statistics to enhance discriminative power. We evaluate our system on a publicly available dataset of free-text typing patterns, demonstrating a significant improvement in authentication accuracy compared to existing methods, achieving an EER reduction of 15% and an AUC improvement of 8%. This research provides a practical and accurate solution for continuous user authentication in free-text environments, enhancing security without compromising user experience."
http://arxiv.org/abs/2108.06302v1,Context Aware Object Geotagging,"Geotagging images by associating them with geographic coordinates has become ubiquitous, yet associating individual objects *within* an image with precise geographic locations remains a challenge. This paper addresses the problem of context-aware object geotagging, where the geographic location of an object is inferred not only from its appearance but also from the surrounding scene context. Our proposed method leverages a novel graph neural network (GNN) architecture that integrates visual features of the object and its surrounding context, represented as a scene graph, along with external geospatial data sources like OpenStreetMap. The GNN propagates information between objects and scene elements, learning contextual embeddings that are then used to predict the object's latitude and longitude. Experiments on a newly curated dataset of street-view images with object-level annotations demonstrate that our context-aware approach significantly outperforms baseline methods that rely solely on object appearance or image-level geotags, achieving a 25% reduction in localization error. This improved accuracy in object geotagging has significant implications for applications like autonomous navigation, augmented reality, and urban planning."
http://arxiv.org/abs/2312.13480v1,InvertibleNetworks.jl: A Julia package for scalable normalizing flows,"Normalizing flows, which transform a simple probability distribution into a complex one via a series of invertible transformations, have become a powerful tool for generative modeling and probabilistic inference. However, implementing and scaling these models can be challenging, particularly with custom architectures or high-dimensional data. We introduce InvertibleNetworks.jl, a Julia package designed to facilitate the development and deployment of scalable normalizing flows. The package provides a modular and composable framework for constructing invertible neural networks using a variety of pre-defined invertible layers, including coupling layers, convolutions, and linear transforms, as well as tools for defining custom invertible blocks. Benchmarks demonstrate significant performance improvements over existing implementations, particularly for large-scale image datasets, while maintaining memory efficiency through careful implementation of adjoint sensitivity analysis. InvertibleNetworks.jl lowers the barrier to entry for researchers and practitioners to explore and apply normalizing flows to a wider range of problems."
http://arxiv.org/abs/1312.0786v2,Image Representation Learning Using Graph Regularized Auto-Encoders,"Image representation learning is a crucial task in computer vision, enabling effective solutions for various downstream applications. Auto-encoders have demonstrated promise in learning compact and informative image representations, but often struggle to explicitly capture the underlying data manifold structure. This paper addresses the problem of learning image representations that are both reconstructive and preserve the inherent relationships between data points. We propose a novel Graph Regularized Auto-Encoder (GRAE) framework that integrates a graph Laplacian regularization term into the auto-encoder's objective function. This regularization encourages the learned representations of neighboring data points in the input space to be close in the latent space, thereby preserving the data manifold structure. Experimental results on benchmark image datasets demonstrate that GRAE achieves superior performance compared to standard auto-encoders and other representation learning methods, as evidenced by improved clustering accuracy and image retrieval performance. The proposed GRAE provides a powerful framework for learning robust and meaningful image representations by effectively leveraging graph-based regularization."
http://arxiv.org/abs/1512.01728v1,Similarity Learning via Adaptive Regression and Its Application to Image Retrieval,"Similarity learning aims to learn a distance metric that reflects the semantic similarity between data points, a crucial task for various computer vision applications. Existing methods often rely on fixed or pre-defined regression targets, limiting their ability to adapt to complex data distributions and potentially hindering performance. This paper introduces a novel similarity learning framework called Adaptive Regression for Similarity (ARS), which dynamically adjusts regression targets based on the local neighborhood structure of the data. ARS iteratively refines the regression targets using a graph-based propagation scheme, enabling the model to learn a more nuanced and context-aware similarity metric. Specifically, we leverage an adaptive graph construction method to capture local data relationships, followed by a label propagation strategy to smooth and refine the regression targets. We evaluate ARS on several image retrieval benchmark datasets, demonstrating significant improvements over state-of-the-art similarity learning methods. The proposed method achieves superior retrieval accuracy and robustness, highlighting the effectiveness of adaptive regression in learning discriminative similarity metrics for image retrieval."
http://arxiv.org/abs/2010.07489v1,Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing,"Deep neural networks are vulnerable to backdoor attacks, where adversaries inject malicious patterns into training data, causing the model to misclassify specific inputs containing the trigger while maintaining high accuracy on clean data. Identifying and mitigating these imperceptible backdoor attacks remains a significant challenge. This paper addresses the problem of reverse engineering imperceptible backdoors to facilitate their detection and removal from poisoned datasets. We propose a novel approach that leverages the activation patterns of neurons in the penultimate layer to identify potential trigger locations and characteristics. By analyzing the gradient of the output with respect to these activations for potentially poisoned inputs, we reconstruct an approximate trigger and use it to score the training data for potential backdoor injection. We demonstrate the effectiveness of our method on several benchmark datasets and network architectures, achieving high accuracy in identifying poisoned samples and reconstructing triggers even when the backdoors are designed to be highly imperceptible. Our approach enables effective cleansing of training datasets, leading to significantly more robust and trustworthy deep learning models."
http://arxiv.org/abs/2210.17360v1,Explainable Deep Learning to Profile Mitochondrial Disease Using High Dimensional Protein Expression Data,"Mitochondrial diseases (MDs) are a heterogeneous group of genetic disorders often exhibiting complex clinical and biochemical presentations, making diagnosis challenging. Current diagnostic approaches often rely on invasive procedures and lack the ability to accurately profile disease subtypes based on underlying molecular mechanisms. This paper addresses the need for a non-invasive, interpretable approach to classify and profile MDs using high-dimensional protein expression data obtained from patient samples. We propose a deep learning framework incorporating attention mechanisms and layer-wise relevance propagation (LRP) to classify MD patients and simultaneously identify key protein biomarkers driving these classifications. The attention mechanism allows the model to focus on the most relevant proteins, while LRP provides a quantitative measure of each protein's contribution to the final prediction, thus enhancing model explainability. Our results demonstrate that the proposed method achieves high accuracy in classifying MD subtypes and identifies biologically relevant proteins consistent with known disease mechanisms, outperforming traditional machine learning approaches. This explainable deep learning framework provides a powerful tool for non-invasive MD diagnosis and offers insights into the complex molecular underpinnings of these disorders, paving the way for personalized treatment strategies."
http://arxiv.org/abs/2111.15518v2,"Detecting Adversaries, yet Faltering to Noise? Leveraging Conditional Variational AutoEncoders for Adversary Detection in the Presence of Noisy Images","Adversarial attacks pose a significant threat to the reliability of deep learning models, prompting research into effective detection mechanisms. However, the robustness of these detectors often diminishes significantly when confronted with real-world noise commonly present in image data. This paper addresses the challenge of detecting adversarial examples in the presence of noisy images, where existing methods often misclassify noisy legitimate images as adversarial. We propose a novel adversary detection framework leveraging Conditional Variational Autoencoders (CVAEs). Our CVAE is conditioned on the predicted class label, enabling it to learn a robust latent space representation of clean images and to effectively reconstruct noisy, legitimate images. Adversarial examples, conversely, exhibit larger reconstruction errors and deviate from the learned latent space, facilitating their detection. Experimental results on benchmark datasets demonstrate that our method significantly outperforms existing state-of-the-art adversary detection techniques under various noise levels, achieving a substantial improvement in detection accuracy without sacrificing performance on clean data. This work provides a more reliable and practical approach to adversarial defense in real-world scenarios where noise is prevalent."
http://arxiv.org/abs/2302.03916v1,QS-ADN: Quasi-Supervised Artifact Disentanglement Network for Low-Dose CT Image Denoising by Local Similarity Among Unpaired Data,"Low-dose computed tomography (LDCT) inevitably introduces artifacts and noise, hindering accurate diagnosis. Supervised deep learning methods for LDCT denoising require paired clean CT images, which are difficult and costly to acquire in practice. To address this challenge, we propose a Quasi-Supervised Artifact Disentanglement Network (QS-ADN) leveraging local similarity among unpaired LDCT and normal-dose CT (NDCT) data. QS-ADN consists of two key components: an artifact disentanglement module that separates artifact-related and structure-related features using adversarial learning and a quasi-supervised refinement module that leverages local similarity to refine the disentangled features. Specifically, we employ a non-local block to capture and utilize local similarity information within and between the LDCT and NDCT domains, guiding the denoising process without explicit paired supervision. Experimental results on benchmark datasets demonstrate that QS-ADN achieves superior denoising performance compared to state-of-the-art unpaired methods, effectively suppressing artifacts and preserving fine details. The proposed method offers a practical and effective solution for LDCT denoising, reducing radiation exposure while maintaining diagnostic image quality."
http://arxiv.org/abs/2405.17035v4,Glauber Generative Model: Discrete Diffusion Models via Binary Classification,"Diffusion models have achieved remarkable success in image generation, but their application to discrete data remains challenging due to the continuous nature of the diffusion process. This paper addresses the problem of developing an effective and efficient discrete diffusion model suitable for generating structured discrete data like shapes and sequences. We introduce the Glauber Generative Model (GGM), a discrete diffusion model inspired by the Glauber dynamics from statistical physics. GGM formulates the diffusion process as a sequence of stochastic binary classifications, iteratively flipping individual bits according to probabilities learned by a classifier. The generative process then reverses this diffusion by learning to predict the original bit value given the noisy state. Experiments on various discrete datasets, including shape generation and protein sequence design, demonstrate that GGM achieves competitive or superior performance compared to existing discrete diffusion models in terms of sample quality and training stability. The proposed approach offers a novel and interpretable framework for discrete data generation, bridging the gap between continuous diffusion models and discrete data domains."
http://arxiv.org/abs/2409.13235v1,Balancing Label Imbalance in Federated Environments Using Only Mixup and Artificially-Labeled Noise,"Federated learning (FL) enables collaborative model training without direct data sharing, but performance can be severely impacted by label imbalance across clients. This paper addresses the challenge of label imbalance in federated learning without relying on complex re-sampling strategies, sensitive data sharing, or proxy datasets. We propose a novel federated learning framework that leverages Mixup data augmentation and artificially-labeled noise injection at the client level. Our approach, dubbed FedMixNoise, balances local data distributions by creating synthetic samples through Mixup and strategically introducing noisy data points with labels selected to counter the local imbalance. Experiments on benchmark datasets demonstrate that FedMixNoise consistently outperforms state-of-the-art federated learning algorithms in imbalanced scenarios, achieving significant improvements in both average accuracy and tail class performance while maintaining privacy. This work provides a simple yet effective solution for mitigating the adverse effects of label imbalance in federated learning environments, promoting fairness and robustness without compromising data privacy."
http://arxiv.org/abs/2410.13925v1,FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model,"Diffusion models have achieved remarkable success in image generation, often relying on computationally expensive U-Nets as their backbone. While recent efforts have explored Vision Transformers (ViTs) to improve scalability and incorporate global context, challenges remain in balancing computational efficiency with performance, especially when adapting ViTs to the unique demands of diffusion model training. This paper introduces FiTv2, a scalable and improved Flexible Vision Transformer specifically designed for diffusion models. FiTv2 leverages a novel multi-resolution attention mechanism that dynamically adjusts the receptive field based on feature importance, coupled with a streamlined architecture that reduces redundant computations. Furthermore, we incorporate a learned positional encoding scheme that adapts to varying input resolutions encountered during the diffusion process. Experimental results on benchmark datasets demonstrate that FiTv2 achieves state-of-the-art image quality with significantly reduced computational costs compared to existing ViT-based and U-Net diffusion models. FiTv2 offers a more efficient and scalable solution for high-resolution image generation with diffusion models, paving the way for broader applications and further research in this rapidly evolving field."
http://arxiv.org/abs/2501.06059v1,COMIX: Compositional Explanations using Prototypes,"Explainable AI (XAI) aims to make opaque machine learning models more transparent and understandable. However, many existing XAI methods struggle to provide explanations that are both faithful to the model's decision-making process and intuitively understandable by humans. We address this limitation by introducing COMIX, a novel approach for generating compositional explanations using prototypes. COMIX learns a set of representative prototypes for each class and then explains individual predictions by identifying the prototypes most relevant to the input. Crucially, COMIX decomposes the input into semantically meaningful components and attributes the contribution of each component to the relevance scores of the selected prototypes. Experimental results on image classification tasks demonstrate that COMIX generates explanations that are more faithful to the model's behavior and more interpretable to humans compared to existing XAI methods, as measured by established metrics like faithfulness and human-subject evaluation. This improved interpretability and faithfulness of explanations can foster greater trust in AI systems and facilitate their responsible deployment."
http://arxiv.org/abs/2502.02351v1,Exploring the Feasibility of AI-Assisted Spine MRI Protocol Optimization Using DICOM Image Metadata,"Spine MRI is a crucial diagnostic tool, but imaging protocols vary significantly, leading to inconsistencies in image quality, scan time, and overall efficiency. This variability highlights the need for optimized and standardized protocols. This study addresses the problem of inefficient and potentially suboptimal spine MRI protocols by exploring the feasibility of leveraging routinely acquired DICOM image metadata to inform AI-assisted protocol optimization. We propose a method that utilizes machine learning algorithms to analyze DICOM metadata from existing spine MRI scans, extracting key parameters such as sequence type, repetition time (TR), echo time (TE), and field of view (FOV). These parameters are then correlated with subjective image quality scores assigned by expert radiologists. Preliminary results demonstrate a strong correlation between specific DICOM metadata parameters and perceived image quality, suggesting the potential to predict image quality based on protocol settings. This approach offers a pathway towards developing AI-driven tools capable of recommending optimized spine MRI protocols tailored to individual patient needs and institutional resources, ultimately improving diagnostic accuracy and workflow efficiency."
http://arxiv.org/abs/2504.07465v1,Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying,"Accurate moisture content prediction is crucial for optimizing apple drying processes, ensuring product quality, and minimizing energy consumption. Current methods often rely on destructive sampling or limited sensor data, hindering real-time monitoring and control. This paper addresses the challenge of accurately predicting moisture content in apple drying by leveraging multi-modal data fusion. We propose a novel approach that integrates hyperspectral imaging, providing detailed spectral information about the apple surface, with thermal imaging, capturing temperature distribution, and combines them through a deep learning framework. Specifically, we employ a convolutional neural network (CNN) architecture to extract features from both modalities, followed by a fusion layer to create a joint representation used for moisture content regression. Experimental results demonstrate that our multi-modal approach significantly outperforms single-modality models, achieving a mean absolute error of 0.8% moisture content, representing a 30% improvement compared to using hyperspectral data alone. This approach offers a non-destructive and accurate method for real-time moisture content monitoring, paving the way for enhanced control and optimization of apple drying processes."
http://arxiv.org/abs/2505.01008v1,Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content,"The proliferation of AI-generated content raises critical questions regarding accountability and the potential for misuse, necessitating robust detection mechanisms. Identifying AI-generated content in a black-box scenario, where access to the generative model is unavailable, poses a significant challenge for establishing liability. We introduce a novel recovery-based approach leveraging the inherent biases and artifacts embedded within AI-generated outputs. Our method trains a lightweight recovery network to reconstruct a latent representation from input images and then analyzes the reconstruction error distribution compared to that of genuine images. Specifically, we hypothesize and demonstrate that AI-generated images exhibit statistically different reconstruction error characteristics due to the constraints imposed by the generative process. Experiments on a diverse set of generative models and image datasets demonstrate a high degree of accuracy (AUC > 0.95) in distinguishing AI-generated content from real images, even across different architectures and resolutions. This recovery-based black-box detection method provides a practical and effective tool for identifying AI-generated content, facilitating the development of responsible AI practices and establishing liability in the generative era."
http://arxiv.org/abs/2010.10050v1,Deep Low-Shot Learning for Biological Image Classification and Visualization from Limited Training Samples,"Biological image classification plays a crucial role in various research areas, including disease diagnosis and drug discovery. However, obtaining large, labeled datasets for specific biological entities is often challenging and expensive, hindering the application of deep learning techniques. This paper addresses the problem of accurate biological image classification when only limited training samples are available, a scenario known as low-shot learning. We propose a novel deep low-shot learning framework that combines a meta-learning approach with a carefully designed feature embedding network tailored for biological image characteristics. Our method utilizes a Siamese network architecture trained with an episodic learning strategy to learn discriminative features from a limited number of labeled examples and generalize effectively to unseen classes. Experimental results on diverse biological image datasets demonstrate that our proposed method significantly outperforms existing state-of-the-art low-shot learning techniques, achieving substantial improvements in classification accuracy and providing insightful visualizations of learned feature representations. This research offers a practical and effective solution for biological image classification with limited data, accelerating scientific discovery and enabling broader applications of deep learning in biology."
http://arxiv.org/abs/2211.09061v1,Squeeze flow of micro-droplets: convolutional neural network with trainable and tunable refinement,"Microfluidic devices offer precise control over droplet generation and manipulation, enabling advancements in drug delivery, chemical synthesis, and biological assays. Accurate prediction of droplet dynamics within these devices, particularly during squeeze flow regimes, is crucial for optimizing device design and operational parameters. However, traditional computational fluid dynamics (CFD) simulations are computationally expensive and time-consuming. This paper addresses the challenge of predicting squeeze flow dynamics of micro-droplets by introducing a novel convolutional neural network (CNN) architecture incorporating trainable and tunable refinement. Our approach leverages a CNN to predict the initial velocity field, followed by a refinement module that allows for adaptive adjustment of the predicted flow based on learned parameters and tunable physical constraints, such as mass conservation. We demonstrate that our method significantly improves prediction accuracy compared to standard CNNs and achieves comparable accuracy to CFD simulations at a fraction of the computational cost, particularly in capturing the complex flow patterns during droplet squeezing. This efficient and accurate prediction framework enables rapid prototyping and optimization of microfluidic devices for various applications."
http://arxiv.org/abs/2301.11962v2,On the Feasibility of Machine Learning Augmented Magnetic Resonance for Point-of-Care Identification of Disease,"Magnetic Resonance Imaging (MRI) offers unparalleled soft tissue contrast, but its cost and complexity limit accessibility, particularly in point-of-care settings. This work addresses the challenge of enabling rapid, on-site disease identification using machine learning to augment low-field, portable MRI. We propose a convolutional neural network architecture trained on simulated and limited high-field MRI data, then fine-tuned on a small dataset acquired from a low-field portable MRI scanner. The network directly classifies image volumes into disease categories, bypassing traditional image reconstruction and segmentation steps. We demonstrate the feasibility of this approach on a simulated dataset of brain pathologies, achieving an average classification accuracy of 87% on unseen low-field MRI data after fine-tuning with only 20 real-world images. These results suggest that machine learning can effectively bridge the gap between high-field training data and low-field portable MRI, enabling rapid and accessible disease diagnosis at the point-of-care."
http://arxiv.org/abs/1502.05134v2,Supervised cross-modal factor analysis for multiple modal data classification,"Multi-modal data, arising from diverse sensors and sources, offers rich information for comprehensive scene understanding. However, effectively integrating information across different modalities with potentially complex interdependencies remains a significant challenge in multi-modal classification. This paper addresses the problem of learning a shared latent space from multiple modalities while simultaneously leveraging supervision to enhance classification performance. We propose Supervised Cross-Modal Factor Analysis (SCMFA), a novel approach that extends traditional cross-modal factor analysis by incorporating supervised information directly into the latent factor learning process. SCMFA learns a shared latent representation that captures the underlying correlations between modalities while optimizing for class discriminability, achieved through a carefully designed objective function that combines reconstruction error and classification loss. Experiments on benchmark multi-modal datasets demonstrate that SCMFA consistently outperforms state-of-the-art methods, achieving significant improvements in classification accuracy. This highlights the effectiveness of our approach in leveraging supervision to learn more discriminative and informative cross-modal representations, leading to improved multi-modal classification performance."
http://arxiv.org/abs/1707.08273v4,MMGAN: Manifold Matching Generative Adversarial Network,"Generative Adversarial Networks (GANs) have demonstrated remarkable success in generating realistic images, but often struggle with mode collapse and generating samples that faithfully represent the underlying data distribution's manifold. This work addresses the challenge of improving GAN performance by explicitly encouraging the generator's output to match the manifold structure of the real data. We introduce the Manifold Matching Generative Adversarial Network (MMGAN), which incorporates a novel regularization term based on manifold learning principles. MMGAN leverages a learned representation of the real data manifold using a k-Nearest Neighbors graph and encourages the generated samples to exhibit similar neighborhood relationships in the feature space of a pre-trained discriminator. This is achieved by minimizing the difference between the neighborhood structures of real and generated data. Experimental results on several benchmark datasets, including CIFAR-10, CelebA, and LSUN, demonstrate that MMGAN achieves improved Inception Score and Frchet Inception Distance compared to baseline GAN models and other regularization techniques. The proposed method effectively promotes diversity and improves the quality of generated samples by enforcing manifold consistency."
http://arxiv.org/abs/1908.07387v1,NLNL: Negative Learning for Noisy Labels,"Learning with noisy labels remains a significant challenge in computer vision, hindering the development of robust and reliable models. The core problem lies in effectively distinguishing between correctly and incorrectly labeled data points during training. This paper introduces Negative Learning for Noisy Labels (NLNL), a novel approach that explicitly leverages negative learning to mitigate the detrimental effects of noisy labels. NLNL operates by identifying potentially mislabeled instances and treating them as negative examples during the learning process, effectively pushing the model away from fitting the incorrect labels. We achieve this by dynamically adjusting the loss contribution of each sample based on its predicted confidence and agreement with the provided label. Experimental results on benchmark datasets with varying noise levels demonstrate that NLNL consistently outperforms state-of-the-art noisy label learning methods in terms of classification accuracy and robustness. This work provides a valuable tool for training accurate models in the presence of label noise, paving the way for more reliable computer vision systems."
http://arxiv.org/abs/1710.10766v3,PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples,"Adversarial examples pose a significant threat to the reliability of deep learning models, exhibiting imperceptible perturbations that can cause misclassification. Defending against these attacks remains a challenging problem due to the high dimensionality of image space and the diverse attack strategies. This paper addresses the challenge of understanding and mitigating adversarial vulnerability by introducing PixelDefend, a novel defense framework that leverages generative adversarial networks (GANs) to project potentially adversarial inputs onto the learned data manifold. PixelDefend utilizes a pre-trained generator network to map random latent vectors to image space, subsequently optimizing the latent vector to reconstruct the input image. By constraining the reconstruction within the generator's learned distribution, PixelDefend effectively smoothes out adversarial perturbations. Our experiments on benchmark datasets, including MNIST, CIFAR-10, and ImageNet, demonstrate that PixelDefend significantly improves the robustness of classifiers against various adversarial attacks, including FGSM, PGD, and C&W, while maintaining high accuracy on clean examples. This work highlights the potential of generative models in creating robust and interpretable defenses against adversarial attacks, offering a promising direction for developing more resilient deep learning systems."
http://arxiv.org/abs/2201.10471v2,GIU-GANs: Global Information Utilization for Generative Adversarial Networks,"Generative Adversarial Networks (GANs) have demonstrated remarkable success in generating realistic images, but often struggle with capturing global dependencies and maintaining structural coherence in complex scenes. This limitation stems from the local receptive fields of convolutional operators used within the generator and discriminator, hindering their ability to leverage holistic image information. To address this, we propose GIU-GANs, a novel architecture that explicitly integrates Global Information Utilization (GIU) modules into both the generator and discriminator networks. These modules leverage self-attention mechanisms and learnable global context vectors to capture long-range dependencies and encode global scene information. We augment the generator with a GIU module to guide the image generation process with global context, while the discriminator utilizes a GIU module to better assess the overall realism and consistency of generated images. Experiments on benchmark datasets, including CelebA-HQ and LSUN, demonstrate that GIU-GANs achieve significant improvements in image quality, structural integrity, and overall FID scores compared to state-of-the-art GAN architectures. This work highlights the importance of global information integration for GANs and provides a promising avenue for generating more realistic and coherent images."
http://arxiv.org/abs/2202.06596v1,Deep Monte Carlo Quantile Regression for Quantifying Aleatoric Uncertainty in Physics-informed Temperature Field Reconstruction,"Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving inverse problems governed by partial differential equations (PDEs), particularly in reconstructing temperature fields from sparse measurements. However, accurately quantifying the inherent aleatoric uncertainty arising from noisy sensor data and model discrepancies remains a significant challenge. This paper introduces Deep Monte Carlo Quantile Regression (DMCQR) for robust aleatoric uncertainty quantification in physics-informed temperature field reconstruction. DMCQR leverages a PINN architecture trained using quantile regression to directly predict a set of conditional quantiles of the temperature field, effectively capturing the data-dependent uncertainty. Furthermore, we employ a Monte Carlo dropout scheme during inference to sample multiple quantile predictions, enabling a more comprehensive and reliable estimation of the uncertainty distribution. Experiments on benchmark heat conduction problems demonstrate that DMCQR significantly outperforms existing deterministic PINN approaches and other uncertainty quantification methods in terms of calibration and sharpness of the predicted uncertainty intervals. This capability to reliably quantify aleatoric uncertainty is crucial for informed decision-making in various engineering applications involving thermal management and control."
http://arxiv.org/abs/2204.13263v2,Covariance-aware Feature Alignment with Pre-computed Source Statistics for Test-time Adaptation to Multiple Image Corruptions,"Unsupervised Test-Time Adaptation (TTA) aims to adapt a pre-trained model to a new, unseen target domain using only unlabeled test data, mitigating performance degradation caused by domain shifts. A major challenge in TTA is adapting to diverse and potentially concurrent image corruptions without access to source data or requiring explicit domain identification. We address this by proposing a novel Covariance-aware Feature Alignment (CaFA) method that aligns the feature distribution of the corrupted test data with pre-computed source statistics. CaFA estimates the covariance matrix of the source features and uses it to guide feature alignment during test time, effectively capturing the underlying structure of the source feature space. Furthermore, we introduce a computationally efficient pre-computation strategy for source statistics, enabling real-time adaptation without significant overhead. Experiments on various benchmark datasets with multiple image corruptions demonstrate that CaFA consistently outperforms existing TTA methods, achieving significant accuracy improvements, particularly under severe corruption levels. This underscores the effectiveness of covariance-aware feature alignment and pre-computed statistics for robust and efficient test-time adaptation in the presence of diverse image corruptions."
http://arxiv.org/abs/2209.04588v2,Extended Feature Space-Based Automatic Melanoma Detection System,"Melanoma, the deadliest form of skin cancer, necessitates early and accurate detection for improved patient outcomes. Existing automated melanoma detection systems often struggle with variations in lesion appearance and subtle feature differences between benign and malignant lesions, leading to suboptimal performance. This paper proposes an extended feature space-based automatic melanoma detection system designed to enhance diagnostic accuracy. Our approach integrates a diverse set of features, including textural, color, shape, and deep learning-derived features extracted from dermoscopic images. Feature selection using a hybrid approach combining variance thresholding and recursive feature elimination is then employed to identify the most relevant features. Finally, a support vector machine (SVM) classifier is trained on the selected feature subset to differentiate between melanoma and benign lesions. Experimental results on the ISIC archive dataset demonstrate that our proposed system achieves a sensitivity of 92.3% and a specificity of 88.7%, outperforming several state-of-the-art melanoma detection methods. The proposed system offers a robust and accurate tool for assisting dermatologists in the early detection of melanoma, potentially leading to improved patient survival rates."
http://arxiv.org/abs/2310.02897v2,How Much Training Data is Memorized in Overparameterized Autoencoders? An Inverse Problem Perspective on Memorization Evaluation,"Overparameterized neural networks are known to memorize training data, potentially leading to poor generalization. Quantifying this memorization, especially in unsupervised models like autoencoders, remains a challenge. This paper addresses the problem of evaluating memorization in overparameterized autoencoders by framing it as an inverse problem. We propose a novel approach that leverages the reconstruction properties of autoencoders to estimate the amount of information retained about specific training examples. Our method involves carefully perturbing the input space around training data points and analyzing the corresponding changes in the reconstructed outputs. By modeling this relationship as an inverse problem, we can estimate the ""memorization score"" for each training sample. Experiments on several datasets demonstrate that our method can effectively identify memorized samples and correlate the memorization score with factors like data complexity and the autoencoder's architecture. Furthermore, we show that samples with high memorization scores are more susceptible to adversarial attacks and contribute less to the learned latent space structure. This work provides a valuable tool for understanding and mitigating memorization in unsupervised representation learning."
http://arxiv.org/abs/2310.11989v3,Image Clustering with External Guidance,"Image clustering aims to group images based on visual similarity without prior knowledge of class labels, a challenging task in unsupervised learning. However, incorporating external guidance, such as textual descriptions or user-provided constraints, can significantly improve clustering performance. This paper addresses the problem of effectively leveraging diverse forms of external guidance to enhance image clustering. We propose a novel framework, Guided Image Clustering Network (GICN), which jointly learns image representations and cluster assignments while explicitly incorporating external guidance through a multi-modal attention mechanism. GICN learns to weight the importance of different guidance modalities and adaptively integrate them into the clustering process, enabling the network to focus on the most relevant information for accurate cluster formation. Experiments on several benchmark datasets demonstrate that GICN consistently outperforms state-of-the-art image clustering methods, achieving significant improvements in clustering accuracy and normalized mutual information. These results highlight the significant potential of external guidance for improving image clustering and offer a promising direction for future research in unsupervised representation learning."
http://arxiv.org/abs/2311.02373v2,From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models,"Diffusion models have achieved remarkable success in image generation, but their vulnerability to data poisoning attacks remains a significant concern. This paper addresses the novel problem of *bilateral* data poisoning in diffusion models, where both training data and user-provided prompts are simultaneously manipulated to induce targeted generation failures. We introduce a framework for crafting such attacks, strategically injecting poisoned data into the training set and subtly modifying user prompts to maximize the attack's efficacy. Our approach leverages a gradient-based optimization strategy to identify optimal poison data perturbations and prompt modifications that steer the diffusion process towards generating semantically incongruent or visually corrupted outputs when triggered by the poisoned prompts. Experiments on various datasets and diffusion model architectures demonstrate that our bilateral poisoning attacks significantly reduce generation fidelity and introduce targeted vulnerabilities, achieving success rates significantly higher than single-faceted attacks. This research highlights the critical need for robust defense mechanisms against sophisticated data poisoning attacks that exploit the interplay between training data and user input in diffusion-based generative systems."
http://arxiv.org/abs/2311.14410v2,Unveiling The Factors of Aesthetic Preferences with Explainable AI,"Aesthetic preference is a subjective and complex phenomenon, influenced by a multitude of factors ranging from low-level image features to high-level semantic concepts. While deep learning models have achieved impressive performance in aesthetic assessment, they often operate as black boxes, hindering our understanding of the underlying factors driving these predictions. This paper addresses the critical need for explainability in aesthetic assessment by unveiling the specific image characteristics that contribute to perceived aesthetic quality. We propose a novel framework that integrates a state-of-the-art aesthetic quality prediction model with explainable AI (XAI) techniques, specifically Grad-CAM and attention visualization, to identify salient image regions and feature activations that influence the model's aesthetic judgments. Furthermore, we quantify the relationship between these identified features and established aesthetic principles, such as the rule of thirds and color harmony, using specifically designed metrics. Our experiments on benchmark aesthetic datasets reveal strong correlations between XAI-identified regions and aesthetic principles, demonstrating the model's sensitivity to these established guidelines and providing insights into its decision-making process. This work contributes to a deeper understanding of aesthetic perception by bridging the gap between data-driven models and established aesthetic theories, paving the way for more interpretable and controllable aesthetic assessment systems."
http://arxiv.org/abs/2401.08364v1,Weighted Spectral Filters for Kernel Interpolation on Spheres: Estimates of Prediction Accuracy for Noisy Data,"Kernel interpolation on the sphere is a fundamental technique for scattered data approximation with applications in geophysics, cosmology, and computer graphics. However, the performance of standard kernel methods degrades significantly in the presence of noise. This paper addresses the problem of improving the prediction accuracy of kernel interpolation on the sphere when the data is corrupted by noise. We introduce a novel approach that leverages weighted spectral filters tailored to the spherical harmonic decomposition of the kernel. Specifically, we derive optimal filter weights by minimizing a bound on the expected prediction error, accounting for both the kernel's spectral properties and the noise variance. We then demonstrate how this method is easily implemented in practice for any spherical kernel. Through theoretical analysis and numerical experiments, we show that our weighted spectral filters significantly reduce the impact of noise, leading to substantial improvements in prediction accuracy compared to standard kernel interpolation methods, particularly when the noise level is high. This work provides a robust and efficient method for kernel interpolation on the sphere, enhancing its applicability in noisy real-world scenarios."
http://arxiv.org/abs/2407.18114v1,Unsupervised Training of Neural Cellular Automata on Edge Devices,"Neural Cellular Automata (NCAs) have emerged as a promising paradigm for decentralized, self-organizing systems capable of generating complex patterns and behaviors. However, training NCAs typically requires significant computational resources and large datasets, hindering their deployment on resource-constrained edge devices. This paper addresses the challenge of training NCAs directly on edge devices in an unsupervised manner. We propose a novel training framework that leverages a self-supervised objective based on local consistency and global pattern emergence. Specifically, each NCA cell aims to predict the next state of its neighbors, while a global discriminator evaluates the overall coherence and target-like structure of the evolving pattern. We demonstrate the feasibility of our approach by training NCAs on embedded systems, achieving comparable performance to centrally trained models in generating target images and exhibiting robustness to perturbations, all while operating with significantly reduced memory and power footprints. This work paves the way for deploying intelligent, adaptive systems directly on edge devices without reliance on cloud-based training infrastructure."
http://arxiv.org/abs/2409.18907v1,In-depth Analysis of Privacy Threats in Federated Learning for Medical Data,"Federated learning (FL) enables collaborative model training across decentralized devices without direct data sharing, holding promise for medical applications where data privacy is paramount. However, recent studies have revealed vulnerabilities in FL systems that can lead to privacy breaches, particularly when dealing with sensitive medical data. This paper presents an in-depth analysis of privacy threats in FL applied to medical imaging and patient records, focusing on gradient leakage attacks and membership inference attacks. We investigate the effectiveness of these attacks under various realistic scenarios, considering factors such as model complexity, data heterogeneity, and defense mechanisms like differential privacy and gradient clipping. Our analysis includes empirical evaluations on benchmark medical datasets, demonstrating that even with common defense strategies, patient information can still be inferred with significant accuracy, with membership inference attacks achieving up to 85% accuracy in identifying whether a patient's data contributed to the model training. This work highlights the critical need for developing more robust privacy-preserving techniques tailored to the unique challenges posed by medical data in federated learning environments."
http://arxiv.org/abs/2410.08837v1,A physics-guided neural network for flooding area detection using SAR imagery and local river gauge observations,"Accurate and timely flood inundation maps are crucial for disaster response and mitigation. Synthetic Aperture Radar (SAR) imagery offers weather-independent flood monitoring capabilities, but interpreting SAR data for flood extent delineation remains challenging due to speckle noise and complex backscattering mechanisms. This paper addresses the problem of improving flood area detection accuracy in SAR imagery by incorporating physical principles and leveraging readily available river gauge observations. We propose a physics-guided neural network (PGNN) that integrates the hydraulic relationship between river water level and inundation extent with deep learning. The PGNN architecture combines a U-Net-based segmentation network for SAR image processing with a physics-informed loss function that penalizes deviations from the expected inundation patterns based on local river gauge readings and a simplified hydraulic model. Experimental results on Sentinel-1 SAR data for multiple flood events demonstrate that the PGNN achieves significantly higher accuracy and robustness compared to purely data-driven deep learning methods and traditional thresholding techniques, particularly in areas with complex topography and vegetation cover. This approach offers a practical and scalable solution for improved flood monitoring and risk assessment."
http://arxiv.org/abs/2501.18405v1,Segmentation of cracks in 3d images of fiber reinforced concrete using deep learning,"Fiber reinforced concrete (FRC) is a widely used construction material, and crack detection is crucial for assessing its structural integrity and durability. Traditional crack detection methods often rely on manual inspection or 2D image analysis, which are time-consuming and lack the ability to capture the complex 3D crack networks within the material. This paper addresses the challenge of automatically and accurately segmenting cracks in 3D images of FRC. We propose a novel deep learning framework based on a 3D U-Net architecture, incorporating a squeeze-and-excitation mechanism to enhance feature representation and attention to relevant crack information. Furthermore, we introduce a custom loss function that combines binary cross-entropy with a Dice loss to address the class imbalance issue inherent in crack segmentation tasks. Experimental results on a newly acquired 3D FRC crack dataset demonstrate that our proposed method significantly outperforms existing 3D segmentation techniques, achieving a Dice score of 0.82 and an Intersection over Union (IoU) of 0.70. This automated and accurate 3D crack segmentation provides a valuable tool for non-destructive evaluation and structural health monitoring of FRC structures."
http://arxiv.org/abs/2504.07611v1,Conditional Conformal Risk Adaptation,"Modern machine learning models often struggle to provide reliable uncertainty estimates, hindering their deployment in safety-critical applications. While conformal prediction offers a distribution-free approach to quantifying uncertainty by constructing prediction sets with guaranteed coverage, standard methods often produce sets that are either overly conservative or fail to adapt to varying risk tolerances across different input regions. This paper addresses the problem of adapting conformal risk to achieve desired coverage levels conditioned on specific input features. We introduce Conditional Conformal Risk Adaptation (CCRA), a novel framework that learns a feature-dependent risk score, enabling the construction of prediction sets with locally calibrated coverage. CCRA leverages a meta-learning approach to dynamically adjust the non-conformity scores based on observed coverage errors in different feature regions. Experiments on image classification and object detection tasks demonstrate that CCRA significantly improves conditional coverage while maintaining marginal coverage guarantees, leading to more efficient and reliable uncertainty quantification. The ability to adapt conformal risk based on input features enhances the practicality of conformal prediction in real-world applications where varying risk tolerances are paramount."
http://arxiv.org/abs/1612.04739v1,"An Architecture for Deep, Hierarchical Generative Models","Deep generative models have demonstrated impressive capabilities in learning complex data distributions, but often struggle with disentangling hierarchical representations. This work addresses the challenge of learning and representing data with inherent hierarchical structures using deep generative models. We introduce a novel architecture, the Hierarchical Generative Network (HGN), which leverages a cascade of variational autoencoders (VAEs) to explicitly model multiple levels of abstraction. The HGN employs a top-down generative process where higher-level latent variables influence the generation of lower-level latent variables, ultimately leading to the reconstruction of the input data. We demonstrate the effectiveness of the HGN on datasets with known hierarchical structure, such as human pose and scene understanding tasks, achieving state-of-the-art results in disentanglement and controllable generation. The proposed architecture provides a powerful framework for learning structured representations and generating complex data with interpretable hierarchical features, paving the way for improved performance in downstream tasks requiring structured understanding."
http://arxiv.org/abs/1904.09029v1,Deep Learning for Power System Security Assessment,"Power system security assessment (PSSA) is crucial for ensuring the reliable and stable operation of electrical grids. Traditional PSSA methods often struggle with the increasing complexity and dimensionality of modern power systems, leading to computational bottlenecks. This paper addresses the challenge of efficient and accurate PSSA by leveraging deep learning techniques. We propose a novel deep neural network architecture, the Security-Aware Transformer Network (SATN), which integrates transformer layers with graph convolutional networks to capture both temporal dependencies and topological information within the power grid. SATN learns complex relationships between system operating conditions and security margins by utilizing historical operational data for training. Experimental results on benchmark power system datasets demonstrate that SATN achieves significantly higher accuracy and faster prediction speeds compared to traditional machine learning methods and existing deep learning approaches for PSSA. This improved performance enables proactive control actions and enhances the overall resilience of power systems against cascading failures."
http://arxiv.org/abs/2106.00203v2,Hybrid Generative Models for Two-Dimensional Datasets,"Generative models have demonstrated remarkable success in synthesizing high-quality data across various domains, including images and audio. However, many real-world datasets exhibit complex structures that are not easily captured by single generative models. This paper addresses the challenge of modeling two-dimensional datasets with intricate dependencies and multi-modal distributions by proposing a hybrid generative approach that combines the strengths of Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). Our method, termed ""VAE-GAN Synergy"" (VGS), leverages a VAE to learn a disentangled latent space representing the underlying data distribution, while a GAN refines the generated samples, enhancing their realism and fidelity. Specifically, the VAE provides a strong inductive bias and regularization, guiding the GAN's generator to produce samples within a plausible data manifold. Experimental results on benchmark datasets demonstrate that VGS achieves superior performance in terms of Frchet Inception Distance (FID) and Kernel Inception Distance (KID) scores compared to state-of-the-art VAEs and GANs. The proposed hybrid approach offers a powerful and flexible framework for generating high-quality samples from complex two-dimensional datasets, paving the way for advancements in various applications, including image synthesis, data augmentation, and anomaly detection."
http://arxiv.org/abs/2106.09667v2,Poisoning and Backdooring Contrastive Learning,"Contrastive learning (CL) has emerged as a powerful paradigm for self-supervised representation learning, achieving state-of-the-art performance across various downstream tasks. However, the vulnerability of CL to adversarial attacks, particularly data poisoning and backdoor attacks, remains largely unexplored. This paper investigates the susceptibility of contrastive learning to such attacks, demonstrating that even subtle data manipulations can significantly degrade the quality of learned representations or embed hidden triggers. We propose two novel attack strategies tailored for CL: a poisoning attack that corrupts a small fraction of the unlabeled training data to globally degrade performance, and a backdoor attack that injects a trigger into specific data instances, causing the model to misclassify samples containing the trigger at inference time. Our experiments on benchmark datasets, including CIFAR-10 and ImageNet, reveal that both poisoning and backdoor attacks can severely compromise the performance of CL models with relatively low poisoning rates (e.g., <5%). Specifically, the backdoor attack achieves >95% attack success rate while maintaining high clean accuracy. These findings highlight the critical need for robust defense mechanisms to safeguard contrastive learning systems against malicious data manipulation, ensuring their reliability and security in real-world applications."
http://arxiv.org/abs/1802.00518v1,Analysis of Fast Alternating Minimization for Structured Dictionary Learning,"Structured dictionary learning aims to learn a dictionary that not only sparsely represents data but also exhibits desirable structural properties, often crucial for interpretability and generalization. Alternating minimization (AM) is a prevalent approach for solving structured dictionary learning problems; however, its computational cost can be prohibitive, especially for large-scale datasets and high-dimensional dictionaries. This paper investigates the performance of fast alternating minimization (FAM) algorithms, specifically those employing efficient proximal operators and accelerated gradient methods, within the context of structured dictionary learning. We propose a novel FAM framework that leverages the structure-inducing regularizer to design computationally efficient proximal steps for both dictionary update and sparse coding stages. Our framework incorporates acceleration techniques to further reduce the iteration complexity. Experimental results on synthetic and real-world datasets, including image denoising and classification tasks, demonstrate that our proposed FAM algorithms achieve significant speedups compared to standard AM approaches while maintaining comparable or even superior reconstruction accuracy and classification performance. This work provides a practical and efficient solution for large-scale structured dictionary learning, enabling its application in resource-constrained environments and complex computer vision tasks."
http://arxiv.org/abs/2107.00003v1,Understanding Adversarial Examples Through Deep Neural Network's Response Surface and Uncertainty Regions,"Deep neural networks are vulnerable to adversarial examples, imperceptible perturbations that can drastically alter network predictions. This sensitivity stems from the complex, high-dimensional response surface of the network, yet a comprehensive understanding of how these perturbations navigate that surface remains elusive. This paper addresses the problem of characterizing the relationship between adversarial perturbations, the DNN's response surface, and the associated uncertainty regions. We propose a novel approach that leverages Bayesian Neural Networks (BNNs) to map the DNN's response surface and estimate prediction uncertainty in the vicinity of adversarial examples. By analyzing the gradient direction, magnitude, and uncertainty estimates, we reveal that adversarial examples often reside in regions of high gradient and elevated uncertainty, indicating areas where the network's decision boundary is highly sensitive and less confident. Our experiments on benchmark datasets demonstrate that adversarial examples are not merely isolated points of misclassification, but rather exist within broader uncertainty regions. This understanding is crucial for developing more robust and reliable deep learning systems, paving the way for more effective defense strategies against adversarial attacks."
http://arxiv.org/abs/1602.03822v8,"A Critical Connectivity Radius for Segmenting Randomly-Generated, High Dimensional Data Points","Clustering high-dimensional data points is a fundamental task in machine learning and computer vision, often relying on distance metrics to define neighborhood relationships. Determining an appropriate connectivity radius for graph-based segmentation methods becomes challenging in high-dimensional spaces due to the curse of dimensionality, leading to either disconnected components or overly dense graphs. We address the problem of identifying a critical connectivity radius that effectively balances these opposing forces for randomly generated data points in high dimensions. Our approach involves deriving a theoretical estimate for this critical radius based on the expected distribution of pairwise distances, incorporating dimensionality reduction techniques to mitigate the curse of dimensionality. We then validate this estimate through extensive experiments on synthetic datasets with varying dimensionality and data point densities, demonstrating a significant improvement in segmentation accuracy compared to using fixed or heuristically chosen radii. This work provides a principled method for selecting the connectivity radius, enabling more robust and accurate segmentation of high-dimensional data in various applications."
http://arxiv.org/abs/2103.11163v2,An Empirical Framework for Domain Generalization in Clinical Settings,"Domain generalization (DG) aims to train models on multiple source domains that can generalize effectively to unseen target domains, a crucial capability for deploying computer vision algorithms in diverse clinical settings. However, existing DG methods often fall short in clinical applications due to the inherent complexity and heterogeneity of medical data. This paper addresses the challenge of developing a robust empirical framework for DG specifically tailored for clinical image analysis. We propose a novel DG strategy that combines adversarial domain alignment with meta-learning to learn domain-invariant features and simulate domain shifts during training. Specifically, we use a gradient reversal layer to encourage domain confusion in the feature space, coupled with a meta-learning objective that optimizes for generalization performance across simulated unseen domains constructed by perturbing existing source domains. We evaluate our framework on three challenging clinical image datasets, including diabetic retinopathy grading, skin lesion classification, and chest X-ray diagnosis, demonstrating consistent improvements in out-of-distribution performance compared to state-of-the-art DG algorithms. Our results highlight the effectiveness of the proposed framework in enhancing the robustness and generalizability of clinical computer vision models, paving the way for more reliable and equitable healthcare applications."
http://arxiv.org/abs/2108.08643v1,Batch Curation for Unsupervised Contrastive Representation Learning,"Unsupervised contrastive learning has shown remarkable success in learning visual representations without explicit labels, often relying on large batch sizes to approximate the underlying data distribution. However, naively increasing batch size can lead to the inclusion of noisy or less informative negative samples, hindering the learning process and potentially collapsing the learned representations. This paper addresses the problem of optimizing batch composition in unsupervised contrastive learning to improve the quality of negative samples. We propose Batch Curation, a novel approach that dynamically selects and weights negative samples within each batch based on their similarity to the anchor sample, using a learned similarity metric. This allows us to prioritize more informative and diverse negative samples, mitigating the impact of noisy negatives. Experiments on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet demonstrate that Batch Curation consistently improves performance over standard contrastive learning methods, achieving significant gains in downstream classification accuracy. These results highlight the importance of carefully curating batches for unsupervised contrastive learning and offer a promising direction for future research in self-supervised representation learning."
http://arxiv.org/abs/2202.06491v5,Adversarial Graph Contrastive Learning with Information Regularization,"Graph contrastive learning (GCL) has emerged as a powerful self-supervised technique for learning node embeddings by maximizing the agreement between different views of the same graph. However, existing GCL methods are vulnerable to adversarial attacks that can significantly degrade the quality of learned representations. To address this vulnerability, we propose Adversarial Graph Contrastive Learning with Information Regularization (AdvGCL-IR), a novel framework that simultaneously learns robust node embeddings and generates adversarial graph perturbations. AdvGCL-IR introduces an adversarial training objective that minimizes the mutual information between node embeddings derived from the original and perturbed graphs, thereby forcing the encoder to learn representations invariant to adversarial attacks. Furthermore, we incorporate an information regularization term that maximizes the entropy of the learned embeddings, preventing mode collapse and encouraging the encoder to capture a broader range of structural information. Empirical evaluations on several benchmark datasets demonstrate that AdvGCL-IR significantly outperforms state-of-the-art GCL methods under various adversarial attack scenarios, achieving improvements of up to 15% in node classification accuracy. This robust and information-rich graph representation learning framework provides a crucial step towards deploying GCL models in security-sensitive applications."
http://arxiv.org/abs/2206.07087v1,Combining Counterfactuals With Shapley Values To Explain Image Models,"Explaining the decisions of image classification models is crucial for building trust and ensuring fairness. While methods like Shapley values and counterfactual explanations offer valuable insights, they often operate in isolation, providing either attribution scores or alternative image scenarios without a direct connection. This paper addresses the need for a more comprehensive explanation framework that bridges the gap between feature importance and actionable image modifications. We propose a novel approach that combines Shapley values with counterfactual generation to provide counterfactual-aware Shapley values. Our method leverages a differentiable approximation of Shapley values and integrates a counterfactual loss during the attribution process. This allows us to generate Shapley maps that emphasize features most relevant to reaching a desired counterfactual outcome, effectively linking attribution to potential image manipulations. Experiments on benchmark datasets demonstrate that our approach generates more targeted and interpretable Shapley maps compared to standard methods, highlighting features that are both important for the original prediction and influential in achieving a specific counterfactual. This unified framework offers a more nuanced understanding of image model decisions, paving the way for more reliable and trustworthy AI systems."
http://arxiv.org/abs/2211.10670v2,Towards Adversarial Robustness of Deep Vision Algorithms,"Deep neural networks have achieved remarkable performance in various computer vision tasks, but their vulnerability to adversarial examples poses a significant challenge for real-world deployment. This paper addresses the problem of improving the adversarial robustness of deep vision algorithms against carefully crafted, imperceptible perturbations. We introduce a novel adversarial training framework, termed ""Feature Scattering Adversarial Training (FSAT),"" that encourages the model to learn more robust feature representations by explicitly maximizing the feature-level distance between clean and adversarial examples. FSAT incorporates a feature scattering loss that penalizes the model when adversarial examples produce features similar to those of clean examples within a batch, effectively forcing the network to learn discriminative features even under attack. Experiments on benchmark datasets, including CIFAR-10 and ImageNet, demonstrate that FSAT significantly improves adversarial robustness against various attack strategies, such as FGSM, PGD, and C&W, while maintaining competitive clean accuracy compared to state-of-the-art adversarial training methods. This approach offers a promising direction for developing more reliable and secure deep vision systems."
http://arxiv.org/abs/2211.13895v1,Identifying Incorrect Annotations in Multi-Label Classification Data,"Multi-label classification, where each instance can be associated with multiple labels simultaneously, is crucial for many real-world applications. However, datasets used for training these models often contain incorrect or missing annotations, which can significantly degrade performance. This paper addresses the problem of automatically identifying and flagging incorrectly annotated instances in multi-label classification datasets. We propose a novel approach that leverages a combination of label co-occurrence statistics and a self-supervised learning framework. Specifically, we train a model to predict masked labels based on observed labels and contextual information, then use the prediction discrepancy to quantify the likelihood of an annotation error. Furthermore, we incorporate a label co-occurrence graph to refine the error estimation, penalizing annotations that deviate significantly from expected label relationships. Experiments on several benchmark multi-label datasets demonstrate that our method effectively identifies incorrectly annotated instances with high precision and recall, outperforming existing baseline approaches. This capability allows for improved data quality, leading to more robust and accurate multi-label classification models."
http://arxiv.org/abs/2303.17080v1,Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling,"Deep learning models are vulnerable to data poisoning attacks, where malicious actors inject carefully crafted samples into the training data to degrade performance. Existing poisoning attacks often focus on either indiscriminate corruption of the training set or targeted misclassification of specific instances. This work addresses a more subtle and potentially more insidious attack: poisoning an image classifier to exhibit degraded performance only on a select subset of benign data  a process we term ""mole recruitment."" We introduce a novel poisoning strategy, Selective Batch Sampling (SBS), that leverages the inherent stochasticity of mini-batch gradient descent to selectively expose the model to poisoned samples during training. SBS biases the sampling process to prioritize poisoned examples when the model encounters data points similar to the targeted benign subset, effectively ""recruiting"" the model to learn a representation that is subtly flawed in that specific region of the feature space. Experiments on CIFAR-10 and Tiny ImageNet demonstrate that SBS can significantly reduce the accuracy on targeted benign sets (up to 30% reduction) while maintaining high performance on the overall dataset. This highlights the vulnerability of image classifiers to targeted poisoning attacks that are difficult to detect due to their localized and subtle impact."
http://arxiv.org/abs/2306.15651v1,Dental CLAIRES: Contrastive LAnguage Image REtrieval Search for Dental Research,"Dental research heavily relies on analyzing dental radiographs and clinical images, often requiring laborious manual searching for specific visual features associated with textual descriptions of pathologies or treatments. This process is time-consuming and prone to human error. To address this challenge, we introduce Dental CLAIRES, a novel Contrastive Language Image Retrieval Search framework specifically designed for dental research. Dental CLAIRES leverages a contrastive learning objective to embed dental images and textual descriptions into a shared latent space, enabling efficient retrieval of relevant images based on textual queries. We fine-tune a pre-trained vision-language model with a carefully curated dataset of dental radiographs and associated textual reports, incorporating techniques to handle the specific challenges of dental image analysis, such as subtle anatomical variations and specialized terminology. Experimental results demonstrate that Dental CLAIRES significantly outperforms baseline methods in retrieving relevant dental images based on textual queries, achieving a substantial improvement in retrieval accuracy (e.g., Mean Average Precision). This framework provides a powerful tool for dental researchers, facilitating faster and more accurate image-based knowledge discovery and diagnosis."
http://arxiv.org/abs/2310.13683v2,CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages,"Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable zero-shot transfer capabilities across various vision tasks, but their performance often lags significantly in low-resource languages due to limited multilingual training data. This paper addresses the challenge of enhancing CLIP performance in low-resource languages without incurring substantial computational costs associated with full fine-tuning or pre-training. We introduce CAPIVARA (Cost-Aware Parameter-efficient Improvement via Visual-linguistic Alignment for low-Resource Adaptation), a novel approach that leverages parameter-efficient fine-tuning techniques combined with a carefully designed visual-linguistic alignment objective. CAPIVARA strategically selects and updates only a small subset of the CLIP model's parameters, guided by a cost-aware optimization strategy that prioritizes parameters crucial for bridging the gap between visual representations and low-resource language embeddings. Experimental results on several low-resource image retrieval and classification benchmarks demonstrate that CAPIVARA significantly improves CLIP's zero-shot performance, achieving gains of up to 15% compared to the vanilla CLIP model while using less than 5% of the original parameters. This work provides a practical and efficient pathway for adapting powerful vision-language models to benefit a wider range of languages and communities."
http://arxiv.org/abs/2407.13382v1,Open-World Visual Reasoning by a Neuro-Symbolic Program of Zero-Shot Symbols,"Visual reasoning demands the integration of perception and symbolic manipulation, yet current approaches struggle with the open-world setting where novel objects and relations are encountered. This paper addresses the challenge of open-world visual reasoning, where the system must reason about previously unseen objects and their relationships without explicit training on these novel concepts. We propose a neuro-symbolic framework that leverages pre-trained vision-language models to ground zero-shot symbols representing novel visual concepts, and then composes these symbols into executable programs for reasoning. Our framework dynamically constructs a program based on the scene and query, using the grounded zero-shot symbols to perform relational reasoning. Experiments on the challenging COG-OOD dataset demonstrate that our approach significantly outperforms existing methods in zero-shot generalization, achieving state-of-the-art results on reasoning about out-of-distribution objects and relations. This work provides a crucial step towards building more adaptable and robust visual reasoning systems capable of operating in complex, real-world environments."
http://arxiv.org/abs/2410.21553v2,Exploring the Design Space of Diffusion Bridge Models,"Diffusion bridge models offer a powerful framework for conditional image generation and manipulation by defining a stochastic process that connects two data distributions. While showing promise in diverse applications, the design space of diffusion bridges remains relatively unexplored, with limited understanding of how different architectural choices and training strategies impact performance. This paper systematically investigates the influence of key design parameters in diffusion bridge models, including noise schedules, network architectures, conditioning strategies, and training objectives. We propose a modular framework that allows for the flexible combination of these design elements and facilitates a comprehensive empirical evaluation. Our experiments reveal that the choice of noise schedule significantly impacts sample quality and generation speed, while incorporating attention mechanisms within the network architecture leads to improved fine-grained detail preservation. Furthermore, we demonstrate that hybrid training objectives, combining variational and score-matching losses, can enhance both fidelity and diversity in the generated samples. These findings provide valuable insights into the design and optimization of diffusion bridge models, paving the way for more effective and controllable generative models."
http://arxiv.org/abs/2502.21187v3,SYN-LUNGS: Towards Simulating Lung Nodules with Anatomy-Informed Digital Twins for AI Training,"The development of robust AI algorithms for lung nodule detection and characterization is hampered by the limited availability of expertly annotated, diverse, and high-quality medical imaging datasets. This paper addresses the challenge of data scarcity by introducing SYN-LUNGS, a novel framework for generating realistic synthetic lung CT scans with embedded nodules, leveraging anatomy-informed digital twins. Our method combines a statistical shape model of the lungs, learned from a large cohort of patient scans, with procedural generation techniques to create diverse lung anatomies. We then integrate realistic nodules, modeled using spherical harmonics and parameterized by size, shape, and texture, into these synthetic lungs, ensuring anatomical plausibility. Experimental results demonstrate that AI models trained on SYN-LUNGS exhibit improved generalization performance on real-world lung CT scans, achieving a statistically significant increase in nodule detection sensitivity compared to models trained on limited real data alone. This work demonstrates the potential of anatomy-informed synthetic data generation to overcome data limitations and accelerate the development of reliable AI solutions for lung cancer screening and diagnosis."
http://arxiv.org/abs/2503.00592v1,SolidMark: Evaluating Image Memorization in Generative Models,"Generative models have achieved remarkable success in producing realistic images, raising concerns about potential memorization of training data. Assessing the extent of this memorization is challenging, particularly in distinguishing genuine generation from near-exact replication. We introduce SolidMark, a novel evaluation framework designed to rigorously quantify image memorization in generative models. SolidMark involves imperceptibly watermarking training images with robust, model-agnostic patterns. After training, we analyze generated images for the presence of these watermarks. The detection rate of SolidMarks provides a direct measure of memorization, allowing us to differentiate between creative synthesis and near-verbatim reproduction. Experiments across various generative architectures and datasets reveal that memorization rates are significantly higher than previously assumed, particularly for models trained on smaller datasets or with limited regularization. SolidMark offers a powerful and practical approach for auditing generative models, enabling a deeper understanding of their capabilities and limitations with respect to data privacy and copyright protection."
http://arxiv.org/abs/2503.11043v1,InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse Problems in Physical Sciences,"Diffusion models have emerged as powerful generative priors for solving ill-posed inverse problems. However, the effectiveness of different plug-and-play diffusion priors (PDP) varies significantly across diverse physical science applications, hindering widespread adoption. This paper introduces InverseBench, a comprehensive benchmark designed to evaluate and compare PDP-based reconstruction methods across a spectrum of inverse problems relevant to physical sciences, including astronomical imaging, seismic imaging, and electron microscopy. InverseBench encompasses a diverse set of datasets, forward operators, and evaluation metrics tailored to each application. We evaluate several state-of-the-art PDP methods, including those based on score matching, denoising diffusion probabilistic models, and consistency models, providing a rigorous analysis of their performance, computational efficiency, and robustness to noise and model misspecification. Our results reveal significant performance variations across different PDP methods and inverse problems, highlighting the importance of problem-specific prior selection and algorithm design. InverseBench serves as a valuable resource for researchers to develop and benchmark novel PDP algorithms, facilitating progress toward reliable and accurate solutions to challenging inverse problems in physical sciences."
http://arxiv.org/abs/2505.02105v1,Deep Representation Learning for Electronic Design Automation,"Electronic Design Automation (EDA) relies heavily on algorithms that operate on complex, high-dimensional representations of circuit designs. Manually crafting effective features for these algorithms is a time-consuming and expertise-dependent process, limiting the potential for automated optimization and design space exploration. This paper addresses the challenge of automatically learning effective representations for EDA tasks directly from raw circuit data. We propose a novel deep learning framework that leverages graph neural networks (GNNs) and contrastive learning to generate task-agnostic, low-dimensional embeddings of circuit designs. Specifically, we train a GNN encoder to map circuit netlists to latent vectors, using a contrastive loss function that encourages similar circuits to have nearby embeddings while pushing dissimilar circuits apart. We demonstrate the effectiveness of our learned representations by applying them to various EDA tasks, including circuit classification, performance prediction, and layout optimization. Our experiments show that our learned representations significantly outperform hand-crafted features and other representation learning techniques, achieving state-of-the-art results on several benchmark datasets. This work provides a powerful and generalizable approach to automating feature engineering for EDA, leading to improved design automation and optimization capabilities."
http://arxiv.org/abs/2505.10950v1,Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and Controllable Image Steganography,"Image steganography, the art of concealing secret information within an image, faces the challenge of balancing high embedding capacity with minimal perceptual distortion and robust extraction. Traditional methods often compromise on lossless recovery of the embedded message or lack fine-grained control over the steganographic process. This paper introduces Shackled Dancing, a novel bit-locked diffusion algorithm designed for lossless and controllable image steganography. Shackled Dancing leverages the generative power of diffusion models, carefully constraining the denoising process to embed the secret message during image reconstruction. Specifically, we introduce a bit-locking mechanism that enforces a deterministic relationship between the embedded message and the diffusion process, ensuring lossless extraction. Additionally, a control mechanism is integrated, allowing users to precisely manage the embedding capacity and perceptual quality trade-off. Experiments demonstrate that Shackled Dancing achieves state-of-the-art embedding capacity while maintaining imperceptible image distortions and guaranteeing lossless message recovery. This work provides a significant advancement in steganography, enabling secure and reliable information hiding with unprecedented control."
http://arxiv.org/abs/2505.24360v3,Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning,"Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality and diverse images from textual descriptions. However, understanding the internal representations and mechanisms that enable these models to translate text into visual content remains a significant challenge. This paper addresses the problem of interpreting the latent space of large text-to-image diffusion models by leveraging dictionary learning techniques. We propose a method to learn a sparse dictionary of visual ""atoms"" from the latent activations of the diffusion model during image generation. Specifically, we extract latent feature maps at different denoising steps and train a dictionary to represent these features as sparse linear combinations of basis vectors. Our experiments on Stable Diffusion reveal that the learned dictionary captures semantically meaningful visual concepts, such as textures, shapes, and object parts, which are activated in response to specific text prompts. By analyzing the activation patterns of these visual atoms, we gain insights into how the model composes complex scenes and manipulates visual attributes based on textual instructions. This work provides a novel framework for interpreting the inner workings of text-to-image diffusion models, facilitating improved model understanding and control."
http://arxiv.org/abs/2503.13212v1,MAME: Multidimensional Adaptive Metamer Exploration with Human Perceptual Feedback,"Metamers, stimuli that elicit the same neural response as a target image but differ in pixel space, offer a powerful tool for probing the human visual system. However, generating metamers that are both perceptually compelling and informative remains challenging, often requiring computationally expensive iterative optimization and lacking explicit control over perceptual attributes. We address this by introducing Multidimensional Adaptive Metamer Exploration (MAME), a novel framework for generating metamers guided by human perceptual feedback within a reduced, perceptually relevant latent space. MAME leverages a pre-trained generative adversarial network (GAN) to map images to a lower-dimensional latent space, enabling efficient exploration. We then employ an adaptive sampling strategy, informed by human similarity judgments between metamers and the target image, to iteratively refine the metamer generation process. Key results demonstrate that MAME can generate metamers that are both perceptually similar to a target image and exhibit controlled variations along specific perceptual dimensions, achieving significantly better alignment with human perception than metamers generated through pixel-based optimization. This interactive, perception-driven approach opens new avenues for understanding and modeling the human visual system by facilitating targeted exploration of the metameric space."
http://arxiv.org/abs/1509.06812v1,Learning Wake-Sleep Recurrent Attention Models,"Recurrent attention models have shown promise in processing sequential data by selectively focusing on relevant parts of the input at each time step, mimicking human attention. However, training these models remains challenging, often relying on reinforcement learning or complex variational inference schemes that can be unstable and difficult to scale. We address this problem by introducing a novel wake-sleep algorithm for training recurrent attention models. Our approach alternates between a 'wake' phase, where the model learns to predict the attention locations based on the input, and a 'sleep' phase, where the model generates the input sequence conditioned on its own past attention selections and hidden states. This allows the model to learn a consistent and interpretable representation of the attended regions. Experiments on benchmark sequence classification and image captioning datasets demonstrate that our wake-sleep recurrent attention model achieves competitive performance compared to existing methods while exhibiting improved training stability and interpretability of attention maps. This work offers a more robust and intuitive framework for learning recurrent attention mechanisms, potentially leading to improved performance and understanding of sequential data processing."
http://arxiv.org/abs/1509.07481v1,Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks,"Temporal data classification is crucial in many applications, ranging from video understanding to medical diagnosis. While Recurrent Neural Networks (RNNs) are traditionally used for this task, Convolutional Neural Networks (CNNs) offer advantages in terms of parallelization and computational efficiency. However, directly applying CNNs to temporal data often struggles to capture long-range dependencies. This paper addresses the challenge of effectively encoding temporal correlations in a spatially amenable format for CNN-based temporal data classification. We propose a novel approach that transforms temporal sequences into 2D spatial representations by encoding temporal relationships through learned displacement vectors. These vectors, derived from self-attention mechanisms, guide the spatial arrangement of temporal features, allowing CNNs to effectively capture both local and global temporal dynamics. We demonstrate the effectiveness of our method on several benchmark datasets, including human activity recognition and video classification, achieving state-of-the-art results while maintaining computational efficiency. This spatial encoding of temporal correlations provides a powerful and versatile framework for leveraging CNNs in temporal data analysis, offering a competitive alternative to traditional recurrent approaches."
http://arxiv.org/abs/1701.06796v2,Discriminative Neural Topic Models,"Neural Topic Models (NTMs) have gained popularity for their ability to learn semantically meaningful topic representations from text data, leveraging the power of neural networks. However, standard NTMs often lack the ability to effectively incorporate document-level labels or auxiliary information during topic inference, hindering their performance in discriminative tasks. This paper addresses the problem of incorporating discriminative information into NTMs for improved topic modeling and classification. We introduce Discriminative Neural Topic Models (DNTMs), a novel framework that integrates a discriminative classifier within the variational autoencoder architecture of NTMs. DNTMs achieve this by conditioning the topic distribution on the document label and simultaneously optimizing the topic model and the classifier through a shared latent space. Experiments on several benchmark text classification datasets demonstrate that DNTMs achieve significantly improved classification accuracy and topic coherence compared to state-of-the-art NTMs and traditional topic models. This work provides a principled approach for leveraging discriminative information within neural topic models, leading to more effective and interpretable topic representations for downstream tasks."
http://arxiv.org/abs/2307.07396v1,Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations,"Biclustering, also known as co-clustering, aims to identify subgroups of rows and columns exhibiting similar patterns within a matrix. Boolean Matrix Factorization (BMF) provides a succinct representation of binary data by decomposing it into a set of Boolean factors, often revealing underlying biclusters. However, visualizing overlapping biclusters obtained from BMF, particularly in large datasets, remains a challenge due to the inherent complexity of representing multiple, potentially intersecting, row and column groupings simultaneously. This paper introduces a novel visualization framework specifically designed for interpreting overlapping biclusters derived from BMF. Our approach leverages a combination of network representations to display bicluster relationships, alongside matrix reordering techniques guided by the BMF factors to highlight the discovered patterns within the original data matrix. We demonstrate the effectiveness of our framework on synthetic and real-world datasets, showcasing its ability to reveal complex relationships between biclusters and improve the interpretability of BMF results. The proposed visualization tool enables a more intuitive understanding of the underlying structure captured by BMF, facilitating knowledge discovery in various application domains."
http://arxiv.org/abs/2412.13321v1,LossLens: Diagnostics for Machine Learning through Loss Landscape Visual Analytics,"Understanding the training dynamics of deep learning models is crucial for achieving optimal performance and robustness. However, the high dimensionality and non-convexity of loss landscapes make it challenging to diagnose training issues such as slow convergence, overfitting, and generalization failures. To address this, we introduce LossLens, a visual analytics framework for interactive exploration and diagnosis of loss landscapes in machine learning. LossLens provides a suite of coordinated visualizations, including dimensionality reduction projections of the loss surface, gradient statistics plots, and parameter space trajectories, enabling users to identify problematic regions and understand the model's behavior during training. Through case studies on image classification and natural language processing tasks, we demonstrate how LossLens facilitates the detection of issues like mode collapse, gradient vanishing, and sensitivity to initialization. This interactive diagnostic approach empowers researchers and practitioners to gain deeper insights into the training process, leading to more effective model development and optimization."
http://arxiv.org/abs/2502.13257v3,Random Forest Autoencoders for Guided Representation Learning,"Autoencoders (AEs) are powerful tools for unsupervised representation learning, but often struggle to disentangle latent features without explicit supervision. This paper addresses the challenge of guiding autoencoder learning towards more structured and interpretable representations. We introduce Random Forest Autoencoders (RFAEs), a novel architecture that integrates random forests into the AE framework. RFAEs leverage random forests trained on labeled data to provide a guiding signal during the autoencoder's training process. Specifically, the feature importance scores derived from the random forest are used to weight the reconstruction loss, emphasizing the reconstruction of features deemed important by the random forest. This encourages the autoencoder to learn representations that are relevant to the task the random forest was trained on. Experiments on image classification and object detection datasets demonstrate that RFAEs produce latent representations that lead to improved performance on downstream tasks compared to standard autoencoders and other guided representation learning techniques. This work highlights the potential of combining discriminative and generative models for learning task-relevant and interpretable representations."
http://arxiv.org/abs/2502.15073v1,Visualizing Machine Learning Models for Enhanced Financial Decision-Making and Risk Management,"Financial decision-making increasingly relies on complex machine learning models for tasks such as fraud detection, credit risk assessment, and algorithmic trading. However, the inherent ""black box"" nature of many advanced models hinders interpretability, limiting trust and hindering effective risk management. This paper addresses the critical need for enhanced model transparency by proposing a novel visualization framework tailored for financial machine learning models. Our framework integrates techniques from explainable AI (XAI), specifically SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), to generate interactive visualizations that reveal the feature importance and decision-making processes of various models, including gradient boosting machines and neural networks. These visualizations are then contextualized within financial domain knowledge through layered information displays, allowing users to explore model behavior at both global and local levels. Experiments using real-world financial datasets demonstrate that our visualization framework significantly improves the understanding of model predictions, enabling financial analysts to identify potential biases, validate model assumptions, and refine decision-making strategies. This work contributes to building more reliable and trustworthy machine learning systems for the financial industry, ultimately fostering better risk management and improved financial outcomes."
http://arxiv.org/abs/2502.15678v2,Testing the Limits of Fine-Tuning for Improving Visual Cognition in Vision Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in tasks requiring joint visual and textual understanding. However, their performance on complex visual cognition tasks, such as visual reasoning and compositional understanding, often lags behind human performance, prompting investigation into effective fine-tuning strategies. This paper addresses the problem of how far fine-tuning can push the visual cognition abilities of pre-trained VLMs, specifically focusing on the trade-offs between task-specific adaptation and the preservation of general visual knowledge. We explore a range of fine-tuning techniques, including multi-task learning with carefully curated datasets designed to target specific cognitive skills, and parameter-efficient fine-tuning methods like LoRA to mitigate catastrophic forgetting. Our experiments on a diverse suite of visual cognition benchmarks, including CLEVR, NLVR2, and GQA, reveal that while targeted fine-tuning significantly improves performance on individual tasks, it can also lead to a degradation of performance on other related tasks, particularly when full fine-tuning is employed. Parameter-efficient methods help to alleviate this issue, achieving a better balance between task-specific improvement and generalizability. These findings highlight the limitations of fine-tuning alone for imbuing VLMs with robust visual cognitive abilities and underscore the need for alternative or complementary approaches, such as architectural modifications or novel training paradigms."
http://arxiv.org/abs/1803.01768v2,An Analysis of the t-SNE Algorithm for Data Visualization,"Data visualization is crucial for understanding high-dimensional datasets, particularly in fields like computer vision where feature spaces are often complex and abstract. The t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become a popular dimensionality reduction technique for visualizing such data, but its inherent stochasticity and parameter sensitivity can lead to inconsistent and potentially misleading interpretations. This paper presents a comprehensive analysis of t-SNE, focusing on the impact of various parameters, including perplexity, learning rate, and initialization methods, on the resulting visualizations. We conduct extensive experiments on several benchmark datasets, systematically varying these parameters and quantifying the stability and quality of the resulting embeddings using metrics such as the Mean Relative Rank Error (MRRE) and visual inspection. Our results demonstrate that perplexity significantly influences the preservation of both local and global data structure, while the learning rate and initialization method can impact the convergence and stability of the algorithm. Furthermore, we provide practical guidelines for selecting appropriate parameter ranges based on dataset characteristics. This work provides a deeper understanding of t-SNE's behavior, enabling researchers to generate more reliable and informative visualizations for data exploration and analysis."
http://arxiv.org/abs/2206.06476v1,Explainable Mixed Data Representation and Lossless Visualization Toolkit for Knowledge Discovery,"The increasing availability of mixed data types (e.g., images, text, and tabular data) presents both opportunities and challenges for knowledge discovery in various domains. A significant hurdle lies in effectively representing and visualizing such data in a way that preserves information while facilitating interpretability and explainability. This paper introduces an explainable mixed data representation and lossless visualization toolkit designed to address this challenge. Our method leverages a novel autoencoder architecture with modality-specific encoders and a shared latent space regularized by contrastive learning to create a unified representation. Subsequently, we employ a dimensionality reduction technique optimized for preserving local and global data structure, enabling lossless visualization through interactive scatterplots and parallel coordinate plots. Experimental results on benchmark datasets demonstrate that our approach achieves superior clustering performance and visualization fidelity compared to existing mixed data representation techniques, while also providing interpretable insights into the relationships between different data modalities. This toolkit offers a powerful and accessible solution for knowledge discovery from complex, heterogeneous datasets."
http://arxiv.org/abs/2310.11186v1,Efficiently Visualizing Large Graphs,"Graph visualization is crucial for understanding complex relationships in various domains, but visualizing large graphs remains a significant challenge. Traditional graph layout algorithms often struggle to produce clear and informative visualizations for graphs with millions of nodes and edges due to computational complexity and visual clutter. This paper introduces a novel approach, GraphZoom, for efficiently visualizing large graphs based on a multi-scale representation and adaptive rendering. GraphZoom pre-computes a hierarchy of graph abstractions using a scalable graph clustering algorithm, enabling interactive exploration at different levels of detail. At each zoom level, it selectively renders a subset of nodes and edges based on their importance and screen space availability, minimizing visual clutter and maximizing information density. We demonstrate that GraphZoom achieves interactive frame rates on graphs with millions of nodes while preserving the global structure and local details. Our results show significant improvements in rendering time and visual clarity compared to existing force-directed layout algorithms, making GraphZoom a valuable tool for analyzing and understanding large-scale graph data."
http://arxiv.org/abs/2504.04164v3,MInCo: Mitigating Information Conflicts in Distracted Visual Model-based Reinforcement Learning,"Model-based reinforcement learning (MBRL) offers sample efficiency by learning a dynamics model of the environment, but its performance degrades in the presence of distractor objects that introduce irrelevant visual information. These distractors often lead to ""information conflicts"" within the learned dynamics model, where the model struggles to disentangle task-relevant state transitions from distractor-related changes in visual observations. To address this, we propose MInCo: Mitigating Information Conflicts, a novel MBRL framework that explicitly identifies and suppresses information conflicts in the learned visual dynamics model. MInCo incorporates a conflict detection module based on mutual information to identify potentially conflicting latent features and subsequently employs an information bottleneck to filter out these features during dynamics prediction. This encourages the model to focus on task-relevant information and prevents distractors from unduly influencing predicted state transitions. Experiments on challenging visual manipulation tasks with high levels of distraction demonstrate that MInCo significantly outperforms state-of-the-art MBRL algorithms, achieving up to a 50% improvement in success rate and improved robustness to unseen distractor configurations. By explicitly addressing information conflicts, MInCo provides a principled approach to learning robust visual dynamics models for MBRL in complex, visually cluttered environments."
http://arxiv.org/abs/1608.07625v1,Large Scale Behavioral Analytics via Topical Interaction,"Understanding human behavior in complex environments is crucial for various applications, ranging from urban planning to personalized recommendations. However, analyzing large-scale behavioral data, particularly interaction sequences, presents significant challenges due to its inherent complexity and the difficulty in identifying meaningful patterns. This paper addresses the problem of extracting interpretable and scalable behavioral insights from large-scale interaction data by introducing a novel Topical Interaction (TI) framework. TI models interaction sequences as a mixture of latent behavioral topics, where each topic represents a characteristic pattern of interaction between entities. We leverage non-negative matrix factorization to learn these topics and represent each interaction sequence as a distribution over them, enabling efficient analysis and comparison of behavioral patterns. Experiments on a large-scale mobile app usage dataset demonstrate that TI effectively captures diverse behavioral patterns, outperforms existing sequence modeling techniques in behavior prediction tasks, and reveals meaningful insights into user engagement and app usage dynamics. This work offers a powerful and interpretable approach for large-scale behavioral analytics, facilitating a deeper understanding of human behavior in complex interactive systems."
http://arxiv.org/abs/2306.17638v1,Geometric Autoencoders -- What You See is What You Decode,"Autoencoders have become a foundational tool for unsupervised representation learning, often prioritizing reconstruction fidelity in a latent space designed for downstream tasks like classification or generation. However, standard autoencoders frequently struggle to produce geometrically accurate reconstructions, leading to distortions and artifacts. This paper introduces Geometric Autoencoders (GAEs), a novel framework that explicitly enforces geometric consistency between the input and the decoded output. GAEs incorporate a differentiable geometric loss function that penalizes deviations in shape, size, and pose, alongside the traditional pixel-wise reconstruction loss. We achieve this by employing a combination of Chamfer distance and a novel orientation alignment loss computed directly on point cloud representations extracted from both input and output images. Experimental results on synthetic and real-world datasets demonstrate that GAEs produce significantly more geometrically accurate reconstructions compared to standard autoencoders and variational autoencoders, while maintaining comparable representation learning capabilities. By prioritizing geometric fidelity, GAEs pave the way for more reliable and interpretable unsupervised learning in computer vision applications."
http://arxiv.org/abs/2009.11112v1,ANNdotNET -- deep learning tool on .NET Platform,"Deep learning has revolutionized various fields, yet its accessibility remains limited by specialized software and hardware requirements, often hindering adoption within existing enterprise ecosystems. This paper addresses the challenge of bringing deep learning capabilities to the .NET platform, a widely used environment for enterprise application development. We introduce ANNdotNET, a novel deep learning tool built entirely on the .NET platform, offering native support for C# and other .NET languages. ANNdotNET provides a comprehensive suite of functionalities, including model definition, training, and inference, leveraging optimized numerical computation libraries tailored for .NET. We demonstrate the effectiveness of ANNdotNET through a series of experiments on image classification and natural language processing tasks, achieving competitive performance compared to established deep learning frameworks like TensorFlow and PyTorch while operating within the .NET ecosystem. ANNdotNET empowers .NET developers to seamlessly integrate deep learning into their existing applications, fostering innovation and expanding the reach of AI-powered solutions."
http://arxiv.org/abs/2108.08003v3,Stochastic Cluster Embedding,"Clustering is a fundamental task in unsupervised learning, aiming to group similar data points together. However, traditional clustering algorithms often struggle with high-dimensional data and may not explicitly represent the uncertainty associated with cluster assignments. This paper addresses the problem of learning robust and probabilistic cluster embeddings that capture both the cluster structure and the inherent uncertainty in data assignments. We introduce Stochastic Cluster Embedding (SCE), a novel framework that learns low-dimensional embeddings while simultaneously modeling the probability of each data point belonging to different clusters. SCE employs a deep neural network to map data points to a latent space, where cluster assignments are modeled using a Gaussian Mixture Model (GMM). The parameters of the GMM, including cluster means and covariances, are learned jointly with the embedding network through an Expectation-Maximization (EM) algorithm adapted for stochastic optimization. Experiments on several benchmark datasets demonstrate that SCE achieves superior clustering performance and provides more informative uncertainty estimates compared to state-of-the-art methods. The learned stochastic embeddings offer a valuable representation for downstream tasks requiring both cluster assignments and measures of confidence."
http://arxiv.org/abs/2308.02764v1,Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting,"Visual data exploration tools often struggle to effectively handle the scale and complexity of modern datasets. Analysts are frequently limited by fixed visualizations and inefficient mechanisms for refining queries to uncover nuanced patterns. We address this challenge by introducing Dataopsy, a novel visual exploration system that leverages aggregate query sculpting for scalable and fluid data analysis. Dataopsy allows users to interactively define and refine complex aggregate queries directly within a visual interface. This is achieved by representing aggregates as visual ""sculpts"" which can be intuitively manipulated to filter, group, and transform data. A key innovation is our adaptive query optimization engine, which dynamically rewrites and executes queries based on user interactions, ensuring real-time responsiveness even on large datasets. Experimental results on benchmark datasets demonstrate that Dataopsy enables users to discover insights up to 5x faster compared to traditional visualization tools, while also supporting more sophisticated data transformations. Dataopsy provides a powerful and scalable approach to visual data exploration, empowering analysts to efficiently extract meaningful insights from complex data landscapes."
http://arxiv.org/abs/2408.01294v2,Feature Clock: High-Dimensional Effects in Two-Dimensional Plots,"Visualizing high-dimensional data through dimensionality reduction techniques like PCA and t-SNE is crucial for understanding complex datasets in computer vision. However, these methods often project high-dimensional features onto two-dimensional plots, potentially obscuring underlying structures and relationships due to the inherent loss of information. This paper addresses the problem of effectively visualizing and interpreting the influence of individual high-dimensional features within these two-dimensional embeddings. We propose ""Feature Clock,"" a novel visualization technique that overlays a polar coordinate system onto a standard 2D embedding. Each feature is assigned an angular position, and the magnitude of its contribution to each data point is represented by a radial displacement from the origin. This allows for a simultaneous visualization of the 2D embedding and the influence of individual high-dimensional features, revealing patterns and correlations often hidden in standard scatter plots. Experiments on benchmark datasets, including image classification and object detection features, demonstrate that Feature Clock can effectively highlight feature importance, reveal feature clusters, and expose the relationship between high-dimensional feature activations and their corresponding positions in the 2D embedding. Feature Clock provides a powerful and intuitive tool for exploring and interpreting high-dimensional data in computer vision, leading to a deeper understanding of feature representations and improved model analysis."
http://arxiv.org/abs/2502.03776v1,StarMAP: Global Neighbor Embedding for Faithful Data Visualization,"Data visualization techniques are crucial for exploring and understanding high-dimensional data. Existing methods often struggle to preserve both global structure and local neighborhood relationships, leading to visual distortions and misinterpretations. This paper introduces StarMAP, a novel global neighbor embedding technique designed to create faithful data visualizations. StarMAP constructs a graph representing data points as nodes and weighted edges based on shared nearest neighbors, emphasizing global connectivity through a carefully tuned affinity function. This graph is then embedded into a low-dimensional space using a force-directed layout algorithm that minimizes stress while prioritizing the preservation of global distances. We demonstrate that StarMAP effectively balances the preservation of global and local structures, resulting in visualizations that are more accurate and interpretable than those produced by state-of-the-art methods like t-SNE and UMAP, as evidenced by quantitative metrics measuring neighborhood preservation and global structure alignment across several benchmark datasets. StarMAP provides a powerful tool for researchers and practitioners seeking to gain deeper insights from complex datasets through improved visualization."
http://arxiv.org/abs/2505.11029v1,Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere,"Vision-Language Models (VLMs) pre-trained on large datasets encode rich semantic information in their high-dimensional embedding spaces, often represented as points on a unit hypersphere. However, the uncertainty associated with these embeddings is rarely uniform, exhibiting an asymmetric structure due to biases in the training data and model architecture. This paper addresses the challenge of explicitly modeling and exploiting this asymmetric uncertainty structure to improve downstream task performance. We propose a novel approach that learns a direction-dependent uncertainty estimate on the unit hypersphere for each embedding. This is achieved by training a lightweight neural network to predict a covariance matrix conditioned on the pre-trained VLM embedding, effectively capturing the local uncertainty landscape. We then leverage this uncertainty information in downstream tasks, such as image retrieval and zero-shot classification, by incorporating it into the similarity metric and decision-making process. Experiments on benchmark datasets demonstrate significant improvements in performance compared to methods that ignore or assume isotropic uncertainty. This work provides a principled framework for understanding and mitigating the impact of asymmetric uncertainty in pre-trained VLMs, leading to more robust and accurate vision-language applications."
http://arxiv.org/abs/2212.08860v1,Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning,"Visual Reinforcement Learning (RL) holds immense potential for robotic control, yet struggles with generalization to unseen environments and tasks due to the high dimensionality and complexity of visual inputs. Learning effective visual representations from scratch within the RL loop is often sample inefficient and prone to overfitting to the training environment. To address this, we propose leveraging a pre-trained image encoder, specifically a contrastively learned model, to extract meaningful features from raw pixel inputs before feeding them into the RL policy. Our approach involves freezing the pre-trained encoder and training only the policy network using these fixed, informative visual features. We demonstrate that this strategy significantly improves generalization performance across a suite of challenging visual manipulation tasks, outperforming end-to-end RL and other representation learning baselines in novel environments with varying object appearances and dynamics. These results highlight the effectiveness of pre-trained image encoders in facilitating robust and generalizable visual RL, paving the way for more practical application of RL in real-world scenarios."
http://arxiv.org/abs/2106.07474v2,Discovering Interpretable Machine Learning Models in Parallel Coordinates,"Interpretable machine learning is crucial for building trust and enabling effective decision-making, yet many models remain black boxes. This paper addresses the challenge of discovering and visualizing interpretable machine learning models directly within the parallel coordinates visualization. We introduce a novel framework that integrates model learning with interactive parallel coordinate exploration. Our approach leverages evolutionary algorithms to optimize model parameters and structure, guided by user-defined constraints and visual feedback directly within the parallel coordinate display. Users can interactively select data subsets, define performance metrics, and specify model complexity, influencing the evolutionary process to discover models tailored to specific regions and requirements. We demonstrate the effectiveness of our framework through case studies on diverse datasets, showcasing the discovery of simple, accurate, and interpretable models such as decision rules and linear classifiers that align with user-defined criteria. This interactive and visually driven approach provides a powerful tool for understanding complex data relationships and building transparent machine learning models."
http://arxiv.org/abs/2012.01281v1,Are Gradient-based Saliency Maps Useful in Deep Reinforcement Learning?,"Deep Reinforcement Learning (DRL) agents are increasingly deployed in complex environments, yet their decision-making processes remain largely opaque. Understanding which input features drive an agent's actions is crucial for debugging, improving trust, and enabling knowledge transfer. This paper investigates the utility of gradient-based saliency maps as explanation tools for DRL agents, specifically examining their ability to identify task-relevant features and predict agent behavior. We propose a suite of evaluation metrics that quantify the alignment between saliency maps and ground-truth feature importance derived from environment dynamics and agent policies. These metrics assess the saliency maps' accuracy in highlighting relevant objects, their consistency across similar states, and their correlation with changes in agent behavior after feature perturbations. Our experiments across several Atari games and a custom gridworld environment reveal that while saliency maps can sometimes highlight relevant regions, they often exhibit instability, sensitivity to hyperparameters, and poor alignment with actual feature importance. These findings suggest that directly applying gradient-based saliency techniques to DRL can be misleading and that more robust explanation methods are needed for reliable interpretation of DRL agent behavior. This work highlights the limitations of current saliency methods in DRL and motivates future research toward developing more effective interpretability tools."
http://arxiv.org/abs/2309.13185v1,Visualizing Topological Importance: A Class-Driven Approach,"Understanding which image regions contribute most to a classifier's decision remains a crucial challenge in explainable AI. While saliency maps highlight pixel-level importance, they often fail to capture high-level semantic relationships critical for robust interpretation. We address the problem of identifying and visualizing topologically significant regions that drive class-specific predictions. Our method, Topological Importance Visualization (TIV), leverages persistent homology to extract topological features from activation maps generated by a convolutional neural network. TIV then calculates the importance of these topological features based on their influence on the classification score, allowing us to visualize regions exhibiting significant topological changes related to a specific class. Experiments on benchmark datasets demonstrate that TIV highlights class-relevant regions more effectively than existing saliency methods, capturing object parts and contextual information crucial for accurate classification. This approach provides a novel and interpretable way to understand the decision-making process of deep learning models by revealing the topological structures that contribute to visual recognition."
http://arxiv.org/abs/2408.08862v4,Visual Agents as Fast and Slow Thinkers,"Visual agents operating in complex environments require efficient decision-making under perceptual uncertainty. Inspired by dual-process theory in cognitive science, which posits the existence of fast, intuitive (""System 1"") and slow, deliberate (""System 2"") thinking, we address the challenge of balancing reactive and deliberative behavior in embodied agents. We propose a novel architecture that integrates a fast, perception-driven policy network with a slow, model-based planning module. The fast policy provides immediate actions based on raw visual input, while the slow planner utilizes a learned world model to simulate future outcomes and refine the agent's trajectory, intervening when the fast policy is deemed unreliable. Experiments in challenging navigation tasks demonstrate that our hybrid approach achieves significantly improved performance compared to purely reactive or model-based agents, exhibiting both rapid response times and robust long-term planning capabilities. This work provides a promising direction for designing more adaptable and intelligent visual agents by explicitly incorporating cognitive principles of dual-process thinking."
http://arxiv.org/abs/2409.18330v1,DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors,"Representation learning is crucial for enabling agents to learn effective control policies from high-dimensional visual inputs. However, the presence of visual distractors, irrelevant visual elements that change independently of the agent's actions, poses a significant challenge to learning robust and generalizable representations. To address this, we introduce DMC-VB, a suite of challenging continuous control tasks built upon the DeepMind Control Suite, augmented with a wide range of realistic and dynamic visual distractors. This benchmark allows for a systematic evaluation of representation learning methods under varying levels of visual noise and complexity. We evaluate several state-of-the-art representation learning algorithms, including contrastive methods and variational autoencoders, on DMC-VB. Our results demonstrate a significant performance gap between these methods, highlighting the difficulty of learning invariant representations in the presence of visual distractors and revealing vulnerabilities in existing approaches. DMC-VB serves as a valuable resource for the community, facilitating the development of more robust and generalizable representation learning techniques for visual control."
http://arxiv.org/abs/2410.14038v4,Sliding Puzzles Gym: A Scalable Benchmark for State Representation in Visual Reinforcement Learning,"Visual Reinforcement Learning (VRL) agents often struggle with complex environments due to the difficulty of learning effective state representations from raw pixel inputs. This work addresses the challenge of benchmarking state representation learning in VRL by introducing Sliding Puzzles Gym (SPGym), a suite of procedurally generated sliding puzzle environments with varying complexity. SPGym offers precise control over visual features, puzzle size, and action space, enabling systematic evaluation of VRL algorithms. We benchmark several state-of-the-art VRL agents, including those employing convolutional and attention-based architectures, on SPGym. Our results reveal significant performance differences across algorithms and puzzle configurations, highlighting the importance of inductive biases for learning robust state representations. SPGym provides a scalable and customizable platform for future research on state representation learning, facilitating the development of more effective and generalizable VRL agents."
http://arxiv.org/abs/2507.10861v1,Visually grounded emotion regulation via diffusion models and user-driven reappraisal,"Emotion regulation, the ability to modulate emotional experiences, is crucial for mental well-being. While cognitive reappraisal, a strategy involving reinterpreting a situation to alter its emotional impact, is effective, its application can be challenging, especially when triggered by visual stimuli. We address the problem of providing visually grounded support for emotion regulation through personalized, user-driven reappraisal. Our approach leverages diffusion models to generate modified images that subtly alter the emotional valence of the original distressing visual content. Users interactively guide the image generation process by providing textual prompts reflecting their desired reappraisal strategies, iteratively refining the image towards a less distressing representation. We demonstrate that our method effectively reduces self-reported negative affect associated with distressing images, while maintaining semantic coherence with the original scene. This work paves the way for novel, personalized interventions for managing emotional responses to visual triggers using generative AI."
http://arxiv.org/abs/1506.00990v10,Unsupervised Learning on Neural Network Outputs: with Application in Zero-shot Learning,"Deep neural networks excel at supervised learning tasks but often struggle with generalization to unseen classes in zero-shot learning (ZSL) scenarios. A key challenge in ZSL is effectively bridging the gap between visual features extracted from images and semantic information associated with unseen classes. This paper addresses the problem of learning robust and transferable representations from neural network outputs without relying on explicit supervision from unseen class labels. We propose a novel unsupervised learning framework that operates directly on the penultimate layer outputs of a pre-trained neural network. Specifically, we leverage clustering and manifold learning techniques to discover inherent structure within the feature space, subsequently learning a mapping from these unsupervised representations to the semantic space. Our experiments on benchmark ZSL datasets demonstrate that this approach significantly improves performance compared to existing methods, achieving state-of-the-art results on several datasets and showing robustness to different pre-trained network architectures. The proposed method offers a powerful and flexible approach for leveraging readily available pre-trained models for zero-shot generalization."
http://arxiv.org/abs/2210.00044v2,Task Formulation Matters When Learning Continually: A Case Study in Visual Question Answering,"Continual learning (CL) aims to enable models to learn new tasks sequentially without forgetting previously acquired knowledge. However, the impact of task formulation on the efficacy of CL strategies in complex vision-language tasks like Visual Question Answering (VQA) remains largely unexplored. This paper investigates how different VQA task formulations affect the performance of various CL algorithms. We systematically explore three common VQA formulations: question-type classification, answer generation, and answer classification, and evaluate their influence on catastrophic forgetting when learned continually. We demonstrate that answer classification, despite its simplicity, offers a surprisingly robust and effective task formulation for CL in VQA, even surpassing more complex generation-based approaches when combined with replay-based CL methods. Specifically, replay combined with answer classification consistently outperforms other task formulation and CL algorithm combinations, achieving up to a 15% improvement in average accuracy across all tasks while mitigating catastrophic forgetting. These findings highlight the critical importance of task formulation in CL and provide valuable insights for designing effective continual learning strategies for vision-language tasks."
http://arxiv.org/abs/1912.00386v2,Active Search for Nearest Neighbors,"Nearest neighbor search is a fundamental problem in computer vision and machine learning, underpinning various applications from image retrieval to classification. Traditional nearest neighbor search methods passively evaluate all data points or rely on pre-computed indices, which can be inefficient for large datasets and dynamic data distributions. This paper addresses the challenge of efficiently identifying nearest neighbors by actively selecting which data points to evaluate. We propose an active search strategy that learns to predict the likelihood of a data point being a near neighbor to a query, using a neural network trained with reinforcement learning. The network adaptively selects data points for evaluation based on the current query and the information gathered so far, balancing exploration and exploitation to minimize the number of evaluations required. Experiments on several benchmark datasets demonstrate that our active search method significantly outperforms passive search strategies and existing approximate nearest neighbor algorithms in terms of query time, while maintaining comparable or superior accuracy. This active search paradigm offers a promising direction for scalable and efficient nearest neighbor search in high-dimensional spaces."
http://arxiv.org/abs/2312.09187v3,Vision-Language Models as a Source of Rewards,"Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and relating visual and textual data, opening avenues for novel applications beyond traditional image classification and captioning. However, leveraging VLMs to guide reinforcement learning (RL) agents for complex, visually-grounded tasks remains a challenge, often requiring carefully engineered reward functions. This paper investigates the potential of VLMs to directly provide reward signals for RL agents, eliminating the need for hand-crafted rewards. We propose a framework where a VLM, specifically CLIP, acts as a reward function by measuring the semantic alignment between the agent's observed environment and a desired textual goal. Our experiments across various simulated robotic manipulation tasks demonstrate that agents trained with VLM-derived rewards learn effective policies that align with the specified goals. Notably, our approach achieves comparable or superior performance to agents trained with traditional, hand-engineered reward functions, while requiring significantly less task-specific engineering. This work highlights the potential of VLMs as a versatile and powerful source of rewards for training RL agents, paving the way for more generalizable and autonomous robotic learning systems."
http://arxiv.org/abs/2406.17916v1,Camera Model Identification Using Audio and Visual Content from Videos,"Camera model identification is crucial for forensic analysis, multimedia provenance verification, and enhancing the reliability of video content. Existing techniques primarily rely on visual cues, neglecting the potential information encoded within the audio component of videos. This paper addresses the challenge of camera model identification by leveraging both audio and visual modalities extracted from video content. We propose a novel multi-modal fusion network that integrates audio features, extracted using Mel-Frequency Cepstral Coefficients (MFCCs) and spectrogram analysis, with visual features derived from convolutional neural networks (CNNs) trained on frame sequences. The network employs a late fusion strategy, concatenating audio and visual feature embeddings before feeding them into a classification layer trained to predict the camera model. Experimental results on a diverse dataset of videos captured by various camera models demonstrate that our multi-modal approach significantly outperforms unimodal (audio-only or visual-only) methods, achieving a relative improvement of over 15% in accuracy. This research highlights the potential of audio-visual synergy for improving camera model identification, leading to more robust and accurate source attribution in multimedia forensics."
http://arxiv.org/abs/2409.02079v1,Synthetic Data Generation and Automated Multidimensional Data Labeling for AI/ML in General and Circular Coordinates,"The increasing demand for labeled data in AI/ML applications is often hindered by the cost and time associated with manual annotation, particularly for complex data structures. This paper addresses the challenge of efficiently generating and labeling synthetic data in multidimensional spaces, specifically focusing on circular coordinate systems. We propose a novel framework that combines procedural generation techniques with automated multidimensional data labeling. Our method leverages parametric models to create diverse synthetic datasets with precise control over object properties and scene characteristics. Simultaneously, we implement an automated labeling pipeline that directly extracts ground truth information from the generation parameters, thereby eliminating the need for manual annotation. Experimental results demonstrate that models trained on our synthetically generated and labeled data achieve comparable or superior performance to those trained on real-world datasets with equivalent manual annotation efforts, especially in tasks involving object detection and pose estimation in circular spaces. This approach significantly reduces the data acquisition bottleneck and facilitates the development of robust and accurate AI/ML models for a wide range of applications."
http://arxiv.org/abs/2502.00466v2,EDELINE: Enhancing Memory in Diffusion-based World Models via Linear-Time Sequence Modeling,"Diffusion-based world models have shown promise in learning representations for downstream reinforcement learning tasks by generating plausible future states. However, these models often struggle with long-term dependencies and maintaining consistent memory of past events, limiting their ability to accurately predict distant future states. To address this limitation, we introduce EDELINE, a novel architecture that enhances memory within diffusion-based world models via linear-time sequence modeling. EDELINE integrates a structured state space model, specifically a simplified Mamba-like architecture, within the latent space of the diffusion model to efficiently capture temporal relationships and maintain a coherent representation of the environment's history. This lightweight sequence model allows for efficient processing of long sequences in linear time, enabling the diffusion model to access and utilize relevant past information during state generation. Experimental results on challenging video prediction benchmarks demonstrate that EDELINE significantly improves long-horizon prediction accuracy and sample quality compared to standard diffusion-based world models and recurrent baselines, particularly in environments requiring memory of past events. EDELINE offers a computationally efficient and effective approach to enhance the memory capabilities of diffusion-based world models, paving the way for more robust and reliable environment forecasting."
http://arxiv.org/abs/2504.03153v1,MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories,"Autonomous laboratories promise to accelerate scientific discovery by automating experimental workflows. However, effectively navigating the complexities of real-world laboratory environments requires agents that can reason about both visual observations and textual information, such as experiment instructions and instrument manuals. This paper addresses the challenge of creating autonomous agents capable of making informed decisions in complex laboratory settings by integrating multimodal information within a reinforcement learning framework. We introduce MORAL (Multimodal Reinforcement Learning), a novel architecture that combines visual encoders with large language models to create a unified representation of the lab environment. MORAL leverages this representation to learn policies that optimize experimental outcomes, using a reward function tailored to specific scientific goals. Experiments conducted in a simulated chemistry lab demonstrate that MORAL significantly outperforms unimodal baselines and alternative multimodal fusion strategies, achieving a 30% improvement in reaction yield. This work demonstrates the potential of multimodal reinforcement learning for creating intelligent agents that can effectively automate and optimize scientific research."
http://arxiv.org/abs/2507.22556v1,VAR: Visual Analysis for Rashomon Set of Machine Learning Models' Performance,"The Rashomon set, the set of near-optimal models for a given task, presents a challenge for machine learning practitioners as it can be difficult to choose a single ""best"" model. This paper addresses the problem of understanding the performance diversity and potential failure modes within the Rashomon set, particularly when dealing with complex visual data. We introduce VAR (Visual Analysis for Rashomon sets), a novel visualization-based framework that leverages dimensionality reduction, clustering, and interactive exploration to analyze the performance characteristics of models within the Rashomon set. VAR allows users to project model performance metrics (e.g., accuracy, precision, recall) onto a lower-dimensional space, identify clusters of models with similar performance profiles, and then visually examine the specific data instances where models within each cluster succeed or fail. Through experiments on image classification and object detection tasks, we demonstrate that VAR effectively reveals nuanced differences in model behavior, highlighting specific classes or object types where certain subsets of the Rashomon set consistently outperform others. This enables more informed model selection and provides valuable insights for targeted data augmentation and model refinement, leading to more robust and reliable visual recognition systems."
http://arxiv.org/abs/2508.00331v1,Embryology of a Language Model,"Large language models (LLMs) have demonstrated remarkable capabilities, yet their development remains largely opaque, akin to observing the final organism without understanding its embryological development. This work addresses the critical gap in understanding how specific architectural choices and training procedures influence the emergent properties of LLMs during the pre-training phase. We introduce a novel framework for dissecting the ""embryology"" of an LLM, systematically ablating and modifying components of the model architecture (e.g., attention heads, feedforward networks) and training data composition at various stages of pre-training, while continuously monitoring performance on a suite of diagnostic tasks designed to probe specific linguistic abilities. Our experiments reveal that certain architectural components are disproportionately responsible for acquiring specific linguistic skills, and that the order in which the model encounters different types of data significantly impacts its final performance and biases. This detailed analysis provides crucial insights into the inner workings of LLMs, enabling more targeted model design and training strategies for improved performance, efficiency, and control over emergent behaviors."
http://arxiv.org/abs/2310.10207v6,Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World,"Bongard problems (BPs) have long served as a benchmark for abstract visual reasoning, traditionally focusing on structured, synthetic domains. However, their applicability to real-world scenarios with free-form visual concepts remains limited. This paper introduces Bongard-OpenWorld, a novel framework extending the BP paradigm to address few-shot reasoning with open-world visual concepts learned directly from real-world images. Our approach leverages a contrastive learning strategy to embed visual concepts from positive and negative example sets into a shared feature space. Subsequently, a transformer-based reasoner operates on these embeddings to identify the underlying distinguishing rule, enabling classification of novel instances. We demonstrate that our framework achieves state-of-the-art performance on a newly curated real-world BP dataset, significantly outperforming existing few-shot learning methods and exhibiting robust generalization capabilities across diverse visual concepts. This work paves the way for more effective and interpretable visual reasoning systems in complex, real-world applications."
http://arxiv.org/abs/2012.01166v1,Improving Interpretability in Medical Imaging Diagnosis using Adversarial Training,"Deep learning models have achieved remarkable performance in medical image diagnosis, yet their ""black box"" nature hinders clinical adoption. A critical challenge lies in improving the interpretability of these models, allowing clinicians to understand the reasoning behind their predictions and build trust in the system. This paper introduces an adversarial training framework to enhance the interpretability of deep learning models used for medical image diagnosis. Our approach incorporates an adversarial loss that encourages the model to focus on clinically relevant features, identified through expert annotations, while discouraging reliance on spurious correlations. Specifically, we train the model to be robust against perturbations that mask or alter non-relevant image regions, forcing it to make predictions based on the salient, annotated areas. Experiments on chest X-ray datasets demonstrate that our adversarially trained models achieve comparable diagnostic accuracy to standard models while significantly improving the quality of generated attention maps, better aligning with clinical findings and expert knowledge. This increased interpretability fosters greater trust and facilitates the integration of deep learning into clinical workflows."
http://arxiv.org/abs/1205.4234v2,Visualization of features of a series of measurements with one-dimensional cellular structure,"One-dimensional cellular structures, such as those found in biological tissues or sensor arrays, often generate sequential measurement data where spatial relationships are crucial for interpretation. Analyzing the underlying features in such data series presents a challenge, particularly when visualizing patterns across multiple measurements. This paper introduces a novel visualization technique, the ""Cellular Feature Map"" (CFM), designed to effectively represent features extracted from a series of one-dimensional cellular measurements. CFM employs a self-organizing map (SOM) to cluster feature vectors derived from each cells measurement profile and visualizes the SOM's weight vectors as representative ""cellular fingerprints."" These fingerprints are then arranged spatially according to the original cellular structure, allowing for immediate visual identification of feature clusters and spatial trends. We demonstrate the utility of CFM on synthetic and real-world datasets, showcasing its ability to reveal subtle variations and spatial correlations not easily discernible with conventional visualization methods. The CFM provides a powerful tool for exploratory data analysis and hypothesis generation in domains characterized by one-dimensional cellular arrangements."
http://arxiv.org/abs/1604.07078v1,Unsupervised Representation Learning of Structured Radio Communication Signals,"Radio communication signals are ubiquitous in modern infrastructure, and their efficient analysis is crucial for various applications, including spectrum monitoring and interference mitigation. However, labeled data for training machine learning models to understand these signals is often scarce and expensive to acquire, hindering the development of robust and generalizable signal processing techniques. This paper addresses the challenge of learning effective representations of structured radio communication signals in an unsupervised manner, without relying on explicit labels or prior knowledge of signal types. We propose a novel contrastive learning framework, SigCLR, that leverages carefully designed data augmentations inspired by common radio impairments, such as frequency offsets, phase noise, and multipath fading, to learn invariant representations. By contrasting differently augmented versions of the same signal, SigCLR learns to disentangle signal structure from noise and channel effects. Experimental results on a diverse dataset of synthetic and real-world radio signals demonstrate that SigCLR significantly outperforms existing unsupervised representation learning methods, achieving state-of-the-art performance in downstream tasks like signal classification and modulation recognition, even with limited labeled data. This work paves the way for more efficient and adaptable signal processing algorithms in dynamic and unpredictable radio environments."
http://arxiv.org/abs/2203.13503v1,Supplemental Material: Lifelong Generative Modelling Using Dynamic Expansion Graph Model,"Lifelong learning aims to enable models to continuously learn from new data distributions without forgetting previously acquired knowledge. However, generative models face significant challenges in lifelong learning scenarios, particularly catastrophic forgetting and the need to adapt their latent space to accommodate new data. This paper introduces a novel lifelong generative modeling framework, the Dynamic Expansion Graph Model (DEGM), designed to mitigate these challenges. DEGM utilizes a graph-structured latent space that dynamically expands to incorporate new data distributions. Each node in the graph represents a learned data distribution, and edges capture the relationships between different distributions. A novel graph expansion and connection strategy allows DEGM to learn new tasks efficiently while preserving previously learned knowledge. Experimental results on a series of benchmark datasets demonstrate that DEGM significantly outperforms existing lifelong generative models in terms of generation quality, knowledge retention, and computational efficiency. This work offers a promising approach to building generative models capable of continuous learning and adaptation in dynamic environments."
http://arxiv.org/abs/2105.00937v1,LFI-CAM: Learning Feature Importance for Better Visual Explanation,"Visual explanations, such as Class Activation Maps (CAMs), aim to highlight image regions important for a model's prediction, enhancing trust and interpretability. However, standard CAM generation often suffers from coarse localization and irrelevant highlighting due to its reliance on unweighted feature maps. This paper addresses the problem of improving the accuracy and fidelity of CAMs by learning feature importance scores. We introduce LFI-CAM, a novel approach that leverages a small, lightweight module trained to predict feature importance weights based on the input image and feature maps. These learned weights are then used to generate more refined CAMs, focusing on truly relevant features. Experimental results on various datasets and architectures demonstrate that LFI-CAM consistently outperforms existing CAM generation techniques, achieving higher localization accuracy and improved visual coherence. LFI-CAM offers a principled way to generate more faithful and informative visual explanations, fostering greater understanding and trust in deep learning models."
http://arxiv.org/abs/2205.04035v1,Visualization of Decision Trees based on General Line Coordinates to Support Explainable Models,"Decision trees are widely used for classification and regression tasks due to their inherent interpretability. However, as tree complexity increases, understanding the decision-making process becomes challenging, hindering effective model explanation and debugging. This paper addresses the need for improved visualization techniques to enhance the interpretability of complex decision trees. We propose a novel visualization method based on general line coordinates, transforming the traditional tree structure into a two-dimensional space where each decision path is represented by a line. This allows for the clear visualization of the relationship between features and predictions, highlighting influential features and identifying potential biases or unexpected decision boundaries. We demonstrate the effectiveness of our method through case studies on both synthetic and real-world datasets, showing improved clarity in visualizing complex decision paths and feature importance compared to existing tree visualization techniques. The proposed visualization provides a valuable tool for understanding and explaining decision tree models, fostering trust and enabling more informed decision-making."
http://arxiv.org/abs/2205.11257v1,Manifold-aligned Neighbor Embedding,"Many machine learning tasks involve data residing on multiple manifolds with potentially differing intrinsic structures. This presents a challenge for traditional dimensionality reduction techniques that assume a single underlying manifold. We address the problem of learning a shared embedding space for data from multiple manifolds while preserving local neighborhood relationships within each manifold and aligning corresponding regions across manifolds. We propose Manifold-aligned Neighbor Embedding (MaNE), a novel non-linear dimensionality reduction method. MaNE first constructs neighborhood graphs for each manifold and then learns a low-dimensional embedding that simultaneously minimizes the reconstruction error within each manifold's neighborhood structure and aligns corresponding neighbors across manifolds using a novel alignment loss. Experiments on synthetic and real-world datasets demonstrate that MaNE outperforms existing methods in preserving manifold structure and achieving superior alignment, leading to improved performance in downstream tasks such as classification and cross-modal retrieval. The ability to effectively align and integrate data from multiple manifolds offers significant advantages in various applications, including multi-view learning and domain adaptation."
http://arxiv.org/abs/2408.00211v1,"Penzai + Treescope: A Toolkit for Interpreting, Visualizing, and Editing Models As Data","Neural networks are increasingly deployed in sensitive applications, yet their inherent complexity often hinders understanding and control. Interpretability tools typically focus on post-hoc explanations or visualizations of existing models, neglecting the potential to directly manipulate and refine model behavior. We introduce Penzai + Treescope, a novel toolkit that treats trained neural network models as structured data, enabling interactive exploration, visualization, and direct editing of model parameters and architecture. Penzai provides a data-centric representation of model components, while Treescope leverages hierarchical tree structures to visualize and navigate model architectures and parameter relationships. This allows users to identify salient features, prune redundant connections, and even transplant learned representations from other models, all within an intuitive visual interface. We demonstrate the utility of Penzai + Treescope on image classification and natural language processing tasks, showcasing significant improvements in model efficiency and interpretability through targeted interventions. By enabling model editing as a core workflow, our toolkit empowers researchers and practitioners to gain deeper insights and exert greater control over the behavior of complex neural networks."
http://arxiv.org/abs/2412.06555v1,"When Dimensionality Reduction Meets Graph (Drawing) Theory: Introducing a Common Framework, Challenges and Opportunities","Dimensionality reduction and graph drawing are seemingly disparate fields: the former focuses on representing high-dimensional data in lower dimensions while preserving key characteristics, and the latter on visually representing relational data as node-link diagrams. This paper addresses the fundamental disconnect between these two areas, arguing that they share a common underlying goal: representing complex data structures in a more manageable and interpretable format. We introduce a novel framework that unifies dimensionality reduction and graph drawing by framing both as optimization problems aiming to minimize distortion in a latent space, where distortion is defined with respect to specific data properties. We achieve this by leveraging graph-theoretic concepts to guide dimensionality reduction and, conversely, employing dimensionality reduction techniques to inform graph layout. Experiments on benchmark datasets demonstrate that this unified approach can yield improved performance in both tasks, resulting in lower stress graph layouts and more accurate low-dimensional embeddings. By bridging the gap between dimensionality reduction and graph drawing, this work opens up new avenues for developing more effective and insightful data visualization and analysis techniques."
http://arxiv.org/abs/2505.23004v1,QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining,"Multi-modal Large Language Models (MLLMs) demonstrate remarkable capabilities in understanding and generating text grounded in visual content. However, efficiently incorporating detailed visual information remains a challenge, often requiring extensive retraining or fine-tuning to adapt to new visual modalities or resolutions. We introduce QLIP, a novel Quadtree vision prior that dynamically encodes visual features, enabling MLLMs to process variable-resolution images without retraining. QLIP leverages a learnable quadtree structure to adaptively decompose images, focusing computational resources on salient regions and providing a hierarchical representation to the MLLM. This dynamic decomposition allows the MLLM to attend to varying levels of detail within an image, improving both accuracy and efficiency. Experiments on several benchmark datasets, including image captioning and visual question answering, demonstrate that QLIP significantly enhances MLLM performance, achieving comparable or superior results to fine-tuned baselines while maintaining the original model's parameters. QLIP offers a computationally efficient and adaptable approach to integrating visual information, promoting the development of more robust and versatile MLLMs."
http://arxiv.org/abs/2506.09044v1,The Decoupled Risk Landscape in Performative Prediction,"Performative prediction, where model predictions influence the underlying data distribution, introduces unique challenges to risk minimization. This work investigates the risk landscape in performative settings, revealing a fundamental decoupling between the observed training risk and the true performative risk. We demonstrate that standard gradient-based optimization can be highly susceptible to converging to suboptimal solutions due to this decoupling. To address this, we propose a novel meta-gradient based algorithm, Performative Risk Correction (PRC), which explicitly estimates and corrects for the discrepancy between the training and performative risk landscapes. PRC leverages a bi-level optimization framework to learn a correction term that aligns the training gradients with the true performative risk gradient. Empirical evaluations on synthetic and real-world datasets demonstrate that PRC significantly outperforms existing performative prediction methods, achieving lower performative risk and improved stability. Our findings highlight the critical importance of accounting for the decoupled risk landscape in performative settings and provide a practical solution for mitigating its adverse effects."
http://arxiv.org/abs/1405.6684v1,Visualizing Random Forest with Self-Organising Map,"Random Forest (RF) is a powerful and widely used machine learning algorithm known for its high accuracy and robustness. However, the inherent complexity of the ensemble model often makes it difficult to interpret and understand its decision-making process, hindering trust and preventing potential improvements. This paper addresses the challenge of visualizing and interpreting the learned feature space of a Random Forest model. We propose a novel approach that leverages Self-Organizing Maps (SOMs) to create a visual representation of the RF's internal feature space. Specifically, we extract the learned representation of data points from the leaf nodes of each tree in the forest and use these representations as input to train a SOM. The resulting SOM then provides a low-dimensional, topologically organized map where similar data points, based on their RF-learned features, are clustered together. We demonstrate the effectiveness of our method on several benchmark datasets, revealing meaningful clusters that correlate with underlying data characteristics and providing insights into the RFs decision boundaries. This visualization technique offers a valuable tool for understanding the inner workings of Random Forests, facilitating model debugging, feature importance analysis, and knowledge discovery within the data."
http://arxiv.org/abs/2112.02694v1,Benchmark for Out-of-Distribution Detection in Deep Reinforcement Learning,"Deep Reinforcement Learning (DRL) agents often struggle to generalize to environments that differ from their training distribution, a critical limitation for real-world deployment. Detecting out-of-distribution (OOD) states during execution is crucial for enabling safe and reliable DRL. This paper introduces a novel benchmark suite specifically designed to evaluate OOD detection methods in DRL settings. Our benchmark, DRL-OOD, encompasses a diverse set of continuous control tasks with varying degrees of distributional shift, including changes in dynamics, reward functions, and environmental parameters. We evaluate a range of existing OOD detection techniques, adapted for DRL, including methods based on density estimation, ensemble disagreement, and uncertainty quantification using Bayesian neural networks. Our results demonstrate that current OOD detection methods struggle to consistently identify OOD states across different tasks and types of distributional shift, highlighting the need for more robust and adaptable techniques. The DRL-OOD benchmark provides a standardized and challenging platform for advancing research in safe and reliable DRL by facilitating the development and evaluation of improved OOD detection algorithms."
http://arxiv.org/abs/2109.12926v2,ML4ML: Automated Invariance Testing for Machine Learning Models,"Machine learning models are increasingly deployed in safety-critical applications, necessitating rigorous testing to ensure reliable and predictable behavior. A crucial aspect of this testing involves verifying invariance properties, where model outputs should remain consistent under specific input transformations. However, manually defining and testing these invariances is a laborious and error-prone process. This paper introduces ML4ML, a novel framework for *automated* invariance testing. ML4ML leverages machine learning itself to learn and generate invariant transformations specific to a given model and dataset. It employs a generative adversarial network (GAN) architecture, where the generator learns to create transformations that preserve model predictions, while the discriminator distinguishes between original and transformed inputs. We evaluate ML4ML on image classification tasks using CIFAR-10 and ImageNet, demonstrating its ability to automatically discover meaningful invariances, such as color shifts, rotations, and subtle texture changes, that expose vulnerabilities in state-of-the-art models. Crucially, these automatically discovered invariances reveal significantly more model weaknesses compared to manually defined invariance tests, leading to a 10-20% increase in the detected failure rate. This automated approach to invariance testing offers a scalable and effective solution for enhancing the robustness and reliability of machine learning models."
http://arxiv.org/abs/1710.03804v3,End-to-End Deep Learning for Steering Autonomous Vehicles Considering Temporal Dependencies,"Autonomous vehicle steering prediction is a critical component of self-driving systems, requiring accurate and timely control actions. Existing deep learning approaches often treat steering prediction as a frame-by-frame regression task, neglecting the inherent temporal dependencies crucial for smooth and safe navigation. This paper addresses the challenge of incorporating temporal information into end-to-end deep learning models for autonomous vehicle steering. We propose a novel architecture that integrates a convolutional neural network (CNN) for feature extraction from raw visual input with a recurrent neural network (RNN), specifically Gated Recurrent Units (GRUs), to model temporal dynamics and predict future steering angles. The CNN-GRU network is trained end-to-end to directly map sequences of camera images to steering commands. Experimental results on the Udacity self-driving car dataset demonstrate that our proposed model outperforms state-of-the-art frame-by-frame approaches, achieving significant improvements in steering accuracy and trajectory smoothness, particularly in challenging driving scenarios. This research highlights the importance of temporal modeling in end-to-end autonomous driving systems and offers a promising direction for developing more robust and reliable self-driving capabilities."
http://arxiv.org/abs/2111.00177v1,On Quantitative Evaluations of Counterfactuals,"Counterfactual explanations are increasingly used to understand and debug complex computer vision models by identifying minimal changes to an input that alter a model's prediction. However, a lack of standardized and robust quantitative evaluation metrics hinders progress in developing and comparing different counterfactual generation methods. This paper addresses the problem of evaluating the quality of counterfactual explanations in computer vision, focusing on the often-conflicting desiderata of proximity, validity, and plausibility. We propose a suite of novel metrics designed to capture these aspects, including perceptual distance measures weighted by learned feature importance, semantic similarity scores based on pre-trained vision-language models, and measures of the causal consistency of counterfactuals based on intervention experiments. Experiments on image classification and object detection tasks demonstrate that our metrics provide a more comprehensive and nuanced evaluation of counterfactual explanations compared to existing metrics, revealing trade-offs between different desiderata. Our work enables more rigorous benchmarking and facilitates the development of more effective and trustworthy counterfactual explanation techniques for computer vision."
http://arxiv.org/abs/2202.09481v2,TransDreamer: Reinforcement Learning with Transformer World Models,"Reinforcement learning (RL) agents often struggle with high-dimensional visual inputs and sparse rewards, necessitating efficient exploration and long-term credit assignment. World models offer a promising solution by learning a compact, abstract representation of the environment, but existing approaches often rely on recurrent neural networks which can suffer from vanishing gradients and limited long-range dependencies. This paper introduces TransDreamer, a novel RL agent that leverages the power of Transformers within a world model framework. Specifically, we replace the recurrent state space model of Dreamer with a Transformer-based architecture, enabling the model to capture longer-range temporal dependencies in the environment dynamics. Furthermore, we introduce a masked autoencoding objective during world model pre-training to improve representation learning and enhance robustness to noisy observations. Our experiments across a range of challenging visual control tasks demonstrate that TransDreamer significantly outperforms Dreamer and other state-of-the-art RL algorithms, achieving higher asymptotic performance and faster learning speeds. TransDreamer's ability to effectively model complex environments with long-term dependencies using Transformers opens new avenues for developing more efficient and robust RL agents."
http://arxiv.org/abs/2208.05280v2,TSInterpret: A unified framework for time series interpretability,"Time series data are ubiquitous in various domains, and the increasing reliance on complex machine learning models for time series analysis necessitates methods for understanding their decision-making processes. However, existing time series interpretability techniques are fragmented, lack standardization, and often focus on specific model types or data characteristics. We address this gap by introducing TSInterpret, a unified open-source framework for time series interpretability. TSInterpret provides a comprehensive suite of tools for generating, evaluating, and comparing explanations across diverse time series models and data modalities. The framework incorporates a modular design, enabling easy integration of new interpretability methods and evaluation metrics. We demonstrate the effectiveness of TSInterpret through comprehensive experiments on benchmark datasets, showcasing its ability to generate meaningful and robust explanations for a variety of tasks including classification, forecasting, and anomaly detection. TSInterpret facilitates a more transparent and trustworthy application of machine learning to time series data, promoting responsible AI development."
http://arxiv.org/abs/2302.00569v2,Agnostic Visual Recommendation Systems: Open Challenges and Future Directions,"Visual recommendation systems leverage the rich information contained within images to enhance the accuracy and personalization of recommendations across various domains. However, current systems often rely on specific datasets and architectures, limiting their generalizability and adaptability to novel visual content or evolving user preferences. This paper addresses the challenge of building agnostic visual recommendation systems, which can effectively generate relevant recommendations across diverse visual domains without requiring extensive retraining or domain-specific knowledge. We propose a modular framework that decouples visual feature extraction, user preference modeling, and recommendation generation, allowing for the flexible integration of pre-trained vision models and adaptive preference learning techniques. Furthermore, we introduce a novel evaluation protocol based on cross-domain transfer learning to assess the true agnosticism of visual recommendation systems. Our experiments demonstrate that the proposed framework achieves competitive performance across multiple datasets, while significantly reducing the need for domain-specific fine-tuning. This work provides a foundation for developing more robust and adaptable visual recommendation systems capable of handling the ever-increasing diversity of visual content online."
http://arxiv.org/abs/2302.02663v1,"Linking data separation, visual separation, and classifier performance using pseudo-labeling by contrastive learning","Data separation, visual separability, and classifier performance are intrinsically linked, yet quantifying these relationships remains challenging, especially in complex datasets. This paper investigates how contrastive learning-based pseudo-labeling can bridge these concepts to improve classifier accuracy. We propose a novel framework, Contrastive Separation Analysis (CSA), which leverages contrastive learning to generate pseudo-labels and simultaneously optimizes for data separation in the embedding space. CSA uses these learned embeddings to quantify visual separability via a novel metric based on neighborhood relationships, enabling a direct comparison with classifier performance on pseudo-labeled data. Experiments on benchmark datasets demonstrate that CSA achieves superior data separation and visual separability compared to standard contrastive learning, resulting in significant improvements in pseudo-label accuracy and downstream classification performance, especially in scenarios with limited labeled data. Our work provides a valuable tool for understanding and improving the interplay between data representation, visual structure, and classifier efficacy."
http://arxiv.org/abs/2302.03858v2,DeepVATS: Deep Visual Analytics for Time Series,"Time series data is ubiquitous across various domains, yet its analysis often relies on separate, specialized techniques for feature extraction, modeling, and visualization, hindering comprehensive understanding. This paper addresses the challenge of unifying these aspects into a single, deep learning framework capable of both learning informative representations and facilitating visual exploration of time series data. We introduce Deep Visual Analytics for Time Series (DeepVATS), a novel architecture that combines a convolutional autoencoder for unsupervised feature learning with a projection network to generate low-dimensional embeddings suitable for interactive visualization. The learned embeddings are optimized to preserve both temporal proximity and semantic similarity, enabling users to identify patterns, anomalies, and clusters within the data through intuitive visual interfaces. Experiments on diverse time series datasets demonstrate that DeepVATS achieves state-of-the-art performance in clustering and anomaly detection tasks, while simultaneously providing interpretable visualizations that enhance data exploration and knowledge discovery. DeepVATS offers a powerful and integrated solution for time series analysis, bridging the gap between automated learning and interactive visual exploration."
http://arxiv.org/abs/2305.07859v1,HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns,"Understanding the complex interplay between cloud properties and climate patterns remains a significant challenge in climate science due to the vast scale and intricate nature of relevant datasets. Analyzing high-resolution satellite imagery to extract meaningful cloud characteristics is often computationally expensive and requires specialized expertise. To address this, we introduce HAiVA, a Hybrid AI-assisted Visual Analysis framework that combines the strengths of automated deep learning techniques with interactive human-in-the-loop analysis. HAiVA employs a pre-trained convolutional neural network for initial cloud segmentation and property estimation, followed by a user-guided refinement module allowing experts to correct errors, validate findings, and explore specific regions of interest using interactive visualization tools. We demonstrate HAiVA's efficacy by analyzing a multi-year dataset of MODIS satellite imagery, achieving a 20% reduction in manual annotation time while maintaining high accuracy in cloud property extraction, and revealing previously unnoticed correlations between cloud microphysics and regional precipitation patterns. This framework facilitates more efficient and accurate analysis of cloud-climate interactions, providing valuable insights for climate modeling and prediction."
http://arxiv.org/abs/2305.18732v1,Wrapped Cauchy Distributed Angular Softmax for Long-Tailed Visual Recognition,"Long-tailed datasets, characterized by a few dominant classes and many rare ones, pose a significant challenge to visual recognition models. Existing softmax-based classifiers struggle with imbalanced data distributions, leading to poor generalization performance on tail classes. To address this issue, we propose a novel angular margin loss function based on the Wrapped Cauchy distribution, termed Wrapped Cauchy Angular Softmax (WCAS). WCAS leverages the heavy-tailed nature of the Wrapped Cauchy distribution to dynamically adjust the decision boundary margin based on class frequency, effectively increasing the margin for tail classes and decreasing it for head classes. This adaptive margin allows the model to better learn discriminative features for rare classes without sacrificing performance on common classes. Experiments on benchmark long-tailed datasets, including CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT, demonstrate that WCAS consistently outperforms state-of-the-art methods, achieving significant improvements in overall accuracy and particularly on tail classes. The proposed WCAS provides a robust and effective solution for long-tailed visual recognition by adaptively addressing class imbalance within the angular softmax framework."
http://arxiv.org/abs/2309.02968v2,CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse,"Variational Autoencoders (VAEs) have emerged as a powerful tool for generative modeling, but are often plagued by the issue of posterior collapse, where the decoder learns to ignore the latent code. This work addresses the problem of posterior collapse in VAEs, which limits their generative capacity and disentanglement properties. We propose a novel Contrastive Regularization VAE (CR-VAE) that encourages the encoder to map different input data points to distinct latent representations by incorporating a contrastive loss in the latent space. Specifically, we minimize the distance between latent representations of augmented versions of the same input while maximizing the distance between latent representations of different inputs. Experimental results on several benchmark datasets, including MNIST, Fashion-MNIST, and CelebA, demonstrate that CR-VAE significantly mitigates posterior collapse, leading to improved reconstruction quality and generation diversity compared to standard VAEs and other regularization techniques. This approach offers a simple yet effective way to improve the performance and robustness of VAEs for various generative tasks."
http://arxiv.org/abs/2405.15135v1,Exploring the Evolution of Hidden Activations with Live-Update Visualization,"Deep neural networks are often treated as black boxes, making it challenging to understand their internal representations and reasoning processes. This opacity hinders effective debugging, optimization, and trust in deployed models. We address this problem by introducing a novel framework for real-time visualization of hidden layer activations during network training and inference. Our approach, ""Live-Update Visualization,"" dynamically displays activation maps, feature distributions, and neuron selectivity within a user-friendly interface. Crucially, the visualization updates in response to live data and training signals, allowing users to observe the evolution of learned representations over time. We demonstrate the utility of our framework through experiments on image classification and object detection tasks, revealing insights into the formation of robust features, the impact of different training strategies, and the emergence of potential biases. Our work provides a powerful tool for researchers and practitioners to gain a deeper understanding of deep learning models, facilitating improved design, debugging, and interpretability."
http://arxiv.org/abs/2411.03978v1,Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning,"Multi-view clustering aims to group data points leveraging complementary information from multiple modalities. However, existing methods often struggle to accommodate user-specified constraints, such as the desired number of clusters for specific views, hindering their applicability in real-world scenarios. To address this limitation, we propose a novel Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning (CMSPL) framework. CMSPL learns a shared latent representation by projecting each view into a subspace and then uses a unified proxy to bridge these subspaces, effectively capturing cross-modal correlations. Critically, our framework allows for customized clustering by incorporating view-specific cluster number constraints directly into the optimization process via a novel regularization term on the subspace projection matrices. Extensive experiments on benchmark multi-view datasets demonstrate that CMSPL significantly outperforms state-of-the-art methods, achieving superior clustering accuracy and Normalized Mutual Information (NMI) scores, particularly when incorporating view-specific cluster number constraints. This framework offers a flexible and effective solution for multi-view clustering with user-defined customization, enabling more targeted and interpretable data analysis."
http://arxiv.org/abs/2503.00854v1,FACROC: a fairness measure for FAir Clustering through ROC curves,"Fair clustering aims to partition data such that subgroups defined by sensitive attributes receive equitable treatment. Existing fairness measures often rely on demographic parity or equal opportunity, which can be overly restrictive or sensitive to class imbalance. We address the need for a more nuanced and robust fairness metric in clustering by introducing FACROC, a fairness measure based on Receiver Operating Characteristic (ROC) curves. FACROC evaluates fairness by comparing the ROC curves generated for different sensitive groups, where the clustering assignment is treated as a binary classifier. By quantifying the area between the ROC curves, FACROC captures the extent to which clustering performance varies across groups, providing a sensitive and comprehensive assessment of fairness. Experiments on synthetic and real-world datasets demonstrate that FACROC effectively identifies unfair clustering solutions that may be missed by traditional metrics, and provides a more detailed understanding of fairness violations within each cluster. FACROC offers a valuable tool for evaluating and improving the fairness of clustering algorithms in various applications, promoting more equitable outcomes."
http://arxiv.org/abs/2503.20633v1,Enhancing Multi-modal Models with Heterogeneous MoE Adapters for Fine-tuning,"Multi-modal models have shown remarkable capabilities across various vision-language tasks, but fine-tuning them for specific applications often necessitates substantial computational resources and can lead to catastrophic forgetting. This paper addresses the challenge of efficient and effective fine-tuning of multi-modal models by introducing Heterogeneous Mixture-of-Experts (MoE) Adapters. Our method strategically integrates lightweight, modality-specific MoE adapters into pre-trained multi-modal architectures. These adapters consist of multiple expert networks, each specialized for a particular feature subspace within either the visual or textual modality, and are dynamically activated based on the input data using a learnable gating network. We demonstrate that fine-tuning only these MoE adapters achieves comparable or superior performance to full fine-tuning on several benchmark datasets, including VQA, image captioning, and visual reasoning, while significantly reducing the number of trainable parameters. The proposed approach offers a practical and scalable solution for adapting large multi-modal models to downstream tasks, facilitating their wider adoption and deployment."
http://arxiv.org/abs/2504.04783v1,Playing Non-Embedded Card-Based Games with Reinforcement Learning,"Card-based games offer a compelling domain for studying sequential decision-making under imperfect information, yet applying reinforcement learning (RL) to non-embedded games presents unique challenges due to the absence of direct state access and the need for robust visual understanding. This paper addresses the problem of training RL agents to play card games directly from camera input, bypassing the need for game engine integration or explicit state representation. We propose a novel architecture that combines convolutional neural networks (CNNs) for visual feature extraction with recurrent neural networks (RNNs) to model the sequential nature of gameplay and maintain a belief state. Furthermore, we introduce a self-play training regime incorporating curriculum learning based on opponent complexity and action masking to encourage exploration and strategic play. We demonstrate the effectiveness of our approach on a simplified card game environment, achieving performance comparable to rule-based agents and surpassing agents trained without visual input. This work demonstrates the potential of RL for learning complex strategies in visually observed, non-embedded card games, paving the way for AI agents capable of playing a wider range of real-world games without relying on privileged information."
http://arxiv.org/abs/2504.18729v1,Multimodal graph representation learning for website generation based on visual sketch,"Website generation from visual sketches offers an intuitive approach to web design, enabling users to rapidly prototype interfaces. However, existing methods often struggle to capture the complex relationships between visual elements and their corresponding HTML code, resulting in suboptimal website structures. This paper addresses the challenge of accurately translating visual sketches into functional websites by leveraging multimodal graph representation learning. Our approach constructs a heterogeneous graph that integrates visual features extracted from the sketch, semantic information derived from predicted UI element types, and structural relationships inferred from the sketch layout. We then employ a graph neural network to learn node embeddings that capture the interplay between these modalities, guiding the generation of HTML code through an attention-based sequence-to-sequence model. Experimental results on benchmark datasets demonstrate that our method significantly outperforms existing approaches in terms of structural similarity and code accuracy, achieving state-of-the-art performance. This work offers a novel framework for sketch-based website generation, facilitating more efficient and accurate web development workflows."
http://arxiv.org/abs/2505.13138v1,Neurosymbolic Diffusion Models,"Diffusion models have demonstrated remarkable success in generative tasks, particularly in image synthesis. However, they often struggle with tasks requiring precise control and compositional reasoning, where symbolic representations excel. This paper addresses the challenge of integrating the strengths of diffusion models with symbolic reasoning to enable controllable and interpretable generative processes. We introduce Neurosymbolic Diffusion Models (NSDMs), a framework that combines a diffusion model with a symbolic scene representation. Our approach utilizes a neural network to translate noisy image embeddings into a symbolic scene graph, which is then refined using symbolic reasoning rules. This refined scene graph guides the denoising process of the diffusion model through a dedicated control mechanism. Experiments on synthetic and real-world datasets demonstrate that NSDMs achieve superior performance in tasks requiring compositional generation and attribute manipulation compared to standard diffusion models and other neurosymbolic approaches. NSDMs offer a promising avenue for developing generative models with enhanced control, interpretability, and reasoning capabilities."
http://arxiv.org/abs/2505.15511v1,NOMAD Projection,"Projecting high-dimensional data into lower dimensions is a fundamental task in computer vision, enabling efficient computation and visualization. Existing linear projection methods often struggle to preserve the underlying non-linear structure of complex datasets, leading to suboptimal performance in downstream tasks like classification and retrieval. We introduce NOMAD Projection, a novel non-linear dimensionality reduction technique that leverages manifold learning principles with an adaptive neighborhood-based optimization strategy. Our method constructs a local neighborhood graph, iteratively refining its structure and embedding by minimizing a stress function that balances local structure preservation with global data distribution. This adaptive process allows NOMAD Projection to effectively capture both fine-grained details and broader relationships within the data. Experimental results on benchmark datasets demonstrate that NOMAD Projection consistently outperforms state-of-the-art linear and non-linear methods, achieving higher accuracy in classification and improved retrieval performance. The ability to accurately represent complex data structures in lower dimensions makes NOMAD Projection a powerful tool for various computer vision applications."
http://arxiv.org/abs/2507.01544v1,MARVIS: Modality Adaptive Reasoning over VISualizations,"Visualizations are powerful tools for understanding complex data, yet reasoning about them often requires integrating information across different modalities, such as the visual representation and associated textual descriptions. Current vision-language models struggle to effectively reason about visualizations due to their limited ability to adapt to the diverse visual encodings and information structures inherent in different visualization types. We introduce MARVIS, a Modality Adaptive Reasoning over VISualizations framework that dynamically adjusts its reasoning strategy based on the detected visualization type. MARVIS employs a visualization type classifier to guide the selection of modality-specific encoders and a cross-modal attention mechanism tailored for the identified type. Experiments on a diverse benchmark of visualization reasoning tasks demonstrate that MARVIS significantly outperforms state-of-the-art vision-language models, achieving a 15% improvement in accuracy on complex reasoning scenarios. This highlights the importance of modality-aware adaptation for effective reasoning about visualizations and opens avenues for developing more robust and generalizable vision-language models."
http://arxiv.org/abs/2507.16844v1,TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning,"Timing diagrams are essential for understanding the temporal behavior of digital circuits and systems. However, interpreting these diagrams can be challenging, especially for complex systems with numerous signals and intricate timing relationships. This paper introduces TD-Interpreter, a novel visual-language learning approach to enhance the understanding and analysis of timing diagrams. Our method leverages a transformer-based architecture, pre-trained on a large corpus of text and images, fine-tuned to understand the visual and textual elements within timing diagrams. Specifically, TD-Interpreter learns to associate visual features of waveforms with their corresponding signal names and timing constraints expressed in natural language. We evaluate TD-Interpreter on a newly curated dataset of diverse timing diagrams, demonstrating significant improvements in tasks such as signal description generation and timing constraint extraction compared to existing OCR and object detection baselines. This work provides a foundation for developing intelligent tools that can automatically interpret and reason about timing diagrams, facilitating faster and more accurate digital system design and verification."
http://arxiv.org/abs/1704.02592v1,MLC Toolbox: A MATLAB/OCTAVE Library for Multi-Label Classification,"Multi-label classification (MLC) is a supervised learning paradigm where each instance can be associated with multiple labels simultaneously, finding applications in diverse fields like image annotation, text categorization, and bioinformatics. Despite the availability of various MLC algorithms, their implementation and evaluation are often fragmented and lack standardization, hindering accessibility and reproducibility, especially for researchers unfamiliar with specific programming languages or complex software packages. This paper introduces the MLC Toolbox, a comprehensive and user-friendly library implemented in MATLAB and OCTAVE, providing a unified framework for multi-label classification. The toolbox encompasses a wide range of established MLC algorithms, evaluation metrics, and data preprocessing techniques, all implemented with efficiency and clarity in mind. We demonstrate the toolbox's utility through extensive experiments on benchmark datasets, showcasing its performance compared to existing implementations and highlighting its ease of use for both novice and experienced researchers. The MLC Toolbox aims to facilitate research and development in multi-label classification by offering a readily accessible, standardized, and well-documented platform for algorithm development, evaluation, and comparison."
http://arxiv.org/abs/2106.11473v1,Sequential Late Fusion Technique for Multi-modal Sentiment Analysis,"Multi-modal sentiment analysis aims to understand subjective opinions and emotions by integrating information from different modalities, such as text, audio, and video. Existing late fusion approaches often treat modalities independently and combine their outputs at the decision level, potentially overlooking valuable temporal relationships between modalities. This paper introduces a novel Sequential Late Fusion (SLF) technique for multi-modal sentiment analysis that leverages the sequential dependencies between modalities to improve performance. Our approach employs Recurrent Neural Networks (RNNs) to model the temporal evolution of sentiment predictions from individual modalities, allowing the model to learn contextual relationships and dependencies between them. Specifically, the output of each modality-specific sentiment classifier is fed sequentially into an RNN, enabling the fusion process to dynamically adapt to the temporal characteristics of the input data. Experimental results on benchmark datasets demonstrate that SLF outperforms state-of-the-art late fusion methods, achieving significant improvements in accuracy and F1-score. This suggests that considering the sequential dependencies between modalities is crucial for robust and accurate multi-modal sentiment analysis."
http://arxiv.org/abs/2212.01415v1,Measuring Competency of Machine Learning Systems and Enforcing Reliability,"Machine learning (ML) systems are increasingly deployed in safety-critical applications, demanding robust and reliable performance. However, current evaluation metrics often provide an incomplete picture of a model's competency across diverse operational conditions, failing to adequately capture its limitations and vulnerabilities. This paper addresses the challenge of accurately measuring and enforcing competency in ML systems, ensuring reliable performance in real-world deployments. We propose a novel framework that integrates uncertainty estimation with adversarial robustness analysis to quantify a model's competency profile. Specifically, our method combines calibrated uncertainty prediction with targeted adversarial attacks to identify regions of input space where the model exhibits both high uncertainty and susceptibility to perturbation. We then enforce reliability by incorporating a competency-aware rejection mechanism that abstains from making predictions when the system operates outside its validated competency envelope. Our experiments on image classification and object detection benchmarks demonstrate that our approach effectively identifies competency boundaries and significantly improves overall system reliability by reducing erroneous predictions in challenging scenarios, without sacrificing performance in well-understood operating conditions. This framework offers a practical approach to building more trustworthy and dependable machine learning systems."
http://arxiv.org/abs/2502.02185v1,Generative Kernel Spectral Clustering,"Kernel Spectral Clustering (KSC) offers a powerful, non-convex approach to data clustering by leveraging kernel methods to implicitly map data into a higher-dimensional space. However, the performance of KSC is highly dependent on the choice of kernel and its associated parameters, often requiring extensive manual tuning or computationally expensive cross-validation. This paper introduces Generative Kernel Spectral Clustering (GKSC), a novel framework that learns an optimal kernel function directly from the data in conjunction with the clustering assignment. GKSC employs a generative adversarial network (GAN) architecture where the generator network learns to produce kernel matrices that maximize a spectral clustering objective, while the discriminator distinguishes between generated and ideal kernel matrices. Experiments on synthetic and real-world datasets demonstrate that GKSC achieves superior clustering accuracy compared to traditional KSC methods with fixed kernels and other state-of-the-art clustering algorithms. The learned kernel effectively captures the underlying data structure, leading to improved and more robust clustering performance without relying on pre-defined kernel choices."
http://arxiv.org/abs/2504.15171v1,Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture,"Aquaculture is rapidly evolving, demanding automated solutions for efficient fish farming, with feeding intensity assessment being a crucial aspect. Existing methods often struggle to adapt to new fish species or evolving feeding behaviors without retraining on previously seen data, a challenge known as catastrophic forgetting. This paper addresses the problem of class-incremental learning in the context of audio-visual fish feeding intensity assessment. We propose a novel audio-visual class-incremental learning framework incorporating a knowledge distillation strategy and a dynamic feature alignment module. The knowledge distillation preserves previously learned knowledge, while the feature alignment module mitigates inter-class feature drift by aligning new class features with historical representations in a shared embedding space. Experiments on a newly collected audio-visual fish feeding dataset demonstrate that our approach significantly outperforms existing class-incremental learning methods, achieving a relative improvement of 15% in average incremental accuracy. This work offers a practical and scalable solution for real-world aquaculture monitoring systems, enabling continuous learning and adaptation to diverse fish species and dynamic feeding environments."
http://arxiv.org/abs/2506.16396v1,GoalLadder: Incremental Goal Discovery with Vision-Language Models,"Vision-language models (VLMs) have shown remarkable capabilities in understanding and interacting with visual environments. However, effectively leveraging VLMs for complex robotic tasks requires defining suitable goals, which is often a manual and challenging process. This paper addresses the problem of automatically discovering a hierarchy of increasingly complex goals that a robot can learn and execute. We introduce GoalLadder, an incremental goal discovery framework that uses a VLM to propose new, reachable goals based on the current state of the environment and the robot's capabilities. GoalLadder iteratively refines these proposals by evaluating their feasibility through a learned reachability function and their novelty using a semantic similarity measure, creating a ladder of increasingly challenging tasks. Experiments in simulated robotic environments demonstrate that GoalLadder can autonomously discover a diverse set of meaningful goals, enabling the robot to progressively expand its skill repertoire and achieve more complex tasks compared to methods relying on pre-defined goal sets. GoalLadder offers a promising approach to automating robot learning and enabling autonomous exploration of complex task spaces."
http://arxiv.org/abs/2106.11112v3,Multivariate Data Explanation by Jumping Emerging Patterns Visualization,"Understanding complex multivariate data is crucial in various domains, yet identifying the underlying relationships and patterns remains a significant challenge. Traditional visualization techniques often struggle to effectively represent and explain the interplay between multiple variables, leading to difficulties in knowledge discovery. This paper addresses the problem of explaining multivariate data through the identification and visualization of emerging patterns that significantly differentiate groups of observations. We introduce a novel approach, Jumping Emerging Patterns Visualization (JEPV), which combines a statistically robust emerging pattern mining algorithm with an interactive visualization framework. JEPV identifies patterns that exhibit a substantial difference in frequency between user-defined groups, and then visualizes these ""jumping"" patterns as interconnected nodes, where node size represents pattern support and edge thickness reflects pattern overlap. The framework allows users to interactively explore these patterns, filter them based on statistical significance and support, and examine the underlying data points associated with each pattern. We demonstrate the effectiveness of JEPV on several real-world datasets, including medical diagnosis and customer behavior analysis, revealing previously hidden relationships and providing actionable insights. JEPV offers a powerful tool for exploring and explaining complex multivariate datasets, facilitating data-driven decision-making across diverse applications."
http://arxiv.org/abs/2110.03882v2,ModeRNN: Harnessing Spatiotemporal Mode Collapse in Unsupervised Predictive Learning,"Unsupervised video prediction aims to learn representations that capture the underlying dynamics of visual scenes. Current approaches often suffer from mode collapse, generating blurry or deterministic predictions that fail to represent the inherent uncertainty and multi-modality of future events. This paper addresses the challenge of harnessing, rather than avoiding, spatiotemporal mode collapse to improve unsupervised predictive learning. We introduce ModeRNN, a novel recurrent architecture that explicitly models distinct spatiotemporal modes of future video frames. ModeRNN utilizes a mixture-of-experts framework within its recurrent cell, allowing it to predict multiple plausible future states based on learned mode representations. A differentiable mode selection mechanism then allows the model to sample and refine these predictions over time. Experiments on benchmark datasets demonstrate that ModeRNN generates sharper, more diverse, and more accurate predictions compared to state-of-the-art methods, achieving significant improvements in both quantitative metrics and qualitative visual fidelity. This work highlights the potential of leveraging mode collapse to learn richer and more robust representations of video dynamics for unsupervised prediction."
http://arxiv.org/abs/1609.01977v2,Doubly Stochastic Neighbor Embedding on Spheres,"Nonlinear dimensionality reduction techniques are crucial for visualizing and analyzing high-dimensional data, often relying on preserving local neighborhood structures. However, standard methods like t-distributed Stochastic Neighbor Embedding (t-SNE) struggle to effectively embed data residing on or near spherical manifolds, leading to distorted representations and loss of global structure. We address this limitation by introducing Doubly Stochastic Neighbor Embedding on Spheres (DSNE-Spheres), an extension of doubly stochastic neighbor embedding (DSNE) that incorporates spherical constraints into the embedding process. DSNE-Spheres utilizes a von Mises-Fisher distribution to model neighborhood probabilities on the hypersphere and enforces orthogonality constraints during gradient descent to maintain spherical embeddings. Experiments on synthetic and real-world datasets, including hyperspectral imagery and document embeddings, demonstrate that DSNE-Spheres outperforms existing methods in preserving both local neighborhood relationships and global spherical geometry, as quantified by geodesic distance correlation and visual inspection. This improved embedding capability enables more accurate data visualization and downstream analysis for datasets with inherent spherical structure."
http://arxiv.org/abs/2011.02701v1,A Black-Box Attack Model for Visually-Aware Recommender Systems,"Visually-aware recommender systems leverage visual information from items to improve recommendation accuracy and user experience. However, the vulnerability of these systems to adversarial attacks remains largely unexplored, particularly under realistic black-box settings where the attacker lacks access to the model's internal parameters and gradients. This paper addresses the challenge of crafting effective black-box attacks against visually-aware recommender systems. We propose a novel query-efficient black-box attack model that combines a surrogate model trained on synthetic data with a transfer-based attack strategy. Specifically, we generate synthetic data by perturbing item images and observing the recommender system's output, then train a surrogate model to mimic the target system's behavior. This surrogate model is subsequently used to craft adversarial examples, which are then transferred to the target system. Our experiments on benchmark datasets demonstrate that our approach significantly degrades the performance of state-of-the-art visually-aware recommender systems with a limited number of queries. These findings highlight the practical vulnerability of visually-aware recommender systems to black-box attacks, emphasizing the need for robust defense mechanisms."
http://arxiv.org/abs/2207.01814v3,Multimodal Frame-Scoring Transformer for Video Summarization,"Video summarization aims to generate concise and representative summaries of long videos, reducing redundancy while preserving crucial information. Existing methods often rely on unimodal visual cues or simplistic fusion strategies for multimodal data, limiting their ability to capture complex relationships between different modalities. This paper introduces a novel Multimodal Frame-Scoring Transformer (MFST) for video summarization. MFST leverages a Transformer encoder to effectively fuse information from visual, audio, and textual modalities, learning modality-specific embeddings and capturing inter-modal dependencies through self-attention mechanisms. A frame-scoring module then predicts the importance of each frame based on the fused multimodal representation, enabling the selection of key frames for the final summary. Experimental results on benchmark datasets, such as TVSum and SumMe, demonstrate that MFST outperforms state-of-the-art methods in terms of F-score and achieves competitive performance in user studies. The proposed MFST offers a powerful and flexible framework for multimodal video summarization, paving the way for more accurate and informative video summaries."
http://arxiv.org/abs/1710.05128v5,Parametric t-Distributed Stochastic Exemplar-centered Embedding,"Stochastic Neighbor Embedding (SNE) and its variants are powerful tools for dimensionality reduction and visualization, particularly for high-dimensional data. However, standard t-distributed SNE (t-SNE) suffers from parameter sensitivity and a tendency to crowd points together, obscuring local structure. To address these limitations, we introduce Parametric t-Distributed Stochastic Exemplar-centered Embedding (PtEx-t-SNE). Our method learns a parametric mapping from the high-dimensional input space to a low-dimensional embedding space while simultaneously optimizing for exemplar-centered similarities based on the t-distribution. This is achieved by selecting a subset of data points as exemplars and explicitly modeling the similarity between all other points and these exemplars, reducing computational complexity and enhancing local structure preservation. Experiments on benchmark datasets demonstrate that PtEx-t-SNE achieves superior performance compared to t-SNE and other related methods in terms of both visualization quality and quantitative metrics, particularly in preserving local neighborhood relationships. This improved embedding offers a more robust and interpretable representation for downstream tasks such as clustering and classification."
http://arxiv.org/abs/2111.15037v3,CO-SNE: Dimensionality Reduction and Visualization for Hyperbolic Data,"Hyperbolic spaces are increasingly recognized as suitable representations for hierarchical and graph-structured data, offering advantages in terms of embedding quality and expressiveness compared to Euclidean spaces. However, visualizing high-dimensional hyperbolic embeddings in a low-dimensional space for human understanding remains a significant challenge due to the inherent distortions involved in projecting from hyperbolic to Euclidean geometry. This paper introduces CO-SNE (Curvature-Optimized Stochastic Neighbor Embedding), a novel dimensionality reduction technique specifically designed for hyperbolic data. CO-SNE leverages the Riemannian geometry of hyperbolic space, incorporating a curvature-aware stress function that minimizes distortions during the embedding process. Furthermore, it introduces a cost function that directly optimizes for preserving local hyperbolic distances, leading to more faithful representations. Experiments on benchmark datasets demonstrate that CO-SNE outperforms existing dimensionality reduction techniques, including t-SNE and UMAP, in terms of preserving both local and global structure within hyperbolic embeddings. This improved visualization capability facilitates a deeper understanding and interpretation of hyperbolic representations, enabling more effective analysis of hierarchical data."
http://arxiv.org/abs/2205.04834v1,Cognitive Visual-learning Environment for PostgreSQL,"PostgreSQL is a powerful open-source relational database management system, yet its complex query language (SQL) presents a significant barrier to entry for novice users. This paper addresses the challenge of improving SQL learning accessibility by developing a cognitive visual-learning environment. Our approach leverages visual representations of database schemas and query operations to scaffold the learning process, integrating a dynamic, interactive interface with immediate feedback on query construction. Specifically, we introduce a novel visual query builder that translates user-defined visual operations into corresponding SQL code, facilitating an intuitive understanding of SQL syntax and semantics. We evaluated our system with a user study involving participants with varying levels of SQL experience. Results demonstrate a significant improvement in both query accuracy and learning efficiency for novice users compared to traditional text-based learning methods, suggesting that visual representations can effectively enhance SQL comprehension. This work contributes a valuable tool for democratizing database access and empowering a broader range of individuals to leverage the power of PostgreSQL."
http://arxiv.org/abs/2206.15049v3,ZeroC: A Neuro-Symbolic Model for Zero-shot Concept Recognition and Acquisition at Inference Time,"Concept recognition in open-world environments demands the ability to identify and understand novel concepts beyond pre-defined categories. This paper addresses the challenge of zero-shot concept recognition and acquisition, where the goal is to recognize and learn new concepts at inference time without any prior training examples. We introduce ZeroC, a neuro-symbolic model that combines the representational power of neural networks with the reasoning capabilities of symbolic logic. ZeroC leverages a pre-trained vision-language model to extract visual and textual features, which are then grounded in a symbolic knowledge base defining relationships between concepts and attributes. At inference, novel concept definitions, provided as textual descriptions, are parsed into logical rules, enabling the model to identify instances of the new concept by reasoning over the visual features and the knowledge base. Experiments on benchmark datasets demonstrate that ZeroC achieves state-of-the-art zero-shot concept recognition accuracy and exhibits a strong ability to acquire and generalize to new concepts described through diverse logical rules. This approach provides a promising direction for building more adaptable and human-interpretable vision systems that can continuously learn and reason about the visual world."
http://arxiv.org/abs/2301.11416v1,Feature space exploration as an alternative for design space exploration beyond the parametric space,"Design space exploration (DSE) is a crucial step in developing computer vision algorithms, often involving tuning parameters to optimize performance. However, focusing solely on parametric variations limits exploration when architectural choices or fundamental feature representations are under consideration. This paper addresses the challenge of expanding DSE beyond the parametric space by introducing a novel feature space exploration (FSE) framework. We propose a method that leverages learned feature embeddings from different network layers and modalities to construct a comprehensive feature space. This space is then systematically explored using techniques inspired by multi-objective optimization, allowing us to identify promising feature combinations and architectures without exhaustively evaluating every possible configuration. Our experiments on image classification and object detection tasks demonstrate that FSE can discover novel feature fusion strategies and architectural modifications that outperform existing hand-crafted approaches and those found through traditional parametric DSE. This work provides a powerful alternative to parametric DSE, enabling a more comprehensive and efficient search for optimal computer vision solutions."
http://arxiv.org/abs/2305.19889v1,Evaluating Machine Learning Models with NERO: Non-Equivariance Revealed on Orbits,"Machine learning models deployed in scientific domains are often expected to respect known symmetries of the underlying physical laws. However, evaluating whether a model truly embodies these symmetries beyond simple invariance is challenging. We address this problem by introducing NERO (Non-Equivariance Revealed on Orbits), a novel evaluation framework that probes models for equivariance violations by analyzing their behavior along symmetry orbits. NERO perturbs input data according to a specified symmetry transformation, generates an orbit of transformed inputs, and then quantifies the deviation of the model's predictions from the expected equivariant transformation. Experiments on image classification and fluid dynamics tasks demonstrate that NERO effectively reveals subtle non-equivariances in models that exhibit high accuracy and near-invariance, exposing inconsistencies in their learned representations. NERO provides a crucial tool for assessing the reliability and trustworthiness of machine learning models in scientific applications where symmetry constraints are paramount."
http://arxiv.org/abs/2306.02137v2,Inconsistent Matters: A Knowledge-guided Dual-consistency Network for Multi-modal Rumor Detection,"Multi-modal rumor detection aims to identify false information spreading online by leveraging heterogeneous data sources like text and images. However, existing methods often overlook the crucial role of inconsistent information within and between modalities, which can be a strong indicator of rumor propagation. To address this limitation, we propose a Knowledge-guided Dual-consistency Network (KDDN) that explicitly models inconsistencies for enhanced rumor detection. KDDN incorporates external knowledge to guide the identification of semantic deviations within each modality and then learns to capture the inconsistency between textual and visual representations. Specifically, we employ a knowledge-aware attention mechanism to highlight inconsistent semantic units within each modality and a cross-modal interaction module to measure the discrepancy between modalities. Experimental results on benchmark datasets demonstrate that KDDN significantly outperforms state-of-the-art methods, achieving improvements in both accuracy and F1-score. This highlights the importance of modeling inconsistencies for effective multi-modal rumor detection."
http://arxiv.org/abs/2309.12628v1,Sequential Action-Induced Invariant Representation for Reinforcement Learning,"Reinforcement learning (RL) agents often struggle with generalization across environments due to sensitivity to irrelevant variations in visual observations. Learning invariant representations can mitigate this issue, but existing methods typically focus on static invariance or require extensive domain knowledge. This paper addresses the challenge of learning robust representations that are invariant to task-irrelevant variations while remaining sensitive to action-relevant dynamics in sequential decision-making problems. We propose Sequential Action-Induced Invariant Representation (SAIR), a novel approach that leverages temporal action sequences as supervisory signals to enforce invariance. SAIR learns a representation by minimizing the discrepancy between representations of states reached by applying the same sequence of actions from different initial states, while simultaneously maximizing the representation's ability to predict future rewards. Our experiments across a suite of challenging visual RL benchmarks demonstrate that SAIR significantly outperforms state-of-the-art invariant representation learning methods and improves generalization to unseen environments with novel visual distractors. This highlights the effectiveness of using sequential action information to learn robust and generalizable representations for RL."
http://arxiv.org/abs/2310.14274v1,Robust Visual Imitation Learning with Inverse Dynamics Representations,"Imitation learning offers a promising avenue for transferring robotic skills from expert demonstrations. However, real-world deployment faces challenges due to the compounding error problem and distribution shift between demonstration and execution environments. This paper addresses the problem of robust imitation learning by leveraging inverse dynamics models to create more generalizable representations. Our approach, Inverse Dynamics Representation Learning (IDRL), learns a latent space where actions are predicted through an inverse dynamics model, enforcing physically plausible transitions and disentangling task-relevant features from irrelevant variations. We train the inverse dynamics model jointly with a downstream imitation learning policy in an end-to-end fashion. Experimental results across a variety of simulated robotic manipulation tasks demonstrate that IDRL significantly improves performance and robustness compared to state-of-the-art imitation learning algorithms, particularly in the presence of environmental perturbations and novel initial conditions. These findings highlight the benefits of incorporating physical priors into representation learning for effective and reliable skill transfer in robotics."
http://arxiv.org/abs/2312.10188v1,"WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data","Extracting structured information from visually rich web documents is crucial for various downstream tasks, including information retrieval, knowledge graph construction, and document understanding. However, automatically processing web pages at scale poses significant challenges due to the diverse layouts, multilingual content, and noisy nature of web crawl data. This paper introduces WordScape, a novel pipeline designed to extract multilingual documents with detailed layout annotations from web crawls. WordScape integrates a series of modules, including language detection, boilerplate removal, optical character recognition (OCR), and layout analysis, optimized for web data. Specifically, we leverage a combination of rule-based and machine learning techniques for robust boilerplate removal and employ a fine-tuned OCR engine to handle diverse fonts and image qualities. Furthermore, we utilize a graph-based approach to infer the document layout and generate structured annotations at the word level, including bounding boxes, font styles, and reading order. Experiments on a large-scale web crawl dataset demonstrate that WordScape achieves state-of-the-art performance in document extraction and layout analysis, significantly improving the accuracy of downstream information extraction tasks. WordScape provides a valuable resource and a robust framework for processing visually rich, multilingual web documents, enabling a wide range of research and application opportunities."
http://arxiv.org/abs/2404.09463v1,PRIME: A CyberGIS Platform for Resilience Inference Measurement and Enhancement,"Understanding and enhancing community resilience to hazards requires integrating diverse data sources, advanced analytical methods, and collaborative decision-making tools. However, existing cyberinfrastructure often lacks the comprehensive capabilities to effectively measure, model, and improve resilience across various hazard types and scales. This paper introduces PRIME (Platform for Resilience Inference Measurement and Enhancement), a CyberGIS platform designed to address these limitations by providing a unified environment for resilience assessment and enhancement. PRIME leverages a modular architecture that incorporates high-resolution spatial data, machine learning-based inference models for hazard impact and recovery, and interactive visualization tools to facilitate stakeholder engagement. We demonstrate PRIME's capabilities through a case study focusing on flood resilience in a coastal community, showcasing its ability to identify vulnerable areas, simulate the effects of mitigation strategies, and quantify resilience improvements. PRIME offers a powerful and adaptable framework for researchers, policymakers, and community stakeholders to collaboratively build more resilient communities."
http://arxiv.org/abs/2406.13754v1,Concept Drift Visualization of SVM with Shifting Window,"Concept drift, the phenomenon where the statistical properties of the target variable change over time, poses a significant challenge to the performance of machine learning models deployed in dynamic environments. This paper addresses the problem of visualizing and understanding concept drift in Support Vector Machines (SVMs) trained on streaming data. We propose a novel visualization technique that utilizes a shifting window approach to continuously retrain an SVM and project the decision boundary and support vectors onto a lower-dimensional space using Principal Component Analysis (PCA). This projection enables the visualization of the decision boundary's evolution and the movement of support vectors over time, highlighting regions where the model adapts to changes in the data distribution. Experimental results on both synthetic and real-world datasets demonstrate that our visualization method effectively captures different types of concept drift, including sudden, gradual, and recurring drifts, providing insights into the model's adaptation process and the nature of the underlying data changes. This visualization tool allows for a more intuitive understanding of SVM behavior in non-stationary environments, aiding in model selection, drift detection, and adaptive learning strategy development."
http://arxiv.org/abs/2408.07724v1,"""Normalized Stress"" is Not Normalized: How to Interpret Stress Correctly","Stress, a measure of local shape deformation, is widely used in computer vision and graphics for tasks like shape analysis, mesh parameterization, and deformation transfer. A common practice is to ""normalize"" stress by dividing it by area, supposedly to obtain a scale-invariant measure. However, this normalization, while intuitively appealing, obscures the true geometric meaning of stress and can lead to incorrect interpretations and suboptimal results. We demonstrate, through theoretical analysis and empirical evaluation, that the standard ""normalized stress"" is not a true normalization and that its magnitude is intrinsically linked to the underlying mesh resolution, invalidating comparisons across meshes with different densities. We propose a revised interpretation of stress, focusing on its relationship to the discrete Laplace-Beltrami operator and the local curvature tensor. Our interpretation allows for a more accurate assessment of shape deformation, leading to improved performance in applications such as feature point detection and shape matching. We show that using the unnormalized stress, along with a careful consideration of mesh resolution, yields more robust and meaningful results compared to relying on the commonly used ""normalized"" variant. This work provides a crucial correction in how stress is understood and applied, leading to more reliable shape analysis in various computer vision tasks."
http://arxiv.org/abs/2503.13964v1,MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding,"Document understanding (DU) requires sophisticated reasoning across visual, textual, and layout modalities. Existing DU models often struggle with complex documents demanding iterative reasoning and information aggregation from diverse sources. To address this limitation, we introduce MDocAgent, a novel multi-modal multi-agent framework for document understanding. MDocAgent leverages a team of specialized agents, each focusing on a specific aspect of the document, such as text extraction, layout analysis, and visual element recognition. These agents interact and collaborate through a shared workspace, iteratively refining their understanding of the document and resolving ambiguities. Experiments on benchmark datasets including DocVQA and Kleister Charity show that MDocAgent significantly outperforms state-of-the-art methods, achieving improvements of up to 5% in accuracy. This demonstrates the effectiveness of a collaborative, agent-based approach for complex document understanding tasks, paving the way for more robust and adaptable DU systems."
http://arxiv.org/abs/2505.21660v1,PreGenie: An Agentic Framework for High-quality Visual Presentation Generation,"Creating compelling and informative visual presentations is a time-consuming process, often requiring expertise in design and content synthesis. Current presentation generation methods often struggle to produce high-quality and engaging visuals that effectively communicate complex information. This paper introduces PreGenie, an agentic framework designed to automate the creation of high-quality visual presentations. PreGenie leverages a multi-agent system comprising specialized agents responsible for content extraction, slide layout design, visual element generation, and iterative refinement, coordinated by a central planning agent. The framework incorporates a novel feedback mechanism, allowing agents to evaluate and improve the presentation based on predefined quality metrics and user preferences. Experimental results demonstrate that PreGenie significantly outperforms existing state-of-the-art methods in terms of visual appeal, information clarity, and overall presentation quality, as evaluated by both quantitative metrics and human studies. PreGenie offers a powerful tool for democratizing presentation creation, enabling users to efficiently generate impactful visual narratives without requiring extensive design skills."
http://arxiv.org/abs/2009.06027v2,ReviewViz: Assisting Developers Perform Empirical Study on Energy Consumption Related Reviews for Mobile Applications,"Mobile application energy consumption is a critical concern for users and developers alike, frequently discussed in app reviews. However, manually analyzing the vast volume of user reviews to understand energy-related issues is a time-consuming and challenging task for developers. This paper addresses the problem of efficiently extracting and summarizing actionable insights regarding energy consumption from user reviews of mobile applications. We introduce ReviewViz, a novel visualization and analysis tool that combines natural language processing techniques with interactive visualizations to assist developers in performing empirical studies on energy consumption related reviews. ReviewViz employs a rule-based system and a fine-tuned BERT model to identify and classify energy-related reviews, followed by sentiment analysis and topic modeling to extract key themes. Interactive visualizations, including sentiment timelines and topic distributions, enable developers to explore the evolution of energy-related issues and identify specific areas for optimization. Our evaluation using real-world app review datasets demonstrates that ReviewViz significantly improves the efficiency of identifying and understanding energy consumption related feedback, allowing developers to prioritize and address user concerns effectively. This tool facilitates data-driven decision-making for energy optimization, ultimately leading to improved user experience and reduced energy waste."
http://arxiv.org/abs/2507.01803v1,Towards Decentralized and Sustainable Foundation Model Training with the Edge,"Foundation models, characterized by their massive size and extensive training data, have revolutionized various fields, but their development is currently limited to resource-rich centralized environments. The computational demands and energy consumption associated with training these models pose significant challenges in terms of accessibility and sustainability. This paper addresses the problem of democratizing foundation model training by exploring a decentralized approach leveraging edge computing resources. We propose a novel federated learning framework, EdgeFM, that incorporates adaptive model partitioning and asynchronous knowledge distillation to enable efficient and collaborative training of foundation models across geographically distributed edge devices with heterogeneous capabilities and intermittent connectivity. EdgeFM minimizes communication overhead and maximizes resource utilization by dynamically distributing model layers based on edge device capabilities and distilling knowledge from edge-trained sub-models to a global foundation model. Experiments on image and language modeling tasks demonstrate that EdgeFM achieves comparable performance to centralized training while significantly reducing the computational burden on individual edge devices and minimizing communication costs. This decentralized paradigm unlocks the potential for sustainable and accessible foundation model development, enabling broader participation and fostering innovation at the edge."
http://arxiv.org/abs/2302.06658v2,In Search for a Generalizable Method for Source Free Domain Adaptation,"Source-free domain adaptation (SFDA) aims to adapt a model pre-trained on a labeled source domain to an unlabeled target domain without accessing the source data, which is crucial for privacy-sensitive applications. However, many existing SFDA methods rely on specific assumptions about the source and target domains, hindering their generalization capability across diverse adaptation scenarios. This paper addresses the challenge of developing a generalizable SFDA method robust to varying domain shifts and task complexities. We propose a novel approach, termed Contrastive Clustering and Alignment (CCA), which leverages self-supervised contrastive learning to discover domain-invariant features and aligns the cluster structures between the source and target domains. Specifically, CCA employs a momentum contrastive learning framework to learn robust feature representations and simultaneously enforces cluster-level alignment via a distribution matching loss, promoting consistent semantic organization across domains. Extensive experiments on multiple benchmark datasets demonstrate that CCA consistently outperforms state-of-the-art SFDA methods, achieving superior adaptation performance across a wide range of domain shifts and task complexities. Our work provides a significant step towards building truly generalizable SFDA algorithms, facilitating broader adoption in real-world applications."
http://arxiv.org/abs/2303.14423v1,Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation,"Continual learning aims to enable models to learn new tasks sequentially without forgetting previously acquired knowledge. However, catastrophic forgetting remains a significant challenge, particularly in complex vision-and-language (V&L) tasks that require intricate cross-modal reasoning. This paper addresses the problem of catastrophic forgetting in continual learning of V&L tasks by proposing a novel Task-Attentive Transformer (TAT) architecture combined with knowledge distillation. The TAT architecture incorporates task-specific attention modules that dynamically adapt the model's parameters based on the current task, enabling targeted knowledge retention and transfer. Furthermore, we leverage knowledge distillation to transfer crucial knowledge from previous tasks to the current task, mitigating forgetting without storing previous data. Extensive experiments on several challenging V&L continual learning benchmarks demonstrate that our approach significantly outperforms existing methods in terms of average accuracy and backward transfer, achieving state-of-the-art results. This work offers a promising direction for building robust and adaptable V&L models capable of continually learning from new experiences."
http://arxiv.org/abs/2408.03480v1,Advancing EEG-Based Gaze Prediction Using Depthwise Separable Convolution and Enhanced Pre-Processing,"Gaze prediction based on electroencephalography (EEG) offers a promising avenue for assistive technologies and brain-computer interfaces, enabling hands-free control and communication. However, the inherent noise and non-stationarity of EEG signals, coupled with the complex relationship between neural activity and eye movements, pose significant challenges to accurate and robust gaze prediction. This paper addresses the problem of improving EEG-based gaze prediction accuracy and efficiency. We propose a novel framework incorporating enhanced signal pre-processing techniques, including independent component analysis (ICA) for artifact removal and wavelet transform for feature extraction, followed by a deep learning model leveraging depthwise separable convolutions to minimize computational complexity and overfitting. Specifically, the depthwise separable convolutional neural network (DSCNN) efficiently learns spatial and temporal EEG patterns related to gaze direction. Our experiments demonstrate that the proposed method significantly outperforms existing state-of-the-art approaches, achieving a 15% reduction in average prediction error and a 30% decrease in model parameters on a benchmark EEG gaze dataset. These improvements highlight the potential of our approach to facilitate real-time, low-latency gaze prediction for practical applications."
http://arxiv.org/abs/2505.23024v1,An Empirical Study of Federated Prompt Learning for Vision Language Model,"Vision-Language Models (VLMs) have demonstrated remarkable zero-shot transfer capabilities, often relying on prompt engineering to adapt to downstream tasks. However, these models are typically trained on centralized datasets, raising privacy concerns and limiting their applicability in decentralized scenarios. This paper investigates the feasibility and effectiveness of federated prompt learning, a privacy-preserving approach to adapting VLMs across multiple clients without sharing raw data. We propose a novel federated prompt learning framework that aggregates prompt updates from distributed clients while incorporating differential privacy to ensure data confidentiality. Our empirical study, conducted on a diverse set of vision-language datasets, demonstrates that federated prompt learning achieves comparable or even superior performance to centralized prompt learning under certain conditions, while significantly reducing communication costs compared to federated fine-tuning. This work highlights the potential of federated learning for adapting VLMs in privacy-sensitive environments, paving the way for more robust and decentralized vision-language applications."
http://arxiv.org/abs/1312.0579v1,SpeedMachines: Anytime Structured Prediction,"Structured prediction tasks, such as semantic segmentation and pose estimation, are computationally expensive, hindering their deployment in resource-constrained environments. This paper addresses the challenge of performing structured prediction under strict time budgets, where a complete solution is required even if the allotted time is interrupted. We introduce SpeedMachines, an anytime structured prediction framework built upon a cascade of progressively refined predictors. Each predictor in the cascade leverages the output of its predecessor as context, enabling faster convergence to a high-quality solution. We train the cascade using a novel loss function that encourages both accuracy and speed, ensuring that earlier predictors are optimized for rapid initial estimates while later predictors focus on refinement. Experiments on benchmark datasets for semantic segmentation and human pose estimation demonstrate that SpeedMachines consistently achieves significantly improved accuracy under tight time constraints compared to state-of-the-art methods, while also providing competitive performance with ample computational resources. Our approach offers a practical solution for deploying complex structured prediction models in real-time applications with varying computational budgets."
http://arxiv.org/abs/2402.02207v2,Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models,"Vision Large Language Models (VLLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content. However, their propensity to generate unsafe or inappropriate content remains a significant barrier to real-world deployment. This paper addresses the challenge of efficiently aligning VLLMs with safety constraints without incurring substantial computational overhead or performance degradation on general vision-language tasks. We introduce a novel safety fine-tuning approach that leverages a small, carefully curated dataset of adversarial examples generated by prompting the pre-trained VLLM itself. This self-generated adversarial data is then used to fine-tune a lightweight adapter module, allowing the model to learn to recognize and mitigate unsafe outputs. Our experiments on a range of safety benchmarks demonstrate that this method significantly reduces the generation of harmful content while preserving the model's original capabilities, achieving comparable performance to full fine-tuning with significantly less computational cost and data. This work establishes a strong, efficient baseline for safety alignment in VLLMs, paving the way for safer and more reliable vision-language applications."
http://arxiv.org/abs/2009.11719v1,Deep Neural Networks with Short Circuits for Improved Gradient Learning,"Deep neural networks, while powerful function approximators, can suffer from vanishing gradients, hindering effective learning, especially in very deep architectures. This paper addresses the problem of gradient degradation during backpropagation in deep networks, which limits the ability of early layers to learn meaningful features. We propose a novel architecture incorporating ""short-circuit"" connections that directly propagate gradients from later layers to earlier layers, bypassing several intervening layers. These connections are implemented as learnable linear transformations, allowing the network to adaptively control the flow of gradient information. Experiments on image classification benchmarks, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that our short-circuit networks achieve significant improvements in accuracy and training convergence compared to standard deep architectures and ResNet baselines, particularly as network depth increases. These results highlight the effectiveness of short-circuit connections in mitigating gradient vanishing and enabling the training of deeper, more powerful neural networks."
http://arxiv.org/abs/2010.09163v2,D2RL: Deep Dense Architectures in Reinforcement Learning,"Reinforcement learning (RL) has achieved remarkable success in various domains, yet sample efficiency remains a significant challenge, particularly in complex environments. This work addresses the problem of sparse reward signals and inefficient exploration in RL by leveraging deep, densely connected neural network architectures to improve value function estimation and policy learning. We introduce Deep Dense Reinforcement Learning (D2RL), a novel framework that incorporates dense connectivity patterns within the deep neural networks used for both value function approximation and policy representation. Specifically, we employ densely connected blocks to facilitate information flow and feature reuse across different layers, promoting more robust and efficient learning. Experimental results across a suite of challenging benchmark environments, including those with sparse rewards and high-dimensional state spaces, demonstrate that D2RL significantly outperforms state-of-the-art RL algorithms in terms of sample efficiency, final performance, and robustness to hyperparameter variations. This improved performance highlights the potential of dense architectures to accelerate learning and improve generalization in reinforcement learning, paving the way for more effective application of RL in real-world scenarios."
http://arxiv.org/abs/2106.06839v2,Intelligent Vision Based Wear Forecasting on Surfaces of Machine Tool Elements,"Machine tool element wear significantly impacts manufacturing precision, efficiency, and cost. Traditional wear monitoring methods are often invasive, time-consuming, or require specialized equipment, hindering real-time and in-situ assessment. This paper addresses the challenge of developing a non-destructive and automated approach for forecasting wear progression on machine tool surfaces. We propose an intelligent vision-based system that integrates advanced image processing techniques, including surface texture analysis and defect detection, with deep learning models, specifically Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), for wear stage classification and future wear prediction. The CNN extracts relevant features from surface images captured over time, while the RNN models the temporal dependencies to forecast future wear based on the observed wear history. Experimental results on a dataset of worn machine tool elements demonstrate a significant improvement in wear stage classification accuracy (over 90%) and a notable reduction in wear forecast error compared to traditional methods. This intelligent vision-based system offers a cost-effective and efficient solution for proactive maintenance, enabling timely interventions and minimizing downtime in manufacturing environments."
http://arxiv.org/abs/2110.13968v2,On the Effects of Artificial Data Modification,"Data augmentation is a cornerstone of modern computer vision, enabling models to generalize better from limited training sets. However, the effects of different augmentation strategies, particularly artificial data modification (ADM) techniques that generate entirely new, synthetic data points, remain poorly understood. This paper addresses the problem of quantifying the impact of various ADM techniques on model performance and generalization ability, going beyond simple accuracy metrics. We propose a novel framework that combines techniques from representation learning and information theory to analyze how ADM affects the learned feature space. This framework allows us to measure the information content added by ADM, the similarity between real and synthetic data representations, and the robustness of the learned features to adversarial perturbations. Our experiments on several benchmark datasets and ADM techniques, including GAN-based data generation and style transfer, reveal that while ADM can boost accuracy, it often leads to a compressed feature space with reduced information capacity and increased vulnerability to adversarial attacks. These findings highlight the need for careful consideration and analysis when deploying ADM techniques, emphasizing that improved accuracy alone is not a sufficient indicator of improved generalization and robustness."
http://arxiv.org/abs/2203.05126v2,PACTran: PAC-Bayesian Metrics for Estimating the Transferability of Pretrained Models to Classification Tasks,"Transfer learning leverages pretrained models to accelerate and improve performance on downstream tasks. However, predicting the transferability of a pretrained model to a specific target dataset remains a challenging problem. This paper addresses the problem of estimating transferability in classification tasks using PAC-Bayesian theory to derive novel, theoretically grounded metrics. We propose PACTran, a suite of transferability metrics based on PAC-Bayesian generalization bounds, quantifying the expected performance of a finetuned model by considering the prior distribution over model weights derived from the pretrained model and the posterior distribution learned during finetuning. These metrics incorporate task similarity and model complexity to provide a robust estimate of transferability. Experiments on diverse image classification datasets demonstrate that PACTran metrics correlate strongly with observed transfer learning performance, outperforming existing heuristic-based transferability measures. This work provides a principled and effective approach for selecting optimal pretrained models for transfer learning, ultimately reducing the computational cost and improving the efficiency of adapting models to new tasks."
http://arxiv.org/abs/2103.02631v3,RotoGrad: Gradient Homogenization in Multitask Learning,"Multitask learning (MTL) aims to improve generalization performance by leveraging shared information across multiple related tasks. However, tasks often exhibit conflicting gradient directions, leading to destructive interference during training and hindering overall performance. This paper addresses the problem of gradient conflict in MTL by proposing RotoGrad, a novel gradient manipulation technique that homogenizes gradient directions within a shared parameter space. RotoGrad operates by projecting task-specific gradients onto a common subspace, aligning them based on a learned rotation matrix that minimizes the angular distance between projected gradients. This rotation matrix is dynamically updated during training, adapting to the evolving task relationships. Experiments on several benchmark MTL datasets, including NYUv2, Cityscapes, and CelebA, demonstrate that RotoGrad consistently improves performance across a range of architectures and MTL strategies, achieving state-of-the-art results in many cases. RotoGrad's ability to mitigate gradient conflict provides a significant advancement in enabling more effective knowledge sharing in multitask learning."
http://arxiv.org/abs/2111.01697v1,Low-Rank+Sparse Tensor Compression for Neural Networks,"Deep neural networks (DNNs) have achieved remarkable performance across various tasks, but their high computational and storage costs hinder deployment on resource-constrained devices. Existing tensor decomposition methods for DNN compression often focus solely on low-rank approximations, neglecting the potential benefits of sparsity, which can further enhance compression and potentially improve generalization. This paper introduces a novel low-rank plus sparse (LR+S) tensor compression framework specifically designed for neural network weight tensors. Our method decomposes each weight tensor into the sum of a low-rank tensor, capturing the essential structural information, and a sparse tensor, representing fine-grained details and outliers. We develop an efficient alternating optimization algorithm to jointly learn the low-rank and sparse components, incorporating regularization terms to control the rank and sparsity levels. Experimental results on several benchmark DNNs and datasets demonstrate that our LR+S tensor compression significantly outperforms state-of-the-art low-rank and sparse compression techniques, achieving higher compression ratios with minimal accuracy loss. This work provides a powerful new tool for compressing and accelerating deep neural networks, facilitating their deployment in resource-limited environments."
http://arxiv.org/abs/2206.15269v4,Deep Reinforcement Learning with Swin Transformers,"Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but often struggles with complex visual environments that require intricate spatial reasoning. Existing DRL agents primarily rely on convolutional neural networks (CNNs) for visual feature extraction, which can be limited in capturing long-range dependencies and global context. This paper addresses the challenge of improving DRL performance in visually complex environments by leveraging the Swin Transformer, a hierarchical vision transformer architecture. We introduce a novel DRL agent, termed Swin-DRL, which incorporates a Swin Transformer-based visual encoder to extract rich, multi-scale representations from the environment's visual input. Furthermore, we integrate a recurrent module to enhance temporal understanding and improve the agent's decision-making process. Experimental results on challenging benchmark environments, including Atari games and robotic manipulation tasks, demonstrate that Swin-DRL significantly outperforms state-of-the-art CNN-based DRL agents in terms of sample efficiency and asymptotic performance. These findings highlight the potential of vision transformers for enhancing DRL agents' capabilities in visually demanding tasks, paving the way for more robust and intelligent autonomous systems."
http://arxiv.org/abs/2306.02010v3,Memorization Capacity of Multi-Head Attention in Transformers,"Transformers have achieved state-of-the-art performance in various sequence modeling tasks, largely attributed to the powerful multi-head attention mechanism. However, the exact memorization capacity of multi-head attention and its relationship to model performance remain poorly understood. This paper investigates the memorization capabilities of multi-head attention layers in transformer networks, specifically focusing on the ability to store and retrieve specific input-output mappings. We introduce a novel probing technique involving the training of transformers on synthetic datasets designed to isolate and quantify the memorization capacity of individual attention heads. Our method involves controlling the complexity and redundancy of the training data and analyzing the learned attention weights and output predictions. We observe a direct correlation between the number of heads required to memorize a dataset and its inherent complexity, demonstrating that individual heads specialize in capturing specific patterns and relationships. Furthermore, we show that increasing the number of heads beyond the minimum required for memorization leads to improved generalization performance, suggesting a regularization effect. These findings provide valuable insights into the functional role of multi-head attention and offer guidance for optimizing transformer architectures for specific tasks."
http://arxiv.org/abs/2310.01651v3,Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in tasks requiring joint understanding of visual and textual information. However, their robustness to adversarial attacks remains a critical concern. This paper investigates the vulnerability of state-of-the-art VLMs to simple input permutations that are semantically negligible to humans but can drastically alter model predictions. We propose a novel attack strategy that leverages embarrassingly simple permutations of image regions and word order to generate adversarial examples. By systematically shuffling image patch arrangements and reordering words within a sentence, we create inputs that maintain high semantic similarity to the original data while effectively misleading the VLM. Our experiments on several popular VLM architectures reveal a significant drop in performance across various tasks, including image captioning and visual question answering, with minimal perceptual distortion. These findings highlight a critical weakness in the current generation of VLMs and underscore the need for more robust architectures that are less susceptible to such easily generated adversarial attacks."
http://arxiv.org/abs/2406.07145v2,"Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models","Large-scale Vision and Language Models (VLMs) have demonstrated remarkable capabilities in complex multimodal tasks, but also exhibit undesirable behaviors such as generating biased content, hallucinating object attributes, or failing in compositional reasoning. This paper addresses the critical problem of characterizing and mitigating these unwanted behaviors in VLMs, which can hinder their reliable and ethical deployment. We introduce a novel framework for systematically identifying failure modes by constructing targeted adversarial examples and developing quantitative metrics to assess the severity of each behavior. Furthermore, we propose a multi-faceted mitigation strategy combining data augmentation with debiasing techniques and a novel regularization term during fine-tuning that penalizes the generation of undesirable outputs based on learned behavior embeddings. Experiments on a diverse set of VLMs and tasks reveal that our framework effectively identifies and reduces the frequency of several unwanted behaviors, including attribute hallucination and demographic bias, while maintaining strong performance on standard benchmarks. These findings highlight the importance of proactive failure analysis and mitigation strategies for building robust and trustworthy VLMs."
http://arxiv.org/abs/2408.04759v1,Confident magnitude-based neural network pruning,"Neural network pruning is crucial for reducing computational costs and memory footprint, enabling deployment on resource-constrained devices. Magnitude-based pruning, which removes connections with small weights, is a popular and efficient technique. However, existing magnitude-based methods often lack a mechanism to confidently identify truly unimportant weights, leading to suboptimal pruning decisions and potential performance degradation, especially at high sparsity levels. We propose a novel confident magnitude-based pruning strategy that incorporates a data-driven confidence score for each weight based on its sensitivity to input perturbations. This score is then used to modulate the pruning threshold, prioritizing the removal of low-magnitude weights with high confidence scores, indicating their limited impact on the network's output. Experiments on CIFAR-10 and ImageNet datasets demonstrate that our method consistently outperforms existing magnitude-based pruning techniques, achieving significantly higher accuracy at comparable sparsity levels, particularly in deep convolutional networks. This confident pruning approach offers a robust and effective way to compress neural networks without sacrificing performance, facilitating efficient deployment in real-world applications."
http://arxiv.org/abs/2502.20587v1,Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Inference,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in complex reasoning tasks, but their high computational cost hinders widespread adoption, especially during inference. This paper addresses the challenge of reducing the inference cost of VLMs without significantly sacrificing accuracy. We introduce Cache-of-Thought (CoT), a novel master-apprentice framework that leverages a large, accurate but computationally expensive ""master"" VLM and a smaller, faster ""apprentice"" VLM. CoT dynamically caches intermediate reasoning steps (""thoughts"") generated by the master model for frequently encountered or similar input queries. When a new query arises, CoT first attempts to retrieve relevant cached thoughts. If a suitable match is found, the apprentice model uses these retrieved thoughts to generate the final answer, bypassing the need for full inference by the master model. Our experiments on several challenging VLM benchmarks demonstrate that CoT can achieve significant inference speedups (up to 4x) and cost reductions (up to 5x) with minimal impact on accuracy (less than 1% drop). The proposed framework offers a practical and efficient approach to deploying VLMs in resource-constrained environments, making sophisticated vision-language reasoning more accessible."
http://arxiv.org/abs/2504.03749v1,Input Resolution Downsizing as a Compression Technique for Vision Deep Learning Systems,"Deep learning models for computer vision tasks are computationally expensive, hindering their deployment on resource-constrained devices. This paper addresses the challenge of reducing the computational burden of vision deep learning systems without significantly sacrificing accuracy. We propose utilizing input resolution downsizing as a compression technique, systematically reducing the input image resolution prior to feeding it into the deep learning model. We analyze the trade-off between computational cost, measured by FLOPs, and accuracy across various network architectures (ResNet, MobileNet) and datasets (CIFAR-10, ImageNet). Our experiments demonstrate that significant reductions in FLOPs (up to 70% in some cases) can be achieved with only a minor drop in accuracy (less than 1% in many scenarios), especially when combined with fine-tuning the model on the lower resolution images. Furthermore, we investigate adaptive resolution selection strategies, dynamically adjusting the input resolution based on image complexity to optimize the accuracy-FLOPs trade-off. This work provides a practical and effective method for compressing vision deep learning models, enabling their deployment in resource-limited environments."
http://arxiv.org/abs/2504.09021v1,A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7,"Competitive racing in modern simulators like Gran Turismo 7 (GT7) presents a complex control problem characterized by high-dimensional visual inputs, intricate vehicle dynamics, and strategic opponent interactions. Achieving human-level performance in such environments remains a significant challenge for autonomous agents. This paper addresses the problem of developing a vision-based reinforcement learning (RL) agent capable of champion-level performance in competitive GT7 races. We propose a novel architecture combining a convolutional neural network (CNN) for feature extraction from raw screen input with a recurrent neural network (RNN) to capture temporal dependencies and improve decision-making. The agent is trained using a proximal policy optimization (PPO) algorithm, augmented with a curriculum learning strategy that progressively increases the difficulty of the racing scenarios. Our agent demonstrates superior performance compared to both built-in GT7 AI drivers and a baseline RL agent, consistently achieving pole positions and race victories across various tracks and vehicle classes. This work demonstrates the potential of vision-based RL for creating highly skilled autonomous agents capable of mastering complex, real-time racing environments."
http://arxiv.org/abs/2505.18570v3,VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis,"Stock time-series analysis is crucial for financial decision-making, yet traditional methods often require extensive training on historical data, limiting their adaptability to rapidly changing market conditions. This paper addresses the challenge of performing stock analysis without the need for training or fine-tuning on historical stock data. We introduce VISTA: Vision-Language Inference for Stock Time-Series Analysis. VISTA leverages pre-trained vision-language models (VLMs) by encoding stock charts as images and formulating analytical queries as text prompts. Specifically, we design prompts to probe the VLM's understanding of chart patterns and their relation to financial concepts, enabling zero-shot inference for tasks such as trend identification and volatility assessment. Experiments on diverse stock datasets demonstrate that VISTA achieves competitive performance compared to traditional machine learning methods, and even outperforms them in scenarios with limited or unseen historical data. These results highlight the potential of VLMs for democratizing access to sophisticated stock analysis tools and enabling more agile financial strategies."
http://arxiv.org/abs/1506.04364v2,Localized Multiple Kernel Learning---A Convex Approach,"Multiple Kernel Learning (MKL) has emerged as a powerful technique for combining the strengths of different kernels, offering improved flexibility and performance in various computer vision tasks. However, traditional MKL often learns a global kernel combination, potentially overlooking the fact that different kernels might be more suitable for different regions of the input space. This paper addresses the problem of learning localized kernel combinations within a convex optimization framework. We propose Localized Multiple Kernel Learning (LMKL), a novel approach that learns spatially varying kernel weights by introducing a location-dependent regularization term. This term encourages smoothness in the kernel weights across the input space while maintaining convexity, ensuring efficient and global optimization. Experimental results on benchmark datasets for image classification and semantic segmentation demonstrate that LMKL achieves significant performance gains compared to global MKL and other state-of-the-art localization techniques. This localized kernel adaptation provides a more nuanced and effective representation, leading to improved accuracy and robustness in visual recognition tasks."
http://arxiv.org/abs/1606.08415v5,Gaussian Error Linear Units (GELUs),"Activation functions are a crucial component of deep neural networks, introducing non-linearities that enable the learning of complex patterns. While ReLU and its variants have become the dominant choice, alternatives with smoother, non-monotonic behavior can offer benefits in certain scenarios. This paper addresses the problem of designing an activation function that combines the advantages of ReLU's simplicity with the benefits of a smoother, probabilistic formulation. We introduce the Gaussian Error Linear Unit (GELU), which weights inputs by their cumulative distribution function of the standard Gaussian distribution. This activation function can be interpreted as probabilistically gating the input, depending on its magnitude relative to other inputs. Experimental results demonstrate that GELU consistently outperforms ReLU and ELU on a range of challenging tasks, including image classification (ImageNet) and natural language processing (BERT). The superior performance of GELU highlights the importance of stochastic regularization and non-monotonicity in activation functions for deep learning models."
http://arxiv.org/abs/2011.06735v2,Investigating Learning in Deep Neural Networks using Layer-Wise Weight Change,"Deep Neural Networks (DNNs) have achieved remarkable success in various computer vision tasks, yet the intricacies of their learning dynamics remain a subject of ongoing research. Understanding how individual layers adapt their weights during training is crucial for improving model performance and interpretability. This paper addresses the problem of characterizing layer-wise learning behavior in DNNs by analyzing the magnitude and direction of weight changes across different layers throughout the training process. We propose a novel method that quantifies weight change using both the L2 norm of the weight update and the cosine similarity between consecutive weight updates within each layer. This allows us to track the speed and consistency of learning in each layer, identifying potential bottlenecks or instability. Our experiments on image classification tasks using standard architectures (e.g., ResNet, VGG) reveal that shallower layers tend to exhibit larger initial weight changes, while deeper layers demonstrate more refined and consistent adjustments later in training. Furthermore, we observe correlations between layer-wise weight change patterns and overall model accuracy. These findings offer valuable insights into the learning process within DNNs, potentially leading to improved training strategies and network architectures."
http://arxiv.org/abs/2104.07792v1,Geometry encoding for numerical simulations,"Numerical simulations are ubiquitous in science and engineering, often relying on accurate geometric representations of physical objects. However, traditional mesh-based representations can be cumbersome to generate, manipulate, and adapt, especially for complex geometries undergoing significant deformation. This paper addresses the challenge of efficiently encoding and utilizing geometric information for numerical simulations by proposing a novel implicit neural representation framework. Our approach leverages signed distance functions (SDFs) encoded by deep neural networks to represent the simulation domain. Crucially, we introduce a tailored network architecture and training strategy that promotes accurate SDF approximation and facilitates efficient computation of geometric properties, such as normals and curvatures, directly from the network. We demonstrate the effectiveness of our method on a variety of benchmark simulation tasks, including fluid dynamics and heat transfer, achieving comparable accuracy to mesh-based methods with significantly reduced preprocessing and memory overhead. This work opens new avenues for integrating deep learning-based geometric representations into numerical simulation pipelines, enabling faster prototyping and analysis of complex physical systems."
http://arxiv.org/abs/2211.02716v1,NLP Inspired Training Mechanics For Modeling Transient Dynamics,"Modeling transient dynamics, such as object interactions and human activities in video, remains a significant challenge in computer vision. Existing approaches often struggle to capture the temporal dependencies and subtle state changes inherent in these dynamic processes. We address this limitation by introducing a novel training paradigm inspired by Natural Language Processing (NLP), specifically drawing parallels between video sequences and textual narratives. Our method, Transient Dynamics Modeling via Narrative-Inspired Training (TraDeNT), leverages concepts like masked sequence prediction and next-sentence prediction, adapted to the visual domain. TraDeNT trains a spatiotemporal encoder to reconstruct masked video frames and predict temporal adjacency between video segments, fostering a more robust understanding of state transitions. Experiments on benchmark datasets for action recognition and video understanding demonstrate that TraDeNT significantly improves performance, achieving state-of-the-art results in capturing nuanced temporal relationships and predicting future states. This NLP-inspired framework provides a powerful new direction for learning and understanding transient dynamics in video."
http://arxiv.org/abs/2310.13935v1,Toward Generative Data Augmentation for Traffic Classification,"Traffic classification is crucial for intelligent transportation systems, enabling applications like congestion management and autonomous driving. However, the performance of traffic classifiers is often limited by the availability of labeled data, particularly for rare or unseen traffic conditions. This paper addresses the challenge of data scarcity in traffic classification by proposing a novel generative data augmentation framework. Our approach leverages a conditional Generative Adversarial Network (GAN) to synthesize realistic traffic images conditioned on class labels. A key innovation is the incorporation of a perceptual loss function, based on pre-trained convolutional neural networks, to enhance the visual fidelity and semantic consistency of the generated images. Experimental results on benchmark traffic datasets demonstrate that training traffic classifiers with our augmented data significantly improves classification accuracy, particularly for minority classes, achieving an average increase of 5-7% compared to baseline augmentation techniques. This work offers a promising avenue for improving the robustness and generalizability of traffic classification models in real-world scenarios."
http://arxiv.org/abs/2402.04359v1,Adaptive Inference: Theoretical Limits and Unexplored Opportunities,"Adaptive inference, where computational resources are dynamically allocated based on input complexity, holds the promise of significantly improving the efficiency of deep neural networks. However, the theoretical limits governing the trade-off between accuracy and computational cost in adaptive inference remain largely unexplored. This paper addresses the challenge of characterizing the fundamental performance bounds of adaptive inference strategies. We develop a novel information-theoretic framework based on rate-distortion theory to model the adaptive inference process as a sequential resource allocation problem. This framework allows us to derive lower bounds on the expected loss as a function of the expected computational cost. Empirical evaluations on image classification and semantic segmentation tasks demonstrate that existing adaptive inference methods operate far from these theoretical limits, highlighting a significant gap between current practice and optimal performance. Our findings suggest that substantial improvements in adaptive inference are achievable through algorithms that more effectively navigate the accuracy-computation trade-off, paving the way for future research in this crucial area."
http://arxiv.org/abs/2111.05326v1,The Internet of Federated Things (IoFT): A Vision for the Future and In-depth Survey of Data-driven Approaches for Federated Learning,"The proliferation of Internet of Things (IoT) devices has generated massive datasets, offering unprecedented opportunities for intelligent applications. However, data privacy concerns and communication constraints hinder the centralized processing of such distributed data. This paper addresses the challenge of leveraging the decentralized data from diverse IoT devices while preserving privacy and minimizing communication overhead. We introduce the concept of the Internet of Federated Things (IoFT), a novel paradigm that combines the ubiquity of IoT with the collaborative learning capabilities of Federated Learning (FL). We present a comprehensive survey of data-driven approaches for FL in IoFT, categorizing and analyzing existing algorithms based on their communication efficiency, privacy guarantees, robustness to heterogeneous data, and adaptability to resource-constrained IoT devices. Furthermore, we identify key research gaps and future directions, focusing on personalized FL, asynchronous communication strategies, and the integration of edge computing. Our analysis reveals that while significant progress has been made, substantial improvements are needed to effectively address the unique challenges posed by the IoFT environment, particularly in handling extreme data heterogeneity and intermittent connectivity. This survey provides a valuable resource for researchers and practitioners seeking to develop and deploy FL solutions in the emerging IoFT landscape."
http://arxiv.org/abs/2209.00945v2,IMG2IMU: Translating Knowledge from Large-Scale Images to IMU Sensing Applications,"Inertial Measurement Units (IMUs) provide robust and efficient self-motion estimates, but often suffer from drift and require careful calibration. This paper addresses the challenge of improving IMU-based motion estimation by leveraging the vast knowledge encoded within large-scale image datasets. We introduce IMG2IMU, a novel framework that translates visual information into synthetic IMU signals, effectively bridging the gap between vision and inertial sensing. Our approach utilizes a conditional Generative Adversarial Network (cGAN) trained on paired image sequences and corresponding simulated IMU data. The cGAN learns to predict realistic IMU signals conditioned on input image features extracted from pre-trained vision transformers. We demonstrate that incorporating these synthetically generated IMU signals as pseudo-measurements within a visual-inertial odometry (VIO) pipeline significantly improves its accuracy and robustness, particularly in challenging environments with limited visual features. Our experiments on benchmark datasets show a reduction in average trajectory error by up to 30% compared to traditional VIO methods, highlighting the potential of large-scale visual knowledge to enhance inertial navigation systems. This work opens new avenues for synergistic fusion of visual and inertial data, leading to more reliable and accurate robot perception."
http://arxiv.org/abs/2209.10901v2,Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning,"Vision-based Deep Reinforcement Learning (DRL) offers the potential to train agents directly from raw pixel inputs, but often suffers from high sample complexity. This challenge is exacerbated when using complex architectures like Vision Transformers (ViTs), which require substantial data for effective training. To address the sample inefficiency of ViTs in vision-based DRL, we propose a novel pretraining paradigm that leverages self-supervised learning (SSL) to initialize the ViT encoder before integrating it into a DRL framework. Specifically, we pretrain the ViT using masked image modeling (MIM) on a large, unlabeled dataset of relevant visual scenes. This pretrained ViT is then used as the visual encoder within a DRL agent, where its weights are further fine-tuned during the reinforcement learning process. Our experiments across a range of challenging vision-based control tasks demonstrate that pretraining with MIM significantly improves both the sample efficiency and asymptotic performance of ViT-based DRL agents, achieving up to a 50% reduction in the number of environment interactions required to reach expert-level performance. This work highlights the effectiveness of SSL pretraining for ViTs in DRL, paving the way for more efficient learning of complex visuomotor policies."
http://arxiv.org/abs/2405.17247v1,An Introduction to Vision-Language Modeling,"Vision-Language Modeling (VLM) has emerged as a powerful paradigm for learning joint representations of visual and textual data, enabling machines to understand and reason about the world in a more human-like manner. Despite the rapid progress, the field lacks a comprehensive overview that synthesizes the diverse approaches and provides a structured understanding of the core concepts. This paper addresses this gap by presenting a systematic introduction to VLM, covering foundational architectures, pre-training objectives, and downstream applications. We categorize VLM approaches based on their architectural choices, contrasting single-stream and dual-stream models, and examine the role of different pre-training tasks, including masked language modeling, image-text matching, and visual question answering. Furthermore, we survey a wide range of downstream tasks, such as image captioning, visual question answering, visual reasoning, and cross-modal retrieval, highlighting the strengths and weaknesses of different VLM models in each setting. This introduction serves as a valuable resource for researchers and practitioners seeking to enter the field, fostering a deeper understanding of the current state-of-the-art and paving the way for future advancements in VLM."
http://arxiv.org/abs/2504.02349v1,Large (Vision) Language Models are Unsupervised In-Context Learners,"Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. Recent works suggest that these models can be adapted for vision tasks by representing images as sequences of visual tokens, enabling them to leverage their pre-trained language understanding. However, effectively adapting LLMs for vision often requires extensive task-specific fine-tuning, limiting their generalization ability and requiring substantial labeled data. In this work, we investigate the unsupervised in-context learning abilities of Large Vision Language Models (LVLMs). We propose a novel prompting strategy that leverages unlabeled image-text pairs to guide LVLMs in performing various vision tasks without explicit fine-tuning. By carefully crafting prompts that define the desired task and provide relevant visual context, we enable the LVLM to infer the underlying relationship between images and text, and consequently, perform the task in an unsupervised manner. Our experiments on several benchmark datasets demonstrate that LVLMs, with appropriate prompting, can achieve competitive performance compared to supervised fine-tuning methods, highlighting their inherent in-context learning capabilities. This finding suggests that LVLMs possess a strong ability to learn and generalize from limited, unlabeled data, paving the way for more data-efficient and adaptable vision systems."
http://arxiv.org/abs/2506.14808v1,PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in complex multimodal tasks, yet a comprehensive understanding of their internal representations and inherent biases remains elusive. This paper addresses the challenge of quantifying and characterizing the symmetries present within VLMs, specifically focusing on how these models respond to semantically equivalent transformations of input data. We introduce PARC (Perturbation Analysis for Representation Consistency), a novel quantitative framework that systematically perturbs both visual and textual inputs using a suite of semantic-preserving transformations. PARC then measures the consistency of the model's internal representations and downstream predictions under these perturbations, allowing us to identify symmetries related to factors like object viewpoint, text paraphrasing, and background variations. Our experiments on several state-of-the-art VLMs reveal significant variations in symmetry across different model architectures and training datasets, highlighting sensitivities to specific perturbation types. These findings provide crucial insights into the robustness and generalization capabilities of VLMs, paving the way for developing more reliable and unbiased multimodal systems."
http://arxiv.org/abs/2506.19023v1,Automating Traffic Monitoring with SHM Sensor Networks via Vision-Supervised Deep Learning,"Traffic monitoring is crucial for urban planning and infrastructure management, traditionally relying on expensive and sparsely deployed sensors or manual observation. This work addresses the challenge of efficiently and accurately estimating traffic flow parameters using structural health monitoring (SHM) sensor networks by leveraging the complementary information provided by visual data. We propose a vision-supervised deep learning framework that fuses vibration data from SHM sensors with features extracted from surveillance camera footage. A convolutional neural network (CNN) processes the visual data, while a recurrent neural network (RNN) analyzes the temporal dynamics of the SHM sensor signals. The CNN is trained with supervision from object detection outputs derived from the video stream, enabling the RNN to learn traffic-correlated patterns in the vibration data. Experiments on a real-world bridge dataset demonstrate that our approach achieves a significant improvement in traffic volume and speed estimation compared to using SHM data alone, reducing the mean absolute error by up to 30%. This vision-supervised learning paradigm offers a cost-effective and scalable solution for enhancing traffic monitoring capabilities using existing SHM infrastructure."
http://arxiv.org/abs/2303.10361v1,DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models,"Large vision models achieve state-of-the-art performance but demand significant computational resources for training and inference, hindering their deployment on resource-constrained edge devices. Directly transferring pre-trained models to edge devices often results in suboptimal performance due to the heterogeneity in data distribution and hardware capabilities between the cloud and edge. This paper introduces Device-Cloud Collaborative Controlled Learning (DC-CCL), a novel framework that leverages both cloud-based global knowledge and device-specific local data to train large vision models efficiently and effectively. DC-CCL employs a knowledge distillation approach where the cloud-trained model acts as a teacher, guiding the edge-based student model. Crucially, we introduce a controlled learning mechanism that dynamically adjusts the relative importance of cloud knowledge and local data based on the device's computational resources and data characteristics, mitigating negative transfer and promoting personalized adaptation. Experimental results on diverse vision tasks, including image classification and object detection, demonstrate that DC-CCL significantly improves the accuracy and efficiency of edge-deployed models compared to traditional transfer learning and fine-tuning approaches. DC-CCL offers a practical and scalable solution for deploying powerful vision models on resource-constrained devices while preserving privacy and adapting to diverse edge environments."
http://arxiv.org/abs/2306.02824v1,COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search,"Mixture of Experts (MoE) models offer a powerful paradigm for scaling neural network capacity by selectively activating different experts for different inputs. However, existing MoE approaches often struggle to effectively balance expert utilization and can lead to instability during training, particularly when constrained by cardinality limitations on the number of active experts. We address the challenge of learning cardinality constrained MoEs by introducing COMET: Cardinality cOnstrained Mixture of Experts with Trees. COMET utilizes a learned tree-structured gating network to route inputs to experts and employs a novel local search algorithm to refine routing decisions based on both expert predictions and load balancing objectives. This local search dynamically adjusts the gating network to enforce strict cardinality constraints while optimizing for overall performance. Experiments on benchmark datasets demonstrate that COMET achieves significantly improved performance compared to existing cardinality constrained MoE methods, exhibiting superior accuracy and more balanced expert utilization. COMET provides a practical and effective approach for scaling MoE models while ensuring efficient resource allocation."
http://arxiv.org/abs/2410.08791v1,Superpipeline: A Universal Approach for Reducing GPU Memory Usage in Large Models,"Large-scale deep learning models have achieved state-of-the-art results in various domains, but their substantial memory footprint often limits deployment on resource-constrained devices and hinders experimentation with larger architectures. Training these models on GPUs is particularly challenging due to the limited GPU memory capacity. To address this problem, we introduce Superpipeline, a novel universal framework for reducing GPU memory usage during both training and inference of large models. Superpipeline dynamically partitions the computation graph into fine-grained stages and intelligently schedules their execution, overlapping computation with communication and memory offloading to host memory. This approach minimizes peak memory consumption by actively managing tensor lifetimes and recomputing intermediate activations when necessary. Experiments on various large models, including Transformers and ResNets, demonstrate that Superpipeline can reduce GPU memory usage by up to 70% with minimal impact on training time, enabling the training and deployment of significantly larger models than previously possible within given hardware constraints. Superpipeline offers a practical and effective solution for democratizing access to large-scale deep learning by reducing hardware requirements."
http://arxiv.org/abs/1712.02427v1,High performance ultra-low-precision convolutions on mobile devices,"Deep convolutional neural networks (CNNs) have achieved remarkable success in various computer vision tasks, but their high computational cost and memory footprint pose significant challenges for deployment on resource-constrained mobile devices. Quantization, especially extreme quantization to ultra-low precision (e.g., 2-bit or 3-bit), offers a promising approach to reduce these costs, but often suffers from significant accuracy degradation or requires specialized hardware. This paper addresses the problem of achieving high performance and minimal accuracy loss when deploying ultra-low-precision quantized CNNs on mobile platforms. We propose a novel quantization-aware training (QAT) scheme that incorporates a mixed-precision strategy, employing higher precision for sensitive layers and ultra-low precision for the majority of the network. Furthermore, we introduce a tailored optimized convolution kernel specifically designed for ARM NEON architectures, exploiting SIMD instructions to accelerate ultra-low-precision matrix multiplication. Experiments on image classification tasks using MobileNetV1 demonstrate that our approach achieves comparable accuracy to the full-precision baseline while providing a 3x speedup on a commercial mobile phone, significantly outperforming existing ultra-low-precision quantization methods. This work provides a practical and efficient solution for deploying deep learning models on mobile devices, enabling real-time performance for a wider range of applications."
http://arxiv.org/abs/1712.09936v2,Gradient Regularization Improves Accuracy of Discriminative Models,"Discriminative models, particularly deep neural networks, have achieved remarkable success in various computer vision tasks. However, these models are often prone to overfitting and can exhibit poor generalization performance, especially when trained on limited or noisy datasets. This paper addresses the problem of improving the accuracy and robustness of discriminative models by introducing a novel gradient regularization technique. Our approach, termed Gradient Norm Scaling (GNS), penalizes large norms of the gradients of the model's output with respect to its inputs, effectively encouraging smoother decision boundaries. Specifically, GNS scales the gradient norm by a factor that decreases as the norm increases, thereby adaptively controlling the regularization strength. Experimental results on benchmark image classification datasets, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that GNS consistently improves the accuracy and robustness of various network architectures. These improvements highlight the potential of gradient regularization to enhance the generalization capabilities of discriminative models, leading to more reliable performance in real-world applications."
http://arxiv.org/abs/2010.03095v1,Gradient-based Causal Structure Learning with Normalizing Flow,"Causal structure learning aims to discover the underlying cause-effect relationships from observational data, a fundamental problem across various scientific disciplines. However, accurately inferring causal graphs from high-dimensional data remains challenging, particularly when relying solely on observational data and facing potential confounding factors. We address this challenge by introducing a novel gradient-based causal structure learning framework that leverages the expressiveness and invertibility of normalizing flows. Our method learns a data transformation using a normalizing flow to minimize the conditional independence violations implied by a given causal graph, parameterized by a differentiable adjacency matrix. This allows us to optimize the causal structure directly using gradient descent, incorporating structural constraints through a continuous relaxation of the acyclicity constraint. Experiments on synthetic and benchmark datasets demonstrate that our approach achieves competitive or superior performance compared to existing methods, particularly in high-dimensional settings and with non-linear relationships. This work offers a scalable and efficient approach for causal discovery, enabling the identification of causal relationships in complex systems with improved accuracy."
http://arxiv.org/abs/2106.09022v1,A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection,"Out-of-distribution (OOD) detection is crucial for deploying robust machine learning systems, especially in safety-critical applications. Mahalanobis distance-based OOD detection, leveraging class-conditional Gaussian distributions, has shown promise, but often struggles with near-OOD samples that lie close to the in-distribution data. This paper addresses the sensitivity of Mahalanobis distance to imprecise covariance matrix estimation, particularly when dealing with high-dimensional features or limited in-distribution data. We propose a simple yet effective regularization technique: adding a scaled identity matrix to the class-conditional covariance matrices before computing the Mahalanobis distance. This regularization effectively mitigates the impact of spurious correlations and noisy feature dimensions. Our experiments on benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet, demonstrate that our regularized Mahalanobis distance significantly improves near-OOD detection performance, achieving state-of-the-art results with minimal computational overhead. This simple fix enhances the reliability of Mahalanobis distance for practical OOD detection scenarios."
http://arxiv.org/abs/2110.08239v2,Learn Proportional Derivative Controllable Latent Space from Pixels,"Controlling image generation through latent space manipulation offers a powerful avenue for content creation and editing. However, existing methods often struggle with precise control over specific image attributes, leading to iterative adjustments and unpredictable outcomes. This paper addresses the challenge of learning a latent space where edits are directly controllable through proportional-derivative (PD) control, mimicking the precise and stable behavior of physical control systems. We propose a novel architecture that integrates a PD controller within the latent space of a generative model. Specifically, we train an encoder to map images to a latent representation, and a decoder to reconstruct images from that latent representation. During training, we apply perturbations to the latent code and utilize a learned PD controller to guide the latent code toward a desired target attribute value. This allows the model to learn a latent space where changes in latent variables directly correspond to predictable and stable changes in image attributes. Experiments on various datasets demonstrate that our method achieves superior control accuracy and stability compared to existing latent space manipulation techniques, enabling precise and intuitive image editing. This work provides a significant step towards more controllable and predictable generative models."
http://arxiv.org/abs/1905.09635v1,Tucker Decomposition Network: Expressive Power and Comparison,"Tensor decomposition methods, particularly Tucker decomposition, have shown promise in compressing and accelerating deep neural networks by exploiting inherent redundancies in weight tensors. However, the expressive power of networks utilizing Tucker decomposition compared to standard convolutional neural networks (CNNs) remains relatively unexplored, hindering a comprehensive understanding of their potential and limitations. This paper investigates the expressive capabilities of Tucker Decomposition Networks (TDNs) and provides an empirical comparison against conventionally structured CNNs. We introduce a novel TDN architecture that incorporates learnable core tensors, enabling adaptive rank selection and improved representation learning. Through extensive experiments on benchmark image classification datasets, including CIFAR-10, CIFAR-100, and ImageNet, we demonstrate that TDNs can achieve competitive or even superior performance compared to standard CNNs with a significantly reduced parameter count, particularly when optimized for specific rank configurations. Our findings highlight the potential of Tucker decomposition as a powerful tool for designing efficient and expressive deep learning models, contributing to the development of resource-constrained applications."
http://arxiv.org/abs/2012.03301v1,Deep Transfer Learning for Industrial Automation: A Review and Discussion of New Techniques for Data-Driven Machine Learning,"Industrial automation increasingly relies on data-driven machine learning, yet the scarcity of labeled data and the high cost of acquiring it often hinder the effective deployment of deep learning models. This paper addresses the challenge of adapting deep learning models trained on source domains with abundant data to target industrial automation tasks with limited labeled data. We present a comprehensive review of deep transfer learning techniques applicable to industrial automation, focusing on domain adaptation, few-shot learning, and self-supervised learning approaches. We categorize these techniques based on their underlying principles, including instance-based, feature-based, and parameter-based transfer learning, and discuss their strengths and weaknesses in the context of industrial applications such as defect detection, robotic manipulation, and predictive maintenance. Furthermore, we introduce novel combinations of existing techniques, such as self-supervised pre-training followed by adversarial domain adaptation, and evaluate their performance on benchmark industrial datasets. Our experiments demonstrate significant improvements in target domain accuracy compared to training from scratch, with some transfer learning strategies achieving up to 20% higher accuracy with only a fraction of the labeled target data. These findings highlight the potential of deep transfer learning to unlock the full potential of data-driven machine learning in industrial automation, enabling the development of robust and efficient solutions even with limited labeled data."
http://arxiv.org/abs/2201.08924v2,Nearest Class-Center Simplification through Intermediate Layers,"Nearest Class-Center (NCC) classifiers offer compelling advantages in terms of simplicity, interpretability, and computational efficiency, making them attractive for resource-constrained applications and explainable AI. However, traditional NCC classifiers, often trained using features extracted from the final layer of a deep neural network, can suffer from suboptimal performance due to the complex and potentially over-specialized nature of these high-level representations. This paper addresses the problem of improving NCC classifier accuracy by strategically selecting intermediate layers within a pre-trained deep neural network for feature extraction. We propose a novel approach that leverages a layer-wise NCC training strategy, progressively evaluating and refining class centers based on features extracted from different intermediate layers. This allows for the identification of layer-specific feature spaces that are more conducive to forming well-separated class clusters, ultimately leading to improved classification accuracy. Experimental results on several benchmark datasets demonstrate that our method significantly outperforms traditional NCC classifiers trained on final-layer features, achieving comparable or even superior performance to more complex classifiers while maintaining the inherent advantages of NCC. This approach offers a simple yet effective way to enhance the performance of NCC classifiers by exploiting the richer, multi-scale feature representations available within deep neural networks."
http://arxiv.org/abs/2205.01138v2,Transformers in Time-series Analysis: A Tutorial,"Time-series analysis is a critical task in various domains, ranging from finance and climate science to healthcare and industrial monitoring. While recurrent neural networks (RNNs) have been the dominant approach for sequential data modeling, they suffer from limitations in capturing long-range dependencies and parallelization. This tutorial addresses the growing interest in applying Transformer architectures, originally developed for natural language processing, to time-series analysis. We provide a comprehensive overview of Transformer-based models adapted for time-series data, including architectural modifications such as the Informer, Autoformer, and FEDformer, which address challenges like quadratic complexity and inherent time-series characteristics (e.g., seasonality). We categorize and explain different adaptation strategies, focusing on techniques for incorporating temporal information, handling irregular sampling, and improving computational efficiency. Through detailed explanations, illustrative examples, and comparisons with traditional methods, we demonstrate the effectiveness of Transformers in various time-series tasks like forecasting, anomaly detection, and classification. This tutorial serves as a valuable resource for researchers and practitioners seeking to understand and apply Transformer-based models to time-series data, fostering further innovation in the field."
http://arxiv.org/abs/2205.15860v2,A Reduction to Binary Approach for Debiasing Multiclass Datasets,"Despite advancements in machine learning, models often inherit and amplify biases present in training data, particularly in multiclass classification. This paper addresses the challenge of mitigating these biases in multiclass datasets without requiring sensitive attribute information or negatively impacting overall accuracy. We propose a novel ""Reduction to Binary"" (RtB) approach that decomposes the multiclass problem into multiple binary classification tasks, strategically designed to isolate and neutralize dominant biases. Each binary classifier is trained on a subset of the original data, carefully selected to minimize the influence of specific biases, and subsequently ensembled to reconstruct the multiclass prediction. Experiments on benchmark datasets demonstrate that RtB effectively reduces bias, as measured by fairness metrics, while maintaining or even improving upon the original multiclass classification accuracy. These results highlight the potential of RtB as a practical and effective technique for developing fairer and more robust machine learning models in real-world applications."
http://arxiv.org/abs/2209.03302v2,Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate Measures?,"Deep learning models are increasingly deployed in safety-critical applications, necessitating accurate quantification of uncertainty. This paper investigates the suitability of conditional entropy and mutual information, commonly used to represent aleatoric and epistemic uncertainty respectively, for reliable uncertainty quantification in machine learning models. We propose a novel analytical framework that decomposes these information-theoretic measures into interpretable components, revealing their sensitivity to model calibration, data density, and latent feature representations. Through rigorous experimentation on benchmark datasets for image classification and semantic segmentation, we demonstrate that high conditional entropy does not always correspond to high aleatoric uncertainty, particularly in miscalibrated models or regions with sparse data. Similarly, we find that high mutual information can be driven by spurious correlations in the latent space rather than genuine epistemic uncertainty. Our findings suggest that while conditional entropy and mutual information can provide useful insights, they should be interpreted cautiously and complemented with alternative uncertainty quantification techniques to ensure robustness and reliability in real-world applications. This work highlights the limitations of relying solely on information-theoretic measures for uncertainty quantification and underscores the need for more nuanced approaches."
http://arxiv.org/abs/2211.16316v1,A3T: Accuracy Aware Adversarial Training,"Adversarial training (AT) has emerged as a prominent defense against adversarial attacks, yet it often suffers from a trade-off between standard and robust accuracy. This paper addresses the problem of robust overfitting in adversarial training, where models become overly specialized to the specific adversarial perturbations used during training, hindering generalization to unseen attacks and sacrificing clean accuracy. We propose Accuracy Aware Adversarial Training (A3T), a novel framework that dynamically adjusts the adversarial perturbation strength during training based on the model's current accuracy on clean and adversarially perturbed examples. A3T leverages a bi-level optimization strategy, where the perturbation strength is adapted to maintain a desired balance between standard and adversarial accuracy, preventing the model from overfitting to either distribution. Experiments on CIFAR-10 and CIFAR-100 demonstrate that A3T consistently outperforms state-of-the-art adversarial training methods, achieving significantly improved robust accuracy without compromising standard accuracy. This work provides a principled approach to mitigating robust overfitting, leading to more generalizable and reliable defenses against adversarial attacks."
http://arxiv.org/abs/2302.04237v2,Black Box Adversarial Prompting for Foundation Models,"Foundation models, pre-trained on massive datasets, exhibit remarkable capabilities across diverse tasks. However, their susceptibility to adversarial attacks, particularly through carefully crafted input prompts, raises significant concerns about their robustness and reliability. This paper addresses the problem of generating effective adversarial prompts for foundation models in a black-box setting, where access to model gradients or internal parameters is unavailable. We introduce a novel black-box adversarial prompting framework that leverages a combination of evolutionary strategies and a learned prompt optimizer. This framework iteratively perturbs and refines prompt candidates based on observed model outputs, guiding the search towards prompts that induce misclassification or desired erroneous behaviors. Experiments across multiple foundation models and diverse tasks, including image classification and text generation, demonstrate the effectiveness of our approach in generating highly transferable adversarial prompts. Our findings highlight the vulnerabilities of foundation models to black-box adversarial attacks and emphasize the need for developing robust defense mechanisms to mitigate these risks in real-world applications."
http://arxiv.org/abs/2309.08374v3,Understanding the limitations of self-supervised learning for tabular anomaly detection,"Self-supervised learning (SSL) has demonstrated remarkable success in computer vision and natural language processing by leveraging unlabeled data to learn robust representations. However, its applicability to tabular data, particularly for anomaly detection, remains relatively unexplored, and its inherent limitations are not well-understood. This paper investigates the efficacy of SSL techniques for tabular anomaly detection and identifies scenarios where they underperform compared to traditional unsupervised methods. We propose a novel evaluation framework that systematically analyzes the impact of feature correlations, anomaly prevalence, and data complexity on the performance of various SSL-based anomaly detection models, including contrastive learning and masked feature prediction. Our results demonstrate that SSL methods often struggle with high-dimensional, weakly correlated tabular datasets, and are sensitive to imbalanced anomaly ratios, leading to suboptimal performance compared to established algorithms like isolation forests and one-class SVMs. This work provides crucial insights into the practical limitations of applying SSL to tabular anomaly detection, guiding future research towards developing more robust and adaptable techniques."
http://arxiv.org/abs/2401.06890v1,An Axiomatic Approach to Model-Agnostic Concept Explanations,"Concept explanations aim to provide human-understandable justifications for a model's prediction by highlighting the influence of specific concepts. However, existing concept explanation methods often lack theoretical grounding and exhibit unpredictable behavior, hindering their reliability and trustworthiness. This work addresses the problem of designing concept explanation methods that are both model-agnostic and satisfy desirable axiomatic properties. We propose a novel framework for concept explanation based on Shapley values, leveraging a carefully constructed coalition structure that reflects the semantic relationships between concepts. Specifically, we define a concept's contribution based on its marginal impact on the model's prediction when added to different concept sets, ensuring properties like completeness, consistency, and symmetry. Experiments on image classification tasks demonstrate that our axiomatic Shapley concept explanations are more faithful to the model's decision-making process and more robust to perturbations compared to existing techniques. Our work provides a principled and reliable foundation for understanding and trusting complex machine learning models through the lens of human-understandable concepts."
http://arxiv.org/abs/2405.02770v2,PhilHumans: Benchmarking Machine Learning for Personal Health,"Personalized medicine promises to revolutionize healthcare through data-driven insights, yet the application of machine learning to individual health trajectories remains challenging due to data scarcity and heterogeneity. This paper addresses the critical need for robust benchmarks to evaluate machine learning models designed to predict and understand personal health. We introduce PhilHumans, a novel benchmark dataset comprising longitudinal physiological data, medical history, and lifestyle information from a cohort of healthy individuals monitored over an extended period. PhilHumans facilitates the development and assessment of algorithms capable of predicting future health states based on past observations, using methods such as recurrent neural networks and transformer architectures tailored for time-series analysis. Our evaluation demonstrates that even state-of-the-art models struggle to achieve high accuracy in long-term health prediction, highlighting significant opportunities for future research in model personalization and feature engineering. PhilHumans provides a valuable resource for advancing machine learning research towards personalized and proactive healthcare solutions."
http://arxiv.org/abs/2406.03662v3,The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders to InceptionV1 Early Vision,"Convolutional Neural Networks (CNNs) have achieved remarkable success in image recognition, yet their internal representations and feature extraction mechanisms remain a subject of ongoing investigation. This paper addresses the gap in understanding the early vision capabilities of InceptionV1, specifically investigating its potential limitations in detecting curved structures, a crucial element in natural image understanding. We propose a novel approach using sparse autoencoders (SAEs) to analyze and augment the early layers of InceptionV1. First, we train SAEs on a dataset of synthetic curved shapes and natural images, extracting a dictionary of curve-sensitive features. Then, we apply these learned features as additional convolutional filters to the first convolutional layer of a pre-trained InceptionV1 model, creating an augmented architecture. Experimental results on standard image classification benchmarks, as well as a specialized curve detection dataset, demonstrate improved performance in both general image recognition and specific curve detection tasks compared to the original InceptionV1. This indicates that augmenting InceptionV1 with learned curve detectors enhances its ability to capture essential image primitives, leading to more robust and accurate visual representations."
http://arxiv.org/abs/2406.15025v1,SiT: Symmetry-Invariant Transformers for Generalisation in Reinforcement Learning,"Reinforcement learning (RL) agents often struggle to generalize to unseen environments, particularly when those environments exhibit symmetries not present in the training data. This sensitivity to symmetry transformations hinders the deployment of RL agents in real-world scenarios where perfect knowledge of the environment is rarely available. To address this, we introduce Symmetry-Invariant Transformers (SiT), a novel architecture that incorporates explicit symmetry awareness into the transformer network. SiT achieves this by learning symmetry transformation operators and applying them to the input embeddings, encouraging the attention mechanism to focus on symmetry-invariant features. Furthermore, we incorporate a symmetry-consistency loss that penalizes inconsistencies in the agents policy under symmetry transformations. Experiments on a suite of challenging continuous control tasks with varying degrees of symmetry demonstrate that SiT significantly outperforms existing state-of-the-art RL algorithms in out-of-distribution generalization, especially when the test environments contain novel symmetry transformations. These results highlight the importance of incorporating symmetry awareness into RL agents for robust and generalizable decision-making."
http://arxiv.org/abs/2406.20046v1,Evaluation of autonomous systems under data distribution shifts,"Autonomous systems are increasingly deployed in real-world environments where they encounter data distributions that differ significantly from their training data, leading to performance degradation. This paper addresses the critical problem of evaluating the robustness of autonomous systems when subjected to such data distribution shifts. We propose a novel evaluation framework based on generating and utilizing a diverse set of synthetic data distributions representing common real-world variations, such as changes in lighting, weather conditions, and object appearance. Our approach employs a generative adversarial network (GAN) conditioned on latent variables that control these environmental factors, enabling the creation of a controllable and comprehensive test suite. We evaluate several state-of-the-art object detection and semantic segmentation models using our framework, demonstrating a significant drop in performance under realistic distribution shifts and highlighting vulnerabilities to specific environmental variations. The results demonstrate the effectiveness of our framework in identifying weaknesses in autonomous systems, paving the way for more robust and reliable deployments."
http://arxiv.org/abs/2408.07869v1,A Systematic Evaluation of Generated Time Series and Their Effects in Self-Supervised Pretraining,"Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning from unlabeled data, with time series benefiting from its application in various domains. However, the impact of synthetic time series data, often used to augment real data during pretraining, remains largely unexplored and lacks systematic evaluation. This paper addresses the critical question of how different generative models for time series impact the quality of learned representations in SSL pretraining. We conduct a comprehensive evaluation of several generative models, including statistical methods like ARIMA and ETS, and deep learning approaches such as GANs and VAEs, assessing their ability to generate realistic and diverse synthetic time series. These generated series are then used in conjunction with real-world datasets to pretrain a temporal convolutional network (TCN) using contrastive learning. Our results demonstrate that the choice of generative model significantly influences the downstream performance, with models capturing complex temporal dependencies leading to superior representations. This study provides valuable insights into the effective utilization of synthetic data for SSL in time series analysis, guiding future research and applications in this domain."
http://arxiv.org/abs/2410.01438v2,The Great Contradiction Showdown: How Jailbreak and Stealth Wrestle in Vision-Language Models?,"Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about images and text, but their susceptibility to adversarial attacks, particularly jailbreaking and stealthy manipulation, raises significant concerns. This paper investigates the inherent tension between jailbreaking, which aims to elicit harmful or unintended responses, and stealth, which seeks to conceal the adversarial nature of the input from detection mechanisms within the VLM. We introduce a novel adversarial attack framework that explicitly optimizes for both jailbreak success and stealth, employing a multi-objective optimization strategy incorporating perceptual similarity metrics and adversarial example transferability. Our experiments on several state-of-the-art VLMs reveal a surprising trade-off: enhancing jailbreak effectiveness often compromises stealth, and vice-versa, demonstrating a fundamental contradiction in adversarial design for these models. We further show that this trade-off is influenced by the VLM's architecture and training data. These findings highlight the complex interplay between adversarial objectives and VLM vulnerabilities, paving the way for more robust defense strategies and a deeper understanding of the security landscape surrounding multimodal AI."
http://arxiv.org/abs/2503.05431v1,Quantum-PEFT: Ultra parameter-efficient fine-tuning,"Parameter-efficient fine-tuning (PEFT) methods have emerged as a crucial approach for adapting large pre-trained models to downstream tasks with limited computational resources. However, existing PEFT techniques still involve training millions of parameters, posing challenges for deployment on resource-constrained devices and scaling to extremely large models. To address this, we introduce Quantum-PEFT, a novel ultra-parameter-efficient fine-tuning method inspired by quantum mechanics. Quantum-PEFT leverages the principles of quantum superposition and entanglement to represent and update model parameters within a drastically reduced parameter space. Specifically, we encode trainable parameters as quantum states and employ quantum gates to perform fine-tuning updates, effectively manipulating the entire parameter space with only a few trainable quantum gate parameters. Experiments on image classification and natural language processing tasks demonstrate that Quantum-PEFT achieves comparable or superior performance to existing PEFT methods while using orders of magnitude fewer trainable parameters. This work paves the way for deploying and fine-tuning large models on resource-limited platforms and unlocks new possibilities for scaling to even larger models with minimal computational overhead."
http://arxiv.org/abs/2503.21166v1,Unveiling the Potential of Superexpressive Networks in Implicit Neural Representations,"Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals and shapes by learning a mapping from coordinates to signal values. While standard multi-layer perceptrons (MLPs) are commonly used as the underlying architecture for INRs, their representational capacity can be a bottleneck, especially for complex scenes and high-frequency details. This paper addresses the problem of enhancing INR expressivity by exploring the potential of superexpressive networks, specifically those with random weights in the early layers. We propose a novel INR architecture that leverages a fixed, randomly weighted initial layer followed by a trainable MLP. Our approach, termed Random Initialized Implicit Network (RIIN), allows for efficient learning and enhanced representation of intricate details. Experimental results on various tasks, including image fitting, shape representation, and novel view synthesis, demonstrate that RIIN achieves significantly improved performance compared to standard MLP-based INRs, particularly in capturing fine-grained structures. This work highlights the benefits of superexpressivity in INRs and opens new avenues for designing more powerful and efficient implicit neural representations."
http://arxiv.org/abs/2504.17232v1,"Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification","Traffic management and urban planning increasingly rely on sophisticated analysis of traffic patterns and safety risks. This paper addresses the challenge of creating a comprehensive traffic analysis system by integrating diverse data modalities for improved forecasting and accident prediction. We propose a novel multi-modal framework that combines time-series forecasting of traffic flow using a hybrid ARIMA-LSTM model, an accident prediction module based on historical accident data and environmental factors using a gradient boosting machine, and an image classification component leveraging convolutional neural networks to identify road conditions and potential hazards from traffic camera feeds. This integrated approach allows the accident prediction module to incorporate both forecasted traffic volume and road condition information extracted from images. Experimental results using a real-world traffic dataset demonstrate that our multi-modal approach significantly outperforms uni-modal baselines in both traffic flow forecasting (reducing RMSE by 15%) and accident prediction (improving F1-score by 8%), particularly in adverse weather conditions. This integrated system provides a more robust and insightful tool for proactive traffic management and enhanced road safety."
http://arxiv.org/abs/2506.11466v1,Position Paper: Rethinking AI/ML for Air Interface in Wireless Networks,"Modern wireless networks face unprecedented demands for higher data rates, lower latency, and massive connectivity, pushing traditional air interface designs to their limits. This position paper addresses the critical need to rethink the application of Artificial Intelligence (AI) and Machine Learning (ML) in the design and optimization of future wireless air interfaces, moving beyond simply augmenting existing methods. We propose a paradigm shift towards AI/ML-native air interface designs, where learning-based techniques are fundamental to the core functionalities rather than applied as post-hoc optimizers. Specifically, we advocate for exploring end-to-end trainable communication systems, intelligent resource allocation schemes based on deep reinforcement learning, and generative models for proactive channel prediction and interference mitigation. We highlight preliminary results demonstrating the potential of these approaches to significantly outperform conventional methods in challenging scenarios with dynamic channel conditions and complex interference patterns. Rethinking air interface design through an AI/ML-native lens will be crucial for realizing the full potential of next-generation wireless networks and enabling innovative applications."
http://arxiv.org/abs/2506.15506v1,Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review,"Adversarial attacks, which introduce imperceptible perturbations to inputs to mislead machine learning models, have been extensively studied in image and text domains. However, the vulnerability of tabular machine learning, prevalent in critical applications like finance and healthcare, to such attacks remains comparatively underexplored. This paper addresses the need for a comprehensive understanding of adversarial attacks targeting tabular data by conducting a systematic literature review. We rigorously analyze existing research, categorizing attacks based on perturbation strategies (e.g., feature manipulation, row addition/deletion), threat models (e.g., white-box, black-box), and defense mechanisms (e.g., adversarial training, input sanitization). Furthermore, we synthesize the reported performance metrics, datasets used, and limitations of current approaches. Our analysis reveals significant variations in attack effectiveness depending on data characteristics, model architecture, and defense strategy, highlighting the lack of standardized evaluation protocols. This review provides valuable insights for researchers and practitioners to develop more robust and trustworthy tabular machine learning systems."
http://arxiv.org/abs/1408.6804v2,A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training Structural SVMs with a Costly max-Oracle,"Structural Support Vector Machines (SVMs) are powerful tools for structured prediction tasks, but training them often involves solving computationally intensive inference problems via a max-oracle. This paper addresses the challenge of efficiently training structural SVMs when the max-oracle is particularly costly, such as in scenarios with complex dependencies or large output spaces. We propose a novel Multi-Plane Block-Coordinate Frank-Wolfe (MP-BCFW) algorithm that leverages block-coordinate descent to decompose the optimization problem and employs multiple cutting planes within each block to accelerate convergence. The algorithm iteratively optimizes over subsets of variables while maintaining a set of cutting planes to approximate the loss function, significantly reducing the number of expensive max-oracle calls. Experimental results on image segmentation and dependency parsing tasks demonstrate that MP-BCFW achieves comparable or superior performance to state-of-the-art methods with substantially fewer max-oracle calls, leading to significant speedups in training time. This efficiency gain makes structural SVMs more practical for applications with computationally demanding inference procedures."
http://arxiv.org/abs/2106.03004v3,Exploring the Limits of Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is crucial for deploying machine learning models in safety-critical applications where encountering unforeseen data is inevitable. This paper addresses the challenge of reliably detecting OOD samples when the distribution shift is subtle and high-dimensional, a scenario where existing methods often falter. We propose a novel framework that leverages a combination of deep ensemble techniques and calibrated uncertainty estimation based on evidential deep learning. Our approach trains an ensemble of models, each regularized to provide well-calibrated uncertainty estimates, and then fuses these estimates using a learned aggregation function that is sensitive to subtle distributional differences. Experiments on a range of image classification datasets, including near-OOD scenarios generated through adversarial perturbations and semantic variations, demonstrate that our method significantly outperforms state-of-the-art OOD detection techniques, achieving higher AUROC and lower false positive rates at 95% TPR. This work advances the robustness and reliability of OOD detection, paving the way for safer and more trustworthy AI systems."
http://arxiv.org/abs/2110.02501v2,On the Surrogate Gap between Contrastive and Supervised Losses,"Contrastive self-supervised learning (SSL) and supervised learning (SL) have demonstrated impressive performance in representation learning. While conceptually distinct, recent works suggest a close relationship between these paradigms, particularly when contrastive losses are viewed as approximations of cross-entropy with specific normalizations. This paper investigates the ""surrogate gap"" arising from the approximation of the true softmax function in supervised cross-entropy by the normalized temperature-scaled cross-entropy (NT-Xent) loss commonly used in contrastive learning. We propose a novel analysis framework that decomposes the surrogate gap into two components: a *normalization gap* stemming from the difference in probability distributions, and a *temperature gap* arising from the temperature scaling factor. Furthermore, we introduce a simple yet effective method to mitigate the surrogate gap by dynamically adjusting the temperature parameter during training based on the estimated normalization gap. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our dynamic temperature adjustment strategy consistently improves the performance of contrastively pre-trained models when fine-tuned with labeled data, bridging the performance gap with fully supervised training. Our findings provide valuable insights into the relationship between contrastive and supervised learning and offer a practical approach to improve the transferability of self-supervised representations."
http://arxiv.org/abs/2110.09276v1,Natural Attribute-based Shift Detection,"Unsupervised domain adaptation aims to mitigate performance degradation when deploying models in new, unseen environments. A critical challenge is detecting when such a ""shift"" has occurred, especially in the absence of labeled target data. This paper addresses the problem of detecting shifts based on changes in the distribution of natural, human-understandable image attributes, such as color, texture, and object presence. We propose a novel shift detection method that leverages pre-trained vision-language models to extract attribute representations from images and then employs statistical hypothesis testing to identify significant differences in attribute distributions between source and target domains. Specifically, we use CLIP to generate attribute embeddings and perform a two-sample Kolmogorov-Smirnov test on these embeddings to quantify distributional differences. Experiments on diverse benchmark datasets, including domain adaptation and object detection tasks, demonstrate that our method accurately detects shifts with high sensitivity and specificity, often outperforming existing unsupervised shift detection techniques. This allows for more reliable deployment of computer vision models in real-world scenarios by providing a mechanism for proactive adaptation."
http://arxiv.org/abs/2110.14056v1,How to transfer algorithmic reasoning knowledge to learn new algorithms?,"Algorithmic reasoning, the ability to understand and execute computational processes, is a crucial aspect of intelligence. However, neural networks often struggle to generalize algorithmic skills to unseen tasks, requiring extensive retraining even for minor variations. This paper addresses the challenge of efficiently transferring knowledge acquired from learning one algorithm to facilitate the learning of novel, related algorithms. We propose a meta-learning framework, Algorithmic Knowledge Transfer Network (AKTN), which learns a shared representation space for different algorithms and a transfer function that maps learned algorithmic knowledge to new algorithms based on their structural similarity. AKTN employs a graph neural network to encode the algorithmic structure and a relation network to learn the transfer function. Experiments on a diverse set of algorithmic tasks, including sorting, searching, and graph algorithms, demonstrate that AKTN significantly improves the learning speed and generalization performance on new algorithms compared to training from scratch or using standard meta-learning techniques. Our work provides a promising approach for enabling neural networks to acquire and reuse algorithmic knowledge, paving the way for more robust and adaptable AI systems."
http://arxiv.org/abs/2203.04750v1,"Using Statistical Models to Detect Occupancy in Buildings through Monitoring VOC, CO$_2$, and other Environmental Factors","Accurate occupancy detection in buildings is crucial for efficient energy management, HVAC control, and space utilization. Existing occupancy detection methods often rely on expensive sensor networks or intrusive video surveillance, posing challenges to privacy and scalability. This paper addresses the problem of non-intrusively and cost-effectively detecting building occupancy by leveraging statistical models applied to environmental factors. We propose a novel approach that combines Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) to analyze time-series data of volatile organic compounds (VOCs), carbon dioxide (CO$_2$), temperature, and humidity. The GMM identifies distinct environmental states, while the HMM learns the temporal transitions between these states, ultimately inferring occupancy probabilities. The proposed method was evaluated on a real-world dataset collected from an office building, demonstrating an average occupancy detection accuracy of 92% and outperforming traditional threshold-based methods by 15%. This research provides a scalable and privacy-preserving solution for intelligent building management through environmental sensing and statistical modeling."
http://arxiv.org/abs/2207.11769v1,CODiT: Conformal Out-of-Distribution Detection in Time-Series Data,"Time-series data streams are ubiquitous in real-world applications, necessitating robust anomaly detection methods. However, traditional anomaly detection often struggles to differentiate between novel, yet valid, in-distribution variations and genuine out-of-distribution (OOD) samples, leading to spurious alerts and reduced system reliability. This paper addresses the problem of effectively and reliably detecting OOD samples in time-series data with guaranteed coverage. We introduce CODiT, a novel Conformal Out-of-Distribution Detection method for Time-Series data. CODiT leverages a sliding window approach combined with a deep learning model to extract meaningful features from time-series segments. These features are then used to construct conformal prediction sets, providing probabilistic guarantees on whether a new time-series segment belongs to the in-distribution data. Extensive experiments on benchmark datasets demonstrate that CODiT achieves state-of-the-art OOD detection performance, exhibiting significantly improved accuracy and reduced false positive rates compared to existing methods while providing valid coverage guarantees. This advancement enables more reliable and trustworthy deployment of time-series analysis systems in safety-critical applications."
http://arxiv.org/abs/2105.04762v1,Deep Convolutional Neural Network Applied to Electroencephalography: Raw Data vs Spectral Features,"Electroencephalography (EEG) is a non-invasive technique widely used for brain activity monitoring and diagnosis of neurological disorders. While traditional EEG analysis relies on handcrafted spectral features, deep learning methods offer the potential to automatically learn relevant representations directly from the data. This paper investigates the performance of deep convolutional neural networks (CNNs) for EEG-based classification using both raw EEG data and pre-computed spectral features as input. We propose a novel CNN architecture optimized for processing time-series EEG signals, incorporating temporal convolutional layers and attention mechanisms. Furthermore, we train separate CNN models on raw EEG and spectral representations, and compare their performance across different classification tasks, including motor imagery and seizure detection. Our results demonstrate that the CNN trained on raw EEG data achieves comparable, and in some cases superior, performance to the CNN trained on spectral features, suggesting that the network can effectively learn relevant spectral information directly from the raw signal. This finding highlights the potential of deep learning to streamline EEG analysis by eliminating the need for manual feature extraction, paving the way for more efficient and automated brain-computer interface systems and diagnostic tools."
http://arxiv.org/abs/2201.08821v1,Representing Long-Range Context for Graph Neural Networks with Global Attention,"Graph Neural Networks (GNNs) have achieved remarkable success in modeling graph-structured data across various domains. However, capturing long-range dependencies between nodes remains a significant challenge due to the limited receptive field of message-passing mechanisms in standard GNN architectures. This paper addresses the problem of effectively representing and utilizing global context information to enhance GNN performance on tasks requiring reasoning over long-range relationships. We introduce a novel Global Attention Graph Neural Network (GAGNN) that augments traditional message passing with a global attention mechanism. GAGNN first aggregates node features into a global context vector using attention, then incorporates this global representation into each node's feature vector before message passing. This allows nodes to directly access and utilize information from all other nodes in the graph, regardless of their distance. Experimental results on benchmark graph classification and node classification datasets demonstrate that GAGNN consistently outperforms existing GNN models, particularly on tasks where long-range dependencies are crucial. Our approach provides a powerful and efficient mechanism for incorporating global context into GNNs, enabling improved performance in a variety of graph-based learning tasks."
http://arxiv.org/abs/2206.03610v1,Towards Scalable Hyperbolic Neural Networks using Taylor Series Approximations,"Hyperbolic spaces offer a compelling alternative to Euclidean spaces for representing hierarchical data due to their inherent tree-like structure and ability to embed data with exponentially growing volume. However, training hyperbolic neural networks (HNNs) remains challenging, particularly with increasing model size and complexity, due to the computational cost of hyperbolic operations and the difficulty of maintaining numerical stability. This paper addresses the scalability limitations of HNNs by introducing a novel approach that leverages Taylor series approximations to efficiently compute key hyperbolic functions, such as the exponential and logarithmic maps. Our method replaces computationally expensive closed-form solutions with truncated Taylor series expansions, significantly reducing the computational burden, especially in high-dimensional spaces. We demonstrate that carefully chosen truncation orders provide a controllable trade-off between accuracy and computational speed. Experimental results on benchmark hierarchical datasets show that our Taylor series approximation-based HNNs achieve comparable or superior performance to existing methods while exhibiting substantial speedups in training time, especially for larger models. This work unlocks the potential for training deeper and wider HNNs, facilitating the application of hyperbolic geometry to more complex and large-scale problems."
http://arxiv.org/abs/2212.04633v1,Mitigation of Spatial Nonstationarity with Vision Transformers,"Spatial nonstationarity, the phenomenon where statistical properties of image data vary across spatial locations, poses a significant challenge to many computer vision tasks. Convolutional Neural Networks (CNNs), while powerful, struggle to explicitly model these spatially varying relationships due to their inherent inductive bias of translation equivariance. This paper addresses the mitigation of spatial nonstationarity in visual data by leveraging the global receptive field and attention mechanism inherent in Vision Transformers (ViTs). We propose a novel adaptation strategy, Spatial-Aware Token Fusion (SATF), that dynamically modulates the interaction between image patches (tokens) based on learned spatial context. SATF incorporates learnable spatial embeddings into the attention mechanism, allowing the model to adapt its processing based on location-specific characteristics. Experiments on synthetic and real-world datasets with varying degrees of spatial nonstationarity demonstrate that our approach significantly outperforms standard ViT architectures and CNN-based methods in tasks such as image classification and semantic segmentation. This highlights the potential of spatially-aware transformers to improve robustness and accuracy in computer vision applications where spatial dependencies are complex and non-uniform."
http://arxiv.org/abs/2212.07035v1,MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning,"Graph Contrastive Learning (GCL) has emerged as a powerful technique for unsupervised graph representation learning, leveraging contrastive objectives to capture structural and semantic information. However, existing GCL methods often rely on simple graph augmentation strategies, potentially limiting their ability to learn robust and generalizable representations. This paper addresses the problem of designing effective graph augmentation techniques that can significantly improve the performance of GCL models. We propose MA-GCL, a novel framework incorporating several model augmentation tricks tailored for GCL. Specifically, we introduce feature masking with learnable parameters, edge perturbation guided by node centrality, and subgraph sampling based on random walks with restart. These augmentations are designed to create diverse and informative views of the graph, encouraging the model to learn invariant features. Extensive experiments on various benchmark datasets demonstrate that MA-GCL consistently outperforms state-of-the-art GCL methods in node classification and graph clustering tasks. The proposed model augmentation strategies provide a simple yet effective way to boost the performance of graph contrastive learning, paving the way for more robust and generalizable graph representation learning."
http://arxiv.org/abs/2302.00775v1,Model Monitoring and Robustness of In-Use Machine Learning Models: Quantifying Data Distribution Shifts Using Population Stability Index,"Machine learning models deployed in real-world applications are susceptible to performance degradation due to evolving data distributions, a phenomenon known as data drift. Detecting and quantifying these drifts is crucial for maintaining model robustness and ensuring reliable predictions. This paper addresses the challenge of quantifying data distribution shifts in in-use machine learning models by leveraging the Population Stability Index (PSI). We propose a novel framework for continuous model monitoring that utilizes PSI to track changes in feature distributions between a baseline (training) dataset and incoming production data. Our method calculates PSI values for individual features and aggregates them to provide a holistic measure of data drift severity. Experiments conducted on synthetic and real-world datasets demonstrate that our PSI-based monitoring framework effectively identifies and quantifies data distribution shifts, correlating strongly with observed model performance degradation. The results show that PSI can serve as a reliable early warning indicator for retraining or adapting machine learning models, ultimately improving the reliability and trustworthiness of AI systems in dynamic environments."
http://arxiv.org/abs/2302.05587v2,Hierarchical Optimization-Derived Learning,"Learning complex visual representations often involves optimizing intricate, non-convex objective functions. However, traditional learning paradigms typically rely on single-stage optimization, potentially leading to suboptimal solutions and limited generalization. This paper addresses the challenge of escaping local optima and improving representation quality by introducing Hierarchical Optimization-Derived Learning (HODL). HODL leverages a multi-stage optimization process where each stage refines the learned representation based on the optimization trajectory of the previous stage. Specifically, the method first trains a model using a standard optimization algorithm. Then, instead of directly using the learned parameters, it analyzes the optimization history (e.g., gradients, parameter updates) to generate a refined objective function for the subsequent training stage. Experimental results on image classification and semantic segmentation tasks demonstrate that HODL consistently outperforms baseline methods, achieving significant improvements in accuracy and robustness. This hierarchical approach offers a novel pathway for improving representation learning by explicitly incorporating information from the optimization process itself."
http://arxiv.org/abs/2303.17235v2,Kaizen: Practical Self-supervised Continual Learning with Continual Fine-tuning,"Continual learning aims to enable models to learn new tasks sequentially without forgetting previously learned knowledge. Existing self-supervised continual learning methods often suffer from catastrophic forgetting and struggle to maintain competitive performance compared to supervised methods, particularly in practical scenarios with limited task-specific data. This paper addresses the challenge of achieving practical and effective self-supervised continual learning. We propose Kaizen, a novel approach that combines self-supervised pre-training with a continual fine-tuning strategy. Kaizen leverages a momentum-updated feature encoder to generate consistent pseudo-labels for unlabeled data within each task, followed by fine-tuning with a masked autoencoding objective and knowledge distillation. This allows the model to adapt to new tasks while preserving previously learned representations. Experiments on standard continual learning benchmarks, including CIFAR-100 and ImageNet-1000, demonstrate that Kaizen significantly outperforms existing self-supervised continual learning methods, achieving performance comparable to supervised continual learning approaches while requiring minimal task-specific labels. Kaizen offers a practical and scalable solution for continual learning in real-world applications where labeled data is scarce and tasks evolve continuously."
http://arxiv.org/abs/2306.02006v1,MA2CL:Masked Attentive Contrastive Learning for Multi-Agent Reinforcement Learning,"Multi-agent reinforcement learning (MARL) has demonstrated promise in solving complex cooperative tasks, but sample inefficiency and difficulty in learning robust agent representations remain significant challenges. Existing methods often struggle to effectively capture the crucial inter-agent dependencies and generalize across diverse scenarios. To address these limitations, we introduce MA2CL: Masked Attentive Contrastive Learning for MARL. MA2CL leverages a novel contrastive learning objective that encourages agents to learn invariant representations by contrasting masked attentive views of the joint action-observation history. Specifically, we employ a transformer-based attention mechanism to model inter-agent relationships and then randomly mask different agents contributions within the attention maps to create diverse, yet semantically related, views. The learned representations are then used to improve policy learning within a standard MARL framework. Experimental results on the StarCraft II micromanagement benchmark demonstrate that MA2CL significantly outperforms state-of-the-art MARL algorithms in terms of sample efficiency and generalization ability. This work provides a new perspective on representation learning in MARL and opens up avenues for developing more robust and efficient multi-agent systems."
http://arxiv.org/abs/2306.13575v3,Scaling MLPs: A Tale of Inductive Bias,"Multi-Layer Perceptrons (MLPs) have recently demonstrated surprising effectiveness in vision tasks, challenging the dominance of convolutional neural networks and transformers. However, the inductive biases that underpin the success of scaled-up MLPs in vision remain poorly understood. This work investigates the implicit inductive biases emerging from different MLP architectures as they are scaled in terms of depth and width, focusing on their impact on generalization and robustness. We propose a novel analysis framework that decomposes MLP behavior into frequency bias, spatial smoothness, and sensitivity to adversarial perturbations, allowing us to quantify the evolution of these biases with increasing model size. Our experiments reveal that wider MLPs exhibit a stronger bias towards learning low-frequency features and smoother functions, leading to improved generalization on clean data, while deeper MLPs become more susceptible to high-frequency noise and adversarial attacks. These findings provide crucial insights into the design and application of MLPs for computer vision, highlighting the importance of carefully considering the interplay between architecture and inductive bias when scaling these models."
http://arxiv.org/abs/2310.04415v2,Why Do We Need Weight Decay in Modern Deep Learning?,"Deep learning models, particularly in computer vision, are often over-parameterized, possessing the capacity to memorize training data rather than learning generalizable features. This necessitates regularization techniques, with weight decay being a widely adopted method. However, the precise reasons for its continued effectiveness in modern architectures and training paradigms, such as adaptive optimizers and batch normalization, remain incompletely understood. This paper investigates the role of weight decay in preventing overfitting and promoting generalization in contemporary deep learning. We propose a theoretical framework, supported by empirical evidence, demonstrating that weight decay implicitly controls the effective learning rate and stabilizes the training dynamics, particularly in the presence of adaptive optimizers like Adam. Through extensive experiments on image classification and semantic segmentation tasks using diverse network architectures, we show that carefully tuned weight decay consistently improves performance, especially when combined with batch normalization. Our findings highlight that weight decay acts as a crucial regularizer by preventing excessive growth of network weights and fostering smoother loss landscapes, leading to better generalization. This work provides valuable insights into the underlying mechanisms of weight decay and underscores its importance for achieving optimal performance in modern deep learning."
http://arxiv.org/abs/2311.16093v3,Visual cognition in multimodal large language models,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and generating text and images, yet the extent to which they truly possess visual cognition remains an open question. This paper addresses the problem of systematically evaluating and understanding the visual reasoning abilities of MLLMs, moving beyond superficial image captioning and visual question answering. We propose a novel evaluation framework, Visual Cognition Assessment Suite (VCAS), comprising a diverse set of tasks designed to probe specific cognitive skills such as object permanence, spatial reasoning, causal understanding, and intuitive physics. VCAS includes both synthetic and real-world visual scenarios, requiring the models to not only perceive visual information but also to reason about it in a manner analogous to human cognition. Our experiments on state-of-the-art MLLMs reveal significant performance variations across different cognitive tasks, indicating limitations in abstract reasoning and generalization, particularly in scenarios requiring understanding of physical laws. These findings highlight critical areas for future research in developing more robust and cognitively plausible MLLMs, paving the way for AI systems that can truly understand and interact with the visual world."
http://arxiv.org/abs/2402.10434v1,Parametric Augmentation for Time Series Contrastive Learning,"Time series data is prevalent across diverse domains, yet learning robust and generalizable representations from unlabeled time series remains a significant challenge. Contrastive learning has emerged as a powerful technique for representation learning, but its performance heavily relies on effective data augmentation strategies. However, designing augmentations for time series data is often domain-specific and requires careful consideration of temporal dependencies. This paper addresses the problem of generating diverse and informative augmentations for time series contrastive learning by introducing a novel parametric augmentation framework. Our approach leverages a learnable augmentation module parameterized by neural networks, which directly optimizes for augmentation policies that improve the downstream performance of the learned representations. Specifically, we train the augmentation module to maximize the mutual information between different augmented views of the same time series while simultaneously minimizing redundancy between augmentations. We demonstrate the effectiveness of our method on several benchmark time series datasets, achieving significant improvements over existing augmentation strategies and state-of-the-art unsupervised time series representation learning methods. This work provides a general and adaptable framework for learning effective data augmentations, leading to more robust and generalizable time series representations."
http://arxiv.org/abs/2402.17457v2,Super Consistency of Neural Network Landscapes and Learning Rate Transfer,"Neural network training is highly sensitive to hyperparameter choices, particularly the learning rate. Transferring learning rates across different architectures or datasets remains a challenging problem due to the complex and often unpredictable nature of neural network loss landscapes. This paper addresses the problem of predicting optimal learning rates for new network architectures or datasets by investigating the consistency of loss landscape geometry across different training scenarios. We propose a novel approach leveraging the concept of ""super consistency,"" where we measure the similarity of eigenvalue distributions of the Hessian matrix around local minima obtained with different architectures and datasets trained to similar levels of performance. Our experiments reveal a strong correlation between the eigenvalue distributions of Hessians at minima found with different architectures and datasets, suggesting a consistent underlying structure. Furthermore, we demonstrate that this super consistency allows for accurate transfer of learning rates: learning rates optimized for one architecture can be effectively adapted for new architectures or datasets by scaling them according to the relative spectral norms of their respective Hessian eigenvalue distributions. This work provides a theoretical justification and practical methodology for learning rate transfer, significantly reducing the computational cost of hyperparameter optimization and accelerating the training of novel neural network architectures."
http://arxiv.org/abs/2405.09021v1,Deep Learning in Earthquake Engineering: A Comprehensive Review,"Earthquake engineering seeks to mitigate the devastating impact of seismic events on civil infrastructure through robust design and assessment methodologies. Traditional approaches often rely on simplified models and empirical relationships, which may lack the precision needed to capture the complex nonlinear behavior of structures under extreme loading. This review addresses the critical need for a comprehensive understanding of how deep learning (DL) techniques are being applied to enhance various aspects of earthquake engineering. We systematically analyze the existing literature, categorizing DL applications into areas such as seismic hazard assessment, structural health monitoring, damage detection, and performance-based design. We further explore the specific DL architectures employed, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their hybrid forms, and discuss their strengths and limitations within each application domain. The review reveals that DL models demonstrate superior performance in predicting ground motion parameters, identifying subtle structural damage patterns, and optimizing structural designs compared to traditional methods. This comprehensive review highlights the transformative potential of deep learning in earthquake engineering, paving the way for more resilient and safer infrastructure in seismically active regions."
http://arxiv.org/abs/2405.16287v1,LoGAH: Predicting 774-Million-Parameter Transformers using Graph HyperNetworks with 1/100 Parameters,"Transformer models have achieved state-of-the-art results in various domains, but their substantial parameter counts pose significant challenges for deployment and training. This paper addresses the problem of efficiently predicting the weights of large-scale Transformer models, specifically those with hundreds of millions of parameters, using a significantly smaller hypernetwork. We introduce LoGAH (Low-rank Graph-based Adaptive Hypernetwork), a novel approach that leverages a graph hypernetwork architecture to generate the weight matrices of the target Transformer. LoGAH employs low-rank factorization and adaptive graph connectivity to minimize the hypernetwork's parameter count while maintaining expressive power. Experiments demonstrate that LoGAH, with only 7.74 million parameters (1/100th of the target Transformer), can predict the weights of a 774-million-parameter Transformer, achieving comparable performance on downstream tasks such as text classification and question answering. This substantial reduction in parameter count opens new avenues for deploying and training large-scale Transformers on resource-constrained devices."
http://arxiv.org/abs/2406.19301v2,MCNC: Manifold-Constrained Reparameterization for Neural Compression,"Neural compression leverages the power of deep learning to achieve high compression ratios for images and videos. However, optimizing neural codecs remains challenging, often leading to artifacts and suboptimal rate-distortion trade-offs, particularly at low bitrates. This paper addresses the problem of effectively constraining the latent space of neural compression models to improve generalization and reduce artifacts. We propose Manifold-Constrained Neural Compression (MCNC), a novel reparameterization technique that encourages latent representations to lie on a learned manifold. MCNC utilizes a variational autoencoder (VAE) architecture where the encoder maps the input to a low-dimensional latent space, and a decoder reconstructs the input. Crucially, we introduce a learned manifold within the latent space, constraining the latent variables to reside near this manifold during training. This is achieved by incorporating a manifold regularization term in the loss function that penalizes deviations from the manifold. Experiments on standard image and video compression benchmarks demonstrate that MCNC achieves significant improvements in rate-distortion performance compared to state-of-the-art neural codecs, particularly at low bitrates, while also reducing visual artifacts. This suggests that manifold constraints can significantly enhance the performance and robustness of neural compression models."
http://arxiv.org/abs/2408.07579v1,TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases,"Deep learning models for tabular data are increasingly used in high-stakes real-world applications, yet their vulnerability to adversarial attacks remains largely unexplored. This paper addresses the critical gap in understanding the adversarial robustness of tabular deep learning models by introducing TabularBench, a novel benchmark specifically designed for evaluating adversarial robustness across diverse real-world tabular datasets. TabularBench comprises a curated collection of datasets spanning various domains, along with standardized evaluation protocols for assessing model performance under different attack strategies, including both white-box and black-box attacks adapted for tabular data. We provide a comprehensive evaluation of several state-of-the-art tabular deep learning models, revealing significant vulnerabilities to even simple adversarial perturbations and highlighting the transferability of attacks across different model architectures and datasets. Our findings demonstrate the urgent need for developing robust tabular deep learning models and provide a valuable resource for future research in this area, ultimately facilitating the deployment of reliable and secure tabular deep learning systems in real-world use-cases."
http://arxiv.org/abs/2408.08684v1,Research on Personalized Compression Algorithm for Pre-trained Models Based on Homomorphic Entropy Increase,"Pre-trained models have achieved remarkable success in various computer vision tasks, but their large size and computational demands pose significant challenges for deployment on resource-constrained devices. Existing model compression techniques often treat all model parameters equally, overlooking the varying importance of individual parameters to different downstream tasks. This paper introduces a novel personalized compression algorithm for pre-trained models based on homomorphic entropy increase, designed to tailor compression strategies to specific downstream tasks. Our method first identifies task-relevant parameters by analyzing the gradient information during fine-tuning. Subsequently, we employ a homomorphic encryption scheme to increase the entropy of less important parameters, making them more amenable to pruning or quantization without significantly impacting performance on the target task. Experimental results on several benchmark datasets demonstrate that our approach achieves superior compression rates compared to state-of-the-art methods while maintaining comparable or even improved accuracy. This personalized compression strategy offers a promising solution for deploying large pre-trained models in resource-limited environments, broadening their applicability and impact."
http://arxiv.org/abs/1209.6195v1,Examples of Artificial Perceptions in Optical Character Recognition and Iris Recognition,"Optical Character Recognition (OCR) and Iris Recognition are established biometrics modalities, yet both are susceptible to errors arising from what can be termed ""artificial perceptions""  instances where the system interprets data in ways that deviate from human intuition. This paper investigates specific examples of such artificial perceptions in both OCR and Iris Recognition systems, highlighting vulnerabilities and potential for adversarial exploitation. We analyze how subtle variations in font rendering or image noise can induce OCR systems to misclassify characters, and demonstrate how adversarial patterns imperceptible to the human eye can significantly alter iris code representations, leading to false matches or non-matches. Our analysis involves manipulating input data using gradient-based optimization techniques to maximize classification error in OCR and perturb iris images to disrupt feature extraction. Experiments demonstrate that even state-of-the-art deep learning models are vulnerable, with targeted attacks achieving high success rates in both modalities. This work underscores the importance of considering the non-human-like perceptions of AI systems when designing robust and secure biometric authentication methods."
http://arxiv.org/abs/2211.13315v1,Bayesian Brain: Computation with Perception to Recognize 3D Objects,"Human object recognition is remarkably robust, effortlessly handling variations in viewpoint, illumination, and occlusion. This suggests the brain employs a sophisticated probabilistic inference mechanism, potentially approximating Bayesian inference. This paper addresses the challenge of developing a computational model that accurately mimics the brain's perceptual abilities in recognizing 3D objects from 2D images, particularly under conditions of uncertainty. We propose a hierarchical Bayesian model that integrates bottom-up visual features with top-down prior knowledge about object shapes and pose. The model uses a learned generative process to synthesize images from 3D object models and an approximate inference scheme based on variational methods to infer object identity, pose, and latent shape parameters given an input image. We demonstrate that our model achieves state-of-the-art performance on challenging 3D object recognition benchmarks, exhibiting robustness to noise and occlusion surpassing existing deep learning approaches. This work provides a step towards understanding the computational principles underlying human visual perception and offers a novel framework for building more robust and interpretable computer vision systems."
http://arxiv.org/abs/2102.00339v1,Enacted Visual Perception: A Computational Model based on Piaget Equilibrium,"Human visual perception is not a passive process but an active exploration of the environment, shaped by action and anticipation. Existing computational models often treat perception as a feedforward process, neglecting the crucial role of embodied interaction and the dynamic interplay between perception and action. We address this gap by introducing a novel computational model of enacted visual perception grounded in Piaget's theory of cognitive equilibrium. Our model employs a hierarchical active inference framework where agents learn to predict sensory consequences of their actions and adapt their internal models to maintain equilibrium between predictions and observations. Specifically, the agent utilizes a variational autoencoder to learn a latent representation of the visual environment and a recurrent neural network to predict future states based on actions. We demonstrate through simulations in a simple 2D environment that our agent learns to actively explore and manipulate objects, achieving a state of equilibrium by minimizing prediction errors and maximizing information gain. This work provides a computational framework for understanding the embodied nature of visual perception and opens new avenues for developing more robust and adaptable artificial intelligence systems."
http://arxiv.org/abs/1706.03576v1,Action and perception for spatiotemporal patterns,"Humans and animals seamlessly integrate action and perception to navigate and interact with dynamic environments, often relying on recognizing and predicting spatiotemporal patterns. A key challenge lies in developing computational models that can learn these complex patterns from raw sensory input and use them to guide purposeful actions. We introduce a novel framework that combines a hierarchical recurrent neural network for spatiotemporal pattern perception with a reinforcement learning agent for action selection. The recurrent network learns to extract relevant features from visual input across time, encoding them into a latent space representation. This representation serves as input to the reinforcement learning agent, which learns an optimal policy to perform tasks based on the perceived spatiotemporal context. We demonstrate the effectiveness of our approach on a simulated navigation task with dynamic obstacles, showing that the agent learns to anticipate obstacle trajectories and proactively adjust its path to avoid collisions, achieving significantly higher success rates compared to reactive control strategies. This work highlights the potential of integrating perception and action through learned spatiotemporal representations for creating intelligent agents capable of operating in complex, dynamic environments."
http://arxiv.org/abs/1903.05937v2,Incremental Learning of Discrete Planning Domains from Continuous Perceptions,"Automated planning enables intelligent agents to achieve complex goals, typically relying on symbolic representations of the environment. However, real-world agents operate in continuous perceptual spaces, requiring robust methods to bridge the gap between perceptions and discrete planning domains. This paper addresses the challenge of incrementally learning a discrete planning domain representation directly from a stream of continuous perceptual data, where the environment and agent's capabilities may evolve over time. We propose a novel framework that integrates a perception module, a domain learner, and a planning module. The perception module uses a neural network to map raw sensory input to symbolic states and actions. The domain learner incrementally refines a symbolic planning domain representation (in PDDL) by observing the agent's interaction with the environment, incorporating new states, actions, and effects as needed. The planning module then leverages the learned domain model to generate plans for new tasks. Experiments in simulated robotic environments demonstrate that our approach can successfully learn accurate and consistent planning domains from continuous perceptions, enabling the agent to adapt to changing environments and solve novel planning problems with increasing efficiency. This work provides a step towards more autonomous and adaptive robotic agents capable of learning and reasoning in complex, real-world settings."
http://arxiv.org/abs/2109.03391v1,"Visual Sensation and Perception Computational Models for Deep Learning: State of the art, Challenges and Prospects","Computational models of visual sensation and perception have long been a cornerstone of understanding human vision. Recent advances in deep learning have revolutionized computer vision, offering unprecedented capabilities in tasks like image recognition and scene understanding. However, a fundamental question remains: how can we effectively integrate and leverage these deep learning architectures to create more biologically plausible and robust models of visual sensation and perception? This paper addresses the critical need for a comprehensive review of current state-of-the-art deep learning models inspired by visual sensation and perception, identifying key challenges and outlining promising future research directions. We present a structured overview of existing models, categorizing them based on the specific aspects of visual processing they aim to replicate, such as early visual processing, object recognition, and scene understanding. Furthermore, we analyze the limitations of current approaches, focusing on issues like explainability, robustness to adversarial attacks, and the ability to generalize to novel environments. Through this analysis, we highlight potential avenues for developing more sophisticated and biologically-inspired deep learning models, including the incorporation of feedback mechanisms, attention mechanisms, and neuromorphic computing principles. This review provides a valuable resource for researchers seeking to bridge the gap between deep learning and human vision, ultimately contributing to the development of more intelligent and robust computer vision systems."
http://arxiv.org/abs/1802.01173v2,Tunneling Neural Perception and Logic Reasoning through Abductive Learning,"Neural-symbolic learning aims to bridge the gap between perception and reasoning by integrating neural networks with symbolic logic. However, existing approaches often struggle with noisy or incomplete perceptual inputs, hindering accurate logical inference. This paper introduces a novel framework, ""Tunneling Neural Perception and Logic Reasoning through Abductive Learning"" (TNPL), designed to enhance robustness in neural-symbolic systems. TNPL leverages abductive reasoning to generate plausible explanations for perceptual discrepancies, effectively ""tunneling"" through noise and ambiguity. Specifically, we train a neural perception module alongside an abductive reasoner that iteratively proposes hypotheses to reconcile observed data with background knowledge encoded in a logical program. The abductive hypotheses are then fed back into the neural network, refining its perception and facilitating more accurate downstream logical inference. We evaluate TNPL on a synthetic visual reasoning task and a real-world scene understanding dataset, demonstrating significant improvements in both accuracy and robustness compared to state-of-the-art neural-symbolic methods, particularly under noisy perceptual conditions. This work presents a promising direction for building more reliable and adaptable AI systems capable of reasoning effectively in complex and uncertain environments."
http://arxiv.org/abs/2011.15067v1,Learning a metacognition for object perception,"Object perception, a cornerstone of visual intelligence, traditionally focuses on improving the accuracy of object recognition and localization. However, a critical yet often overlooked aspect is the ability of a system to assess its own confidence and uncertainty in its perception  its metacognition. We address the problem of learning a metacognitive model for object perception that predicts the reliability of object detectors in complex scenes. We propose a novel framework that trains a meta-network to predict the Intersection-over-Union (IoU) between predicted and ground truth bounding boxes, leveraging contextual features and detector outputs. This meta-network is trained using a self-supervised approach, without requiring additional annotations beyond standard object detection datasets. Our experiments on benchmark datasets demonstrate that the learned metacognition accurately predicts object detection quality, enabling improved filtering of unreliable detections and calibration of confidence scores. This work offers a significant step towards building more robust and trustworthy object perception systems, facilitating their deployment in safety-critical applications."
http://arxiv.org/abs/2502.14264v1,SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game Dynamics,"Perception-Reinforcement Learning (PRL) agents operating in multi-agent environments face challenges in adapting to the dynamic and often unpredictable behaviors of other agents, especially when those agents exhibit strategic decision-making. This paper addresses the problem of designing PRL agents that can effectively learn to interact with strategic agents modeled as Stackelberg leaders, where the PRL agent acts as a follower, without explicit knowledge of the leader's policy. We introduce Stackelberg Perception-Reinforcement learning with Internal Game dynamics (SPRIG), a novel approach that integrates a differentiable game solver within the PRL agent's architecture. SPRIG learns a latent representation of the environment and the Stackelberg leader's potential strategies, enabling it to anticipate the leader's actions and optimize its own policy accordingly, by simulating internal game dynamics. Empirical evaluations on simulated navigation and resource gathering tasks demonstrate that SPRIG significantly outperforms traditional PRL methods and imitation learning baselines in terms of reward acquisition and adaptation to different leader behaviors. This work highlights the importance of incorporating game-theoretic reasoning into PRL for effective multi-agent interaction and offers a promising direction for developing more robust and adaptable autonomous agents."
http://arxiv.org/abs/2102.07246v3,Responsibility Management through Responsibility Networks,"The increasing complexity of computer vision systems necessitates methods for understanding and managing the intricate relationships between components and their contributions to overall system behavior. A critical aspect of this management is the clear assignment and tracking of responsibility for specific functionalities and potential failures. This paper addresses the challenge of explicitly representing and reasoning about responsibility within complex computer vision pipelines, which is often implicit and difficult to trace. We introduce Responsibility Networks, a novel graph-based framework that models individual components as nodes and their dependencies and influence on other components as weighted edges representing responsibility flow. This framework allows for the formalization of responsibility assignment, propagation, and attribution, facilitating the identification of critical components and potential bottlenecks. We demonstrate the effectiveness of Responsibility Networks on a synthetic image processing pipeline and a real-world object detection system, showcasing their ability to pinpoint failure sources and quantify the impact of individual module malfunctions on the overall system performance. This approach offers a principled methodology for enhancing the robustness, explainability, and maintainability of complex computer vision systems through proactive responsibility management."
http://arxiv.org/abs/2405.04443v2,POV Learning: Individual Alignment of Multimodal Models using Human Perception,"Multimodal models excel at integrating information from diverse sources, yet often struggle to align with individual human perception due to inherent biases in training data and subjective interpretations. This paper addresses the challenge of personalizing multimodal models to better reflect individual viewpoints, specifically focusing on aligning model predictions with individual human ratings of visual and textual data. We introduce ""POV Learning,"" a novel training framework that leverages personalized contrastive learning coupled with a viewpoint-aware attention mechanism. The contrastive loss encourages the model to embed multimodal data points closer together if they are rated similarly by an individual, while the attention mechanism allows the model to emphasize features deemed relevant by that individual. Experiments on a newly collected dataset of image-text pairs with personalized human ratings demonstrate that POV Learning significantly improves the correlation between model predictions and individual ratings compared to standard multimodal training techniques. This personalized alignment of multimodal models has the potential to enhance human-computer interaction and create more intuitive and user-centric AI systems."
http://arxiv.org/abs/2411.07722v2,Is Cognition Consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding,"Multimodal document understanding relies on integrating visual and textual information, yet inconsistencies between these modalities can significantly hinder performance. This paper addresses the problem of knowledge conflicts arising from discrepancies between perceptual cues from document images and semantic information extracted from text. We propose a novel framework, the Cognition-Consistent Perception Network (CCPN), that explicitly assesses and mitigates these conflicts. CCPN employs a cross-modal attention mechanism to identify potentially conflicting regions between visual and textual representations, and then uses a knowledge-aware fusion module to re-calibrate the representation of the text based on the visual perception. This fusion is guided by an external knowledge base to ensure semantic plausibility and consistency. Experiments on several document understanding benchmarks, including DocVQA and VisualMRC, demonstrate that CCPN achieves significant improvements over state-of-the-art methods, particularly in scenarios with high levels of visual-textual discrepancy. Our findings highlight the importance of explicitly modeling and resolving knowledge conflicts for robust and accurate document understanding."
http://arxiv.org/abs/2412.12000v2,CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception,"Collaborative perception using Bird's Eye View (BEV) representations is crucial for autonomous driving, enabling vehicles to extend their sensing range and improve environmental awareness. However, this collaborative setting is vulnerable to malicious agents injecting false or misleading information, severely degrading the performance of the entire system. This paper introduces CP-Guard, a novel framework for detecting and defending against malicious agents in collaborative BEV perception. CP-Guard employs a two-stage approach: first, a cross-attention-based anomaly detection module identifies potentially malicious agents by analyzing the consistency of their BEV features with those of trusted agents. Second, a robust aggregation module mitigates the impact of malicious inputs by adaptively weighting the contributions of different agents based on their trustworthiness scores, effectively filtering out corrupted information. Experimental results on the OPV2V dataset demonstrate that CP-Guard significantly improves the robustness of collaborative BEV perception under various attack scenarios, achieving up to 20% improvement in detection accuracy compared to baseline methods when facing strong adversaries. CP-Guard provides a practical and effective defense mechanism, paving the way for secure and reliable collaborative autonomous driving systems."
http://arxiv.org/abs/2502.10705v1,CoPEFT: Fast Adaptation Framework for Multi-Agent Collaborative Perception with Parameter-Efficient Fine-Tuning,"Multi-agent collaborative perception (MCP) enhances the perception capabilities of autonomous systems by sharing information among agents, leading to improved environmental understanding. However, deploying MCP in dynamic environments requires rapid adaptation to novel scenarios and sensor configurations, a process that can be computationally expensive and data-intensive with traditional fine-tuning methods. This paper addresses the challenge of efficiently adapting pre-trained MCP models to new collaborative environments with limited data. We introduce CoPEFT, a novel framework leveraging parameter-efficient fine-tuning (PEFT) techniques to enable fast adaptation for MCP. CoPEFT strategically integrates learnable adapter modules into a pre-trained collaborative perception backbone and optimizes only these modules during adaptation, significantly reducing the number of trainable parameters. Our experiments on multiple collaborative perception datasets demonstrate that CoPEFT achieves comparable or superior performance to full fine-tuning while reducing training time and parameter updates by up to 90%. This efficient adaptation framework facilitates the rapid deployment of robust and adaptable multi-agent perception systems in real-world scenarios."
http://arxiv.org/abs/1301.6359v2,Subjective Reality and Strong Artificial Intelligence,"The pursuit of Strong Artificial Intelligence (AI), possessing human-level general intelligence and consciousness, hinges on understanding the nature of subjective experience. Current AI systems, while excelling at specific tasks, lack the capacity for subjective reality, a fundamental aspect of human cognition. This paper addresses the problem of bridging the gap between objective data processing in AI and the subjective, qualia-rich experience inherent in human consciousness. We propose a computational framework that integrates predictive processing with recurrent neural networks to model the embodied agent's interaction with the environment and generate internal representations that mimic the characteristics of subjective experience. Specifically, we incorporate hierarchical predictive coding to simulate the inference of causes behind sensory inputs and employ recurrent connections to maintain internal states reflective of past experiences and expectations. Our experiments demonstrate that the proposed architecture exhibits emergent properties such as contextual sensitivity and the capacity to generate novel, internally consistent ""perceptual worlds,"" exhibiting behaviors not explicitly programmed. These findings suggest a potential pathway toward developing AI systems capable of more nuanced and human-like understanding of the world, ultimately contributing to a more robust approach to Strong AI."
http://arxiv.org/abs/2403.16101v3,Public Perceptions of Fairness Metrics Across Borders,"Algorithmic fairness has become a critical concern as machine learning systems are increasingly deployed in high-stakes domains, yet fairness definitions are often mathematically incompatible and culturally dependent. This work addresses the gap in understanding how different fairness metrics are perceived by the public across diverse cultural contexts. We conducted a large-scale online survey across five countries  the United States, United Kingdom, Germany, India, and Brazil  presenting participants with hypothetical scenarios involving algorithmic decision-making and asking them to rank the fairness of different outcomes as defined by common fairness metrics such as demographic parity, equal opportunity, and predictive parity. Our analysis reveals significant cross-cultural variations in preferences for fairness metrics, with individuals in some countries prioritizing equal opportunity while others favor demographic parity, even when presented with identical scenarios. These preferences are correlated with national-level indicators of social inequality and cultural values. These findings highlight the importance of considering cultural context when designing and deploying fair algorithms and suggest that a one-size-fits-all approach to fairness is inadequate."
http://arxiv.org/abs/2409.17659v1,Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception with Deep Reinforcement Learning,"Autonomous driving demands robust perception and intelligent decision-making in complex environments. Existing end-to-end autonomous driving systems often struggle to effectively integrate high-dimensional sensory input with control policies, leading to suboptimal performance and limited generalization. This paper addresses the challenge of learning a unified perception and control policy by proposing a hierarchical end-to-end autonomous driving framework that integrates Bird's-Eye-View (BEV) perception with deep reinforcement learning (DRL). Our approach first employs a convolutional neural network to transform multi-camera images into a BEV representation, capturing the spatial layout of the surrounding environment. Subsequently, a DRL agent, conditioned on this BEV feature map, learns to navigate and control the vehicle via a hierarchical action space encompassing both high-level maneuver selection and low-level control commands. Experimental results on the CARLA simulator demonstrate that our hierarchical BEV-DRL agent achieves significantly improved driving performance, exhibiting greater success rates and reduced collision rates compared to baseline end-to-end approaches. This work highlights the potential of integrating structured perception with reinforcement learning for developing more robust and adaptable autonomous driving systems."
http://arxiv.org/abs/1804.05906v1,An information-theoretic on-line update principle for perception-action coupling,"Perception-action coupling is crucial for intelligent agents to interact with dynamic environments, requiring continuous adaptation to maintain goal-directed behavior. However, many existing approaches struggle with efficiently integrating new information while preserving previously learned knowledge, especially in non-stationary environments. This paper introduces an information-theoretic on-line update principle for perception-action coupling based on minimizing the information loss between consecutive policy updates. Specifically, we derive a novel update rule that minimizes the Kullback-Leibler divergence between the old and new policy distributions, subject to constraints imposed by new sensory information and task objectives. We demonstrate the effectiveness of our approach in simulated robotic navigation and manipulation tasks, showing significant improvements in adaptation speed and robustness compared to traditional reinforcement learning methods and other policy distillation techniques. Our results indicate that the proposed principle facilitates efficient and stable learning in dynamic environments, leading to improved performance and adaptability in perception-action systems. This work offers a principled framework for designing adaptive agents that can effectively learn and operate in complex, real-world scenarios."
http://arxiv.org/abs/2109.13392v6,"The Tensor Brain: A Unified Theory of Perception, Memory and Semantic Decoding","The human brain seamlessly integrates perception, memory, and semantic understanding, yet current computational models typically treat these as distinct modules. This paper addresses the challenge of developing a unified framework capable of capturing the intricate interplay between these cognitive functions. We propose ""The Tensor Brain,"" a novel architecture that represents information as high-order tensors, enabling a dynamic and relational encoding of sensory inputs, episodic memories, and semantic knowledge. Perception is modeled as tensor decomposition and completion, memory as tensor-based associative recall, and semantic decoding as tensor network reasoning. We demonstrate that The Tensor Brain achieves state-of-the-art performance on benchmark datasets for visual scene understanding, knowledge graph completion, and language modeling, surpassing existing models in accuracy and efficiency. This unified tensor-based framework offers a promising avenue for building more intelligent and human-like artificial intelligence systems."
http://arxiv.org/abs/2305.00813v1,"Neurosymbolic AI -- Why, What, and How","Neurosymbolic AI aims to bridge the gap between the perception capabilities of deep learning and the reasoning prowess of symbolic AI. Despite the successes of deep learning in tasks like image recognition and natural language processing, these models often lack robustness, explainability, and the ability to perform abstract reasoning. This paper addresses the challenge of integrating neural and symbolic approaches to overcome the limitations of each paradigm individually. We present a comprehensive overview of neurosymbolic AI, categorizing existing approaches based on their architectural integration strategies: neural modules enhancing symbolic systems, symbolic reasoning guiding neural networks, and tightly coupled hybrid architectures. We further analyze these approaches along axes of knowledge representation, learning mechanisms, and inference procedures, highlighting their strengths and weaknesses. Our analysis reveals that tightly coupled architectures demonstrate the greatest potential for complex reasoning tasks while requiring careful design to manage complexity and ensure efficient training. This survey provides a structured understanding of the neurosymbolic landscape, facilitating future research and development towards more robust, explainable, and reasoning-capable AI systems."
http://arxiv.org/abs/2310.00013v4,Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving,"Collaborative perception among autonomous vehicles holds immense promise for enhancing environmental awareness and safety. However, bandwidth limitations and heterogeneous sensor configurations across vehicles pose significant challenges for effective information sharing. This paper addresses the problem of efficient and robust collaborative perception in the face of limited communication bandwidth and domain discrepancies arising from variations in sensor modalities and calibration. We propose an adaptive communication framework that dynamically selects and transmits only the most informative features based on a novel information gain metric, while simultaneously employing a domain alignment module to mitigate the impact of sensor heterogeneity. The domain alignment module learns to transform features from different sensor domains into a common space, enabling seamless fusion and improved perception accuracy. Experiments on a simulated autonomous driving environment demonstrate that our approach achieves significant improvements in object detection accuracy, particularly for distant and occluded objects, compared to existing collaborative perception methods, while substantially reducing communication overhead. These findings highlight the potential of adaptive communication and domain alignment for realizing robust and scalable collaborative perception in real-world autonomous driving scenarios."
http://arxiv.org/abs/1109.6030v1,Probabilistic Hybrid Action Models for Predicting Concurrent Percept-driven Robot Behavior,"Predicting robot behavior is crucial for safe and efficient human-robot interaction and autonomous task planning. However, modeling complex, concurrent robot actions driven by noisy perceptual inputs remains a significant challenge. This paper addresses the problem of predicting a robot's future actions when those actions are hybrid (both discrete and continuous) and executed concurrently based on probabilistic interpretations of sensory data. We propose a novel probabilistic hybrid action model that integrates a hierarchical hidden semi-Markov model (HSMM) with Gaussian process latent variable models (GPLVMs). The HSMM captures the temporal dependencies and discrete action sequences, while the GPLVMs model the continuous action parameters conditioned on latent representations of the robots perceived environment. We evaluate our model on a real-world dataset of a robot performing a table-setting task, demonstrating that it significantly outperforms baseline methods in predicting both the discrete and continuous components of concurrent robot actions, even with noisy and incomplete perceptual information. This improved prediction accuracy enables more robust robot planning and safer human-robot collaboration in dynamic environments."
http://arxiv.org/abs/2010.14289v3,Affordance as general value function: A computational model,"Affordances, the action possibilities offered by an environment to an agent, are crucial for embodied intelligence. However, current computational models often treat affordance prediction as a discrete classification problem, limiting their generalization and adaptability. This paper addresses the challenge of representing affordances in a more flexible and continuous manner, enabling agents to reason about potential actions and their associated values across diverse environments. We propose a novel framework that models affordances as General Value Functions (GVFs), where each GVF predicts the expected cumulative reward of executing a particular action policy from a given state. This allows us to learn a rich, state-dependent representation of action possibilities and their associated utilities. We demonstrate the effectiveness of our approach on simulated robotic manipulation tasks, showing improved generalization to novel objects and environments compared to traditional affordance classification methods. Our GVF-based affordance representation provides a powerful and generalizable framework for robot learning and decision-making, paving the way for more adaptive and intelligent agents."
http://arxiv.org/abs/2004.06213v2,Combined Model for Partially-Observable and Non-Observable Task Switching: Solving Hierarchical Reinforcement Learning Problems Statically and Dynamically with Transfer Learning,"Hierarchical Reinforcement Learning (HRL) offers a promising approach to solving complex tasks by decomposing them into sub-tasks. However, many real-world scenarios involve partial observability and the need to dynamically switch between tasks, presenting significant challenges for HRL agents. This paper addresses the problem of enabling HRL agents to effectively switch between partially-observable and non-observable tasks, both statically and dynamically, leveraging transfer learning to accelerate the learning process. We propose a Combined Model (CM) that integrates a hierarchical policy with an attention mechanism to selectively focus on relevant observations and a task-switching module that learns to predict the optimal task sequence based on the current state and history. The CM is further enhanced with a transfer learning strategy that pre-trains the low-level policies on simpler tasks and then fine-tunes them on more complex ones. Our experiments on a suite of simulated robotic manipulation tasks demonstrate that the CM significantly outperforms existing HRL methods in terms of learning speed, sample efficiency, and overall performance, especially in dynamic task-switching scenarios. The proposed approach offers a practical solution for deploying HRL agents in real-world environments characterized by partial observability and dynamic task requirements."
http://arxiv.org/abs/2103.03429v1,Human-Understandable Decision Making for Visual Recognition,"Deep learning models have achieved remarkable success in visual recognition tasks, but their opaque decision-making processes hinder trust and adoption, particularly in high-stakes scenarios. This paper addresses the problem of generating human-understandable explanations for the decisions made by visual recognition models. We propose a novel framework that combines concept bottleneck models with a post-hoc explanation module to provide both inherent interpretability and faithful justifications. Our method first forces the model to make predictions based on a predefined set of human-understandable concepts. Then, a separate explanation module learns to select and weight these concepts to justify the model's final prediction in a concise and intuitive manner, using a contrastive learning approach to highlight the most relevant concepts. Experimental results on benchmark datasets demonstrate that our framework achieves competitive accuracy while providing explanations that are both more accurate and more preferred by human evaluators compared to existing post-hoc explanation methods. This work contributes towards building more transparent and reliable visual recognition systems that can be effectively used in real-world applications."
http://arxiv.org/abs/2202.13794v1,Inkorrect: Online Handwriting Spelling Correction,"Online handwriting recognition systems often struggle with spelling errors due to the inherent variability in handwriting styles and the challenges of accurately segmenting and interpreting handwritten strokes. This paper addresses the problem of real-time spelling correction for online handwriting recognition, aiming to improve the accuracy and usability of such systems. We propose Inkorrect, an online handwriting spelling correction framework that integrates a novel stroke-based error detection module with a dynamic, context-aware correction model. The error detection module leverages recurrent neural networks to identify likely misspellings based on stroke sequences and contextual information. The correction model then utilizes a weighted combination of character-level language models, visual similarity metrics based on stroke alignment, and user interaction history to suggest the most probable corrections. Experimental results on a newly collected dataset of handwritten text demonstrate that Inkorrect significantly reduces the word error rate compared to existing offline spelling correction methods and naive online approaches, achieving a relative improvement of over 15%. This work provides a practical and effective solution for enhancing the accuracy and user experience of online handwriting recognition systems."
http://arxiv.org/abs/2206.13174v1,Towards Unifying Perceptual Reasoning and Logical Reasoning,"Perceptual reasoning and logical reasoning are often treated as distinct cognitive processes, despite their inherent interconnectedness in intelligent systems. This separation hinders the development of truly integrated AI systems capable of understanding and interacting with the world in a manner akin to human cognition. This paper addresses the challenge of unifying these two reasoning paradigms within a single computational framework. We propose a novel neuro-symbolic architecture that leverages differentiable rendering to bridge the gap between visual perception and symbolic logic. Our approach learns to extract scene descriptions from images in a differentiable manner, allowing for end-to-end training of a system that can both ""see"" and ""reason"" about the perceived world using logical rules. Experimental results on synthetic and real-world datasets demonstrate that our unified model achieves superior performance on complex visual reasoning tasks compared to traditional modular approaches. This work presents a significant step towards building more robust and interpretable AI systems capable of seamlessly integrating perception and logical inference."
http://arxiv.org/abs/2403.17873v1,Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach,"Large Language Models (LLMs) are increasingly integrated into societal decision-making processes, leading to concerns about potential social misattributions, where LLM outputs are incorrectly perceived as reflecting human biases or intentions. This paper addresses the critical problem of mitigating and understanding these social misattributions in LLM-driven applications. We propose a Human-Centered Explainable AI (HCXAI) framework integrating counterfactual explanations, sensitivity analysis, and user studies to disentangle the influence of training data, model architecture, and prompting strategies on perceived social biases. Our approach facilitates the identification of specific input features and model parameters that contribute to misattributions. Empirical results demonstrate that our HCXAI framework effectively highlights the sources of perceived bias, allowing for targeted interventions such as data re-balancing and prompt engineering to reduce misattribution rates by up to 30% in sensitive social domains. This work offers a crucial step towards responsible LLM deployment by providing actionable insights for mitigating social misattributions and fostering user trust."
http://arxiv.org/abs/2405.01394v1,Analysis of a Modular Autonomous Driving Architecture: The Top Submission to CARLA Leaderboard 2.0 Challenge,"Autonomous driving research is rapidly advancing, with simulation environments like CARLA playing a crucial role in development and benchmarking. The CARLA Leaderboard 2.0 challenge provides a standardized platform to evaluate the performance of autonomous driving systems across diverse and challenging scenarios. However, designing a robust and generalizable autonomous driving architecture that effectively handles the complexities of urban environments remains a significant problem. This paper presents a comprehensive analysis of a modular autonomous driving architecture developed for the CARLA Leaderboard 2.0 challenge. Our system employs a perception module leveraging LiDAR and camera data for environment understanding, a behavior planning module based on hierarchical reinforcement learning for decision-making, and a model predictive control (MPC) based motion planning module for precise vehicle control. The modular design allows for independent optimization and adaptation of each component. Our architecture achieved top performance in the CARLA Leaderboard 2.0 challenge, demonstrating robust navigation capabilities with a success rate of 91% on the challenging NoCrash benchmark and a competitive score on other metrics. This work provides valuable insights into the design and implementation of effective autonomous driving systems and highlights the importance of modularity and hierarchical control strategies for tackling complex driving scenarios."
http://arxiv.org/abs/2503.18641v1,From Fragment to One Piece: A Survey on AI-Driven Graphic Design,"Graphic design, traditionally a human-driven creative process, is increasingly influenced by advancements in Artificial Intelligence (AI). This survey examines the landscape of AI-driven graphic design, focusing on techniques that leverage machine learning to automate and enhance various stages of the design workflow. We address the challenge of synthesizing fragmented AI approaches into a coherent understanding of their potential and limitations within the broader design context. Our survey categorizes existing research based on design tasks, including image generation, layout design, typography, color palette selection, and design evaluation. We further analyze the underlying AI techniques employed, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and reinforcement learning, highlighting their strengths and weaknesses in specific applications. We present a comprehensive overview of the current state-of-the-art, identifying key trends and open challenges in the field. Our analysis reveals that AI excels at automating repetitive tasks and generating novel design variations, but struggles with high-level creative direction and contextual understanding. This survey provides a valuable resource for researchers and practitioners seeking to understand and leverage the transformative potential of AI in graphic design."
http://arxiv.org/abs/2507.11079v1,Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander,"Multi-Unmanned Ground Vehicle (UGV) confrontation scenarios demand sophisticated tactical decision-making for mission success. Existing UGV control systems often rely on pre-programmed behaviors or lack the adaptability to dynamically evolving battlefield conditions. This paper addresses the challenge of enabling UGVs to make intelligent tactical decisions in adversarial environments using a novel Vision-Language Model (VLM)-based commander. Our approach integrates a VLM, fine-tuned on a curated dataset of tactical scenarios, to interpret visual battlefield information and generate high-level strategic commands expressed in natural language. These commands are then translated into low-level UGV control actions via a separate action execution module. Experimental results in a simulated multi-UGV combat environment demonstrate that our VLM-based commander significantly outperforms rule-based systems and achieves comparable performance to human strategists in terms of mission success rate and resource utilization. This work paves the way for more autonomous and adaptable UGV deployments in complex and dynamic environments, enhancing their effectiveness in tactical operations."
http://arxiv.org/abs/2507.11633v1,General Modular Harness for LLM Agents in Multi-Turn Gaming Environments,"Large Language Models (LLMs) have shown promise as agents in interactive environments, yet their application in complex, multi-turn games remains challenging due to issues with planning, memory, and adapting to diverse game mechanics. This paper addresses the problem of effectively harnessing LLMs for robust and generalizable agent behavior across a variety of multi-turn gaming environments. We introduce a General Modular Harness (GMH) that decomposes the agent's decision-making process into distinct, interoperable modules responsible for observation processing, action planning, memory management, and environment interaction. This modular design allows for targeted improvements and adaptations to specific game requirements. We evaluate GMH across a suite of diverse text-based games, demonstrating significant improvements in task completion rate, reward acquisition, and generalization to unseen game variations compared to monolithic LLM agents and agents using simpler harness designs. Our results highlight the importance of modularity in enabling LLM agents to effectively navigate the complexities of multi-turn gaming environments, paving the way for more adaptable and robust AI game players."
http://arxiv.org/abs/2507.21637v1,Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models,"Vision-Language Models (VLMs) have demonstrated impressive capabilities in complex reasoning and multimodal understanding, yet their deployment in safety-critical applications is hindered by their potential to generate harmful or biased content. This paper addresses the challenge of mitigating unsafe outputs from VLMs by introducing Self-Aware Safety Augmentation (SASA), a novel framework that leverages the model's internal semantic understanding to proactively identify and correct potentially unsafe generations. SASA employs a self-diagnosis module to assess the semantic content of intermediate outputs, identifying concepts associated with known safety risks. Based on this assessment, the generation process is augmented with targeted interventions, such as concept suppression or counterfactual prompting, to steer the model towards safer and more responsible outputs. Experimental results on a diverse set of safety benchmarks demonstrate that SASA significantly reduces the generation of harmful content across various categories, including hate speech, toxicity, and misinformation, while preserving the overall quality and coherence of the generated text. By enhancing VLMs' inherent awareness of potential safety violations, SASA offers a promising approach to building more reliable and trustworthy AI systems for real-world applications."
http://arxiv.org/abs/cs/0703124v1,Modelling Complexity in Musical Rhythm,"Musical rhythm, often perceived as a straightforward sequence of events, exhibits intricate patterns crucial for musical expression and understanding. Quantifying and modeling this rhythmic complexity remains a challenge, hindering accurate music information retrieval and generation. This paper introduces a novel computational framework for modeling rhythmic complexity by combining information-theoretic measures with hierarchical symbolic representations. Our approach employs a variable-order Markov model to capture dependencies between rhythmic events at different temporal scales. We then calculate entropy and entropy rate across these scales to quantify the predictability and randomness inherent in the rhythmic structure. Experiments conducted on a diverse dataset of musical excerpts demonstrate that our model effectively captures the perceived complexity of various rhythmic styles, outperforming existing methods based on fixed-order Markov models and simple statistical features. The proposed framework provides a valuable tool for analyzing, classifying, and generating music with nuanced rhythmic characteristics, advancing our understanding of musical structure and opening new avenues for computational creativity."
http://arxiv.org/abs/1106.3932v1,Coincidences and the encounter problem: A formal account,"Understanding visual coincidences, such as alignment and proximity, is fundamental to many aspects of human and machine vision, including perceptual grouping and object recognition. However, a formal treatment of coincidences and their relationship to the ""encounter problem""  the challenge of efficiently identifying potentially meaningful relationships within a vast space of possibilities  remains underdeveloped. This paper introduces a novel probabilistic framework for modeling visual coincidences, explicitly addressing the encounter problem by quantifying the surprise associated with observed spatial relationships. Our approach utilizes a Bayesian formulation to estimate the prior probability of different spatial configurations and then computes the likelihood of observing a particular configuration given various underlying generative models. We demonstrate that our framework can effectively distinguish between chance alignments and those arising from structured scenes, even in the presence of significant clutter. Furthermore, we show how this framework can be integrated into a hypothesis generation and testing scheme, enabling efficient exploration of potential object relationships. This work provides a principled and computationally tractable approach to understanding visual coincidences, with broad implications for scene understanding and object recognition."
http://arxiv.org/abs/1709.03879v1,"Ultimate Intelligence Part III: Measures of Intelligence, Perception and Intelligent Agents","Artificial intelligence research strives to create systems exhibiting human-level or superhuman intelligence, yet a comprehensive understanding and quantifiable measures of intelligence remain elusive. This paper addresses the critical need for robust and unified metrics to evaluate intelligence, particularly focusing on perception and embodied intelligent agents. We propose a novel framework, the Embodied Perception Quotient (EPQ), that integrates multiple dimensions of intelligence, including perceptual accuracy, adaptability to novel environments, and efficiency in goal-directed behavior within simulated physical worlds. The EPQ utilizes a suite of benchmark tasks spanning visual understanding, navigation, object manipulation, and social interaction, evaluated within diverse and dynamically changing environments. Experiments conducted on several state-of-the-art AI agents demonstrate that EPQ effectively differentiates their performance across various cognitive skills and reveals strengths and weaknesses not captured by existing single-metric evaluations. The EPQ provides a more nuanced and holistic assessment of intelligence, facilitating targeted improvements in AI agent design and fostering progress towards more capable and generalizable artificial intelligence."
http://arxiv.org/abs/2406.12147v1,Metacognitive AI: Framework and the Case for a Neurosymbolic Approach,"Metacognition, the ability to reason about one's own cognitive processes, is a crucial aspect of human intelligence, enabling adaptability and robust decision-making in complex environments. Current AI systems often lack this self-awareness, resulting in brittle performance and limited generalization capabilities. This paper addresses the challenge of instilling metacognitive abilities in artificial intelligence through the development of a novel framework termed ""Metacognitive AI"" (MCAI). Our approach leverages a neurosymbolic architecture, combining the strengths of neural networks for perception and symbolic reasoning for higher-level cognitive functions. Specifically, we propose a modular architecture comprising a perception module, a reasoning module, and a metacognitive control module that monitors and regulates the interaction between the former two. We demonstrate the efficacy of our MCAI framework on a visual question answering task requiring both scene understanding and strategic reasoning. Experimental results show that our MCAI system significantly outperforms baseline models lacking metacognitive capabilities, exhibiting improved accuracy and robustness to noisy inputs and adversarial attacks. This work provides a foundational framework for building more robust, adaptable, and human-like AI systems capable of self-reflection and strategic decision-making."
http://arxiv.org/abs/2412.03903v1,Using SlowFast Networks for Near-Miss Incident Analysis in Dashcam Videos,"Dashcam videos offer a valuable source of data for understanding and preventing traffic accidents. However, identifying near-miss incidents within these videos remains challenging due to their subtle and often fleeting nature. This paper addresses the problem of automatically detecting and classifying near-miss incidents in dashcam videos by leveraging the SlowFast network architecture. We propose a modified SlowFast network incorporating attention mechanisms to better capture the crucial temporal and spatial features associated with near-miss events. The Slow pathway processes static contextual information at a low frame rate, while the Fast pathway focuses on capturing rapid motion dynamics. The attention mechanism allows the network to focus on the most relevant spatial regions and temporal segments within the video. Experimental results on a newly curated dataset of near-miss incidents demonstrate that our proposed method achieves state-of-the-art performance in near-miss detection and classification, outperforming existing action recognition models. This research provides a valuable tool for proactive road safety analysis and the development of advanced driver-assistance systems."
http://arxiv.org/abs/2505.17882v1,Formalizing Embeddedness Failures in Universal Artificial Intelligence,"Universal Artificial Intelligence (UAI) aims to develop agents capable of performing well across a vast range of environments. However, the theoretical foundations often overlook the critical aspect of embodied interaction, leading to potential failures when UAI agents are deployed in complex, real-world settings. This paper addresses the problem of formally characterizing and predicting ""embeddedness failures,"" where an agent's performance degrades due to unforeseen interactions between its internal state, its actions, and the environment. We introduce a novel framework based on information-theoretic measures and dynamical systems theory to model the flow of information within the agent-environment loop. Specifically, we define metrics for ""informational fragility"" and ""environmental coupling"" that quantify an agent's susceptibility to disruptions and its dependence on specific environmental properties, respectively. We demonstrate the effectiveness of our framework through simulations involving simulated robots navigating diverse terrains and manipulating objects. The results show a strong correlation between high informational fragility and environmental coupling scores and observed performance degradation in challenging environments. This work provides a formal foundation for understanding and mitigating embeddedness failures, paving the way for more robust and reliable UAI agents in the real world."
http://arxiv.org/abs/2103.11218v3,Evaluating Perceived Usefulness and Ease of Use of CMMN and DCR,"Business Process Management (BPM) systems are increasingly used to automate and manage complex, knowledge-intensive processes. Case Management Model and Notation (CMMN) and Decision Model and Notation (DCR) graphs are two prominent standards for modeling such adaptive and flexible processes. However, the perceived usefulness and ease of use of these notations by process modelers remain underexplored, hindering their effective adoption and application. This paper addresses this gap by empirically evaluating the perceived usefulness and ease of use of CMMN and DCR graphs. We conducted a controlled experiment with 60 participants, using a between-subjects design, where participants were tasked with modeling real-world case scenarios using either CMMN or DCR. After the modeling task, participants completed a validated questionnaire based on the Technology Acceptance Model (TAM). Our results indicate that while DCR graphs are perceived as significantly easier to use compared to CMMN, there is no significant difference in perceived usefulness between the two notations. These findings provide valuable insights for BPM tool developers and process modelers, informing the design of more user-friendly modeling tools and facilitating the wider adoption of appropriate case management notations."
http://arxiv.org/abs/2401.12247v1,Exploring consumers response to text-based chatbots in e-commerce: The moderating role of task complexity and chatbot disclosure,"Text-based chatbots are increasingly deployed in e-commerce to enhance customer service and drive sales. However, the effectiveness of these chatbots hinges on various factors, including the nature of the task and the transparency surrounding the chatbot's artificial identity. This study investigates how task complexity and chatbot disclosure (i.e., whether the chatbot is explicitly identified as a bot) moderate consumer responses to text-based chatbots in an e-commerce setting. We conducted a 2x2 between-subjects experiment, manipulating task complexity (simple vs. complex product inquiry) and chatbot disclosure (disclosed vs. undisclosed). Participants interacted with a simulated e-commerce chatbot to complete a purchase-related task. We measured perceived usefulness, satisfaction, and purchase intention. Our findings reveal that chatbot disclosure positively influences consumer satisfaction and purchase intention, particularly when dealing with complex tasks. Conversely, in simple tasks, undisclosed chatbots yield comparable or even slightly better results, suggesting that users may not prioritize knowing the chatbot's identity when the interaction is straightforward. These results highlight the importance of carefully considering task complexity and chatbot disclosure strategies to optimize consumer engagement and achieve desired business outcomes in e-commerce chatbot deployments."
http://arxiv.org/abs/2505.15146v2,lmgame-Bench: How Good are LLMs at Playing Games?,"Large Language Models (LLMs) have shown remarkable capabilities in diverse domains, including code generation and reasoning. However, their ability to perform complex tasks requiring sequential decision-making and interaction with dynamic environments, such as playing games, remains relatively unexplored. This paper addresses the question: how effectively can LLMs play games, particularly those demanding strategic planning and adaptation? We introduce lmgame-Bench, a novel benchmark comprising a diverse set of text-based games with varying complexities and rule sets. We evaluate several state-of-the-art LLMs, prompting them to generate game actions based on the game state description and their past interactions. Our evaluation considers both zero-shot and few-shot learning paradigms, as well as the impact of different prompting strategies. Our results demonstrate that while LLMs can achieve some success in simpler games, they struggle with games requiring long-term planning, memory of past events, and adaptation to unexpected situations, achieving significantly lower scores compared to human players or specialized game-playing agents. This benchmark and analysis highlight the limitations of current LLMs in sequential decision-making and provide a valuable resource for future research aimed at developing more robust and adaptable AI agents capable of mastering complex interactive environments."
http://arxiv.org/abs/2505.21322v1,Assured Autonomy with Neuro-Symbolic Perception,"Assured autonomy in complex environments demands robust perception that can handle uncertainty and provide verifiable guarantees. Current perception systems, primarily driven by deep learning, often lack explainability and struggle with out-of-distribution scenarios, hindering their deployment in safety-critical applications. This paper addresses the challenge of creating perception systems with verifiable properties by introducing a novel neuro-symbolic framework. Our approach integrates learned neural perception modules with symbolic reasoning to provide both accurate scene understanding and formal guarantees on system behavior. Specifically, we use neural networks for object detection and attribute recognition, and then leverage a knowledge base and logical rules to infer higher-level scene properties and verify safety constraints. We demonstrate the effectiveness of our framework in a simulated autonomous driving scenario, showing significant improvements in both perception accuracy and the ability to detect and prevent unsafe situations compared to purely neural or symbolic approaches. This hybrid approach represents a significant step towards building trustworthy and verifiable autonomous systems."
http://arxiv.org/abs/1401.3854v1,A Constraint Satisfaction Framework for Executing Perceptions and Actions in Diagrammatic Reasoning,"Diagrammatic reasoning, the ability to solve problems using diagrams, is a core aspect of human intelligence, yet remains a challenge for artificial intelligence. This paper addresses the problem of effectively integrating perception and action within a computational framework for diagrammatic reasoning, where actions modify the diagram and perception provides updated information. We propose a novel constraint satisfaction framework that represents both the perceptual interpretation of a diagram and the effects of actions as constraints. The reasoning process involves iteratively applying actions to the diagram, updating the constraint network based on the perceived changes, and searching for a solution that satisfies all constraints. We demonstrate the effectiveness of our framework on a geometric analogy problem, showing its ability to correctly identify and execute actions leading to the desired solution despite noisy perception. This work offers a promising approach to building more robust and adaptable diagrammatic reasoning systems by explicitly managing the interplay between perception and action."
http://arxiv.org/abs/cs/0211027v1,Adaptive Development of Koncepts in Virtual Animats: Insights into the Development of Knowledge,"Understanding the developmental origins of knowledge remains a central challenge in cognitive science. This work investigates how structured knowledge can emerge in artificial agents (animats) through adaptive interaction with a virtual environment, focusing on the development of reusable, abstract ""koncepts."" We address the problem of how animats can autonomously discover and refine perceptual abstractions that support increasingly complex behavior without explicit programming. Our method utilizes a neural network architecture where koncepts are represented as latent variables learned through a variational autoencoder conditioned on both sensory input and motor actions. Crucially, the learning process is driven by intrinsic motivation, specifically curiosity, which guides the animat to explore and interact with the environment in ways that maximize its learning progress. We demonstrate that animats can develop a hierarchical understanding of their environment, learning to associate sensory patterns with specific actions and outcomes, and generalizing these associations to novel situations. The resulting koncepts not only improve task performance but also exhibit properties analogous to symbolic representations, suggesting a potential pathway for the emergence of grounded knowledge from sensorimotor experience. This research provides a computational framework for studying the development of knowledge and offers insights into the mechanisms underlying abstraction and generalization in artificial and biological systems."
http://arxiv.org/abs/cs/0410049v1,Intransitivity and Vagueness,"Intransitivity and vagueness are pervasive challenges in tasks requiring comparative judgments, such as image ranking and attribute evaluation. These inconsistencies arise when human annotators exhibit cyclic preferences (intransitivity) or struggle to define precise boundaries for subjective concepts (vagueness), leading to noisy and unreliable labels. This paper addresses the problem of learning robust models from data exhibiting both intransitivity and vagueness in comparative annotations. We propose a novel probabilistic framework that explicitly models both intransitivity and vagueness using a mixture model of pairwise preference distributions. Specifically, we represent each comparison as a noisy observation drawn from a mixture of a consistent preference model and a random guessing process, where the mixing weights are learned alongside the model parameters to capture the degree of intransitivity. Furthermore, we introduce a vagueness parameter that modulates the uncertainty in the preference distribution based on the proximity of the compared items in the feature space. Experiments on synthetic and real-world image ranking datasets demonstrate that our method significantly outperforms existing approaches in terms of ranking accuracy and robustness to noisy annotations. Our approach provides a principled framework for handling subjective and inconsistent data, leading to more accurate and reliable visual understanding systems."
http://arxiv.org/abs/1604.02509v2,Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds,"Situated language comprehension, where meaning is grounded in perception and action within a physical environment, is crucial for cognitive agents to effectively interact with the world. However, current computational models often struggle to dynamically integrate linguistic input with real-time sensory information and embodied experience. This paper addresses the challenge of building an indexical model of situated language comprehension that enables cognitive agents to understand and execute instructions in complex physical environments. We propose a novel framework that combines probabilistic language parsing with a visual-spatial attention mechanism, allowing the agent to dynamically index linguistic referents to perceived objects and spatial relations. The model iteratively refines its understanding by predicting and verifying the consequences of its actions through interaction with a physics engine. We demonstrate that our model significantly outperforms baseline approaches in a series of simulated manipulation tasks, exhibiting improved accuracy in object identification and instruction following. This work represents a significant step towards creating cognitive agents that can effectively ground language in the physical world, enabling more natural and intuitive human-robot interaction."
http://arxiv.org/abs/1707.09661v1,A Vision For Continuous Automated Game Design,"Procedural Content Generation via Machine Learning (PCGML) has shown promise in automating aspects of game design, but current approaches often focus on generating static content or require significant human intervention for evaluation and refinement. This paper addresses the challenge of creating a continuous automated game design system capable of iterative generation, evaluation, and refinement of game content without explicit human guidance. We propose a novel framework that integrates a Variational Autoencoder (VAE) for generating game level representations with a reinforcement learning (RL) agent trained to evaluate and optimize game level quality based on learned player behavior models. The RL agent provides a continuous feedback loop, guiding the VAE to generate levels that maximize desired player engagement metrics predicted by the learned behavior model. Experiments on a 2D platformer game demonstrate that our system can autonomously generate increasingly complex and engaging levels over time, surpassing the performance of levels generated by purely generative or evolutionary approaches. This work represents a significant step towards truly autonomous game design, opening up possibilities for personalized gaming experiences and accelerated game development workflows."
http://arxiv.org/abs/1902.04245v2,VERIFAI: A Toolkit for the Design and Analysis of Artificial Intelligence-Based Systems,"The increasing deployment of Artificial Intelligence (AI) in safety-critical applications necessitates rigorous verification and validation methodologies. However, the complexity and opacity of AI-based systems pose significant challenges to ensuring their reliability and trustworthiness. This paper introduces VERIFAI, a comprehensive toolkit designed to facilitate the design and analysis of AI-based systems, with a focus on computer vision applications. VERIFAI integrates formal methods, simulation-based testing, and explainable AI techniques within a unified framework. It provides functionalities for requirement specification, property verification using reachability analysis and falsification, robustness evaluation against adversarial attacks, and explainability analysis through feature attribution methods. We demonstrate the utility of VERIFAI through case studies involving autonomous driving and medical image analysis, showcasing its ability to identify potential vulnerabilities and enhance system understanding. VERIFAI enables developers and researchers to systematically assess and improve the safety and robustness of AI-based systems, ultimately contributing to their responsible deployment in real-world scenarios."
http://arxiv.org/abs/1905.00607v2,A knowledge-based intelligent system for control of dirt recognition process in the smart washing machines,"Modern smart washing machines increasingly rely on computer vision to optimize washing cycles based on the type and amount of dirt detected. However, current dirt recognition systems often struggle with the variability of real-world dirt appearances, leading to suboptimal washing performance and increased resource consumption. This paper addresses the challenge of improving dirt recognition accuracy and robustness in smart washing machines by introducing a novel knowledge-based intelligent system. Our approach integrates a convolutional neural network (CNN) for initial dirt classification with a knowledge base containing dirt characteristics, textile types, and optimal washing parameters. A rule-based inference engine then leverages this knowledge to refine the CNN's initial predictions, resolving ambiguities and adapting to specific dirt-textile combinations. Experimental results on a diverse dataset of soiled fabrics demonstrate a significant improvement in dirt classification accuracy compared to standalone CNN models, reducing misclassification rates by up to 15%. This improved dirt recognition accuracy leads to more efficient and effective washing cycles, minimizing water and energy consumption while extending textile lifespan."
http://arxiv.org/abs/2103.15975v1,Platform for Situated Intelligence,"Situated intelligence, where AI agents operate and learn within complex, real-world environments, demands a robust and flexible platform for experimentation and deployment. However, current AI development often relies on isolated simulations or narrowly focused robotic systems, hindering the transfer of learned skills to diverse and unpredictable real-world scenarios. This paper introduces a novel ""Platform for Situated Intelligence"" (PSI) designed to bridge this gap. PSI provides a modular and extensible framework integrating multi-modal sensor streams (vision, audio, tactile), diverse robotic actuators (mobile manipulators, drones), and a hierarchical control architecture allowing for both low-level reactive control and high-level task planning. We demonstrate the effectiveness of PSI through a series of experiments involving object manipulation, navigation, and human-robot interaction in a dynamic office environment. Our results show significant improvements in task completion rates and robustness to environmental variations compared to baseline methods, indicating the platform's capacity to facilitate the development of more adaptable and reliable situated AI agents. PSI accelerates research in situated intelligence by providing a standardized and versatile environment for developing and evaluating AI algorithms in real-world contexts."
http://arxiv.org/abs/2205.07635v1,Relating Information and Proof,"The interplay between information theory and proof theory has long been recognized, yet a concrete, unifying framework remains elusive. This paper addresses the problem of formally relating the information content of data with the complexity of proving statements derived from that data, specifically within the context of computer vision tasks. We propose a novel information-theoretic proof system based on a modified intuitionistic type theory, where types represent visual concepts and proofs correspond to object recognition and scene understanding processes. The system incorporates a cost function reflecting the information required to construct a proof, derived from the minimum description length (MDL) principle applied to visual data encoding. We demonstrate, through experiments on image classification and object detection tasks using both synthetic and real-world datasets, a strong correlation between the information cost of a proof and its empirical verification rate. This framework provides a formal bridge between information and proof, offering a new perspective on the reliability and interpretability of vision algorithms."
http://arxiv.org/abs/2206.05922v2,"From Perception to Programs: Regularize, Overparameterize, and Amortize","Learning to map perceptual inputs directly to executable programs offers a powerful paradigm for intelligent systems. However, effectively bridging the gap between noisy, high-dimensional sensory data and the discrete, structured space of programs remains a significant challenge. We address this problem by introducing a novel framework, Regularize, Overparameterize, and Amortize (ROA), designed to improve the generalizability and efficiency of perception-to-program learning. ROA leverages overparameterized neural networks to learn a rich, implicit representation of the program space, regularized by a novel program-structure-aware loss function. Subsequently, it amortizes the search for the optimal program by training a decoder that directly maps perceptual features to program parameters, conditioned on the learned implicit representation. Experiments on diverse program synthesis tasks, including visual reasoning and robot manipulation, demonstrate that ROA significantly outperforms state-of-the-art approaches in terms of both accuracy and generalization ability, achieving up to a 25% improvement in program execution success rate on unseen test environments. This work establishes a new direction for learning programs from perception by effectively combining the strengths of representation learning, regularization, and amortized inference."
http://arxiv.org/abs/2304.00004v1,Disentangling Domain Ontologies,"Domain adaptation aims to generalize models trained on a source domain to a target domain with different data distributions. A key challenge in domain adaptation is the presence of entangled domain-specific and domain-invariant features, hindering effective knowledge transfer. This paper addresses the problem of disentangling these entangled domain ontologies for improved domain adaptation performance. We propose a novel framework, Domain Ontology Disentanglement Network (DODNet), which utilizes adversarial learning and a novel ontology contrastive loss to explicitly separate domain-specific and domain-invariant feature representations. DODNet employs a shared encoder to extract features, followed by two domain-specific branches and a domain-invariant branch, each optimized to capture distinct aspects of the data. Experiments on several benchmark datasets, including Office-Home and VisDA-C, demonstrate that DODNet achieves state-of-the-art performance in unsupervised domain adaptation. The proposed method effectively disentangles domain ontologies, leading to more robust and generalizable models across diverse domains."
http://arxiv.org/abs/2404.08543v1,Memory Traces: Are Transformers Tulving Machines?,"Transformers have achieved remarkable success in various sequence modeling tasks, prompting investigations into their underlying mechanisms and representational capabilities. A key question is whether Transformers can emulate aspects of human memory, particularly episodic memory, which involves consciously recalling past experiences and their contexts. This paper investigates whether Transformers develop internal representations analogous to ""memory traces""  persistent neural changes encoding specific experiences  and whether these traces support episodic-like recall. We introduce a novel methodology for probing Transformer activations, specifically examining the evolution of hidden states across layers in response to sequential input. We then analyze the learned representations using techniques inspired by cognitive neuroscience, searching for evidence of distinct, contextualized memory traces. Our experiments on language modeling and synthetic tasks reveal that Transformers do indeed form identifiable memory traces, which become more distinct and context-specific with increasing layer depth. Furthermore, we demonstrate that these traces can be selectively reactivated by cues related to the original experience, enabling a form of episodic-like recall. These findings suggest that Transformers, despite their architectural simplicity, possess emergent properties that resemble aspects of human episodic memory, opening avenues for more biologically plausible and efficient sequence learning models."
http://arxiv.org/abs/2503.12687v1,"AI Agents: Evolution, Architecture, and Real-World Applications","Artificial intelligence (AI) agents, autonomous entities capable of perceiving their environment and acting to achieve specific goals, are rapidly transforming various sectors. This paper addresses the challenge of providing a comprehensive overview of the evolution, architectural diversity, and practical deployments of AI agents in real-world scenarios. We conduct a systematic review of the historical development of AI agents, tracing their lineage from early symbolic systems to contemporary deep reinforcement learning approaches. Furthermore, we categorize and analyze prominent agent architectures, including deliberative, reactive, hybrid, and hierarchical models, highlighting their strengths and weaknesses in different application domains. Finally, we examine real-world applications of AI agents across diverse fields such as robotics, game playing, healthcare, finance, and autonomous driving, showcasing their impact and potential. Our analysis reveals a trend towards increasingly sophisticated and adaptable agent architectures leveraging deep learning and reinforcement learning techniques, leading to significant performance gains in complex and dynamic environments. This review provides valuable insights for researchers and practitioners seeking to understand the current state and future directions of AI agent technology and its transformative potential."
http://arxiv.org/abs/2505.06328v1,A Grounded Memory System For Smart Personal Assistants,"Smart Personal Assistants (SPAs) increasingly rely on memory to provide personalized and context-aware assistance. However, current memory systems often lack grounding, leading to difficulties in relating abstract knowledge to real-world sensory experiences and user interactions. This paper addresses the challenge of building a grounded memory system capable of associating perceptual input with symbolic knowledge for improved SPA performance. We propose a novel architecture that integrates a visual perception module, a knowledge graph, and a neural memory network. The visual perception module extracts features from the environment, which are then linked to relevant entities in the knowledge graph. The neural memory network uses these grounded representations to store and retrieve information, enabling the SPA to reason about the physical world and user activity. Experiments demonstrate that our grounded memory system improves the accuracy and relevance of SPA responses by 15% compared to ungrounded approaches in a simulated environment. This work highlights the importance of grounding memory systems in sensory data for building more intelligent and context-aware SPAs."
http://arxiv.org/abs/1504.05381v3,How do you revise your belief set with %$;@*?,"Belief revision, the process of updating one's beliefs in light of new, potentially contradictory information, is a fundamental problem in artificial intelligence and cognitive science. This paper addresses the challenge of belief revision when the incoming information, denoted by the symbolic placeholder ""%$;@*,"" is noisy, uncertain, or represented in a non-standard format, making direct integration with existing belief sets problematic. We propose a novel framework that leverages variational inference to probabilistically model the relationship between the observed ""%$;@*"" and the underlying, true state of the world. Our approach learns a latent representation of ""%$;@*"" and then utilizes this representation to update the belief set by minimizing a divergence measure between the pre- and post-revision belief distributions, conditioned on the latent state. Experiments on synthetic and real-world datasets demonstrate that our method outperforms traditional belief revision techniques, particularly when ""%$;@*"" contains significant noise or ambiguity, leading to more accurate and robust belief updates. This work provides a principled approach for incorporating complex and potentially unreliable information into existing belief systems, advancing the field of knowledge representation and reasoning."
http://arxiv.org/abs/1703.04368v1,Symbol Grounding via Chaining of Morphisms,"Symbol grounding, the problem of connecting abstract symbols to perceptual experience, remains a fundamental challenge in artificial intelligence. Traditional approaches often rely on direct association between symbols and sensory data, limiting their ability to generalize to novel situations or handle complex, compositional meanings. This paper addresses the symbol grounding problem by proposing a novel framework that grounds symbols through a chain of learned morphisms, representing transformations between different representational spaces. Our method learns to map symbols to abstract scene graphs, then further transforms these graphs into lower-level perceptual representations, ultimately grounding symbols in raw visual data. By composing learned morphisms, we enable the system to understand complex relationships between symbols and their visual counterparts, facilitating compositional generalization. Experimental results on synthetic and real-world datasets demonstrate that our approach outperforms existing methods in grounding symbols accurately and generalizing to unseen combinations of objects and relations. This work establishes a new paradigm for symbol grounding, emphasizing compositional reasoning and offering a pathway towards more robust and adaptable AI systems."
http://arxiv.org/abs/1910.10393v2,RTOP: A Conceptual and Computational Framework for General Intelligence,"The pursuit of Artificial General Intelligence (AGI) demands frameworks capable of representing and manipulating knowledge across diverse domains. However, current AI systems often struggle with transferring knowledge and reasoning flexibly in novel situations. We introduce RTOP: a novel conceptual and computational framework for general intelligence grounded in the principles of Representation, Transformation, Operation, and Planning. RTOP decomposes complex problems into these four fundamental components, facilitating modular knowledge representation and enabling the dynamic generation of solution pathways. Specifically, RTOP utilizes a graph-based knowledge representation where nodes represent concepts and edges define relationships, allowing for rule-based transformations and goal-oriented planning. We demonstrate RTOP's efficacy on a suite of benchmark problems spanning visual reasoning, symbolic manipulation, and commonsense question answering. Our results show that RTOP achieves superior performance compared to existing methods, particularly in tasks requiring compositional reasoning and generalization to unseen scenarios. This framework offers a significant step towards building more robust and adaptable AI systems capable of approaching human-level intelligence."
http://arxiv.org/abs/2009.10256v1,Extending Answer Set Programs with Neural Networks,"Answer Set Programming (ASP) is a powerful declarative problem-solving paradigm, particularly effective for knowledge representation and reasoning tasks. However, ASP struggles with tasks requiring perception or learning from raw data. This paper addresses the limitation of traditional ASP by introducing a novel framework that seamlessly integrates neural networks within ASP programs, enabling reasoning with learned representations. Our approach, called Neuro-ASP, allows ASP rules to directly query neural networks for predictions, treating network outputs as atoms within the logic program. We define a formal semantics for Neuro-ASP and develop a prototype solver that efficiently grounds and solves Neuro-ASP programs, leveraging existing ASP solvers. Experimental results demonstrate that Neuro-ASP significantly enhances the capabilities of ASP, enabling the solution of complex hybrid problems involving both logical reasoning and perception, such as visual question answering and commonsense reasoning tasks requiring image understanding. The Neuro-ASP framework opens new avenues for building intelligent systems that combine the strengths of symbolic and sub-symbolic AI."
http://arxiv.org/abs/2110.06477v1,Feudal Reinforcement Learning by Reading Manuals,"Hierarchical reinforcement learning (HRL) offers a promising approach to tackling complex, long-horizon tasks by decomposing them into sub-goals. However, learning effective hierarchies and sub-policies remains a significant challenge, often requiring extensive exploration or shaped reward functions. This paper addresses the problem of efficiently learning HRL policies by leveraging readily available, human-authored task manuals. We introduce a novel feudal reinforcement learning framework where a manager learns to generate sub-goals by extracting relevant instructions from the manual, and workers learn to execute these sub-goals in the environment. The manager uses a transformer-based architecture to ground textual instructions into visual sub-goals, providing intrinsic motivation for the workers. Our experiments across various simulated robotic manipulation tasks demonstrate that our method significantly outperforms flat and other hierarchical reinforcement learning baselines in terms of sample efficiency and final performance, enabling agents to solve complex tasks with minimal environment interaction. This work demonstrates the potential of integrating human knowledge with reinforcement learning to accelerate the acquisition of intelligent agents capable of solving complex tasks."
http://arxiv.org/abs/2110.09378v1,Forecasting Nonverbal Social Signals during Dyadic Interactions with Generative Adversarial Neural Networks,"Accurate forecasting of nonverbal social signals is crucial for enabling socially intelligent agents in human-computer interaction. Predicting future human behavior, particularly during dyadic interactions, remains a challenging task due to the complex interplay of multimodal cues and inherent uncertainty. We propose a novel generative adversarial network (GAN) architecture, the Social Signal Forecasting GAN (SSF-GAN), to address this challenge. SSF-GAN leverages a spatiotemporal encoder-decoder network to capture the underlying dynamics of nonverbal signals, such as head pose, facial expressions, and body movements. A conditional discriminator is employed to enforce realistic and temporally coherent predictions, effectively mitigating the blurring often observed in regression-based forecasting methods. Experimental results on benchmark datasets demonstrate that SSF-GAN significantly outperforms state-of-the-art methods in predicting future nonverbal behaviors, achieving lower prediction errors and generating more realistic and diverse forecasts. This advancement offers a promising direction for developing more natural and responsive social agents capable of anticipating and adapting to human behavior in real-time."
http://arxiv.org/abs/2210.15236v1,Painting the black box white: experimental findings from applying XAI to an ECG reading setting,"Electrocardiogram (ECG) readings are crucial for diagnosing various cardiac conditions, and deep learning models have shown promise in automating this process. However, the ""black box"" nature of these models limits clinical trust and understanding of their decision-making processes. This paper addresses the need for explainable AI (XAI) in ECG analysis by investigating the application of several XAI techniques to a deep learning model trained for arrhythmia classification using a large ECG dataset. We implemented and compared gradient-based methods (Grad-CAM, GradInput), perturbation-based methods (LIME, SHAP), and attention mechanisms to highlight regions of the ECG signal that contribute most significantly to the model's predictions. Our results demonstrate that XAI methods can effectively identify clinically relevant features, such as P-waves, QRS complexes, and T-waves, aligning with expert knowledge. Furthermore, we found that specific XAI techniques provided more consistent and interpretable explanations across different arrhythmia classes. These findings contribute to increased transparency and trust in AI-driven ECG analysis, paving the way for safer and more effective clinical deployment."
http://arxiv.org/abs/2105.00762v1,VECA : A Toolkit for Building Virtual Environments to Train and Test Human-like Agents,"Creating realistic and diverse virtual environments is crucial for training and evaluating embodied AI agents, especially those designed to interact with humans. However, the process of building such environments is often time-consuming and requires specialized expertise in 3D modeling and rendering. This paper introduces VECA, a novel toolkit designed to streamline the creation of Virtual Environments for Cognitive Agents. VECA provides a modular and extensible framework built upon a physically-based rendering engine and a library of pre-built assets, offering tools for procedural environment generation, interactive object placement, and customizable agent embodiment. Key features include a user-friendly interface for defining scene layouts, scripting agent behaviors, and generating diverse datasets with ground truth annotations. We demonstrate VECA's capabilities by constructing several virtual environments ranging from indoor scenes to urban landscapes, and training agents to perform tasks such as navigation and object manipulation. These environments facilitate the development of more robust and human-like AI agents capable of operating in complex and unpredictable real-world scenarios."
http://arxiv.org/abs/2209.02414v3,From Smart Sensing to Consciousness: An info-structural model of computational consciousness for non-interacting agents,"Computational consciousness research seeks to understand and replicate subjective experience in artificial systems. A significant hurdle lies in defining and operationalizing consciousness in non-biological, particularly non-interacting, agents lacking the complex embodied and social interactions often considered crucial for its emergence. This paper addresses the challenge of modeling consciousness in such isolated agents by introducing an info-structural framework based on smart sensing and hierarchical information processing. We propose a model where consciousness emerges from the agent's internal representation of its environment, constructed through multi-modal smart sensors and organized into a hierarchical structure reflecting the complexity of perceived information. The model quantifies consciousness as a function of the integrated information capacity within this structure, specifically considering the diversity and interdependence of sensory inputs and their subsequent processing. Simulation results demonstrate that agents with more sophisticated sensory processing and hierarchical organization exhibit higher levels of integrated information, correlating with increased complexity in their simulated behavior. This work offers a novel, computationally tractable approach to studying consciousness in isolated artificial agents, potentially bridging the gap between smart sensing and the emergence of subjective experience."
http://arxiv.org/abs/2211.04009v1,SOTIF Entropy: Online SOTIF Risk Quantification and Mitigation for Autonomous Driving,"Safety of the Intended Functionality (SOTIF) is crucial for the safe deployment of autonomous driving systems, requiring comprehensive risk assessment and mitigation strategies. Current SOTIF methodologies primarily focus on design-time analysis, often neglecting the dynamic and unpredictable nature of real-world operational environments. This paper addresses the challenge of online, real-time SOTIF risk quantification and mitigation. We introduce ""SOTIF Entropy,"" a novel framework that leverages information entropy, derived from onboard sensor data and environmental context, to quantify the uncertainty and potential hazards associated with the current driving scene. SOTIF Entropy is then used to dynamically adjust the autonomous vehicle's operational parameters, such as speed and following distance, to proactively mitigate identified risks and maintain acceptable safety levels. Experimental results, conducted in a high-fidelity driving simulator, demonstrate a significant reduction in the frequency and severity of hazardous events compared to baseline autonomous driving systems without online SOTIF risk mitigation. This real-time SOTIF risk management approach enhances the safety and robustness of autonomous driving systems in complex and uncertain environments."
http://arxiv.org/abs/2301.02983v1,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,"Generalized zero-shot logical reasoning over text remains a significant challenge due to the difficulty in effectively transferring knowledge from seen to unseen logical structures. Existing methods often struggle with accurately identifying and utilizing the semantic types of entities within textual premises, hindering their ability to perform accurate inference in novel scenarios. To address this, we introduce Mind Reasoning Manners (MRM), a novel framework that enhances type perception by explicitly modeling the reasoning process through a combination of a type-aware knowledge graph and a dynamic reasoning module. MRM first constructs a knowledge graph that captures the relationships between entity types and their associated properties. It then employs a dynamic reasoning module that iteratively refines type representations by propagating information along the knowledge graph, allowing the model to infer nuanced type information even for unseen entities. Experimental results on benchmark datasets demonstrate that MRM significantly outperforms existing zero-shot logical reasoning approaches, achieving state-of-the-art performance and exhibiting superior generalization capabilities across diverse logical structures. This enhanced type perception and reasoning framework provides a crucial step towards building more robust and adaptable AI systems capable of handling complex logical reasoning tasks in real-world scenarios."
http://arxiv.org/abs/2304.00002v2,Beyond Interpretable Benchmarks: Contextual Learning through Cognitive and Multimodal Perception,"Current computer vision benchmarks often prioritize isolated object recognition, neglecting the crucial role of contextual understanding inherent in human perception. This paper addresses the limitations of existing benchmarks by exploring contextual learning through the integration of cognitive and multimodal perception. We propose a novel framework that leverages cognitive architectures to simulate human-like reasoning and attention mechanisms, coupled with multimodal fusion techniques to integrate visual, auditory, and textual information. Specifically, we employ a neuro-symbolic approach to model contextual relationships and infer scene-level understanding, guiding the attention of a deep learning model towards relevant features in the multimodal input. Our experiments on a newly curated dataset containing complex, context-rich scenarios demonstrate significant improvements in scene understanding and reasoning compared to state-of-the-art unimodal and multimodal baselines. This work highlights the potential of cognitive-inspired multimodal perception for developing more robust and human-aligned computer vision systems capable of reasoning beyond superficial object recognition."
http://arxiv.org/abs/2310.08803v1,Advancing Perception in Artificial Intelligence through Principles of Cognitive Science,"Artificial intelligence has achieved remarkable progress in perception tasks, yet often lacks the robustness and generalizability observed in human cognition. Current AI perception systems primarily rely on data-driven approaches, neglecting the underlying cognitive principles that guide human perception. This paper addresses the problem of brittle and inflexible AI perception by integrating insights from cognitive science into the design of novel perception architectures. We propose a framework that incorporates principles of hierarchical processing, attention mechanisms, and contextual inference, mimicking the human cognitive architecture. Specifically, we introduce a biologically-inspired attention module that dynamically focuses computational resources on salient regions of the input, coupled with a hierarchical feature extraction network designed to mirror the ventral stream processing in the brain. Experimental results on benchmark datasets demonstrate that our cognitively-inspired approach significantly improves robustness to adversarial attacks and outperforms state-of-the-art models in few-shot learning scenarios. This work highlights the potential of leveraging cognitive science to build more robust, adaptable, and human-like artificial perception systems."
http://arxiv.org/abs/2311.15209v3,See and Think: Embodied Agent in Virtual Environment,"Embodied agents operating in virtual environments hold immense potential for developing general-purpose AI capable of perception, reasoning, and action. However, effectively integrating visual perception with high-level cognitive reasoning remains a significant challenge for these agents. This paper introduces a novel ""See and Think"" framework that tightly couples a deep reinforcement learning agent with a symbolic reasoning module for enhanced decision-making in complex, visually rich environments. Our method leverages a visual encoder to extract relevant features from the agent's observation, which are then translated into symbolic facts used by a planner to generate action sequences. The reinforcement learning agent learns to refine these plans and adapt to dynamic environments by incorporating feedback from both the planner and the environment. We evaluate our framework on a simulated household environment, demonstrating that it significantly outperforms end-to-end reinforcement learning approaches and ablated versions of our model in terms of task completion rate and planning efficiency. This work highlights the benefits of integrating symbolic reasoning with deep learning for creating more robust and interpretable embodied agents."
http://arxiv.org/abs/2312.06034v1,Modeling Uncertainty in Personalized Emotion Prediction with Normalizing Flows,"Accurate prediction of an individual's emotional response to stimuli is crucial for various applications, including personalized content recommendation and mental health monitoring. However, inherent subjectivity and individual differences introduce significant uncertainty in emotion prediction, which is often ignored by deterministic models. This work addresses the problem of modeling personalized uncertainty in emotion prediction by proposing a novel framework leveraging normalizing flows. Our approach, Personal Flow Network (PFN), learns a personalized latent emotion distribution conditioned on both stimulus features and individual user embeddings. PFN transforms a simple base distribution into a complex, user-specific emotion distribution through a series of invertible transformations, enabling both accurate point predictions and uncertainty quantification. Experiments on benchmark emotion recognition datasets demonstrate that PFN achieves comparable or superior predictive performance to state-of-the-art deterministic models while simultaneously providing well-calibrated uncertainty estimates. The ability to quantify and understand uncertainty in personalized emotion prediction has the potential to improve the reliability and trustworthiness of emotion-aware systems."
http://arxiv.org/abs/2312.09546v1,On a Functional Definition of Intelligence,"Artificial Intelligence (AI) research strives to create systems that exhibit intelligent behavior, yet a universally accepted definition of intelligence remains elusive. This ambiguity hinders progress in evaluating and comparing different AI approaches, often leading to subjective assessments of performance. This paper addresses the problem of defining intelligence in a functional and measurable way, independent of specific cognitive architectures or biological constraints. We propose a functional definition of intelligence based on an agent's ability to effectively achieve goals within a complex, dynamic environment. This definition is formalized through a novel metric that quantifies an agent's resourcefulness, adaptability, and efficiency in navigating unforeseen challenges and optimizing reward acquisition. We demonstrate the utility of our framework by evaluating the performance of diverse AI agents across a suite of simulated environments, revealing nuanced distinctions in their problem-solving capabilities and highlighting areas for improvement. This functional definition provides a practical and objective framework for evaluating and comparing AI systems, fostering more targeted research towards the development of truly intelligent machines."
http://arxiv.org/abs/2312.11935v1,Parameterized Decision-making with Multi-modal Perception for Autonomous Driving,"Autonomous driving necessitates robust decision-making in complex, dynamic environments, often requiring the integration of diverse sensor modalities. Existing decision-making frameworks often struggle to generalize across varying environmental conditions and driving styles due to their reliance on hand-crafted rules or limited adaptability. This paper addresses the challenge of creating a flexible and adaptable decision-making system that can effectively leverage multi-modal perception for autonomous driving. We propose a novel parameterized decision-making framework that learns a continuous, latent space representation of driving behaviors conditioned on multi-modal sensory inputs, including LiDAR point clouds, camera images, and vehicle telemetry. This parameterized space allows for fine-grained control over driving actions and enables adaptation to different driving styles and environmental contexts through learned parameter adjustments. Experimental results in a realistic driving simulator demonstrate that our approach outperforms rule-based and imitation learning baselines in terms of safety, comfort, and task completion rate, particularly in challenging scenarios with diverse traffic conditions. This work presents a significant step towards more adaptable and personalized autonomous driving systems capable of navigating complex real-world environments."
http://arxiv.org/abs/2402.04370v1,Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception,"Pedestrian crossing decisions are crucial for urban safety, yet the underlying cognitive processes remain incompletely understood. This paper addresses the problem of how pedestrians integrate noisy visual information about approaching vehicles to make safe and efficient crossing decisions. We propose a bounded optimal decision-making model that incorporates both perceptual uncertainty and cognitive constraints, such as limited planning horizon and risk aversion. Our model assumes pedestrians estimate vehicle time-to-arrival (TTA) with inherent perceptual noise and choose to cross only when the expected utility, considering the probability of collision and the cost of delay, exceeds a certain threshold. We validate our model using a large-scale dataset of real-world pedestrian crossing behaviors, demonstrating that it accurately predicts crossing decisions and captures individual differences in risk tolerance. The results show that perceptual noise significantly influences crossing decisions, and that individuals adopt strategies that balance safety and efficiency within their cognitive limitations. This framework provides a powerful tool for understanding and predicting pedestrian behavior, with implications for designing safer and more efficient urban environments."
http://arxiv.org/abs/2403.03017v1,OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,"Embodied instruction following (EIF) tasks challenge agents to navigate and interact with realistic environments based on natural language instructions, often leveraging Large Language Models (LLMs) for planning and reasoning. However, the intricate interplay between LLMs and other components within these agents makes it difficult to isolate the impact of individual modules on overall performance. This paper introduces OPEx, a novel framework for conducting a component-wise ablation study of LLM-centric agents in EIF. OPEx systematically replaces or modifies key components, such as the LLM planner, perception module, and execution module, with simplified alternatives or oracle information. We then evaluate the resulting agent variants across multiple EIF benchmarks, allowing us to quantify the contribution of each component to success. Our experiments reveal that while LLMs contribute significantly to high-level planning, the robustness of the perception module and the precision of the execution module are often critical bottlenecks limiting overall performance. OPEx provides valuable insights into the strengths and weaknesses of current LLM-centric EIF agents, paving the way for more targeted improvements and a deeper understanding of their inner workings."
http://arxiv.org/abs/2405.18510v1,"Improved Emotional Alignment of AI and Humans: Human Ratings of Emotions Expressed by Stable Diffusion v1, DALL-E 2, and DALL-E 3","Recent advancements in text-to-image (TTI) models have enabled the generation of photorealistic images from textual descriptions, including those evoking specific emotions. However, the degree to which AI-generated images reliably convey intended emotions to human observers remains an open question, hindering effective human-AI communication and creative collaboration. This study investigates the emotional alignment between AI and humans by evaluating how accurately human raters perceive emotions expressed in images generated by Stable Diffusion v1, DALL-E 2, and DALL-E 3. We prompted each model with a diverse set of emotion-laden text descriptions and collected human ratings on perceived emotion categories and intensity using a fine-grained emotion annotation scheme. We then compared the distribution of human-rated emotions with the intended emotions specified in the prompts. Our findings indicate significant variations in emotional alignment across different TTI models, with DALL-E 3 demonstrating a superior ability to generate images that consistently evoke the intended emotions in human observers compared to Stable Diffusion v1 and DALL-E 2. Specifically, DALL-E 3 exhibited higher accuracy in conveying complex emotions such as awe and serenity. These results highlight the importance of evaluating and improving the emotional expressiveness of TTI models to facilitate more intuitive and effective human-AI interaction."
http://arxiv.org/abs/2406.14132v2,Enhancing Monotonic Modeling with Spatio-Temporal Adaptive Awareness in Diverse Marketing,"Monotonic modeling, which enforces predefined order relationships between input features and the target variable, is crucial for interpretability and trust in marketing applications like customer lifetime value prediction and ad campaign optimization. However, existing monotonic models often struggle to effectively capture the complex spatio-temporal dynamics inherent in diverse marketing data, leading to suboptimal predictive performance. This paper introduces a novel approach, Spatio-Temporal Adaptive Awareness for Monotonic Modeling (STAAMM), which dynamically adjusts the monotonic constraints based on both spatial and temporal context. STAAMM utilizes a learnable attention mechanism to weigh the influence of neighboring data points in both feature space and time, allowing for flexible adaptation of the monotonic relationships. We demonstrate the efficacy of STAAMM on several real-world marketing datasets, showing significant improvements in prediction accuracy and model calibration compared to state-of-the-art monotonic and non-monotonic models. STAAMM offers a powerful and interpretable framework for enhancing monotonic modeling in diverse marketing scenarios, leading to more reliable and actionable insights."
http://arxiv.org/abs/2407.06088v1,Qualitative Event Perception: Leveraging Spatiotemporal Episodic Memory for Learning Combat in a Strategy Game,"In complex strategy games, understanding and reacting to emergent gameplay events is crucial for developing intelligent agents. However, directly learning from raw game states is challenging due to the high dimensionality and the delayed consequences of actions. We address the problem of learning combat strategies from a qualitative perspective, focusing on recognizing and reacting to meaningful spatiotemporal events within the game. Our approach leverages a novel spatiotemporal episodic memory architecture to store and retrieve sequences of qualitative state representations extracted from combat engagements. By encoding game states into qualitative descriptions based on relative positioning and unit health, we reduce the state space complexity. We then train a reinforcement learning agent to select actions based on retrieved episodic memories, allowing it to generalize across similar combat scenarios. Experimental results in a simplified StarCraft II environment demonstrate that our agent learns effective combat strategies and outperforms agents trained directly on raw game states or without episodic memory, particularly in unseen combat scenarios. This work highlights the potential of qualitative representations and episodic memory for enabling efficient and generalizable learning in complex, event-driven environments."
http://arxiv.org/abs/2408.00257v1,RoCo:Robust Collaborative Perception By Iterative Object Matching and Pose Adjustment,"Collaborative perception leverages the complementary viewpoints of multiple agents to enhance environmental understanding, particularly in challenging scenarios like occlusions or limited sensor range. However, accurately fusing information from different agents requires precise knowledge of their relative poses and robust object association across diverse observations, a task susceptible to noise and inaccuracies in pose estimation and object detection. We introduce RoCo, a robust collaborative perception framework that iteratively refines object matching and pose alignment. RoCo initiates with initial pose estimates and object detections from each agent. Subsequently, it alternates between two key steps: (1) establishing reliable object correspondences using a learned similarity metric and a robust matching algorithm resistant to outliers, and (2) refining agent poses by minimizing the discrepancy between corresponding object locations via a novel pose adjustment strategy that incorporates uncertainty estimates. Experiments on simulated and real-world datasets demonstrate that RoCo significantly improves object detection accuracy and pose estimation precision compared to existing collaborative perception methods, especially in environments with noisy sensor data and inaccurate initial poses. RoCo's iterative refinement strategy offers a more resilient and accurate approach to collaborative perception, facilitating safer and more reliable autonomous systems."
http://arxiv.org/abs/2410.04320v2,Channel-Aware Throughput Maximization for Cooperative Data Fusion in CAV,"Cooperative perception in connected and autonomous vehicles (CAV) leverages data fusion from multiple vehicles to enhance environmental awareness. However, wireless communication channels introduce significant variations in data transmission rates, directly impacting the overall throughput of cooperative perception systems. This paper addresses the problem of maximizing the effective throughput of fused data in a CAV network by intelligently selecting data sources and adapting transmission strategies based on channel conditions. We propose a novel channel-aware data fusion algorithm that dynamically prioritizes data streams from vehicles exhibiting superior channel quality and employs a reinforcement learning-based approach to optimize data compression and transmission power allocation. Our simulations, conducted using realistic channel models and CAV mobility patterns, demonstrate a significant improvement in the effective throughput of fused data, achieving up to a 30% increase compared to conventional data fusion methods that do not consider channel characteristics. This work provides a crucial step towards realizing reliable and efficient cooperative perception systems for enhanced safety and autonomy in CAV environments."
http://arxiv.org/abs/2501.10768v2,MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science,"Multi-modal reasoning, combining visual and textual information, is crucial for expert-level problem-solving, particularly in physical sciences where diagrams and equations are indispensable. However, existing multi-modal models often struggle with the complex relationships and abstract concepts inherent in these domains. We introduce Multi-modal Abstraction and Physical reasoning System (MAPS), a novel framework designed to enhance multi-modal reasoning in expert-level physical science scenarios. MAPS utilizes a hierarchical abstraction module to decompose complex diagrams into simpler, semantically meaningful components, followed by a physics-aware reasoning engine that leverages both visual and textual cues to infer physical quantities and relationships. This engine incorporates a symbolic solver and a neural module, allowing it to both perform precise calculations and reason about qualitative physics principles. Evaluated on a challenging dataset of physics problems requiring multi-modal reasoning, MAPS significantly outperforms state-of-the-art methods, achieving a 25% improvement in accuracy. MAPS demonstrates a significant step towards building AI systems capable of expert-level reasoning in complex scientific domains."
http://arxiv.org/abs/2502.19915v2,LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty,"Knowledge tracing (KT) aims to model student knowledge states over time, predicting their future performance on learning materials. Existing KT models often struggle to accurately assess the difficulty of questions, leading to suboptimal knowledge state estimation. This paper addresses the limitation of single-channel difficulty representation in KT by proposing a novel LLM-driven knowledge tracing framework that integrates dual-channel difficulty. Our method leverages Large Language Models (LLMs) to extract both intrinsic difficulty, reflecting the inherent complexity of the question content, and extrinsic difficulty, representing the observed student performance on the question. These difficulty representations are then fused within a deep KT model to provide a more comprehensive understanding of question difficulty and its impact on student learning. Experimental results on benchmark KT datasets demonstrate that our approach significantly outperforms state-of-the-art KT models, achieving improved prediction accuracy and enhanced knowledge state representation. This work provides a promising direction for developing more effective and reliable KT systems by incorporating richer, LLM-driven difficulty information."
http://arxiv.org/abs/2504.20464v2,A Survey on GUI Agents with Foundation Models Enhanced by Reinforcement Learning,"Graphical User Interface (GUI) agents automate tasks by interacting directly with software applications, offering potential for increased efficiency and accessibility. However, crafting robust GUI agents capable of generalizing across diverse applications and dynamically adapting to changing interfaces remains a significant challenge. This survey investigates the emerging paradigm of GUI agents leveraging the capabilities of Foundation Models (FMs), such as large language models (LLMs) and vision-language models (VLMs), enhanced by reinforcement learning (RL). We systematically review existing literature, categorizing approaches based on their FM architecture, RL algorithm, and training methodology. We analyze how FMs provide essential capabilities like visual understanding, language processing, and common-sense reasoning, while RL enables adaptive decision-making within the interactive GUI environment. Our analysis reveals that FM-RL agents demonstrate superior performance in complex task completion, improved generalization to unseen applications, and enhanced robustness to GUI variations compared to traditional methods. These advancements signify a substantial step towards creating truly autonomous and adaptable GUI agents, paving the way for more intuitive and efficient human-computer interaction."
http://arxiv.org/abs/2506.10753v1,Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering,"Counterfactual Question Answering (CQA) requires reasoning about hypothetical scenarios that deviate from observed reality, posing a significant challenge for current vision-language models. Existing approaches often rely on brute-force simulation, directly generating visual and textual representations for each counterfactual scenario, which can be computationally expensive and prone to generating implausible or irrelevant information. To address this, we propose a novel framework that leverages symbolic reasoning to orchestrate neural computation for CQA. Our method first parses the counterfactual question into a symbolic representation that explicitly identifies the relevant objects, attributes, and relationships involved in the hypothetical change. This symbolic representation then guides the neural computation, selectively activating and modifying only the necessary visual and textual features. Specifically, we use the symbolic representation to condition a learned ""scene editor"" that alters the original image and text to reflect the counterfactual scenario, minimizing unnecessary modifications. Experiments on multiple CQA datasets demonstrate that our approach achieves state-of-the-art performance while significantly reducing computational cost compared to simulation-heavy baselines. This highlights the potential of integrating symbolic reasoning and neural computation for more efficient and accurate counterfactual reasoning in vision-language tasks."
http://arxiv.org/abs/2506.15196v2,HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges,"Combinatorial optimization problems are ubiquitous across various scientific and engineering disciplines, yet solving them often requires specialized expertise and computationally intensive algorithms. This paper addresses the challenge of automating and improving the design of effective heuristics for complex combinatorial optimization problems. We introduce HeurAgenix, a novel framework that leverages large language models (LLMs) to generate, evaluate, and refine heuristic algorithms. HeurAgenix employs a prompt engineering strategy to guide the LLM in generating candidate heuristic steps, which are then evaluated on a set of problem instances using a custom-designed simulation environment. Feedback from the simulation is used to iteratively refine the LLM's heuristic generation process through a reinforcement learning mechanism. Empirical results on benchmark instances of the Traveling Salesperson Problem (TSP) and the Vehicle Routing Problem (VRP) demonstrate that HeurAgenix can discover heuristics that outperform hand-crafted algorithms and existing learning-based approaches in terms of solution quality and computational efficiency. This work demonstrates the potential of LLMs as powerful tools for automating the design of high-performance algorithms for complex optimization tasks."
http://arxiv.org/abs/2507.04464v1,Anomalous Decision Discovery using Inverse Reinforcement Learning,"Identifying anomalous behaviors in complex systems is crucial for safety and efficiency. However, defining anomalies a priori is often challenging, especially when based on decision-making processes. This paper addresses the problem of discovering anomalous decisions made by an agent operating within a complex environment without explicit knowledge of the reward function guiding the agent's behavior. We propose a novel approach that leverages Inverse Reinforcement Learning (IRL) to first infer the agent's underlying reward function from observed behavior, and then uses this learned reward function to evaluate the optimality of individual decisions. By comparing the expected return of the observed action with the expected return of the optimal action under the inferred reward, we identify decisions that deviate significantly from optimal behavior, thus flagging them as anomalous. We demonstrate the effectiveness of our approach on synthetic and real-world datasets, showing its ability to accurately identify anomalous decisions even in the presence of noisy and incomplete observations. Our method provides a powerful tool for understanding and improving decision-making in complex systems by highlighting deviations from expected behavior, enabling targeted interventions and system improvements."
http://arxiv.org/abs/2507.21872v3,MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors,"Generating and editing realistic driving scenarios is crucial for training and evaluating autonomous driving systems. However, existing methods often struggle with precise object manipulation and multimodal control in complex environments. This paper introduces MultiEditor, a novel framework for controllable multimodal object editing in driving scenarios, leveraging 3D Gaussian Splatting (3D-GS) priors. Our method learns a disentangled latent space representation of 3D-GS scenes, enabling precise object selection and manipulation through user-defined masks and multimodal conditioning (e.g., text prompts, reference images). We then utilize a diffusion model operating in this latent space to perform edits, ensuring scene consistency and realism. Experiments on the nuScenes dataset demonstrate that MultiEditor allows for intuitive and high-quality object editing, including modifications to object appearance, pose, and even the addition of new objects, outperforming existing state-of-the-art methods in terms of both visual fidelity and editing accuracy. MultiEditor provides a powerful tool for creating diverse and controllable driving scenarios, significantly advancing the development and testing of autonomous driving technologies."
http://arxiv.org/abs/0811.1711v1,Artificial Intelligence Techniques for Steam Generator Modelling,"Steam generators are critical components in power plants, responsible for converting water into steam to drive turbines. Accurate and efficient modeling of steam generator dynamics is essential for optimizing plant operations, ensuring safety, and enabling predictive maintenance. This paper addresses the challenge of developing a robust and accurate model of a steam generator using artificial intelligence (AI) techniques. We propose a hybrid approach that combines physics-informed neural networks (PINNs) with recurrent neural networks (RNNs). PINNs leverage known physical laws governing steam generator behavior to constrain the training process, while RNNs capture the temporal dependencies inherent in the system's dynamics. The model is trained and validated using historical operational data from a real-world power plant. Results demonstrate that the proposed AI-driven model achieves significantly improved accuracy in predicting key parameters, such as steam temperature and pressure, compared to traditional empirical models, while also offering enhanced computational efficiency. This work provides a promising pathway for developing advanced monitoring and control strategies for steam generators, leading to improved power plant performance and reliability."
http://arxiv.org/abs/1101.2378v1,Extracting Features from Ratings: The Role of Factor Models,"Recommender systems often rely on user-item rating matrices to predict preferences and personalize recommendations. However, these matrices are typically sparse, hindering the direct application of many computer vision techniques for feature extraction. This paper addresses the challenge of extracting meaningful feature representations from sparse rating data by leveraging the power of factor models. We propose a novel approach that first decomposes the rating matrix using matrix factorization techniques, such as Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF), to obtain latent user and item embeddings. These embeddings are then treated as feature vectors, enabling the application of computer vision methods like clustering and dimensionality reduction to uncover hidden patterns and relationships within the user-item interaction space. Our experiments on benchmark datasets demonstrate that features extracted from factorized rating matrices significantly improve the performance of downstream tasks, including user profiling and item categorization, compared to using raw rating data. This work highlights the potential of combining factor models with computer vision techniques for enhanced analysis and understanding of recommender system data."
http://arxiv.org/abs/1209.4290v1,Cognitive Bias for Universal Algorithmic Intelligence,"Algorithmic intelligence is increasingly prevalent across diverse domains, yet its inherent biases can lead to unfair or undesirable outcomes. This paper addresses the problem of how cognitive biases, traditionally studied in human psychology, manifest and can be mitigated in the pursuit of universal algorithmic intelligence. We propose a novel framework that integrates cognitive bias detection modules into machine learning pipelines, enabling the identification and quantification of biases such as anchoring, confirmation bias, and availability heuristic within datasets and model architectures. This framework leverages explainable AI techniques to expose the decision-making processes contributing to biased outputs and employs adversarial training strategies to improve model robustness against these biases. Experiments on benchmark datasets across image classification and natural language processing tasks demonstrate a significant reduction in bias metrics, such as disparate impact and equal opportunity difference, while maintaining or improving overall accuracy. These findings highlight the importance of explicitly accounting for cognitive biases in the design and evaluation of algorithms striving for universal intelligence and societal benefit."
http://arxiv.org/abs/1608.04672v1,Informal Physical Reasoning Processes,"Humans possess an intuitive understanding of physics, enabling them to predict and reason about the physical world with remarkable efficiency, often without formal training. However, replicating this informal physical reasoning in artificial intelligence remains a significant challenge. This paper addresses the problem of developing a computational model capable of mimicking human-like physical reasoning, specifically focusing on qualitative, rather than quantitative, predictions about object interactions. We propose a novel framework that integrates visual perception with a rule-based inference engine, utilizing a learned representation of object properties and relationships to simulate intuitive physics principles, such as stability and support. The system learns to extract relevant features from visual input and then applies a set of pre-defined rules to infer the likelihood of different physical outcomes. Our experiments on a variety of scenes involving object arrangements demonstrate that our model achieves a high degree of accuracy in predicting stability and collapse events, outperforming several baseline methods. This work offers a step towards creating AI systems that can reason about the physical world in a manner more aligned with human intuition, with potential applications in robotics, simulation, and augmented reality."
http://arxiv.org/abs/1706.00355v1,Grounding Symbols in Multi-Modal Instructions,"Humans seamlessly integrate language and perception to follow instructions, often relying on symbolic references like ""the red block"" or ""the one on the left."" Teaching robots to understand such grounded symbols remains a significant challenge in multi-modal instruction following. This paper addresses the problem of accurately grounding symbolic references within multi-modal instructions, where the symbols refer to objects or regions in the visual scene. We introduce a novel attention-based neural network architecture that jointly processes visual and textual information to predict the grounding of symbols. Our method incorporates a dynamic symbol embedding module that adapts the representation of each symbol based on the context provided by both the instruction and the visual scene. Furthermore, we leverage a contrastive learning objective to encourage the model to learn discriminative representations for correct and incorrect grounding candidates. Experiments on a synthetic dataset and a real-world robotic manipulation dataset demonstrate that our approach significantly outperforms existing methods in grounding accuracy, particularly in scenarios with complex instructions and visually similar objects. This improved grounding capability enables robots to more effectively understand and execute multi-modal instructions, bridging the gap between human communication and robotic action."
http://arxiv.org/abs/1302.6442v1,A Modelling Approach Based on Fuzzy Agents,"Agent-based modelling offers a powerful paradigm for simulating complex systems by representing individual entities and their interactions. However, traditional agent models often struggle to represent the inherent uncertainty and imprecision present in real-world environments and agent decision-making. This paper addresses the challenge of incorporating fuzzy logic within an agent-based framework to better model these uncertainties. We propose a novel modelling approach based on fuzzy agents, where each agent's behavior is governed by fuzzy rules, allowing for nuanced responses to ambiguous situations and gradual transitions between behavioral states. This framework is implemented and tested in a simulated environment mimicking a dynamic resource allocation problem. The results demonstrate that the fuzzy agent model outperforms traditional rule-based agents in terms of adaptability, robustness to noisy data, and overall system efficiency. This approach offers a more realistic and flexible method for modelling complex systems with inherent uncertainties, applicable to a wide range of domains including robotics, environmental modelling, and social simulations."
http://arxiv.org/abs/1805.04749v1,A Cognitive Approach to Real-time Rescheduling using SOAR-RL,"Real-time rescheduling is a critical capability in dynamic environments, requiring rapid adaptation to unexpected events while maintaining operational efficiency. Traditional rescheduling approaches often struggle with the complexity of integrating diverse constraints and adapting to unforeseen disruptions in a timely manner. This paper addresses the challenge of developing a cognitive architecture for real-time rescheduling that can effectively balance competing objectives and react to dynamic changes. We propose a novel approach, SOAR-RL, which integrates the SOAR cognitive architecture with reinforcement learning (RL) to enable adaptive and efficient rescheduling. SOAR provides a symbolic reasoning framework for representing constraints and goals, while RL learns optimal rescheduling policies through interaction with the environment. Experimental results demonstrate that SOAR-RL significantly outperforms rule-based and standard RL approaches in terms of solution quality and response time, especially when faced with complex and unpredictable disruptions. This research demonstrates the potential of cognitive architectures to enhance real-time decision-making in dynamic and complex scheduling problems."
http://arxiv.org/abs/1512.04976v1,Conditions for Normative Decision Making at the Fire Ground,"Fireground decision-making is a complex cognitive process performed under extreme stress, time pressure, and uncertainty, often leading to deviations from normative decision-making models. This paper investigates the conditions that influence firefighters' adherence to, or departure from, normative decision-making strategies during active fire suppression. We propose a novel analytical framework that integrates cognitive task analysis with contextual factors extracted from fireground incident reports and firefighter interviews. This framework facilitates the identification of specific environmental and operational characteristics, such as fire intensity, structural complexity, and communication clarity, that correlate with the observed decision-making processes. Our analysis reveals that adherence to normative decision-making is significantly compromised by high workload, ambiguous information, and perceived discrepancies between training and real-world scenarios. Understanding these conditions is crucial for developing targeted training programs and decision support tools that promote effective and safe fireground operations."
http://arxiv.org/abs/1512.07943v1,Toward a Research Agenda in Adversarial Reasoning: Computational Approaches to Anticipating the Opponent's Intent and Actions,"Adversarial reasoning, the ability to anticipate and counteract an opponent's strategies, is crucial in various domains, including security, robotics, and game playing. However, current computational models often struggle to effectively reason about intelligent adversaries with complex goals and capabilities. This paper addresses the challenge of developing a comprehensive research agenda for adversarial reasoning, focusing on computational approaches to predict an opponent's intent and actions. We propose a multi-faceted approach that integrates game-theoretic models, inverse reinforcement learning, and predictive planning algorithms. Specifically, we leverage game theory to formalize adversarial interactions, employ inverse reinforcement learning to infer the opponent's reward function from observed behavior, and utilize predictive planning to forecast their future actions based on the inferred intent. We demonstrate the efficacy of this integrated approach through simulations in a simulated security game, showing improved accuracy in predicting adversarial behaviors compared to baseline methods. This work lays the groundwork for developing more robust and intelligent systems capable of proactively addressing adversarial threats in complex, real-world scenarios."
http://arxiv.org/abs/1903.07008v4,Leveling the Playing Field -- Fairness in AI Versus Human Game Benchmarks,"Artificial intelligence (AI) agents have surpassed human performance in numerous game environments, leading to their widespread use as benchmarks. However, direct comparisons between AI and human performance often overlook inherent advantages afforded to AI, such as perfect information, faster reaction times, and tireless consistency, potentially skewing the perceived fairness of these benchmarks. This paper addresses the problem of fairly comparing AI and human performance in game environments by accounting for these inherent advantages. We propose a methodology to ""level the playing field"" by introducing constraints on AI agents that mimic human limitations, including reaction time delays, perceptual noise, and fatigue-induced performance degradation. We then evaluate AI agents in StarCraft II and Dota 2 under these constraints, comparing their performance against human players at various skill levels. Our results demonstrate that after accounting for human limitations, the performance gap between AI and humans significantly narrows, and in some cases, human players outperform constrained AI agents. This work highlights the importance of considering fairness when evaluating AI performance against human benchmarks and provides a framework for developing more equitable and informative comparisons."
http://arxiv.org/abs/2106.02578v1,"Alexa, Google, Siri: What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants","Conversational assistants (CAs) like Alexa, Google Assistant, and Siri are increasingly integrated into daily life, yet their design often reflects and reinforces gender stereotypes. This paper investigates how the perceived gender of CAs, influenced by factors like voice, name, and pronoun usage (or lack thereof), impacts user anthropomorphism and subsequent interactions. We conducted a mixed-methods study, manipulating pronoun usage (he/she/they/none) and voice characteristics across different CA names. Participants interacted with the CAs in simulated scenarios, and their perceptions of gender, trustworthiness, intelligence, and overall user experience were measured through surveys and qualitative feedback. Our findings reveal that explicitly assigned pronouns, particularly ""she,"" significantly increase the perception of CAs as female and, surprisingly, enhance feelings of trustworthiness, while the use of ""they"" or no pronouns leads to perceptions of neutrality but can also decrease perceived intelligence. This research highlights the critical role of design choices in shaping user perceptions of CAs and has implications for creating more equitable and inclusive technologies."
http://arxiv.org/abs/1409.1170v2,Hybrid Systems Knowledge Representation Using Modelling Environment System Techniques Artificial Intelligence,"Hybrid systems, integrating continuous dynamics with discrete events, present a significant challenge for knowledge representation and reasoning in complex, real-world applications. Existing approaches often struggle to effectively capture the intricate interplay between these different modalities, hindering the development of robust and adaptable AI systems. This paper addresses the problem of effectively representing and reasoning about hybrid systems by proposing a novel knowledge representation framework based on Modelling Environment System Techniques (MEST) integrated with Artificial Intelligence (AI) methodologies. Our approach leverages the hierarchical and modular structure of MEST to represent the continuous dynamics and discrete events of hybrid systems, while employing AI techniques such as knowledge graphs and rule-based reasoning to manage complexity and enable automated reasoning. We demonstrate the effectiveness of our framework through a case study involving the intelligent control of a hybrid manufacturing system, showing significant improvements in system adaptability and fault diagnosis compared to traditional methods. This work provides a foundation for building more intelligent and resilient hybrid systems, facilitating their deployment in diverse domains."
http://arxiv.org/abs/2301.10571v1,Leveraging Planning Landmarks for Hybrid Online Goal Recognition,"Goal recognition aims to infer an agent's intended goal from observed behavior, a critical capability for effective human-robot interaction. Online goal recognition, which reasons about goals in real-time as observations arrive, presents a significant challenge, particularly when dealing with complex, partially observable environments. This paper addresses the problem of improving the accuracy and efficiency of online goal recognition by leveraging planning landmarks. We propose a hybrid approach that combines a probabilistic Bayesian framework with symbolic planning representations. Specifically, we extract landmark information from offline planning and integrate it into the observation likelihood calculation within a Hidden Markov Model (HMM). This landmark-augmented HMM dynamically updates goal probabilities based on the agent's progress towards achieving crucial intermediate states. Experimental results on benchmark goal recognition datasets demonstrate that our approach significantly outperforms state-of-the-art online methods, especially in scenarios with noisy or incomplete observations. This integrated framework provides a robust and interpretable solution for real-time goal understanding in complex environments, bridging the gap between probabilistic inference and symbolic reasoning."
http://arxiv.org/abs/2307.07876v2,Real-time goal recognition using approximations in Euclidean space,"Goal recognition, the task of inferring an agent's intended destination from observed behavior, is crucial for effective human-robot interaction and autonomous navigation. However, accurately inferring goals in real-time remains challenging, particularly in complex environments with numerous potential goals and noisy observations. This paper addresses the computational bottleneck inherent in probabilistic goal recognition approaches that rely on computationally expensive shortest-path calculations within graph-based representations of the environment. We propose a novel method that approximates shortest-path distances using Euclidean distance calculations, combined with learned correction factors to account for obstacles and environment structure. Our approach uses a small set of strategically placed landmark nodes to efficiently estimate path costs, significantly reducing the computational burden of goal likelihood estimation. Experiments on simulated and real-world datasets demonstrate that our method achieves comparable goal recognition accuracy to traditional graph-based methods, while providing a speedup of up to two orders of magnitude. This enables real-time goal recognition for applications requiring rapid response and adaptation to changing agent behavior."
http://arxiv.org/abs/1103.3420v1,Extraction of handwritten areas from colored image of bank checks by an hybrid method,"Bank check processing relies heavily on accurate extraction of handwritten information, but colored backgrounds and diverse layouts pose significant challenges. This paper addresses the problem of reliably extracting handwritten regions from colored bank check images, a crucial step for subsequent tasks like amount recognition and signature verification. We propose a hybrid method combining color-based segmentation with texture analysis and morphological operations. Initially, the color image is transformed into different color spaces, and Otsu's thresholding is applied to isolate potential handwritten regions. Subsequently, a Gabor filter bank is employed to extract texture features, which are then used to refine the segmentation by differentiating between handwritten strokes and background patterns. Finally, morphological operations are performed to remove noise and connect fragmented handwritten components. Experimental results on a dataset of real bank check images demonstrate that our method achieves a significantly higher F1-score compared to existing techniques, particularly in cases with complex backgrounds and overlapping printed elements. The proposed approach offers a robust and effective solution for handwritten area extraction, improving the accuracy and efficiency of automated check processing systems."
http://arxiv.org/abs/1209.4535v1,Application of Fuzzy Mathematics to Speech-to-Text Conversion by Elimination of Paralinguistic Content,"Speech-to-text (STT) systems are increasingly prevalent, but their accuracy can be significantly impacted by paralinguistic information embedded within speech signals, such as emotion, accent, and speaking style. This paper addresses the challenge of improving STT accuracy by filtering out paralinguistic content from the acoustic signal before transcription. We propose a novel approach leveraging fuzzy mathematics to model and subsequently suppress paralinguistic features. Specifically, we employ fuzzy clustering to identify acoustic features representative of paralinguistic cues and then utilize fuzzy inference systems to dynamically adjust the spectral components associated with these cues. The resulting modified speech signal, with reduced paralinguistic information, is then fed into a standard STT system. Experimental results on benchmark datasets demonstrate a significant reduction in word error rate (WER) compared to the baseline STT system and other state-of-the-art paralinguistic removal techniques. This work offers a promising direction for enhancing the robustness and accuracy of STT systems by effectively mitigating the negative impact of paralinguistic variations."
http://arxiv.org/abs/2407.09103v1,DANIEL: A fast Document Attention Network for Information Extraction and Labelling of handwritten documents,"Information Extraction (IE) from handwritten documents is a challenging task due to the variability in writing styles and the noisy nature of scanned images. Existing deep learning approaches often rely on computationally expensive recurrent neural networks or transformers to capture long-range dependencies within the document, hindering their applicability in resource-constrained environments. This paper introduces DANIEL, a fast Document Attention Network for IE and labelling of handwritten documents. DANIEL leverages a novel combination of lightweight convolutional feature extractors and a sparse attention mechanism, specifically designed to efficiently model contextual relationships between text regions. The sparse attention focuses on relevant document segments, reducing computational complexity while preserving accuracy. Experimental results on benchmark datasets for handwritten document IE demonstrate that DANIEL achieves comparable or superior performance to state-of-the-art methods, while significantly reducing inference time and memory footprint. DANIEL offers a practical solution for deploying IE systems on handwritten documents in real-world applications with limited computational resources."
http://arxiv.org/abs/1307.1388v1,Introducing Memory and Association Mechanism into a Biologically Inspired Visual Model,"Biologically inspired visual models have shown promise in replicating aspects of human visual processing, but often lack the ability to retain and associate past experiences effectively. This limitation hinders their performance in complex tasks requiring contextual understanding and recognition of temporally correlated events. To address this, we introduce a novel memory and association mechanism into a hierarchical biologically inspired visual model. The proposed mechanism incorporates a recurrent neural network module at each level of the hierarchy, enabling the model to encode and retrieve past activations. Furthermore, an associative learning rule is implemented to strengthen connections between co-occurring patterns across different levels, allowing the model to learn relationships between visual features and their temporal context. Experiments on dynamic texture recognition and video classification tasks demonstrate that our augmented model significantly outperforms the baseline model and achieves competitive results compared to state-of-the-art methods. These findings highlight the importance of incorporating memory and association mechanisms into biologically inspired visual models for improved performance and more human-like visual understanding."
http://arxiv.org/abs/1301.6700v1,A New Model of Plan Recognition,"Plan recognition, the task of inferring an agent's underlying intentions from observed actions, is crucial for human-robot interaction and autonomous systems. Existing plan recognition models often struggle with noisy observations and the inherent ambiguity of human behavior, leading to inaccurate inferences and suboptimal system performance. This paper introduces a novel probabilistic model for plan recognition, termed Hierarchical Bayesian Plan Network (HBPN), which leverages a hierarchical representation of plans and incorporates contextual information to improve robustness and accuracy. HBPN employs a Bayesian framework to model the relationships between observed actions, plan steps, and overall goals, while also integrating contextual cues such as object affordances and environmental constraints. We demonstrate the effectiveness of HBPN on a benchmark dataset of human activity recognition, showing a significant improvement in plan recognition accuracy compared to state-of-the-art methods, particularly in scenarios with incomplete or noisy observations. The proposed model offers a more robust and adaptable approach to plan recognition, facilitating more natural and effective human-agent collaboration."
http://arxiv.org/abs/1605.05807v2,"Heuristics for Planning, Plan Recognition and Parsing","Planning, plan recognition, and parsing are fundamental cognitive tasks with applications spanning robotics, human-computer interaction, and natural language understanding. These tasks often involve searching through vast combinatorial spaces, making efficient algorithms crucial. This paper addresses the challenge of improving the efficiency of algorithms for planning, plan recognition, and parsing in complex domains. We propose a novel framework that leverages domain-specific heuristics, learned from data and expert knowledge, to guide search and prune irrelevant branches. Specifically, we introduce a hybrid approach integrating symbolic planning with learned heuristic functions and probabilistic parsing models. Our experiments on benchmark planning domains and activity recognition datasets demonstrate significant improvements in computational efficiency, achieving speedups of up to an order of magnitude compared to state-of-the-art methods while maintaining accuracy. This research provides a practical approach to scaling planning, plan recognition, and parsing algorithms to more complex and realistic scenarios."
http://arxiv.org/abs/2011.01832v1,Goal recognition via model-based and model-free techniques,"Goal recognition, the task of inferring an agent's intended goal from observed actions, is crucial for effective human-robot interaction and proactive assistance. This paper addresses the challenge of robust and adaptable goal recognition in dynamic environments where agents may deviate from optimal behavior. We propose a hybrid approach that combines model-based planning with model-free reinforcement learning. The model-based component leverages a learned transition model to predict future states and evaluate the likelihood of different goal hypotheses, while the model-free component employs an inverse reinforcement learning (IRL) framework to learn a reward function directly from observed trajectories, capturing complex and potentially non-rational agent behaviors. Experiments conducted in simulated grid-world environments and a more complex blocks-world domain demonstrate that our hybrid approach outperforms both purely model-based and purely model-free methods, achieving higher accuracy and robustness to noisy observations and suboptimal agent behavior. The proposed method offers a significant advancement in goal recognition by effectively integrating the strengths of both planning and learning-based techniques, leading to more reliable and adaptable systems."
http://arxiv.org/abs/1709.09839v1,Heuristic Online Goal Recognition in Continuous Domains,"Goal recognition is a fundamental problem in artificial intelligence, enabling agents to understand and anticipate the intentions of others based on observed behavior. Existing goal recognition techniques often struggle with the complexities of continuous domains, where actions and states are not discrete and enumerable. This paper addresses the challenge of online goal recognition in continuous domains by developing a novel heuristic approach that balances computational efficiency with accuracy. Our method employs a receding horizon strategy, coupled with a set of learned heuristics that approximate the cost-to-go for different goal hypotheses. These heuristics are trained offline using supervised learning on simulated trajectories, allowing for rapid online evaluation of goal likelihood given observed actions. We demonstrate the effectiveness of our approach in simulated continuous navigation environments, showing significant improvements in recognition accuracy and speed compared to traditional planning-based methods. This work provides a practical and scalable solution for online goal recognition in complex, real-world scenarios."
http://arxiv.org/abs/2209.04189v1,Conversion of Acoustic Signal (Speech) Into Text By Digital Filter using Natural Language Processing,"Automatic Speech Recognition (ASR) is a long-standing challenge with numerous applications, yet remains imperfect due to variations in accents, background noise, and speaking styles. This paper addresses the problem of improving the accuracy and robustness of ASR systems, particularly in noisy environments. We propose a novel ASR pipeline that integrates digital filtering techniques with Natural Language Processing (NLP) methods. Our approach utilizes a cascade of digital filters, optimized using a genetic algorithm, to pre-process the acoustic signal, reducing noise and enhancing speech features. Subsequently, the filtered signal is processed by a deep learning-based acoustic model, and the resulting transcriptions are refined using an NLP-based language model incorporating contextual information. Experimental results on the LibriSpeech dataset demonstrate a significant reduction in word error rate (WER) compared to baseline ASR systems, especially in the presence of additive noise. This integrated approach offers a pathway towards more reliable and accurate speech-to-text conversion, paving the way for improved human-computer interaction and accessibility applications."
http://arxiv.org/abs/1904.11739v2,Landmark-Based Approaches for Goal Recognition as Planning,"Goal recognition as planning (GRAP) aims to infer the most likely goal of an agent by framing the agent's observed behavior as (partially) optimal plan execution. Existing GRAP approaches often struggle with scalability and the need for complete domain knowledge. This paper addresses the challenge of improving the efficiency and robustness of GRAP by leveraging landmark-based heuristics. We propose a novel landmark-based GRAP framework that utilizes landmarks, which are necessary subgoals for achieving a set of potential goals, to constrain the search space and guide the planning process. Our method incorporates landmark extraction and goal-conditioned planning to efficiently evaluate the likelihood of different goal hypotheses. We evaluate our approach on benchmark domains, demonstrating significant improvements in computational efficiency compared to state-of-the-art GRAP algorithms, while maintaining comparable accuracy. This landmark-based framework provides a more scalable and practical solution for goal recognition in complex, real-world scenarios."
http://arxiv.org/abs/1804.05917v1,Heuristic Approaches for Goal Recognition in Incomplete Domain Models,"Goal recognition, the task of inferring an agent's intended goal from observed actions, is crucial for effective human-robot interaction and automated planning. However, real-world applications often involve incomplete domain models, where the available knowledge about actions, preconditions, and effects is limited or uncertain, leading to inaccurate goal predictions. This paper addresses the challenge of goal recognition in such incomplete domain models by introducing novel heuristic approaches that leverage partial knowledge and observed behavior to improve inference accuracy. We propose two complementary methods: a heuristic search algorithm that explores the space of possible goal explanations by prioritizing likely action sequences based on observed frequencies and action similarity, and a knowledge-based refinement strategy that iteratively updates the domain model by incorporating observed actions and their inferred effects. Empirical evaluation on benchmark datasets, modified to simulate domain incompleteness, demonstrates that our approaches significantly outperform existing state-of-the-art goal recognition techniques, achieving higher accuracy and robustness in identifying the correct goal despite limited domain knowledge. These findings highlight the potential of heuristic methods to enhance goal recognition in complex and uncertain environments, paving the way for more adaptable and reliable AI systems."
http://arxiv.org/abs/1802.04086v1,The Complex Event Recognition Group,"Complex Event Recognition (CER) aims to identify high-level events composed of simpler activities and relationships within streaming data. However, existing CER systems often struggle with the inherent ambiguity and variability present in real-world scenarios, particularly when dealing with multiple, interacting entities performing diverse actions. This paper introduces the Complex Event Recognition Group (CERG), a novel framework designed to enhance robustness and adaptability in CER by explicitly modeling entity groupings and their collective behaviors. CERG utilizes a hierarchical graph structure, where nodes represent individual entities or groups of entities, and edges represent relationships between them. A novel group-aware reasoning mechanism propagates evidence across the graph, enabling the system to infer complex events based on both individual actions and the collective behavior of related entities. Experimental results on synthetic and real-world datasets demonstrate that CERG achieves significantly improved accuracy and robustness compared to state-of-the-art CER methods, particularly in scenarios with occlusions, noisy sensor data, and complex inter-entity dependencies. The CERG framework offers a powerful and flexible approach to understanding complex activities in dynamic environments, paving the way for more reliable and insightful event understanding systems."
http://arxiv.org/abs/2004.13482v1,HAPRec: Hybrid Activity and Plan Recognizer,"Human activity recognition (HAR) and plan recognition are crucial for understanding complex human behaviors in various applications. However, existing methods often treat these tasks separately, neglecting the inherent hierarchical relationship between activities and plans. We address the problem of simultaneously recognizing activities and plans in a unified framework, enabling a more comprehensive understanding of human behavior. We propose HAPRec, a Hybrid Activity and Plan Recognizer, which leverages a hierarchical recurrent neural network architecture. HAPRec first encodes low-level sensor data into activity embeddings using a recurrent layer. These embeddings are then fed into a higher-level recurrent layer that infers the underlying plan. Furthermore, we introduce a novel attention mechanism that allows the model to dynamically focus on the most relevant activities for plan recognition. Experimental results on benchmark datasets demonstrate that HAPRec achieves state-of-the-art performance in both activity and plan recognition tasks, outperforming existing methods that address these tasks independently. This unified approach to activity and plan recognition provides a more robust and interpretable understanding of human behavior, paving the way for more intelligent and context-aware systems."
http://arxiv.org/abs/1111.2763v1,8-Valent Fuzzy Logic for Iris Recognition and Biometry,"Iris recognition is a widely used biometric authentication technique, relying on the unique and complex patterns of the iris. However, traditional iris recognition systems often struggle with variations in image quality, illumination, and noise, leading to decreased accuracy and robustness. This paper addresses the limitations of conventional binary or multi-valued logic in representing the inherent uncertainty and vagueness within iris biometric data. We propose an innovative 8-Valent Fuzzy Logic (8VFL) framework for iris recognition, replacing the traditional binarization of iris codes with a richer representation that captures varying degrees of membership. Our method involves encoding iris texture features using Gabor filters and then mapping the filter responses onto eight distinct fuzzy sets, each representing a different level of feature activation. The resulting fuzzy iris code is then compared using a modified Hamming distance metric adapted for fuzzy sets. Experimental results on benchmark iris datasets demonstrate that the proposed 8VFL approach significantly improves recognition accuracy and robustness compared to conventional binary iris code matching, particularly in the presence of noise and off-angle images. This novel fuzzy logic-based approach offers a more nuanced and robust representation of iris biometric data, leading to improved performance and reliability in real-world iris recognition systems."
http://arxiv.org/abs/1703.00838v1,SLIM: Semi-Lazy Inference Mechanism for Plan Recognition,"Plan recognition, the task of inferring an agent's underlying goals and intentions from observed actions, is crucial for effective human-robot interaction. However, existing plan recognition systems often suffer from computational bottlenecks due to either exhaustive search or reliance on fully pre-computed knowledge, limiting their applicability in dynamic and complex environments. This paper introduces SLIM, a Semi-Lazy Inference Mechanism for plan recognition, which strategically balances proactive and reactive reasoning to optimize inference efficiency. SLIM employs a hierarchical plan library and dynamically expands only the necessary plan branches based on incoming observations, leveraging a cost-benefit analysis to determine when to pre-compute likely plan continuations. Experiments on benchmark plan recognition datasets demonstrate that SLIM achieves comparable accuracy to state-of-the-art methods while significantly reducing computation time, especially in scenarios with partial or noisy observations. SLIM's adaptive inference strategy provides a more efficient and scalable approach to plan recognition, enabling more responsive and intelligent agents."
http://arxiv.org/abs/1808.05249v2,LSTM-Based Goal Recognition in Latent Space,"Goal recognition, the task of inferring an agent's intended goal from observed actions, is crucial for effective human-robot interaction and autonomous system planning. However, real-world observations are often noisy, incomplete, and high-dimensional, making accurate and robust goal prediction challenging. This paper addresses the problem of goal recognition from sequential observations by learning a latent representation of the observed actions and leveraging temporal dependencies for improved accuracy. We propose an LSTM-based architecture that first encodes the observed action sequence into a low-dimensional latent space using a variational autoencoder (VAE). An LSTM network then processes this latent representation to predict the agent's goal. We evaluate our method on both synthetic and real-world datasets, demonstrating superior performance compared to state-of-the-art methods, particularly in scenarios with partial and noisy observations. The learned latent space provides a compact and informative representation of the agent's behavior, enabling more robust and accurate goal recognition."
http://arxiv.org/abs/2102.11791v1,Inferring Agents Preferences as Priors for Probabilistic Goal Recognition,"Goal recognition aims to infer the intended goal of an agent based on observed behavior. A key challenge lies in effectively incorporating prior knowledge about agent preferences to improve inference accuracy and efficiency, especially in complex and ambiguous scenarios. This paper addresses the problem of learning and representing agent preferences as priors for probabilistic goal recognition. We propose a novel framework that leverages inverse reinforcement learning (IRL) to infer a distribution over reward functions from demonstrations of agent behavior, effectively capturing agent preferences. This learned reward distribution is then used to construct a prior distribution over possible goals, which is integrated into a Bayesian goal recognition model. We demonstrate, through extensive experiments on simulated and real-world datasets, that our approach significantly improves goal recognition accuracy and robustness compared to methods using uniform or hand-crafted priors, particularly when dealing with noisy or incomplete observations. By automatically learning and incorporating agent preferences, our method provides a more principled and effective approach to probabilistic goal recognition, enabling more reliable and interpretable agent behavior understanding."
http://arxiv.org/abs/1304.1492v1,Map Learning with Indistinguishable Locations,"Simultaneous Localization and Mapping (SLAM) relies on the ability to distinguish locations to create consistent maps. However, many environments contain perceptual aliasing, where different locations appear indistinguishable to the robot's sensors, leading to map corruption and localization failure. This paper addresses the problem of learning accurate maps in environments with indistinguishable locations, focusing on mitigating the impact of perceptual aliasing on map consistency. We propose a novel approach that combines a learned similarity metric with a probabilistic mapping framework. The learned metric, trained using contrastive learning, estimates the likelihood that two observations originate from the same location, even if they appear visually similar. This likelihood is then integrated into a Bayesian filter to update the map and robot pose, allowing the system to maintain multiple hypotheses and recover from incorrect loop closures caused by perceptual aliasing. Experiments conducted in simulated and real-world environments with significant perceptual aliasing demonstrate that our approach significantly improves map accuracy and localization robustness compared to traditional SLAM methods. The proposed method offers a practical solution for enabling reliable robotic navigation in challenging, visually ambiguous environments."
http://arxiv.org/abs/2005.02986v1,The More the Merrier?! Evaluating the Effect of Landmark Extraction Algorithms on Landmark-Based Goal Recognition,"Landmark-based goal recognition leverages identifiable events within an environment to infer an agent's intended goal. The accuracy of this approach heavily relies on the quality and relevance of extracted landmarks, yet the impact of utilizing multiple, potentially redundant, landmark extraction algorithms remains largely unexplored. This paper addresses the question of whether combining landmarks extracted by different algorithms consistently improves goal recognition performance. We propose a framework to systematically evaluate the effect of integrating landmark sets generated by diverse algorithms, specifically focusing on algorithms based on object permanence, affordance, and spatial relations. We evaluate our framework using a simulated environment and a real-world video dataset, analyzing the precision, recall, and F1-score of goal recognition models trained with various combinations of landmark sets. Our results demonstrate that while combining landmark sets can improve recall, it does not always translate to a consistent increase in precision, and in some cases, leads to a decrease in overall performance due to the introduction of noisy or irrelevant landmarks. This highlights the importance of careful selection and filtering of landmarks when employing multiple extraction algorithms for effective goal recognition."
http://arxiv.org/abs/2306.08680v1,Temporally Extended Goal Recognition in Fully Observable Non-Deterministic Domain Models,"Goal recognition aims to infer the intended goal of an agent based on observed behavior. Existing approaches often struggle with non-deterministic environments and focus primarily on instantaneous goal inference, neglecting the temporal evolution of goal likelihoods. This paper addresses the challenge of temporally extended goal recognition in fully observable, non-deterministic domain models, where actions may have multiple possible outcomes. We propose a novel Bayesian approach that integrates a probabilistic planner to model the agent's potential future behavior under each candidate goal, and leverages particle filtering to maintain a belief distribution over goals as the agent's actions unfold over time. The planner accounts for non-determinism by considering multiple possible execution paths. Experiments conducted on benchmark domains demonstrate that our method accurately tracks the evolving probabilities of different goals, achieving significantly higher accuracy and lower latency compared to state-of-the-art approaches, particularly in scenarios with high levels of non-determinism. This work provides a robust framework for understanding agent intent in complex, uncertain environments, enabling more effective human-robot interaction and autonomous system coordination."
http://arxiv.org/abs/cs/9612102v1,Quantitative Results Comparing Three Intelligent Interfaces for Information Capture: A Case Study Adding Name Information into an Electronic Personal Organizer,"Intelligent interfaces promise to streamline information capture, yet a comprehensive quantitative comparison of different approaches remains a challenge. This paper addresses the problem of efficiently adding name information into an electronic personal organizer (EPO) using three distinct intelligent interfaces: a rule-based system, a statistical natural language processing (NLP) model, and a hybrid approach combining both. Each interface was designed to parse name strings from various sources (e.g., business cards, emails) and populate corresponding fields in the EPO. We evaluated the interfaces on a dataset of 500 name strings, measuring precision, recall, and F1-score for accurate field assignment. The hybrid approach achieved the highest F1-score (0.92), significantly outperforming the rule-based (0.85) and NLP-only (0.88) systems, demonstrating a marked improvement in both precision and recall. This study provides empirical evidence for the effectiveness of hybrid intelligent interfaces in information capture tasks, offering valuable insights for designing more user-friendly and accurate data entry systems."
http://arxiv.org/abs/cs/0310023v1,Application of Kullback-Leibler Metric to Speech Recognition,"Automatic Speech Recognition (ASR) systems rely on accurate modeling of acoustic features and language models. Traditional ASR often employs cross-entropy loss for training, which may not fully capture the distributional nuances between predicted and target speech probabilities. This paper addresses the problem of improving ASR performance by directly minimizing the Kullback-Leibler (KL) divergence between the predicted posterior distribution of phonemes/words and the target distribution generated from the ground truth transcriptions. We propose a novel training regime that incorporates the KL divergence as a primary loss function, supplementing it with cross-entropy to maintain stability and prevent mode collapse. We investigate the effect of different smoothing techniques on the target distribution to mitigate the impact of sparse training data. Experimental results on the LibriSpeech dataset demonstrate that our KL-divergence-based training approach achieves a relative word error rate (WER) reduction of 5% compared to a baseline system trained solely with cross-entropy. This indicates that directly minimizing the KL divergence can lead to more accurate and robust speech recognition models by better aligning the predicted and target probability distributions."
http://arxiv.org/abs/1110.6589v1,A cognitive diversity framework for radar target classification,"Radar target classification is crucial in numerous applications, ranging from autonomous driving to defense systems. However, accurately classifying radar targets remains challenging due to factors such as noise, clutter, and the inherent variability of radar signatures. This paper addresses the problem of improving radar target classification performance by leveraging a cognitive diversity framework. Our approach introduces a novel ensemble learning strategy that combines multiple classifiers, each trained on a distinct subset of radar features and employing different machine learning algorithms (e.g., Support Vector Machines, Random Forests, and Convolutional Neural Networks). The diversity is further enhanced by incorporating data augmentation techniques tailored to radar signals, creating a more robust and generalized classification system. Experimental results on both simulated and real-world radar datasets demonstrate that our cognitive diversity framework significantly outperforms individual classifiers and traditional ensemble methods, achieving an average improvement of 8-12% in classification accuracy. This work offers a promising avenue for developing more reliable and accurate radar target classification systems by effectively harnessing the power of diverse perspectives in feature representation and classification methodologies."
http://arxiv.org/abs/1106.0171v1,Proposal of Pattern Recognition as a necessary and sufficient Principle to Cognitive Science,"Cognitive science seeks to understand the nature of intelligence, encompassing perception, learning, reasoning, and action. A central challenge lies in identifying unifying principles that can explain the diverse range of cognitive phenomena observed across biological and artificial systems. This paper addresses the question of whether pattern recognition, broadly defined as the ability to detect and categorize regularities in sensory input and internal representations, can serve as a necessary and sufficient principle for cognitive function. We propose a computational framework where all cognitive processes are decomposed into hierarchical pattern recognition modules. These modules learn, represent, and manipulate patterns at various levels of abstraction, from raw sensory data to abstract concepts, using probabilistic inference and predictive coding. We demonstrate, through simulations of several benchmark cognitive tasks, that this pattern recognition-centric architecture can replicate key aspects of human performance, including generalization, robustness to noise, and adaptation to novel environments. Our findings suggest that pattern recognition provides a powerful and parsimonious framework for understanding cognition, potentially unifying diverse theoretical perspectives and offering a foundation for building more intelligent artificial systems."
http://arxiv.org/abs/2406.13694v1,An Embedded Intelligent System for Attendance Monitoring,"Automated attendance monitoring is crucial for various sectors, including education and workplace management, to improve efficiency and resource allocation. However, current systems often rely on manual processes or centralized, computationally expensive solutions. This paper introduces an embedded intelligent system for real-time attendance monitoring utilizing edge computing principles. Our approach integrates a low-power embedded platform with a lightweight convolutional neural network (CNN) optimized for face detection and recognition. The system captures images via an integrated camera, performs on-device facial analysis to identify individuals, and autonomously logs attendance data with associated timestamps. Experimental results demonstrate a high accuracy of 95% in face recognition under varying lighting conditions, with an average processing time of 0.3 seconds per face on the embedded platform. This embedded system offers a cost-effective, privacy-preserving, and scalable solution for automated attendance monitoring, paving the way for wider deployment in resource-constrained environments."
http://arxiv.org/abs/1712.01949v2,Recognizing Plans by Learning Embeddings from Observed Action Distributions,"Inferring the underlying plan behind an agent's observed actions is a fundamental problem in AI, with applications ranging from robotics to human-computer interaction. However, traditional plan recognition approaches often rely on explicit domain knowledge or assume fully observable actions, which limits their applicability in complex, real-world scenarios. This paper addresses the problem of recognizing plans from noisy and incomplete observations of action distributions. We propose a novel embedding-based approach that learns to represent plans and observed action distributions in a shared latent space. Our method trains a neural network to map observed action frequencies to plan embeddings, using a contrastive loss function that encourages similar plans to be mapped to nearby points in the embedding space, regardless of slight variations in observed action distributions. We evaluate our method on both synthetic and real-world datasets, demonstrating superior performance compared to existing baselines in terms of plan recognition accuracy, especially in scenarios with noisy or incomplete observations. This learned embedding space allows for efficient and robust plan recognition from observed action distributions, enabling more effective human-robot collaboration and activity understanding in complex environments."
http://arxiv.org/abs/1106.0672v1,Policy Recognition in the Abstract Hidden Markov Model,"Policy recognition, the task of inferring an agent's goals and strategies from observed behavior, is crucial for enabling effective human-robot interaction and autonomous system coordination. Current approaches often struggle with noisy, incomplete, or abstract observations of agent actions, limiting their applicability in real-world scenarios. This paper addresses the challenge of policy recognition from abstract observations by introducing a novel approach based on the Abstract Hidden Markov Model (AHMM). Our method represents policies as probability distributions over abstract action sequences, learned from expert demonstrations or prior knowledge. We then employ a Bayesian inference framework to estimate the posterior probability of each policy given a sequence of abstract observations, leveraging the AHMM to handle uncertainty and ambiguity in the observation process. Experiments on a simulated navigation task and a real-world manipulation task demonstrate that our AHMM-based approach significantly outperforms traditional Hidden Markov Model (HMM) and Inverse Reinforcement Learning (IRL) methods, particularly in scenarios with high levels of abstraction and observation noise. The proposed method provides a robust and efficient solution for policy recognition in complex environments, paving the way for more intelligent and adaptive autonomous systems."
http://arxiv.org/abs/2005.05712v1,Goal Recognition over Imperfect Domain Models,"Goal recognition, the task of inferring an agent's intended goal from observed actions, typically relies on accurate domain models that define the possible actions and their effects. However, real-world environments are often complex and incompletely specified, leading to imperfect domain models that hinder accurate goal recognition. This paper addresses the problem of robust goal recognition in the face of inaccuracies and incompleteness in the domain model. We introduce a novel approach that learns a residual correction model to compensate for discrepancies between the provided domain model and the true environment dynamics. This model is learned jointly with the goal recognition process by incorporating a variational inference framework that marginalizes over possible residual corrections while inferring the agent's goal. Experimental results on synthetic and real-world datasets demonstrate that our approach significantly outperforms existing methods when the provided domain model is imperfect, achieving higher accuracy and robustness to noise. This work provides a crucial step towards deploying goal recognition systems in complex, real-world environments where perfect domain knowledge is often unattainable."
http://arxiv.org/abs/2306.15362v2,Planning Landmark Based Goal Recognition Revisited: Does Using Initial State Landmarks Make Sense?,"Goal recognition aims to infer the most likely intention of an agent based on observed behavior. Planning-based approaches leverage the agent's presumed optimality and domain knowledge to identify goals that best explain the observed actions, often utilizing landmarks  necessary subgoals  to prune the search space. While landmarks derived from potential goal states are widely used, the potential benefit of incorporating landmarks derived from the *initial* state remains underexplored and is the focus of this paper. We investigate whether including initial state landmarks improves the accuracy and efficiency of planning-based goal recognition. We propose a novel algorithm that integrates initial state landmarks into the goal recognition process by identifying goals whose plans not only achieve the goal but also satisfy the identified initial state landmarks. Through extensive experiments on benchmark domains, we demonstrate that incorporating initial state landmarks can significantly improve goal recognition accuracy, particularly in scenarios with sparse observations or ambiguous action sequences, without significantly increasing computational cost. This finding highlights the importance of considering both goal-directed and initial state constraints for effective goal recognition."
http://arxiv.org/abs/2311.14426v1,Human-Machine Cooperative Multimodal Learning Method for Cross-subject Olfactory Preference Recognition,"Olfactory preference recognition, crucial for personalized experiences in various applications, faces significant challenges due to the subjective and complex nature of human olfaction. Accurately recognizing olfactory preferences across different individuals remains a difficult task. To address this, we propose a novel Human-Machine Cooperative Multimodal Learning (HMC-MML) method for cross-subject olfactory preference recognition. HMC-MML leverages both human-provided pairwise preference labels and machine-extracted features from electroencephalogram (EEG) and respiration signals during olfactory stimulation. Specifically, we employ a Siamese network architecture trained with a contrastive loss function to learn a shared embedding space for both human and machine representations of olfactory stimuli. This allows the model to effectively integrate human knowledge with machine-learned patterns, leading to improved generalization across subjects. Experimental results on a publicly available olfactory EEG dataset demonstrate that HMC-MML significantly outperforms state-of-the-art methods in cross-subject olfactory preference recognition, achieving an average accuracy improvement of over 8%. This work highlights the potential of human-machine cooperation in advancing the field of olfactory preference recognition and offers a promising avenue for developing personalized olfactory-driven technologies."
http://arxiv.org/abs/1703.01083v1,Sequential Plan Recognition,"Plan recognition, the task of inferring an agent's intentions from observed actions, is crucial for effective human-robot interaction and autonomous systems. A key challenge lies in accurately recognizing plans in dynamic environments where observations arrive sequentially and may be noisy or incomplete. This paper addresses the problem of sequential plan recognition, focusing on maintaining and updating beliefs about an agent's plan as new observations become available. We propose a novel Bayesian approach that integrates hierarchical plan structures with a recurrent neural network (RNN) to model temporal dependencies and handle uncertainty in the observation stream. The RNN learns to predict the likelihood of different actions given the current plan belief, which is then used to update the belief via Bayesian inference. Experiments on synthetic and real-world activity datasets demonstrate that our method outperforms existing approaches in terms of accuracy and robustness to noisy observations. Our approach achieves a 15% improvement in F1-score compared to state-of-the-art Hidden Markov Model (HMM) based methods in challenging scenarios with partial observability. This work provides a robust and adaptable framework for sequential plan recognition, facilitating more intuitive and reliable human-machine collaboration."
http://arxiv.org/abs/1706.06328v1,Session Analysis using Plan Recognition,"Understanding user behavior in interactive sessions is crucial for optimizing system design and providing personalized experiences. Analyzing these sessions as sequences of actions often overlooks the underlying goals and intentions driving user behavior. This paper addresses the problem of automatically inferring user plans from observed interaction sessions to provide a more comprehensive understanding of user activity. We propose a novel plan recognition framework that combines a hierarchical representation of possible user plans with a probabilistic model for action selection and plan execution. Our model leverages both the temporal dependencies between actions and the semantic relationships between plans to accurately infer the most likely user plan given an observed session. Experiments on a real-world dataset of software usage demonstrate that our approach significantly outperforms existing methods in accurately identifying user plans, achieving a 20% improvement in F1-score. This research provides a valuable tool for developers and researchers seeking to gain deeper insights into user behavior and improve the usability and effectiveness of interactive systems."
http://arxiv.org/abs/1911.05876v1,"Partial-Order, Partially-Seen Observations of Fluents or Actions for Plan Recognition as Planning","Plan recognition, the task of inferring an agent's plan from observed actions, is crucial for intelligent human-robot interaction. However, real-world observations are often partial, providing only a fragmented view of the agent's activities and their temporal relationships. This paper addresses the challenge of plan recognition from *partial-order*, *partially-seen* observations of fluents or actions, a scenario where the observer only perceives a subset of the agent's actions with uncertain temporal ordering. We introduce a novel approach that casts plan recognition as a planning problem, leveraging classical planning techniques to search for the most likely plan consistent with the observed partial information. Our method encodes the observed actions and their partial order constraints as planning goals and uses a probabilistic domain model to capture the likelihood of different plans. Experimental results on benchmark plan recognition datasets demonstrate that our approach significantly outperforms existing methods in handling noisy and incomplete observations, particularly when temporal constraints are sparse. This work advances the state-of-the-art in robust plan recognition, enabling more effective collaboration between humans and AI agents in complex, real-world environments."
http://arxiv.org/abs/1301.2295v1,Recognition Networks for Approximate Inference in BN20 Networks,"Bayesian networks (BNs) offer a powerful framework for probabilistic reasoning under uncertainty, but exact inference in complex BNs is often computationally intractable. Approximate inference methods, such as variational inference, provide a scalable alternative, but require careful design and can be computationally expensive themselves. This paper addresses the challenge of efficient approximate inference in BN2O networks, a class of BNs frequently encountered in object recognition and scene understanding. We propose a novel approach that leverages recognition networks, specifically convolutional neural networks (CNNs), to directly approximate the posterior distribution over latent variables in the BN2O network. The CNN is trained to map observed evidence to parameters of a tractable distribution (e.g., Gaussian), enabling fast and amortized inference. Experiments on synthetic and real-world datasets demonstrate that our recognition network-based approach achieves comparable or superior accuracy to traditional variational inference methods while significantly reducing inference time. This accelerated inference capability enables the deployment of complex BN2O models in real-time computer vision applications."
