#!/usr/bin/env python3
"""
DistilBERTæ¨¡å‹å¾®è°ƒè„šæœ¬
ç”¨äºè®­ç»ƒä¸€ä¸ªèƒ½å¤ŸåŒºåˆ†AIç”Ÿæˆæ–‡æœ¬å’Œäººç±»æ’°å†™æ–‡æœ¬çš„åˆ†ç±»å™¨
"""

import pandas as pd
import numpy as np
import torch
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
)
import os
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# === Hugging Face é•œåƒé…ç½®ï¼ˆä¸­å›½å¤§é™†ç”¨æˆ·ï¼‰ ===
# å¦‚æœéœ€è¦ä½¿ç”¨é•œåƒï¼Œè¯·å°† USE_MIRROR è®¾ç½®ä¸º True
USE_MIRROR = True  # è®¾ç½®ä¸º True ä½¿ç”¨é•œåƒæº

if USE_MIRROR:
    # æ–¹æ³•1ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
    import os
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    
    # æ–¹æ³•2ï¼šç›´æ¥ä¿®æ”¹ transformers çš„ä¸‹è½½åœ°å€
    import transformers
    # è®¾ç½®é•œåƒURL
    transformers.utils.hub.HUGGINGFACE_CO_PREFIX = "https://hf-mirror.com"
    
    print("ğŸŒ å·²é…ç½®ä½¿ç”¨ Hugging Face é•œåƒæº: https://hf-mirror.com")
    print("   å¦‚éœ€æ›´æ¢é•œåƒæºï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„é•œåƒé…ç½®\n")
    
    # å¤‡é€‰é•œåƒæºåˆ—è¡¨
    # 1. hf-mirror.com (æ¨è)
    # 2. mirrors.bfsu.edu.cn/hugging-face-models
    # 3. mirrors.tuna.tsinghua.edu.cn/hugging-face-models

# --- é…ç½®å‚æ•° ---
FINAL_DATASET_PATH = '/hy-tmp/Detector/data/processed/final_labeled_dataset.csv'
# ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹
MODEL_NAME = './model_cache/distilbert-base-uncased'  # æ”¹ä¸ºæœ¬åœ°è·¯å¾„
OUTPUT_DIR = '/hy-tmp/Detector/results/model'
LOGGING_DIR = '/hy-tmp/Detector/results/logs'
BEST_MODEL_DIR = '/hy-tmp/Detector/results/best_model'

# è®­ç»ƒè¶…å‚æ•°
NUM_TRAIN_EPOCHS = 3
BATCH_SIZE = 16  # RTX 3090 å¯ä»¥ç”¨æ›´å¤§çš„æ‰¹æ¬¡
MAX_TOKEN_LENGTH = 256  # æ‘˜è¦é€šå¸¸ä¸ä¼šå¤ªé•¿ï¼Œ256è¶³å¤Ÿ
LEARNING_RATE = 2e-5
WARMUP_STEPS = 500
WEIGHT_DECAY = 0.01

# æ—©åœå‚æ•°
EARLY_STOPPING_PATIENCE = 2

def print_separator(title="", char="=", length=60):
    """æ‰“å°åˆ†éš”ç¬¦"""
    if title:
        print(f"\n{char * length}")
        print(f"{title.center(length)}")
        print(f"{char * length}")
    else:
        print(f"{char * length}")

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print_separator("ç¯å¢ƒæ£€æŸ¥")
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        print(f"âœ“ CUDAå¯ç”¨")
        print(f"  GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("âš  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šè¾ƒæ…¢ï¼‰")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if os.path.exists(FINAL_DATASET_PATH):
        print(f"âœ“ æ•°æ®é›†æ–‡ä»¶å­˜åœ¨: {FINAL_DATASET_PATH}")
    else:
        print(f"âœ— æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {FINAL_DATASET_PATH}")
        return False
    
    return True

def download_model_with_retry(model_name, max_retries=3):
    """
    åŠ è½½æœ¬åœ°æ¨¡å‹æˆ–ä¸‹è½½æ¨¡å‹
    """
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æœ¬åœ°è·¯å¾„
    if os.path.exists(model_name):
        print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name}")
        try:
            # ç›´æ¥ä»æœ¬åœ°åŠ è½½
            tokenizer = DistilBertTokenizerFast.from_pretrained(
                model_name,
                local_files_only=True,  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            
            model = DistilBertForSequenceClassification.from_pretrained(
                model_name,
                num_labels=2,
                id2label={0: "Human", 1: "AI"},
                label2id={"Human": 0, "AI": 1},
                local_files_only=True,  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            
            print("  âœ“ æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return tokenizer, model
            
        except Exception as e:
            print(f"  âœ— æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise e
    
    # å¦‚æœä¸æ˜¯æœ¬åœ°è·¯å¾„ï¼Œå°è¯•ä¸‹è½½ï¼ˆåŸæœ‰çš„ä¸‹è½½é€»è¾‘ï¼‰
    print(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    
    # é•œåƒæºåˆ—è¡¨
    mirror_urls = [
        "https://hf-mirror.com",
        "https://mirrors.bfsu.edu.cn/hugging-face-models",
        "https://mirrors.tuna.tsinghua.edu.cn/hugging-face-models",
    ]
    
    for mirror_idx, mirror_url in enumerate(mirror_urls):
        print(f"\nå°è¯•é•œåƒæº {mirror_idx + 1}: {mirror_url}")
        
        # è®¾ç½®å½“å‰é•œåƒ
        os.environ['HF_ENDPOINT'] = mirror_url
        
        # å¦‚æœä½¿ç”¨ hf-mirror.comï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if "hf-mirror" in mirror_url:
            # æ„å»ºå®Œæ•´çš„æ¨¡å‹URL
            model_url = f"{mirror_url}/{model_name}"
        else:
            model_url = model_name
            
        for attempt in range(max_retries):
            try:
                print(f"  å°è¯• {attempt + 1}/{max_retries}...")
                
                # å°è¯•ä½¿ç”¨é•œåƒURLä¸‹è½½
                if "hf-mirror" in mirror_url:
                    # å¯¹äº hf-mirrorï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´URL
                    tokenizer = DistilBertTokenizerFast.from_pretrained(
                        model_url,
                        cache_dir="./model_cache",
                        resume_download=True,
                        force_download=False,
                        local_files_only=False,
                        trust_remote_code=True,
                    )
                    
                    model = DistilBertForSequenceClassification.from_pretrained(
                        model_url,
                        num_labels=2,
                        id2label={0: "Human", 1: "AI"},
                        label2id={"Human": 0, "AI": 1},
                        cache_dir="./model_cache",
                        resume_download=True,
                        force_download=False,
                        local_files_only=False,
                        trust_remote_code=True,
                    )
                else:
                    # å¯¹äºå…¶ä»–é•œåƒï¼Œä½¿ç”¨æ¨¡å‹å
                    tokenizer = DistilBertTokenizerFast.from_pretrained(
                        model_name,
                        cache_dir="./model_cache",
                        resume_download=True,
                        force_download=False,
                    )
                    
                    model = DistilBertForSequenceClassification.from_pretrained(
                        model_name,
                        num_labels=2,
                        id2label={0: "Human", 1: "AI"},
                        label2id={"Human": 0, "AI": 1},
                        cache_dir="./model_cache",
                        resume_download=True,
                        force_download=False,
                    )
                
                print("  âœ“ æ¨¡å‹ä¸‹è½½æˆåŠŸï¼")
                return tokenizer, model
                
            except Exception as e:
                print(f"  âœ— ä¸‹è½½å¤±è´¥: {str(e)[:200]}")
                if attempt < max_retries - 1:
                    print(f"  ç­‰å¾…3ç§’åé‡è¯•...")
                    import time
                    time.sleep(3)
                else:
                    print(f"  è¯¥é•œåƒæºå¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                    break
    
    # å¦‚æœæ‰€æœ‰é•œåƒéƒ½å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨ä¸‹è½½æ–¹æ¡ˆ
    print("\nâŒ æ‰€æœ‰é•œåƒæºéƒ½ä¸‹è½½å¤±è´¥ï¼")
    print("\nğŸ“Œ æ‰‹åŠ¨ä¸‹è½½è§£å†³æ–¹æ¡ˆï¼š")
    print("1. è®¿é—®ä»¥ä¸‹ä»»ä¸€é“¾æ¥ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼š")
    print("   - https://hf-mirror.com/distilbert-base-uncased")
    print("   - https://huggingface.co/distilbert-base-uncased")
    print("\n2. ä¸‹è½½ä»¥ä¸‹æ–‡ä»¶åˆ° ./model_cache/distilbert-base-uncased/ ç›®å½•ï¼š")
    print("   - config.json")
    print("   - pytorch_model.bin")
    print("   - tokenizer_config.json")
    print("   - tokenizer.json")
    print("   - vocab.txt")
    print("\n3. ä½¿ç”¨ä»£ç†æ–¹æ¡ˆï¼š")
    print("   export https_proxy=http://your-proxy:port")
    print("   export http_proxy=http://your-proxy:port")
    print("\n4. æˆ–ä½¿ç”¨ huggingface-cli ä¸‹è½½ï¼š")
    print("   pip install -U huggingface_hub")
    print("   export HF_ENDPOINT=https://hf-mirror.com")
    print("   huggingface-cli download distilbert-base-uncased --local-dir ./model_cache/distilbert-base-uncased")
    
    raise Exception("æ— æ³•ä¸‹è½½æ¨¡å‹ï¼Œè¯·å°è¯•æ‰‹åŠ¨ä¸‹è½½")

def load_and_prepare_dataset():
    """åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
    print_separator("åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†")
    
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(FINAL_DATASET_PATH)
    print(f"æ€»æ•°æ®é‡: {len(df)} æ¡")
    
    # æ£€æŸ¥åˆ—åå¹¶ç¡®å®šæ–‡æœ¬åˆ—
    text_column = None
    for col in ['abstract', 'text', 'content']:
        if col in df.columns:
            text_column = col
            break
    
    if text_column is None:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªélabelåˆ—ä½œä¸ºæ–‡æœ¬åˆ—
        cols = [col for col in df.columns if col != 'label']
        if cols:
            text_column = cols[0]
    
    print(f"ä½¿ç”¨æ–‡æœ¬åˆ—: '{text_column}'")
    
    # ç¡®ä¿æ–‡æœ¬åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
    df[text_column] = df[text_column].astype(str)
    
    # å¦‚æœåŸå§‹åˆ—åä¸æ˜¯'text'ï¼Œé‡å‘½åä¸º'text'ä»¥ç»Ÿä¸€å¤„ç†
    if text_column != 'text':
        df = df.rename(columns={text_column: 'text'})
    
    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    label_counts = df['label'].value_counts().sort_index()
    print("\næ ‡ç­¾åˆ†å¸ƒ:")
    print(f"  äººç±»æ–‡æœ¬ (label=0): {label_counts.get(0, 0)} æ¡")
    print(f"  AIæ–‡æœ¬ (label=1): {label_counts.get(1, 0)} æ¡")
    
    # åˆ›å»ºHugging Face Datasetå¯¹è±¡
    dataset = Dataset.from_pandas(df[['text', 'label']])
    
    # 80/20 åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_test_split_dict = dataset.train_test_split(test_size=0.2, seed=42)
    
    print("\næ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_test_split_dict['train'])} æ¡")
    print(f"  æµ‹è¯•é›†: {len(train_test_split_dict['test'])} æ¡")
    
    return train_test_split_dict

def tokenize_data(dataset_dict, tokenizer):
    """å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯"""
    print_separator("æ•°æ®åˆ†è¯")
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=MAX_TOKEN_LENGTH
        )
    
    print("æ­£åœ¨å¯¹è®­ç»ƒé›†è¿›è¡Œåˆ†è¯...")
    tokenized_datasets = dataset_dict.map(
        tokenize_function, 
        batched=True,
        desc="Tokenizing"
    )
    
    print("âœ“ åˆ†è¯å®Œæˆ")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {MAX_TOKEN_LENGTH}")
    
    return tokenized_datasets

def compute_metrics(pred):
    """å®šä¹‰è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•°"""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='binary'
    )
    acc = accuracy_score(labels, preds)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(labels, preds)
    
    # åˆ†åˆ«è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(
        labels, preds, average=None
    )
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'precision_human': precision_per_class[0] if len(precision_per_class) > 0 else 0,
        'precision_ai': precision_per_class[1] if len(precision_per_class) > 1 else 0,
        'recall_human': recall_per_class[0] if len(recall_per_class) > 0 else 0,
        'recall_ai': recall_per_class[1] if len(recall_per_class) > 1 else 0,
    }

def create_model_card(trainer, training_args, model_path):
    """åˆ›å»ºæ¨¡å‹å¡ç‰‡ï¼Œè®°å½•æ¨¡å‹ä¿¡æ¯"""
    model_card = {
        "model_name": "DistilBERT AI Text Detector",
        "base_model": MODEL_NAME,
        "task": "Binary Classification (AI vs Human Text)",
        "training_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "training_parameters": {
            "num_epochs": NUM_TRAIN_EPOCHS,
            "batch_size": BATCH_SIZE,
            "learning_rate": LEARNING_RATE,
            "max_token_length": MAX_TOKEN_LENGTH,
            "warmup_steps": WARMUP_STEPS,
            "weight_decay": WEIGHT_DECAY,
        },
        "dataset_info": {
            "path": FINAL_DATASET_PATH,
            "train_size": len(trainer.train_dataset),
            "test_size": len(trainer.eval_dataset),
        },
        "performance_metrics": None  # å°†åœ¨è®­ç»ƒåæ›´æ–°
    }
    
    # ä¿å­˜æ¨¡å‹å¡ç‰‡
    card_path = os.path.join(model_path, "model_card.json")
    with open(card_path, 'w', encoding='utf-8') as f:
        json.dump(model_card, f, indent=2, ensure_ascii=False)
    
    return card_path

def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ¨¡å‹å¾®è°ƒå…¨è¿‡ç¨‹"""
    print_separator("DistilBERTæ¨¡å‹å¾®è°ƒ - AIæ–‡æœ¬æ£€æµ‹å™¨", "=", 60)
    
    # 0. ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        print("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å­˜åœ¨")
        return
    
    # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†
    dataset_dict = load_and_prepare_dataset()
    
    # 2. åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹ï¼ˆä½¿ç”¨é‡è¯•æœºåˆ¶ï¼‰
    print_separator("åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨")
    print(f"æ¨¡å‹åç§°: {MODEL_NAME}")
    
    if USE_MIRROR:
        print("ä½¿ç”¨é•œåƒæºä¸‹è½½æ¨¡å‹...")
    
    try:
        tokenizer, model = download_model_with_retry(MODEL_NAME)
    except Exception as e:
        print(f"\næ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return
    
    # 3. åˆ†è¯
    tokenized_datasets = tokenize_data(dataset_dict, tokenizer)
    
    # 4. é…ç½®æ¨¡å‹è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âœ“ æ¨¡å‹å°†åœ¨ {device.upper()} ä¸Šè¿›è¡Œè®­ç»ƒ")
    if device == "cuda":
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    model.to(device)
    
    # 5. å®šä¹‰è®­ç»ƒå‚æ•°
    print_separator("é…ç½®è®­ç»ƒå‚æ•°")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(LOGGING_DIR, exist_ok=True)
    os.makedirs(BEST_MODEL_DIR, exist_ok=True)
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_TRAIN_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        warmup_steps=WARMUP_STEPS,
        weight_decay=WEIGHT_DECAY,
        logging_dir=LOGGING_DIR,
        logging_steps=50,
        eval_strategy="epoch",  # ä½¿ç”¨æ–°çš„å‚æ•°å
        save_strategy="epoch",  # æ¯ä¸ªepochç»“æŸåä¿å­˜æ¨¡å‹
        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
        metric_for_best_model="f1",  # ä½¿ç”¨f1åˆ†æ•°æ¥åˆ¤æ–­æœ€ä½³æ¨¡å‹
        greater_is_better=True,
        save_total_limit=2,  # åªä¿ç•™æœ€å¥½çš„2ä¸ªcheckpoint
        report_to=["tensorboard"],  # RTX 3090 å¯ä»¥ä½¿ç”¨ tensorboard
        fp16=True,  # RTX 3090 æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼ŒåŠ é€Ÿè®­ç»ƒ
        gradient_checkpointing=False,  # RTX 3090 æœ‰è¶³å¤Ÿæ˜¾å­˜ï¼Œä¸éœ€è¦
        optim="adamw_torch",  # ä¼˜åŒ–å™¨
        seed=42,  # éšæœºç§å­
        max_grad_norm=1.0,
    )
    
    print("è®­ç»ƒé…ç½®:")
    print(f"  è®­ç»ƒè½®æ•°: {NUM_TRAIN_EPOCHS}")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {MAX_TOKEN_LENGTH}")
    print(f"  æ··åˆç²¾åº¦è®­ç»ƒ: {training_args.fp16}")
    
    # 6. åˆå§‹åŒ–Trainer
    print_separator("åˆå§‹åŒ–Trainer")
    
    # åˆ›å»ºæ—©åœå›è°ƒ
    early_stopping = EarlyStoppingCallback(
        early_stopping_patience=EARLY_STOPPING_PATIENCE,
        early_stopping_threshold=0.001
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[early_stopping],
    )
    
    # 7. å¼€å§‹è®­ç»ƒï¼
    print_separator("å¼€å§‹è®­ç»ƒ", "â–¶", 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è®­ç»ƒ
    train_result = trainer.train()
    
    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print_separator("è®­ç»ƒå®Œæˆ", "âœ“", 60)
    
    # 8. è¯„ä¼°æœ€ç»ˆæ¨¡å‹
    print_separator("æ¨¡å‹è¯„ä¼°")
    eval_result = trainer.evaluate()
    
    print("æµ‹è¯•é›†æ€§èƒ½:")
    for key, value in eval_result.items():
        if key.startswith('eval_'):
            metric_name = key.replace('eval_', '')
            if isinstance(value, float):
                print(f"  {metric_name:15s}: {value:.4f}")
    
    # 9. ä¿å­˜æœ€ä½³æ¨¡å‹
    print_separator("ä¿å­˜æ¨¡å‹")
    
    # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
    trainer.save_model(BEST_MODEL_DIR)
    tokenizer.save_pretrained(BEST_MODEL_DIR)
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜è‡³: {BEST_MODEL_DIR}")
    
    # åˆ›å»ºå¹¶ä¿å­˜æ¨¡å‹å¡ç‰‡
    model_card_path = create_model_card(trainer, training_args, BEST_MODEL_DIR)
    
    # æ›´æ–°æ¨¡å‹å¡ç‰‡çš„æ€§èƒ½æŒ‡æ ‡
    with open(model_card_path, 'r') as f:
        model_card = json.load(f)
    
    model_card['performance_metrics'] = {
        k.replace('eval_', ''): v 
        for k, v in eval_result.items() 
        if k.startswith('eval_')
    }
    
    with open(model_card_path, 'w') as f:
        json.dump(model_card, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ æ¨¡å‹å¡ç‰‡å·²ä¿å­˜è‡³: {model_card_path}")
    
    # 10. æ€»ç»“
    print_separator("è®­ç»ƒæ€»ç»“", "ğŸ“Š", 60)
    print(f"æœ€ä½³F1åˆ†æ•°: {eval_result.get('eval_f1', 0):.4f}")
    print(f"å‡†ç¡®ç‡: {eval_result.get('eval_accuracy', 0):.4f}")
    print(f"ç²¾ç¡®ç‡: {eval_result.get('eval_precision', 0):.4f}")
    print(f"å¬å›ç‡: {eval_result.get('eval_recall', 0):.4f}")
    print("\nåˆ†ç±»åˆ«æ€§èƒ½:")
    print(f"  äººç±»æ–‡æœ¬ - ç²¾ç¡®ç‡: {eval_result.get('eval_precision_human', 0):.4f}, "
          f"å¬å›ç‡: {eval_result.get('eval_recall_human', 0):.4f}")
    print(f"  AIæ–‡æœ¬ - ç²¾ç¡®ç‡: {eval_result.get('eval_precision_ai', 0):.4f}, "
          f"å¬å›ç‡: {eval_result.get('eval_recall_ai', 0):.4f}")
    
    print_separator("", "=", 60)
    print("ğŸ‰ æ­å–œï¼AIæ–‡æœ¬æ£€æµ‹å™¨è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä½ç½®: {BEST_MODEL_DIR}")
    print("ğŸ“ æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ¥æ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬äº†")
    print_separator("", "=", 60)

if __name__ == '__main__':
    main()