#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ·±åº¦é”™è¯¯åˆ†æè„šæœ¬ï¼ˆæ— éœ€NLTKï¼‰
è¯¦ç»†åˆ†ææ¨¡å‹é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ï¼Œæ‰¾å‡ºå…±åŒç‰¹å¾
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import os

# é…ç½®å‚æ•°
ERROR_ANALYSIS_PATH = '/hy-tmp/Detector/results/evaluation/error_analysis.csv'
OUTPUT_DIR = '/hy-tmp/Detector/results/evaluation/error_deep_analysis'

os.makedirs(OUTPUT_DIR, exist_ok=True)

def simple_tokenize(text):
    """ç®€å•çš„åˆ†è¯å‡½æ•°"""
    # ç§»é™¤æ ‡ç‚¹ï¼Œè½¬å°å†™ï¼Œåˆ†è¯
    words = re.findall(r'\b[a-z]+\b', text.lower())
    return words

def sentence_split(text):
    """ç®€å•çš„å¥å­åˆ†å‰²"""
    # ä½¿ç”¨å¸¸è§çš„å¥å­ç»“æŸç¬¦åˆ†å‰²
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences

def analyze_text_features(text):
    """åˆ†ææ–‡æœ¬çš„å„ç§ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    features = {}
    
    # åŸºæœ¬ç»Ÿè®¡
    features['length'] = len(text)
    words = simple_tokenize(text)
    features['word_count'] = len(words)
    sentences = sentence_split(text)
    features['sentence_count'] = len(sentences)
    features['avg_word_length'] = np.mean([len(word) for word in words]) if words else 0
    features['avg_sentence_length'] = features['word_count'] / features['sentence_count'] if features['sentence_count'] > 0 else 0
    
    # è¯æ±‡å¤šæ ·æ€§
    features['unique_words'] = len(set(words))
    features['lexical_diversity'] = features['unique_words'] / features['word_count'] if features['word_count'] > 0 else 0
    
    # æ ‡ç‚¹ç¬¦å·ä½¿ç”¨
    features['punctuation_count'] = len(re.findall(r'[.,;:!?\-\(\)]', text))
    features['punctuation_ratio'] = features['punctuation_count'] / features['word_count'] if features['word_count'] > 0 else 0
    
    # å¤§å†™è¯æ¯”ä¾‹ï¼ˆå¯èƒ½è¡¨ç¤ºä¸“æœ‰åè¯ã€ç¼©å†™ç­‰ï¼‰
    all_words = re.findall(r'\b[A-Za-z]+\b', text)
    features['capitalized_words'] = len([w for w in all_words if w[0].isupper()])
    features['capital_ratio'] = features['capitalized_words'] / len(all_words) if all_words else 0
    
    # æ•°å­—å’Œç¬¦å·
    features['contains_numbers'] = bool(re.search(r'\d', text))
    features['number_count'] = len(re.findall(r'\d+', text))
    
    return features

def analyze_linguistic_patterns(text):
    """åˆ†æè¯­è¨€æ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    patterns = {}
    
    # å¸¸è§çš„AI/å­¦æœ¯å†™ä½œæ¨¡å¼
    ai_keywords = [
        'we propose', 'we present', 'we introduce', 'our approach', 'our method',
        'in this paper', 'in this work', 'furthermore', 'moreover', 'specifically',
        'comprehensive', 'significant', 'novel', 'state-of-the-art', 'experimental results',
        'we demonstrate', 'extensive experiments', 'outperforms', 'baseline'
    ]
    
    patterns['ai_pattern_count'] = 0
    patterns['ai_patterns_found'] = []
    for keyword in ai_keywords:
        if keyword in text.lower():
            patterns['ai_pattern_count'] += text.lower().count(keyword)
            patterns['ai_patterns_found'].append(keyword)
    
    # äººç±»å†™ä½œå¯èƒ½çš„ç‰¹å¾
    human_keywords = [
        'surprisingly', 'interestingly', 'we find', 'we show', 'turns out',
        'it seems', 'perhaps', 'maybe', 'probably', 'might', 'could be'
    ]
    
    patterns['human_pattern_count'] = 0
    patterns['human_patterns_found'] = []
    for keyword in human_keywords:
        if keyword in text.lower():
            patterns['human_pattern_count'] += text.lower().count(keyword)
            patterns['human_patterns_found'].append(keyword)
    
    # æ£€æŸ¥ç‰¹æ®Šç¬¦å·
    patterns['question_marks'] = text.count('?')
    patterns['exclamation_marks'] = text.count('!')
    patterns['parentheses'] = text.count('(')
    patterns['quotes'] = text.count('"')
    
    # æ£€æŸ¥å¼€å¤´æ¨¡å¼
    text_lower = text.strip().lower()
    patterns['starts_with_we'] = text_lower.startswith('we ')
    patterns['starts_with_this'] = text_lower.startswith('this ')
    patterns['starts_with_in'] = text_lower.startswith('in ')
    patterns['starts_with_the'] = text_lower.startswith('the ')
    
    return patterns

def load_and_analyze_errors():
    """åŠ è½½å¹¶åˆ†æé”™è¯¯æ ·æœ¬"""
    print("=" * 60)
    print("æ·±åº¦é”™è¯¯åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("=" * 60)
    
    # åŠ è½½é”™è¯¯åˆ†ææ–‡ä»¶
    if not os.path.exists(ERROR_ANALYSIS_PATH):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {ERROR_ANALYSIS_PATH}")
        print("è¯·å…ˆè¿è¡Œè¯„ä¼°è„šæœ¬ç”Ÿæˆ error_analysis.csv")
        return None, None, None
    
    error_df = pd.read_csv(ERROR_ANALYSIS_PATH)
    
    print(f"\næ€»é”™è¯¯æ•°: {len(error_df)}")
    print(f"é”™è¯¯ç‡: {len(error_df)/1000*100:.1f}% (å‡è®¾æµ‹è¯•é›†1000ä¸ªæ ·æœ¬)")
    
    # åˆ†ç±»é”™è¯¯ç±»å‹
    false_positives = error_df[error_df['label'] == 0]  # Humanè¯¯åˆ¤ä¸ºAI
    false_negatives = error_df[error_df['label'] == 1]  # AIè¯¯åˆ¤ä¸ºHuman
    
    print(f"\nFalse Positives (Humanâ†’AI): {len(false_positives)}")
    print(f"False Negatives (AIâ†’Human): {len(false_negatives)}")
    
    return error_df, false_positives, false_negatives

def display_detailed_analysis(error_df, false_positives, false_negatives):
    """è¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªé”™è¯¯æ ·æœ¬çš„åˆ†æ"""
    
    print("\n" + "=" * 60)
    print("é”™è¯¯æ ·æœ¬è¯¦ç»†åˆ†æ")
    print("=" * 60)
    
    # ä¿å­˜è¯¦ç»†åˆ†æåˆ°æ–‡ä»¶
    detailed_analysis_path = os.path.join(OUTPUT_DIR, 'detailed_error_analysis.txt')
    
    with open(detailed_analysis_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("æ·±åº¦é”™è¯¯åˆ†ææŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        
        # åˆ†æFalse Positives (Humanâ†’AI)
        f.write("\n" + "="*60 + "\n")
        f.write("FALSE POSITIVES (äººç±»æ–‡æœ¬è¢«è¯¯åˆ¤ä¸ºAI)\n")
        f.write(f"å…± {len(false_positives)} ä¸ªæ ·æœ¬\n")
        f.write("="*60 + "\n\n")
        
        for i, (idx, row) in enumerate(false_positives.iterrows(), 1):
            f.write(f"\nã€æ ·æœ¬ {i}ã€‘ (åŸç´¢å¼•: {idx}, ç½®ä¿¡åº¦: {row['prediction_confidence']:.3f})\n")
            f.write("-" * 40 + "\n")
            
            text = row['text']
            f.write(f"æ–‡æœ¬å†…å®¹:\n{text}\n\n")
            
            # åˆ†æç‰¹å¾
            features = analyze_text_features(text)
            patterns = analyze_linguistic_patterns(text)
            
            f.write("æ–‡æœ¬ç‰¹å¾:\n")
            f.write(f"  - æ€»é•¿åº¦: {features['length']} å­—ç¬¦\n")
            f.write(f"  - è¯æ•°: {features['word_count']}\n")
            f.write(f"  - å¥å­æ•°: {features['sentence_count']}\n")
            f.write(f"  - å¹³å‡å¥é•¿: {features['avg_sentence_length']:.1f} è¯\n")
            f.write(f"  - ç‹¬ç‰¹è¯æ±‡æ•°: {features['unique_words']}\n")
            f.write(f"  - è¯æ±‡å¤šæ ·æ€§: {features['lexical_diversity']:.3f}\n")
            f.write(f"  - æ ‡ç‚¹ç¬¦å·æ¯”ä¾‹: {features['punctuation_ratio']:.3f}\n")
            f.write(f"  - å¤§å†™è¯æ¯”ä¾‹: {features['capital_ratio']:.3f}\n")
            
            f.write("\nè¯­è¨€æ¨¡å¼:\n")
            f.write(f"  - AIæ¨¡å¼è¯æ±‡æ•°: {patterns['ai_pattern_count']}\n")
            if patterns['ai_patterns_found']:
                f.write(f"    å‘ç°çš„AIæ¨¡å¼: {', '.join(patterns['ai_patterns_found'])}\n")
            f.write(f"  - äººç±»æ¨¡å¼è¯æ±‡æ•°: {patterns['human_pattern_count']}\n")
            if patterns['human_patterns_found']:
                f.write(f"    å‘ç°çš„äººç±»æ¨¡å¼: {', '.join(patterns['human_patterns_found'])}\n")
            
            f.write("\nå¯èƒ½çš„è¯¯åˆ¤åŸå› :\n")
            reasons = []
            if patterns['ai_pattern_count'] > 3:
                reasons.append("ä½¿ç”¨äº†å¤§é‡å­¦æœ¯/AIå†™ä½œå¸¸è§è¯æ±‡")
            if features['lexical_diversity'] < 0.5:
                reasons.append("è¯æ±‡å¤šæ ·æ€§è¾ƒä½ï¼Œæ–‡æœ¬å¯èƒ½æ˜¾å¾—å…¬å¼åŒ–")
            if patterns['starts_with_we'] or patterns['starts_with_this']:
                reasons.append("ä½¿ç”¨äº†å…¸å‹çš„å­¦æœ¯è®ºæ–‡å¼€å¤´æ¨¡å¼")
            if features['avg_sentence_length'] > 25:
                reasons.append("å¥å­è¾ƒé•¿ä¸”ç»“æ„å¤æ‚")
            if not patterns['question_marks'] and not patterns['exclamation_marks']:
                reasons.append("ç¼ºä¹ç–‘é—®å¥æˆ–æ„Ÿå¹å¥ç­‰äººç±»ç‰¹å¾")
            
            for reason in reasons:
                f.write(f"  - {reason}\n")
            
            f.write("\n" + "="*60 + "\n")
        
        # åˆ†æFalse Negatives (AIâ†’Human)
        f.write("\n" + "="*60 + "\n")
        f.write("FALSE NEGATIVES (AIæ–‡æœ¬è¢«è¯¯åˆ¤ä¸ºäººç±»)\n")
        f.write(f"å…± {len(false_negatives)} ä¸ªæ ·æœ¬\n")
        f.write("="*60 + "\n\n")
        
        for i, (idx, row) in enumerate(false_negatives.iterrows(), 1):
            f.write(f"\nã€æ ·æœ¬ {i}ã€‘ (åŸç´¢å¼•: {idx}, ç½®ä¿¡åº¦: {row['prediction_confidence']:.3f})\n")
            f.write("-" * 40 + "\n")
            
            text = row['text']
            f.write(f"æ–‡æœ¬å†…å®¹:\n{text}\n\n")
            
            # åˆ†æç‰¹å¾
            features = analyze_text_features(text)
            patterns = analyze_linguistic_patterns(text)
            
            f.write("æ–‡æœ¬ç‰¹å¾:\n")
            f.write(f"  - æ€»é•¿åº¦: {features['length']} å­—ç¬¦\n")
            f.write(f"  - è¯æ•°: {features['word_count']}\n")
            f.write(f"  - å¥å­æ•°: {features['sentence_count']}\n")
            f.write(f"  - å¹³å‡å¥é•¿: {features['avg_sentence_length']:.1f} è¯\n")
            f.write(f"  - ç‹¬ç‰¹è¯æ±‡æ•°: {features['unique_words']}\n")
            f.write(f"  - è¯æ±‡å¤šæ ·æ€§: {features['lexical_diversity']:.3f}\n")
            
            f.write("\nè¯­è¨€æ¨¡å¼:\n")
            f.write(f"  - AIæ¨¡å¼è¯æ±‡æ•°: {patterns['ai_pattern_count']}\n")
            f.write(f"  - äººç±»æ¨¡å¼è¯æ±‡æ•°: {patterns['human_pattern_count']}\n")
            if patterns['human_patterns_found']:
                f.write(f"    å‘ç°çš„äººç±»æ¨¡å¼: {', '.join(patterns['human_patterns_found'])}\n")
            f.write(f"  - ç–‘é—®å¥: {patterns['question_marks']} ä¸ª\n")
            f.write(f"  - æ„Ÿå¹å¥: {patterns['exclamation_marks']} ä¸ª\n")
            
            f.write("\nå¯èƒ½çš„è¯¯åˆ¤åŸå› :\n")
            reasons = []
            if patterns['human_pattern_count'] > 2:
                reasons.append("åŒ…å«äº†è¾ƒå¤šäººç±»å†™ä½œç‰¹å¾è¯æ±‡")
            if features['lexical_diversity'] > 0.6:
                reasons.append("è¯æ±‡å¤šæ ·æ€§é«˜ï¼Œæ˜¾å¾—æ›´è‡ªç„¶")
            if patterns['question_marks'] > 0:
                reasons.append("åŒ…å«ç–‘é—®å¥")
            if features['avg_sentence_length'] < 20:
                reasons.append("å¥å­è¾ƒçŸ­ï¼Œæ›´åƒäººç±»çš„ç®€æ´è¡¨è¾¾")
            if patterns['ai_pattern_count'] < 2:
                reasons.append("ç¼ºå°‘å…¸å‹çš„AI/å­¦æœ¯å†™ä½œæ¨¡å¼")
            
            for reason in reasons:
                f.write(f"  - {reason}\n")
            
            f.write("\n" + "="*60 + "\n")
    
    print(f"\nâœ“ è¯¦ç»†åˆ†æå·²ä¿å­˜è‡³: {detailed_analysis_path}")
    
    # æ‰“å°ç®€è¦æ€»ç»“åˆ°æ§åˆ¶å°
    print("\n### é”™è¯¯æ ·æœ¬ç®€è¦é¢„è§ˆ ###\n")
    
    if len(false_positives) > 0:
        print("FALSE POSITIVES (Humanâ†’AI):")
        for i, (idx, row) in enumerate(false_positives.head(3).iterrows(), 1):
            print(f"\n{i}. ç½®ä¿¡åº¦: {row['prediction_confidence']:.3f}")
            print(f"   æ–‡æœ¬é¢„è§ˆ: {row['text'][:200]}...")
    
    if len(false_negatives) > 0:
        print("\nFALSE NEGATIVES (AIâ†’Human):")
        for i, (idx, row) in enumerate(false_negatives.head(3).iterrows(), 1):
            print(f"\n{i}. ç½®ä¿¡åº¦: {row['prediction_confidence']:.3f}")
            print(f"   æ–‡æœ¬é¢„è§ˆ: {row['text'][:200]}...")

def create_feature_comparison_chart(error_df):
    """åˆ›å»ºç‰¹å¾å¯¹æ¯”å›¾è¡¨ï¼ˆè®ºæ–‡çº§åˆ«ï¼‰"""
    
    print("\næ­£åœ¨ç”Ÿæˆç‰¹å¾å¯¹æ¯”å›¾è¡¨...")
    
    # è®¾ç½®matplotlibä¸ºè®ºæ–‡å‘è¡¨è´¨é‡
    import matplotlib as mpl
    mpl.rcParams['pdf.fonttype'] = 42  # TrueTypeå­—ä½“
    mpl.rcParams['ps.fonttype'] = 42   # TrueTypeå­—ä½“
    mpl.rcParams['font.size'] = 11
    mpl.rcParams['axes.labelsize'] = 12
    mpl.rcParams['axes.titlesize'] = 13
    mpl.rcParams['xtick.labelsize'] = 10
    mpl.rcParams['ytick.labelsize'] = 10
    mpl.rcParams['legend.fontsize'] = 10
    mpl.rcParams['figure.dpi'] = 300
    mpl.rcParams['savefig.dpi'] = 300
    
    # æ”¶é›†ç‰¹å¾
    all_features = []
    for idx, row in error_df.iterrows():
        features = analyze_text_features(row['text'])
        features['error_type'] = 'FP' if row['label'] == 0 else 'FN'
        features['confidence'] = row['prediction_confidence']
        all_features.append(features)
    
    features_df = pd.DataFrame(all_features)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # å­å›¾æ ‡ç­¾æ ·å¼
    subplot_labels = ['(a)', '(b)', '(c)', '(d)']
    label_props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)
    
    # 1. æ–‡æœ¬é•¿åº¦å¯¹æ¯” - å­å›¾(a)
    ax = axes[0, 0]
    fp_data = features_df[features_df['error_type'] == 'FP']['length']
    fn_data = features_df[features_df['error_type'] == 'FN']['length']
    
    if len(fp_data) > 0 and len(fn_data) > 0:
        bp = ax.boxplot([fp_data, fn_data], 
                        labels=['False Positive\n(Humanâ†’AI)', 'False Negative\n(AIâ†’Human)'],
                        patch_artist=True)
        # è®¾ç½®ç®±ä½“é¢œè‰²
        bp['boxes'][0].set_facecolor('lightcoral')
        bp['boxes'][1].set_facecolor('lightblue')
    elif len(fp_data) > 0:
        bp = ax.boxplot([fp_data], labels=['False Positive\n(Humanâ†’AI)'], patch_artist=True)
        bp['boxes'][0].set_facecolor('lightcoral')
    elif len(fn_data) > 0:
        bp = ax.boxplot([fn_data], labels=['False Negative\n(AIâ†’Human)'], patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
    
    ax.set_ylabel('Text Length (characters)', fontweight='bold')
    ax.set_title('Text Length Comparison', fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.text(0.02, 0.98, subplot_labels[0], transform=ax.transAxes, 
            fontsize=12, fontweight='bold', va='top', bbox=label_props)
    
    # 2. è¯æ±‡å¤šæ ·æ€§å¯¹æ¯” - å­å›¾(b)
    ax = axes[0, 1]
    fp_diversity = features_df[features_df['error_type'] == 'FP']['lexical_diversity']
    fn_diversity = features_df[features_df['error_type'] == 'FN']['lexical_diversity']
    
    if len(fp_diversity) > 0 and len(fn_diversity) > 0:
        bp = ax.boxplot([fp_diversity, fn_diversity], 
                        labels=['False Positive\n(Humanâ†’AI)', 'False Negative\n(AIâ†’Human)'],
                        patch_artist=True)
        bp['boxes'][0].set_facecolor('lightcoral')
        bp['boxes'][1].set_facecolor('lightblue')
    elif len(fp_diversity) > 0:
        bp = ax.boxplot([fp_diversity], labels=['False Positive\n(Humanâ†’AI)'], patch_artist=True)
        bp['boxes'][0].set_facecolor('lightcoral')
    elif len(fn_diversity) > 0:
        bp = ax.boxplot([fn_diversity], labels=['False Negative\n(AIâ†’Human)'], patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
    
    ax.set_ylabel('Lexical Diversity', fontweight='bold')
    ax.set_title('Vocabulary Diversity Comparison', fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.text(0.02, 0.98, subplot_labels[1], transform=ax.transAxes, 
            fontsize=12, fontweight='bold', va='top', bbox=label_props)
    
    # 3. å¹³å‡å¥é•¿å¯¹æ¯” - å­å›¾(c)
    ax = axes[1, 0]
    fp_sent = features_df[features_df['error_type'] == 'FP']['avg_sentence_length']
    fn_sent = features_df[features_df['error_type'] == 'FN']['avg_sentence_length']
    
    if len(fp_sent) > 0 and len(fn_sent) > 0:
        bp = ax.boxplot([fp_sent, fn_sent], 
                        labels=['False Positive\n(Humanâ†’AI)', 'False Negative\n(AIâ†’Human)'],
                        patch_artist=True)
        bp['boxes'][0].set_facecolor('lightcoral')
        bp['boxes'][1].set_facecolor('lightblue')
    elif len(fp_sent) > 0:
        bp = ax.boxplot([fp_sent], labels=['False Positive\n(Humanâ†’AI)'], patch_artist=True)
        bp['boxes'][0].set_facecolor('lightcoral')
    elif len(fn_sent) > 0:
        bp = ax.boxplot([fn_sent], labels=['False Negative\n(AIâ†’Human)'], patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
    
    ax.set_ylabel('Average Sentence Length (words)', fontweight='bold')
    ax.set_title('Sentence Length Comparison', fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.text(0.02, 0.98, subplot_labels[2], transform=ax.transAxes, 
            fontsize=12, fontweight='bold', va='top', bbox=label_props)
    
    # 4. ç½®ä¿¡åº¦åˆ†å¸ƒ - å­å›¾(d)
    ax = axes[1, 1]
    fp_conf = features_df[features_df['error_type'] == 'FP']['confidence']
    fn_conf = features_df[features_df['error_type'] == 'FN']['confidence']
    
    if len(fp_conf) > 0:
        n1, bins1, patches1 = ax.hist(fp_conf, alpha=0.6, label='False Positive', 
                                      color='red', bins=10, edgecolor='black', linewidth=1)
    if len(fn_conf) > 0:
        n2, bins2, patches2 = ax.hist(fn_conf, alpha=0.6, label='False Negative', 
                                      color='blue', bins=10, edgecolor='black', linewidth=1)
    
    ax.set_xlabel('Prediction Confidence', fontweight='bold')
    ax.set_ylabel('Count', fontweight='bold')
    ax.set_title('Confidence Distribution of Errors', fontweight='bold')
    ax.legend(loc='upper right', frameon=True, fancybox=True, shadow=True)
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.text(0.02, 0.98, subplot_labels[3], transform=ax.transAxes, 
            fontsize=12, fontweight='bold', va='top', bbox=label_props)
    
    # è®¾ç½®æ€»æ ‡é¢˜
    plt.suptitle('Error Sample Feature Analysis', fontsize=15, fontweight='bold', y=0.98)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # è°ƒæ•´å¸ƒå±€ä»¥é€‚åº”æ€»æ ‡é¢˜
    
    # ä¿å­˜ä¸ºPDFçŸ¢é‡å›¾ï¼ˆè®ºæ–‡ç”¨ï¼‰
    pdf_path = os.path.join(OUTPUT_DIR, 'error_features_comparison.pdf')
    plt.savefig(pdf_path, format='pdf', dpi=300, bbox_inches='tight')
    print(f"âœ“ ç‰¹å¾å¯¹æ¯”å›¾(PDF)å·²ä¿å­˜è‡³: {pdf_path}")
    
    # åŒæ—¶ä¿å­˜PNGé¢„è§ˆ
    png_path = os.path.join(OUTPUT_DIR, 'error_features_comparison.png')
    plt.savefig(png_path, format='png', dpi=300, bbox_inches='tight')
    print(f"âœ“ ç‰¹å¾å¯¹æ¯”å›¾(PNG)å·²ä¿å­˜è‡³: {png_path}")
    
    plt.close()
    
    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n### ç‰¹å¾ç»Ÿè®¡æ‘˜è¦ ###")
    
    if len(features_df[features_df['error_type'] == 'FP']) > 0:
        fp_stats = features_df[features_df['error_type'] == 'FP']
        print("\nFalse Positives (Humanâ†’AI):")
        print(f"  å¹³å‡é•¿åº¦: {fp_stats['length'].mean():.0f} å­—ç¬¦ (Â±{fp_stats['length'].std():.0f})")
        print(f"  å¹³å‡è¯æ•°: {fp_stats['word_count'].mean():.0f} (Â±{fp_stats['word_count'].std():.0f})")
        print(f"  å¹³å‡è¯æ±‡å¤šæ ·æ€§: {fp_stats['lexical_diversity'].mean():.3f} (Â±{fp_stats['lexical_diversity'].std():.3f})")
        print(f"  å¹³å‡å¥é•¿: {fp_stats['avg_sentence_length'].mean():.1f} è¯ (Â±{fp_stats['avg_sentence_length'].std():.1f})")
        print(f"  å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {fp_stats['confidence'].mean():.3f}")
    
    if len(features_df[features_df['error_type'] == 'FN']) > 0:
        fn_stats = features_df[features_df['error_type'] == 'FN']
        print("\nFalse Negatives (AIâ†’Human):")
        print(f"  å¹³å‡é•¿åº¦: {fn_stats['length'].mean():.0f} å­—ç¬¦ (Â±{fn_stats['length'].std():.0f})")
        print(f"  å¹³å‡è¯æ•°: {fn_stats['word_count'].mean():.0f} (Â±{fn_stats['word_count'].std():.0f})")
        print(f"  å¹³å‡è¯æ±‡å¤šæ ·æ€§: {fn_stats['lexical_diversity'].mean():.3f} (Â±{fn_stats['lexical_diversity'].std():.3f})")
        print(f"  å¹³å‡å¥é•¿: {fn_stats['avg_sentence_length'].mean():.1f} è¯ (Â±{fn_stats['avg_sentence_length'].std():.1f})")
        print(f"  å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {fn_stats['confidence'].mean():.3f}")

def generate_insights_report(error_df, false_positives, false_negatives):
    """ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š"""
    
    print("\n" + "=" * 60)
    print("å…³é”®æ´å¯Ÿä¸æ”¹è¿›å»ºè®®")
    print("=" * 60)
    
    insights = []
    
    # åˆ†æå…±åŒç‰¹å¾
    if len(false_positives) > 0:
        fp_features = [analyze_text_features(text) for text in false_positives['text']]
        fp_patterns = [analyze_linguistic_patterns(text) for text in false_positives['text']]
        
        avg_length = np.mean([f['length'] for f in fp_features])
        avg_diversity = np.mean([f['lexical_diversity'] for f in fp_features])
        total_ai_patterns = sum([p['ai_pattern_count'] for p in fp_patterns])
        
        insights.append(f"ã€False Positives åˆ†æã€‘")
        insights.append(f"  â€¢ å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        insights.append(f"  â€¢ å¹³å‡è¯æ±‡å¤šæ ·æ€§: {avg_diversity:.3f}")
        insights.append(f"  â€¢ å‘ç° {total_ai_patterns} ä¸ªAIå†™ä½œæ¨¡å¼")
        
        if avg_length > 1500:
            insights.append("  â€¢ å€¾å‘äºè¾ƒé•¿çš„æ–‡æœ¬")
        if avg_diversity < 0.5:
            insights.append("  â€¢ è¯æ±‡é‡å¤ç‡é«˜ï¼Œå†™ä½œé£æ ¼è§„èŒƒåŒ–")
    
    if len(false_negatives) > 0:
        fn_features = [analyze_text_features(text) for text in false_negatives['text']]
        fn_patterns = [analyze_linguistic_patterns(text) for text in false_negatives['text']]
        
        avg_length = np.mean([f['length'] for f in fn_features])
        avg_diversity = np.mean([f['lexical_diversity'] for f in fn_features])
        total_human_patterns = sum([p['human_pattern_count'] for p in fn_patterns])
        
        insights.append(f"\nã€False Negatives åˆ†æã€‘")
        insights.append(f"  â€¢ å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        insights.append(f"  â€¢ å¹³å‡è¯æ±‡å¤šæ ·æ€§: {avg_diversity:.3f}")
        insights.append(f"  â€¢ å‘ç° {total_human_patterns} ä¸ªäººç±»å†™ä½œæ¨¡å¼")
        
        if avg_diversity > 0.6:
            insights.append("  â€¢ è¯æ±‡å¤šæ ·æ€§é«˜ï¼Œæ›´åƒè‡ªç„¶è¯­è¨€")
    
    # æ‰“å°æ´å¯Ÿ
    for insight in insights:
        print(insight)
    
    print("\n### æ”¹è¿›å»ºè®® ###")
    suggestions = [
        "1. æ•°æ®å¢å¼ºï¼šå¢åŠ è¾¹ç•Œæ¡ˆä¾‹çš„è®­ç»ƒæ ·æœ¬",
        "2. ç‰¹å¾å·¥ç¨‹ï¼šè€ƒè™‘åŠ å…¥æ–‡æœ¬é•¿åº¦ã€è¯æ±‡å¤šæ ·æ€§ä½œä¸ºè¾…åŠ©ç‰¹å¾",
        "3. æ¨¡å‹æ”¹è¿›ï¼šä½¿ç”¨æ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹æˆ–é›†æˆå­¦ä¹ ",
        "4. åå¤„ç†ï¼šå¯¹ä½ç½®ä¿¡åº¦é¢„æµ‹è¿›è¡ŒäºŒæ¬¡éªŒè¯",
        "5. é¢†åŸŸé€‚åº”ï¼šé’ˆå¯¹ç‰¹å®šå†™ä½œé£æ ¼è¿›è¡Œå¾®è°ƒ"
    ]
    
    for suggestion in suggestions:
        print(suggestion)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(OUTPUT_DIR, 'insights_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("é”™è¯¯åˆ†ææ´å¯ŸæŠ¥å‘Š\n")
        f.write("=" * 40 + "\n\n")
        
        for insight in insights:
            f.write(insight + "\n")
        
        f.write("\næ”¹è¿›å»ºè®®:\n")
        for suggestion in suggestions:
            f.write(suggestion + "\n")
    
    print(f"\nâœ“ æ´å¯ŸæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é”™è¯¯æ ·æœ¬
    error_df, false_positives, false_negatives = load_and_analyze_errors()
    
    if error_df is None:
        return
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªé”™è¯¯æ ·æœ¬
    display_detailed_analysis(error_df, false_positives, false_negatives)
    
    # åˆ›å»ºç‰¹å¾å¯¹æ¯”å›¾è¡¨
    create_feature_comparison_chart(error_df)
    
    # ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š
    generate_insights_report(error_df, false_positives, false_negatives)
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 60)
    print(f"\næ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“„ detailed_error_analysis.txt - æ¯ä¸ªé”™è¯¯æ ·æœ¬çš„è¯¦ç»†åˆ†æ")
    print("  ğŸ“Š error_features_comparison.pdf - ç‰¹å¾å¯¹æ¯”å›¾è¡¨(çŸ¢é‡å›¾)")
    print("  ğŸ“Š error_features_comparison.png - ç‰¹å¾å¯¹æ¯”å›¾è¡¨(é¢„è§ˆ)")
    print("  ğŸ“ insights_report.txt - æ´å¯Ÿä¸æ”¹è¿›å»ºè®®")

if __name__ == '__main__':
    main()