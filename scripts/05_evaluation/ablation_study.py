#!/usr/bin/env python3
"""
æ¶ˆèå®éªŒè„šæœ¬
ä½¿ç”¨ä¼ ç»Ÿçš„TF-IDF + Logistic Regressionä½œä¸ºåŸºçº¿æ¨¡å‹
ç”¨äºå¯¹æ¯”æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½æå‡
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    classification_report, 
    accuracy_score,
    confusion_matrix,
    roc_curve,
    auc,
    roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
import joblib

# --- é…ç½®å‚æ•° ---
FINAL_DATASET_PATH = '/hy-tmp/Detector/data/processed/final_labeled_dataset.csv'
OUTPUT_DIR = '/hy-tmp/Detector/results/ablation'

def ensure_output_dir():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

def run_ablation_study():
    """
    è¿è¡Œå¤šä¸ªåŸºçº¿æ¨¡å‹çš„æ¶ˆèç ”ç©¶ï¼ŒåŒ…æ‹¬ï¼š
    1. TF-IDF + Logistic Regression
    2. TF-IDF + Random Forest
    3. TF-IDF + SVM
    """
    ensure_output_dir()
    
    print("=" * 60)
    print("æ¶ˆèç ”ç©¶ - åŸºçº¿æ¨¡å‹å¯¹æ¯”")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®é›†
    if not os.path.exists(FINAL_DATASET_PATH):
        print(f"âŒ é”™è¯¯: æ•°æ®é›† '{FINAL_DATASET_PATH}' æœªæ‰¾åˆ°ã€‚")
        return

    print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
    df = pd.read_csv(FINAL_DATASET_PATH)
    
    # æ£€æŸ¥åˆ—åå¹¶ç¡®å®šæ–‡æœ¬åˆ—
    text_column = None
    for col in ['abstract', 'text', 'content']:
        if col in df.columns:
            text_column = col
            break
    
    if text_column is None:
        cols = [col for col in df.columns if col != 'label']
        if cols:
            text_column = cols[0]
    
    print(f"ä½¿ç”¨æ–‡æœ¬åˆ—: '{text_column}'")
    df[text_column] = df[text_column].astype(str)

    # 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆä½¿ç”¨ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸åŒçš„åˆ’åˆ†ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        df[text_column], df['label'], 
        test_size=0.2, 
        random_state=42, 
        stratify=df['label']
    )

    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ¡")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ¡")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: Human={sum(y_train==0)}/{sum(y_test==0)}, AI={sum(y_train==1)}/{sum(y_test==1)}")

    # 3. ç‰¹å¾æå– (TF-IDF)
    print("\næ­£åœ¨ä½¿ç”¨TF-IDFè¿›è¡Œç‰¹å¾æå–...")
    print("  é…ç½®: max_features=5000, ngram_range=(1,2)")
    
    start_time = time.time()
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words='english',
        ngram_range=(1, 2),  # ä½¿ç”¨unigramå’Œbigram
        min_df=2,
        max_df=0.95
    )
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)  # ä¿®å¤ï¼šè¿™é‡Œä¹‹å‰å†™æˆäº†vector.transform
    
    feature_time = time.time() - start_time
    print(f"  âœ“ ç‰¹å¾æå–å®Œæˆ (è€—æ—¶: {feature_time:.2f}ç§’)")
    print(f"  ç‰¹å¾ç»´åº¦: {X_train_tfidf.shape[1]}")

    # ä¿å­˜vectorizerä¾›åç»­åˆ†æ
    vectorizer_path = os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl')
    joblib.dump(vectorizer, vectorizer_path)
    print(f"  âœ“ TF-IDF vectorizerå·²ä¿å­˜è‡³: {vectorizer_path}")

    # 4. è®­ç»ƒå¤šä¸ªåŸºçº¿æ¨¡å‹
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42)
    }
    
    results = {}
    
    for model_name, model in models.items():
        print(f"\n{'='*40}")
        print(f"è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"{'='*40}")
        
        # è®­ç»ƒæ¨¡å‹
        start_time = time.time()
        model.fit(X_train_tfidf, y_train)
        train_time = time.time() - start_time
        
        # é¢„æµ‹
        y_pred = model.predict(X_test_tfidf)
        y_proba = model.predict_proba(X_test_tfidf)[:, 1] if hasattr(model, 'predict_proba') else None
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, target_names=['Human', 'AI'], output_dict=True)
        
        # è®¡ç®—AUCï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒæ¦‚ç‡é¢„æµ‹ï¼‰
        if y_proba is not None:
            auc_score = roc_auc_score(y_test, y_proba)
        else:
            auc_score = None
        
        # ä¿å­˜ç»“æœ
        results[model_name] = {
            'accuracy': accuracy,
            'precision_ai': report['AI']['precision'],
            'recall_ai': report['AI']['recall'],
            'f1_ai': report['AI']['f1-score'],
            'auc': auc_score,
            'train_time': train_time,
            'y_pred': y_pred,
            'y_proba': y_proba,
            'model': model
        }
        
        # æ‰“å°ç»“æœ
        print(f"\nè®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’")
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        if auc_score:
            print(f"ROC-AUC: {auc_score:.4f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['Human', 'AI']))
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(OUTPUT_DIR, f'{model_name.lower().replace(" ", "_")}_model.pkl')
        joblib.dump(model, model_path)
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    # 5. ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå¯¹æ¯”åˆ†æå›¾è¡¨")
    print("=" * 60)
    
    # 5.1 æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # å‡†å¤‡æ•°æ®
    model_names = list(results.keys())
    metrics = ['accuracy', 'precision_ai', 'recall_ai', 'f1_ai']
    metric_labels = ['Accuracy', 'Precision (AI)', 'Recall (AI)', 'F1-Score (AI)']
    
    # å·¦å›¾ï¼šä¸»è¦æŒ‡æ ‡å¯¹æ¯”
    ax1 = axes[0]
    x = np.arange(len(model_names))
    width = 0.2
    
    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
        values = [results[m][metric] for m in model_names]
        ax1.bar(x + i*width, values, width, label=label)
    
    ax1.set_xlabel('Models')
    ax1.set_ylabel('Score')
    ax1.set_title('Performance Comparison - Baseline Models')
    ax1.set_xticks(x + width * 1.5)
    ax1.set_xticklabels(model_names, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim([0, 1.05])
    
    # å³å›¾ï¼šè®­ç»ƒæ—¶é—´å¯¹æ¯”
    ax2 = axes[1]
    train_times = [results[m]['train_time'] for m in model_names]
    bars = ax2.bar(model_names, train_times, color='skyblue')
    ax2.set_xlabel('Models')
    ax2.set_ylabel('Training Time (seconds)')
    ax2.set_title('Training Time Comparison')
    ax2.set_xticklabels(model_names, rotation=45, ha='right')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, time_val in zip(bars, train_times):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{time_val:.2f}s', ha='center', va='bottom')
    
    plt.tight_layout()
    comparison_path = os.path.join(OUTPUT_DIR, 'baseline_comparison.png')
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ“ å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜è‡³: {comparison_path}")
    
    # 5.2 ROCæ›²çº¿å¯¹æ¯”
    plt.figure(figsize=(10, 8))
    
    for model_name, result in results.items():
        if result['y_proba'] is not None:
            fpr, tpr, _ = roc_curve(y_test, result['y_proba'])
            auc_score = result['auc']
            plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {auc_score:.4f})')
    
    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('ROC Curves - Baseline Models Comparison', fontsize=14)
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    roc_comparison_path = os.path.join(OUTPUT_DIR, 'roc_curves_comparison.png')
    plt.tight_layout()
    plt.savefig(roc_comparison_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ“ ROCæ›²çº¿å¯¹æ¯”å·²ä¿å­˜è‡³: {roc_comparison_path}")
    
    # 6. ç”Ÿæˆç»“æœæ±‡æ€»è¡¨
    summary_data = []
    for model_name, result in results.items():
        summary_data.append({
            'Model': model_name,
            'Accuracy': f"{result['accuracy']:.4f}",
            'Precision (AI)': f"{result['precision_ai']:.4f}",
            'Recall (AI)': f"{result['recall_ai']:.4f}",
            'F1-Score (AI)': f"{result['f1_ai']:.4f}",
            'ROC-AUC': f"{result['auc']:.4f}" if result['auc'] else 'N/A',
            'Training Time (s)': f"{result['train_time']:.2f}"
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = os.path.join(OUTPUT_DIR, 'baseline_models_summary.csv')
    summary_df.to_csv(summary_path, index=False)
    
    print("\n" + "=" * 60)
    print("æ¶ˆèç ”ç©¶æ€»ç»“")
    print("=" * 60)
    print("\nåŸºçº¿æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print(summary_df.to_string(index=False))
    
    print(f"\nâœ“ æ±‡æ€»è¡¨å·²ä¿å­˜è‡³: {summary_path}")
    
    # 7. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆé’ˆå¯¹Logistic Regressionï¼‰
    print("\n" + "=" * 60)
    print("ç‰¹å¾é‡è¦æ€§åˆ†æ (TF-IDF + Logistic Regression)")
    print("=" * 60)
    
    lr_model = results['Logistic Regression']['model']
    feature_names = vectorizer.get_feature_names_out()
    
    # è·å–æœ€é‡è¦çš„ç‰¹å¾ï¼ˆåŒºåˆ†AIå’ŒHumançš„ï¼‰
    coef = lr_model.coef_[0]
    top_ai_indices = np.argsort(coef)[-20:][::-1]  # Top 20 AI indicators
    top_human_indices = np.argsort(coef)[:20]  # Top 20 Human indicators
    
    print("\nTop 20 ç‰¹å¾ - æŒ‡ç¤ºAIç”Ÿæˆæ–‡æœ¬:")
    for i, idx in enumerate(top_ai_indices, 1):
        print(f"  {i:2d}. {feature_names[idx]:30s} (æƒé‡: {coef[idx]:.4f})")
    
    print("\nTop 20 ç‰¹å¾ - æŒ‡ç¤ºäººç±»æ’°å†™æ–‡æœ¬:")
    for i, idx in enumerate(top_human_indices, 1):
        print(f"  {i:2d}. {feature_names[idx]:30s} (æƒé‡: {coef[idx]:.4f})")
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'AI_Features': [feature_names[i] for i in top_ai_indices],
        'AI_Weights': [coef[i] for i in top_ai_indices],
        'Human_Features': [feature_names[i] for i in top_human_indices],
        'Human_Weights': [coef[i] for i in top_human_indices]
    })
    
    feature_path = os.path.join(OUTPUT_DIR, 'feature_importance.csv')
    feature_importance.to_csv(feature_path, index=False)
    print(f"\nâœ“ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³: {feature_path}")
    
    print("\n" + "=" * 60)
    print("âœ… æ¶ˆèç ”ç©¶å®Œæˆï¼")
    print("=" * 60)
    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print("\nğŸ’¡ å¯¹æ¯”å»ºè®®:")
    print("1. å°†è¿™äº›åŸºçº¿æ¨¡å‹çš„ç»“æœä¸DistilBERTæ¨¡å‹è¿›è¡Œå¯¹æ¯”")
    print("2. å…³æ³¨æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å“ªäº›æŒ‡æ ‡ä¸Šæœ‰æ˜¾è‘—æå‡")
    print("3. åˆ†æä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯")

if __name__ == '__main__':
    run_ablation_study()