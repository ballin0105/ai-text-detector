#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°ä¸æ·±åº¦åˆ†æè„šæœ¬ - è®ºæ–‡å‘è¡¨çº§åˆ«
ç”Ÿæˆé«˜è´¨é‡çŸ¢é‡å›¾PDFï¼Œå¯ç›´æ¥ç”¨äºå­¦æœ¯è®ºæ–‡
"""

import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
)
from sklearn.metrics import (
    classification_report, 
    confusion_matrix,
    roc_curve,
    auc,
    roc_auc_score,
    precision_recall_curve,
    average_precision_score
)
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.backends.backend_pdf import PdfPages
import os
import numpy as np
from scipy.special import softmax

# ========== é…ç½®matplotlibä¸ºè®ºæ–‡å‘è¡¨è´¨é‡ ==========
# è®¾ç½®å…¨å±€å­—ä½“å’Œæ ·å¼
plt.rcParams.update({
    # å­—ä½“è®¾ç½®
    'font.family': 'serif',
    'font.serif': ['Times New Roman', 'DejaVu Serif'],
    'font.size': 12,
    
    # æ•°å­¦æ–‡æœ¬
    'mathtext.fontset': 'cm',
    'mathtext.rm': 'serif',
    
    # è½´çº¿è®¾ç½®
    'axes.linewidth': 1.0,
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'axes.grid': True,
    
    # åˆ»åº¦è®¾ç½®
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'xtick.major.width': 1.0,
    'ytick.major.width': 1.0,
    'xtick.major.size': 5,
    'ytick.major.size': 5,
    
    # å›¾ä¾‹è®¾ç½®
    'legend.fontsize': 12,
    'legend.frameon': True,
    'legend.framealpha': 0.8,
    'legend.fancybox': True,
    
    # çº¿æ¡è®¾ç½®
    'lines.linewidth': 2.0,
    'lines.markersize': 8,
    
    # å›¾å½¢è¾“å‡ºè®¾ç½®
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.1,
    
    # PDFè®¾ç½®
    'pdf.fonttype': 42,  # TrueTypeå­—ä½“
    'ps.fonttype': 42,   # TrueTypeå­—ä½“
})

# Seabornæ ·å¼è®¾ç½®
sns.set_style("whitegrid")
sns.set_context("paper", font_scale=1.2)

# --- é…ç½®å‚æ•° ---
FINAL_DATASET_PATH = '/hy-tmp/Detector/data/processed/final_labeled_dataset.csv'
BEST_MODEL_PATH = '/hy-tmp/Detector/results/best_model' 
OUTPUT_DIR = '/hy-tmp/Detector/results/evaluation'

MAX_TOKEN_LENGTH = 256  # å¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´

def ensure_output_dir():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(os.path.join(OUTPUT_DIR, 'figures'), exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

def create_publication_confusion_matrix(cm, output_path):
    """åˆ›å»ºè®ºæ–‡çº§åˆ«çš„æ··æ·†çŸ©é˜µå›¾"""
    fig, ax = plt.subplots(figsize=(8, 6))
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ³¨é‡Šæ–‡æœ¬ï¼ˆæ˜¾ç¤ºæ•°é‡å’Œç™¾åˆ†æ¯”ï¼‰
    annotations = np.empty_like(cm, dtype=object)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            annotations[i, j] = f'{cm[i, j]}\n({cm_percent[i, j]:.1f}%)'
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues',
                cbar_kws={'label': 'Number of Samples'},
                xticklabels=['Human', 'AI'],
                yticklabels=['Human', 'AI'],
                square=True,
                linewidths=1,
                linecolor='gray',
                cbar=True,
                vmin=0)
    
    ax.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')
    ax.set_ylabel('True Label', fontsize=14, fontweight='bold')
    ax.set_title('Confusion Matrix of AI Text Detection Model', 
                 fontsize=16, fontweight='bold', pad=20)
    
    plt.tight_layout()
    
    # ä¿å­˜ä¸ºPDFçŸ¢é‡å›¾
    fig.savefig(output_path, format='pdf', dpi=300, bbox_inches='tight')
    # åŒæ—¶ä¿å­˜PNGä¾›é¢„è§ˆ
    fig.savefig(output_path.replace('.pdf', '.png'), format='png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return fig

def create_publication_roc_curve(fpr, tpr, roc_auc, output_path):
    """åˆ›å»ºè®ºæ–‡çº§åˆ«çš„ROCæ›²çº¿å›¾"""
    fig, ax = plt.subplots(figsize=(8, 8))
    
    # ç»˜åˆ¶ROCæ›²çº¿
    ax.plot(fpr, tpr, color='#FF6B6B', lw=2.5, 
            label=f'DistilBERT (AUC = {roc_auc:.3f})', 
            linestyle='-', marker='', alpha=0.9)
    
    # ç»˜åˆ¶å¯¹è§’çº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰
    ax.plot([0, 1], [0, 1], color='#4ECDC4', lw=2, 
            linestyle='--', label='Random Classifier (AUC = 0.500)', alpha=0.7)
    
    # å¡«å……ROCæ›²çº¿ä¸‹æ–¹åŒºåŸŸ
    ax.fill_between(fpr, 0, tpr, alpha=0.15, color='#FF6B6B')
    
    # è®¾ç½®è½´æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')
    ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')
    ax.set_title('Receiver Operating Characteristic (ROC) Curve', 
                 fontsize=16, fontweight='bold', pad=20)
    
    # è®¾ç½®è½´èŒƒå›´å’Œåˆ»åº¦
    ax.set_xlim([-0.02, 1.02])
    ax.set_ylim([-0.02, 1.02])
    ax.set_xticks(np.arange(0, 1.1, 0.2))
    ax.set_yticks(np.arange(0, 1.1, 0.2))
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle=':', linewidth=0.5)
    ax.set_axisbelow(True)
    
    # è®¾ç½®å›¾ä¾‹
    ax.legend(loc='lower right', frameon=True, fancybox=True, 
              shadow=True, framealpha=0.95)
    
    # æ·»åŠ æ€§èƒ½ç‚¹æ ‡è®°ï¼ˆä¾‹å¦‚ï¼Œæœ€ä½³é˜ˆå€¼ç‚¹ï¼‰
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = (fpr[optimal_idx], tpr[optimal_idx])
    ax.scatter(*optimal_threshold, color='red', s=100, zorder=5, 
               label=f'Optimal Threshold ({optimal_threshold[0]:.3f}, {optimal_threshold[1]:.3f})')
    
    # è®¾ç½®ç­‰æ¯”ä¾‹
    ax.set_aspect('equal', adjustable='box')
    
    plt.tight_layout()
    
    # ä¿å­˜ä¸ºPDFçŸ¢é‡å›¾
    fig.savefig(output_path, format='pdf', dpi=300, bbox_inches='tight')
    fig.savefig(output_path.replace('.pdf', '.png'), format='png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return fig

def create_publication_pr_curve(precision, recall, avg_precision, output_path):
    """åˆ›å»ºè®ºæ–‡çº§åˆ«çš„Precision-Recallæ›²çº¿å›¾"""
    fig, ax = plt.subplots(figsize=(8, 8))
    
    # ç»˜åˆ¶PRæ›²çº¿
    ax.plot(recall, precision, color='#6C5CE7', lw=2.5,
            label=f'DistilBERT (AP = {avg_precision:.3f})',
            linestyle='-', alpha=0.9)
    
    # å¡«å……PRæ›²çº¿ä¸‹æ–¹åŒºåŸŸ
    ax.fill_between(recall, 0, precision, alpha=0.15, color='#6C5CE7')
    
    # æ·»åŠ åŸºçº¿ï¼ˆéšæœºåˆ†ç±»å™¨çš„ç²¾ç¡®ç‡ï¼‰
    baseline_precision = 0.5  # å‡è®¾ç±»åˆ«å¹³è¡¡
    ax.axhline(y=baseline_precision, color='#FD79A8', lw=2, 
               linestyle='--', label=f'Random Classifier (AP = {baseline_precision:.3f})', 
               alpha=0.7)
    
    # è®¾ç½®è½´æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel('Recall', fontsize=14, fontweight='bold')
    ax.set_ylabel('Precision', fontsize=14, fontweight='bold')
    ax.set_title('Precision-Recall Curve', 
                 fontsize=16, fontweight='bold', pad=20)
    
    # è®¾ç½®è½´èŒƒå›´å’Œåˆ»åº¦
    ax.set_xlim([-0.02, 1.02])
    ax.set_ylim([-0.02, 1.02])
    ax.set_xticks(np.arange(0, 1.1, 0.2))
    ax.set_yticks(np.arange(0, 1.1, 0.2))
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle=':', linewidth=0.5)
    ax.set_axisbelow(True)
    
    # è®¾ç½®å›¾ä¾‹
    ax.legend(loc='lower left', frameon=True, fancybox=True, 
              shadow=True, framealpha=0.95)
    
    # è®¾ç½®ç­‰æ¯”ä¾‹
    ax.set_aspect('equal', adjustable='box')
    
    plt.tight_layout()
    
    # ä¿å­˜ä¸ºPDFçŸ¢é‡å›¾
    fig.savefig(output_path, format='pdf', dpi=300, bbox_inches='tight')
    fig.savefig(output_path.replace('.pdf', '.png'), format='png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return fig

def create_combined_performance_plot(results_dict, output_path):
    """åˆ›å»ºç»¼åˆæ€§èƒ½å¯¹æ¯”å›¾ï¼ˆæ¡å½¢å›¾ï¼‰"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # å·¦å›¾ï¼šä¸»è¦æŒ‡æ ‡å¯¹æ¯”
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = results_dict['main_metrics']
    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']
    
    bars = ax1.bar(metrics, values, color=colors, edgecolor='black', linewidth=1.5)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars, values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax1.set_ylim([0, 1.1])
    ax1.set_ylabel('Score', fontsize=14, fontweight='bold')
    ax1.set_title('Classification Performance Metrics', fontsize=14, fontweight='bold')
    ax1.grid(True, axis='y', alpha=0.3, linestyle=':')
    ax1.set_axisbelow(True)
    
    # å³å›¾ï¼šROC-AUCå’ŒPR-AUCå¯¹æ¯”
    auc_metrics = ['ROC-AUC', 'PR-AUC']
    auc_values = results_dict['auc_metrics']
    colors2 = ['#9b59b6', '#1abc9c']
    
    bars2 = ax2.bar(auc_metrics, auc_values, color=colors2, edgecolor='black', linewidth=1.5)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars2, auc_values):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax2.set_ylim([0, 1.1])
    ax2.set_ylabel('Score', fontsize=14, fontweight='bold')
    ax2.set_title('Area Under Curve Metrics', fontsize=14, fontweight='bold')
    ax2.grid(True, axis='y', alpha=0.3, linestyle=':')
    ax2.set_axisbelow(True)
    
    # è°ƒæ•´å¸ƒå±€
    plt.suptitle('AI Text Detection Model Performance Summary', 
                 fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    # ä¿å­˜ä¸ºPDFçŸ¢é‡å›¾
    fig.savefig(output_path, format='pdf', dpi=300, bbox_inches='tight')
    fig.savefig(output_path.replace('.pdf', '.png'), format='png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return fig

def create_error_distribution_plot(error_df, output_path):
    """åˆ›å»ºé”™è¯¯åˆ†å¸ƒå›¾"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # å·¦å›¾ï¼šé”™è¯¯ç±»å‹åˆ†å¸ƒ
    error_counts = error_df['error_type'].value_counts()
    colors = ['#e74c3c', '#3498db']
    wedges, texts, autotexts = ax1.pie(error_counts.values, 
                                        labels=error_counts.index,
                                        colors=colors,
                                        autopct='%1.1f%%',
                                        startangle=90,
                                        explode=(0.05, 0.05))
    
    # ç¾åŒ–é¥¼å›¾æ–‡æœ¬
    for text in texts:
        text.set_fontsize(12)
        text.set_fontweight('bold')
    for autotext in autotexts:
        autotext.set_fontsize(11)
        autotext.set_fontweight('bold')
        autotext.set_color('white')
    
    ax1.set_title('Error Type Distribution', fontsize=14, fontweight='bold')
    
    # å³å›¾ï¼šç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
    ax2.hist(error_df['prediction_confidence'], bins=20, 
             color='#34495e', edgecolor='black', alpha=0.7)
    ax2.set_xlabel('Prediction Confidence', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Number of Errors', fontsize=12, fontweight='bold')
    ax2.set_title('Error Confidence Distribution', fontsize=14, fontweight='bold')
    ax2.grid(True, axis='y', alpha=0.3, linestyle=':')
    ax2.set_axisbelow(True)
    
    # æ·»åŠ å‡å€¼çº¿
    mean_conf = error_df['prediction_confidence'].mean()
    ax2.axvline(mean_conf, color='red', linestyle='--', linewidth=2, 
                label=f'Mean: {mean_conf:.3f}')
    ax2.legend()
    
    plt.suptitle('Error Analysis', fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    # ä¿å­˜ä¸ºPDFçŸ¢é‡å›¾
    fig.savefig(output_path, format='pdf', dpi=300, bbox_inches='tight')
    fig.savefig(output_path.replace('.pdf', '.png'), format='png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return fig

def create_all_figures_pdf(all_figures, output_path):
    """å°†æ‰€æœ‰å›¾å½¢åˆå¹¶åˆ°ä¸€ä¸ªPDFæ–‡ä»¶ä¸­"""
    with PdfPages(output_path) as pdf:
        for fig in all_figures:
            pdf.savefig(fig, bbox_inches='tight')
        
        # æ·»åŠ å…ƒæ•°æ®
        d = pdf.infodict()
        d['Title'] = 'AI Text Detection Model Evaluation Results'
        d['Author'] = 'AI Text Detector'
        d['Subject'] = 'Model Performance Analysis'
        d['Keywords'] = 'Machine Learning, NLP, Text Classification, DistilBERT'

def analyze_model_performance():
    """
    åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç”Ÿæˆè®ºæ–‡çº§åˆ«çš„å¯è§†åŒ–ç»“æœã€‚
    """
    ensure_output_dir()
    
    # å­˜å‚¨æ‰€æœ‰å›¾å½¢å¯¹è±¡
    all_figures = []
    
    # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†
    print("=" * 60)
    print("æ¨¡å‹è¯„ä¼°ä¸åˆ†æ - è®ºæ–‡å‘è¡¨çº§åˆ«")
    print("=" * 60)
    
    print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
    df = pd.read_csv(FINAL_DATASET_PATH)
    
    # æ£€æŸ¥åˆ—åå¹¶ç¡®å®šæ–‡æœ¬åˆ—
    text_column = None
    for col in ['abstract', 'text', 'content']:
        if col in df.columns:
            text_column = col
            break
    
    if text_column is None:
        cols = [col for col in df.columns if col != 'label']
        if cols:
            text_column = cols[0]
    
    print(f"ä½¿ç”¨æ–‡æœ¬åˆ—: '{text_column}'")
    df[text_column] = df[text_column].astype(str)
    
    # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­åˆ’åˆ†æ•°æ®é›†
    from sklearn.model_selection import train_test_split
    train_df, test_df = train_test_split(
        df, test_size=0.2, random_state=42, stratify=df['label']
    )
    
    if text_column != 'text':
        test_df = test_df.rename(columns={text_column: 'text'})
    
    test_dataset = Dataset.from_pandas(test_df[['text', 'label']])
    
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    print(f"  äººç±»æ–‡æœ¬: {sum(test_df['label'] == 0)} æ¡")
    print(f"  AIæ–‡æœ¬: {sum(test_df['label'] == 1)} æ¡")

    # 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print(f"\næ­£åœ¨ä» '{BEST_MODEL_PATH}' åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    if not os.path.exists(BEST_MODEL_PATH):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚")
        return
        
    tokenizer = DistilBertTokenizerFast.from_pretrained(BEST_MODEL_PATH)
    model = DistilBertForSequenceClassification.from_pretrained(BEST_MODEL_PATH)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()
    print(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ° {device.upper()}")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"], 
            padding="max_length", 
            truncation=True, 
            max_length=MAX_TOKEN_LENGTH
        )
    
    print("\næ­£åœ¨å¯¹æµ‹è¯•é›†è¿›è¡Œåˆ†è¯...")
    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

    # 3. è¿›è¡Œé¢„æµ‹
    print("\næ­£åœ¨å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
    )
    predictions = trainer.predict(tokenized_test_dataset)
    
    y_pred = np.argmax(predictions.predictions, axis=1)
    y_true = predictions.label_ids
    y_proba = softmax(predictions.predictions, axis=1)
    y_proba_ai = y_proba[:, 1]

    # 4. ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("åˆ†ç±»æŠ¥å‘Š")
    print("=" * 60)
    report = classification_report(y_true, y_pred, target_names=['Human', 'AI'], digits=4)
    print(report)
    
    report_dict = classification_report(y_true, y_pred, target_names=['Human', 'AI'], output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose()
    report_path = os.path.join(OUTPUT_DIR, 'classification_report.csv')
    report_df.to_csv(report_path)

    # 5. ç”Ÿæˆæ··æ·†çŸ©é˜µï¼ˆPDFï¼‰
    print("\næ­£åœ¨ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    cm = confusion_matrix(y_true, y_pred)
    cm_path = os.path.join(OUTPUT_DIR, 'figures', 'confusion_matrix.pdf')
    fig_cm = create_publication_confusion_matrix(cm, cm_path)
    print(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {cm_path}")

    # 6. ç”ŸæˆROCæ›²çº¿ï¼ˆPDFï¼‰
    print("\næ­£åœ¨ç”ŸæˆROC/AUCæ›²çº¿...")
    fpr, tpr, thresholds = roc_curve(y_true, y_proba_ai)
    roc_auc = auc(fpr, tpr)
    roc_path = os.path.join(OUTPUT_DIR, 'figures', 'roc_curve.pdf')
    fig_roc = create_publication_roc_curve(fpr, tpr, roc_auc, roc_path)
    print(f"âœ“ ROCæ›²çº¿å·²ä¿å­˜è‡³: {roc_path}")
    print(f"  AUCåˆ†æ•°: {roc_auc:.4f}")

    # 7. ç”ŸæˆPrecision-Recallæ›²çº¿ï¼ˆPDFï¼‰
    print("\næ­£åœ¨ç”ŸæˆPrecision-Recallæ›²çº¿...")
    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_proba_ai)
    avg_precision = average_precision_score(y_true, y_proba_ai)
    pr_path = os.path.join(OUTPUT_DIR, 'figures', 'pr_curve.pdf')
    fig_pr = create_publication_pr_curve(precision, recall, avg_precision, pr_path)
    print(f"âœ“ Precision-Recallæ›²çº¿å·²ä¿å­˜è‡³: {pr_path}")
    print(f"  å¹³å‡ç²¾ç¡®ç‡: {avg_precision:.4f}")

    # 8. è¯¯å·®åˆ†æ
    print("\næ­£åœ¨è¿›è¡Œè¯¯å·®åˆ†æ...")
    test_df_copy = test_df.copy()
    test_df_copy['predicted_label'] = y_pred
    test_df_copy['prediction_confidence'] = y_proba_ai
    test_df_copy['prediction_correct'] = (test_df_copy['label'] == test_df_copy['predicted_label'])
    
    error_df = test_df_copy[~test_df_copy['prediction_correct']].copy()
    error_df['error_type'] = error_df.apply(
        lambda row: 'False Positive' if row['label'] == 0 else 'False Negative',
        axis=1
    )
    
    error_df = error_df.sort_values('prediction_confidence', ascending=False)
    error_analysis_path = os.path.join(OUTPUT_DIR, 'error_analysis.csv')
    error_df.to_csv(error_analysis_path, index=False, encoding='utf-8')
    
    # ç”Ÿæˆé”™è¯¯åˆ†å¸ƒå›¾ï¼ˆPDFï¼‰
    if len(error_df) > 0:
        error_plot_path = os.path.join(OUTPUT_DIR, 'figures', 'error_distribution.pdf')
        fig_error = create_error_distribution_plot(error_df, error_plot_path)
        print(f"âœ“ é”™è¯¯åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {error_plot_path}")

    # 9. ç”Ÿæˆç»¼åˆæ€§èƒ½å›¾ï¼ˆPDFï¼‰
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    accuracy = accuracy_score(y_true, y_pred)
    precision_val = precision_score(y_true, y_pred)
    recall_val = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    
    results_dict = {
        'main_metrics': [accuracy, precision_val, recall_val, f1],
        'auc_metrics': [roc_auc, avg_precision]
    }
    
    performance_path = os.path.join(OUTPUT_DIR, 'figures', 'performance_summary.pdf')
    fig_perf = create_combined_performance_plot(results_dict, performance_path)
    print(f"âœ“ æ€§èƒ½æ€»ç»“å›¾å·²ä¿å­˜è‡³: {performance_path}")
    
    # 10. åˆ›å»ºåˆå¹¶çš„PDFæ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰å›¾å½¢ï¼‰
    print("\næ­£åœ¨åˆ›å»ºåŒ…å«æ‰€æœ‰å›¾å½¢çš„PDFæ–‡ä»¶...")
    combined_pdf_path = os.path.join(OUTPUT_DIR, 'all_figures_combined.pdf')
    
    # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬å·²ç»å…³é—­äº†å„ä¸ªå›¾å½¢ï¼Œè¿™é‡Œæˆ‘ä»¬é‡æ–°æ‰“å¼€PDFæ–‡ä»¶
    from PyPDF2 import PdfMerger
    merger = PdfMerger()
    
    pdf_files = [
        os.path.join(OUTPUT_DIR, 'figures', 'confusion_matrix.pdf'),
        os.path.join(OUTPUT_DIR, 'figures', 'roc_curve.pdf'),
        os.path.join(OUTPUT_DIR, 'figures', 'pr_curve.pdf'),
        os.path.join(OUTPUT_DIR, 'figures', 'performance_summary.pdf'),
    ]
    
    if len(error_df) > 0:
        pdf_files.append(os.path.join(OUTPUT_DIR, 'figures', 'error_distribution.pdf'))
    
    for pdf_file in pdf_files:
        if os.path.exists(pdf_file):
            merger.append(pdf_file)
    
    merger.write(combined_pdf_path)
    merger.close()
    print(f"âœ“ åˆå¹¶PDFå·²ä¿å­˜è‡³: {combined_pdf_path}")
    
    # 11. ç”ŸæˆLaTeXè¡¨æ ¼ä»£ç 
    print("\nç”ŸæˆLaTeXè¡¨æ ¼ä»£ç ...")
    latex_path = os.path.join(OUTPUT_DIR, 'latex_tables.tex')
    with open(latex_path, 'w') as f:
        f.write("% Classification Report Table\n")
        f.write("\\begin{table}[h!]\n")
        f.write("\\centering\n")
        f.write("\\caption{Classification Report}\n")
        f.write("\\begin{tabular}{lcccc}\n")
        f.write("\\hline\n")
        f.write("Class & Precision & Recall & F1-Score & Support \\\\\n")
        f.write("\\hline\n")
        f.write(f"Human & {report_dict['Human']['precision']:.3f} & ")
        f.write(f"{report_dict['Human']['recall']:.3f} & ")
        f.write(f"{report_dict['Human']['f1-score']:.3f} & ")
        f.write(f"{int(report_dict['Human']['support'])} \\\\\n")
        f.write(f"AI & {report_dict['AI']['precision']:.3f} & ")
        f.write(f"{report_dict['AI']['recall']:.3f} & ")
        f.write(f"{report_dict['AI']['f1-score']:.3f} & ")
        f.write(f"{int(report_dict['AI']['support'])} \\\\\n")
        f.write("\\hline\n")
        f.write(f"Accuracy & \\multicolumn{{3}}{{c}}{{ {accuracy:.3f} }} & {len(y_true)} \\\\\n")
        f.write("\\hline\n")
        f.write("\\end{tabular}\n")
        f.write("\\end{table}\n\n")
        
        f.write("% Performance Metrics Table\n")
        f.write("\\begin{table}[h!]\n")
        f.write("\\centering\n")
        f.write("\\caption{Performance Metrics}\n")
        f.write("\\begin{tabular}{lc}\n")
        f.write("\\hline\n")
        f.write("Metric & Score \\\\\n")
        f.write("\\hline\n")
        f.write(f"Accuracy & {accuracy:.3f} \\\\\n")
        f.write(f"Precision & {precision_val:.3f} \\\\\n")
        f.write(f"Recall & {recall_val:.3f} \\\\\n")
        f.write(f"F1-Score & {f1:.3f} \\\\\n")
        f.write(f"ROC-AUC & {roc_auc:.3f} \\\\\n")
        f.write(f"PR-AUC & {avg_precision:.3f} \\\\\n")
        f.write("\\hline\n")
        f.write("\\end{tabular}\n")
        f.write("\\end{table}\n")
    
    print(f"âœ“ LaTeXè¡¨æ ¼ä»£ç å·²ä¿å­˜è‡³: {latex_path}")
    
    print("\n" + "=" * 60)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 60)
    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print("\nğŸ“Š è®ºæ–‡çº§åˆ«å›¾å½¢æ–‡ä»¶:")
    print(f"  PDFçŸ¢é‡å›¾: {OUTPUT_DIR}/figures/*.pdf")
    print(f"  PNGé¢„è§ˆå›¾: {OUTPUT_DIR}/figures/*.png")
    print(f"  åˆå¹¶PDF: {OUTPUT_DIR}/all_figures_combined.pdf")
    print(f"  LaTeXä»£ç : {OUTPUT_DIR}/latex_tables.tex")
    print("\nè¿™äº›å›¾å½¢å¯ä»¥ç›´æ¥ç”¨äºå­¦æœ¯è®ºæ–‡å‘è¡¨ï¼")

if __name__ == '__main__':
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†PyPDF2
    try:
        import PyPDF2
    except ImportError:
        print("è¯·å®‰è£…PyPDF2ä»¥åˆå¹¶PDFæ–‡ä»¶: pip install PyPDF2")
    
    analyze_model_performance()